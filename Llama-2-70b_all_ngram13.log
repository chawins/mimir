nohup: ignoring input
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
results_new/neo125_github_experiment/meta-llama_Llama-2-70b-hf/wikipedia_(en)_ngram_13_<0.8_truncated
Saving results to absolute path: /home/chawins/mimir/tmp_results/neo125_github_experiment/meta-llama_Llama-2-70b-hf/wikipedia_(en)_ngram_13_<0.8_truncated
LOG: cache_dir is /home/chawins/.cache
Using cache dir /home/chawins/.cache
Loading BASE model meta-llama/Llama-2-70b-hf...
Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.embed_tokens.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:   7%|â–‹         | 1/15 [00:00<00:02,  5.79it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  13%|â–ˆâ–Ž        | 2/15 [00:00<00:02,  5.90it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 3/15 [00:00<00:02,  5.88it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:00<00:01,  5.91it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:00<00:01,  5.84it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:01<00:01,  5.79it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:01<00:01,  5.74it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:01<00:01,  5.70it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:01<00:01,  5.67it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:01<00:00,  5.70it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:01<00:00,  5.77it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:02<00:00,  5.76it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:02<00:00,  5.73it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:02<00:00,  5.77it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for lm_head.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:02<00:00,  5.90it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:02<00:00,  5.80it/s]
  0%|          | 0/57 [00:00<?, ?w/s]model.embed_tokens.weight:   0%|          | 0/57 [00:00<?, ?w/s, dev=0]model.embed_tokens.weight:   2%|â–         | 1/57 [00:00<00:10,  5.33w/s, dev=0]model.layers.0.input_layernorm.weight:   2%|â–         | 1/57 [00:00<00:10,  5.32w/s, dev=0]model.layers.0.mlp.down_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:05, 10.62w/s, dev=0]  model.layers.0.mlp.down_proj.weight:   5%|â–Œ         | 3/57 [00:00<00:06,  8.57w/s, dev=0]model.layers.0.mlp.gate_proj.weight:   5%|â–Œ         | 3/57 [00:00<00:06,  8.56w/s, dev=0]model.layers.0.mlp.up_proj.weight:   7%|â–‹         | 4/57 [00:00<00:06,  7.70w/s, dev=0]  model.layers.0.mlp.up_proj.weight:   9%|â–‰         | 5/57 [00:00<00:07,  7.39w/s, dev=0]model.layers.0.post_attention_layernorm.weight:   9%|â–‰         | 5/57 [00:00<00:07,  7.39w/s, dev=0]model.layers.0.self_attn.k_proj.weight:  11%|â–ˆ         | 6/57 [00:00<00:05,  8.86w/s, dev=0]        model.layers.0.self_attn.o_proj.weight:  12%|â–ˆâ–        | 7/57 [00:00<00:04, 10.24w/s, dev=0]model.layers.0.self_attn.q_proj.weight:  14%|â–ˆâ–        | 8/57 [00:00<00:04, 10.90w/s, dev=0]model.layers.0.self_attn.q_proj.weight:  16%|â–ˆâ–Œ        | 9/57 [00:00<00:04, 11.51w/s, dev=0]model.layers.0.self_attn.rotary_emb.inv_freq:  16%|â–ˆâ–Œ        | 9/57 [00:00<00:04, 11.50w/s, dev=0]model.layers.0.self_attn.v_proj.weight:  18%|â–ˆâ–Š        | 10/57 [00:00<00:03, 12.77w/s, dev=0]     model.layers.1.input_layernorm.weight:  19%|â–ˆâ–‰        | 11/57 [00:00<00:03, 13.92w/s, dev=0] model.layers.1.mlp.down_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:00<00:02, 15.18w/s, dev=0]  model.layers.1.mlp.down_proj.weight:  23%|â–ˆâ–ˆâ–Ž       | 13/57 [00:00<00:03, 13.48w/s, dev=0]model.layers.1.mlp.gate_proj.weight:  23%|â–ˆâ–ˆâ–Ž       | 13/57 [00:00<00:03, 13.48w/s, dev=0]model.layers.1.mlp.up_proj.weight:  25%|â–ˆâ–ˆâ–       | 14/57 [00:01<00:03, 12.28w/s, dev=0]  model.layers.1.post_attention_layernorm.weight:  26%|â–ˆâ–ˆâ–‹       | 15/57 [00:01<00:03, 11.42w/s, dev=0]model.layers.1.self_attn.k_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:01<00:03, 12.18w/s, dev=0]        model.layers.1.self_attn.k_proj.weight:  30%|â–ˆâ–ˆâ–‰       | 17/57 [00:01<00:03, 12.86w/s, dev=0]model.layers.1.self_attn.o_proj.weight:  30%|â–ˆâ–ˆâ–‰       | 17/57 [00:01<00:03, 12.86w/s, dev=0]model.layers.1.self_attn.q_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 18/57 [00:01<00:02, 13.16w/s, dev=0]model.layers.1.self_attn.rotary_emb.inv_freq:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 19/57 [00:01<00:02, 13.43w/s, dev=0]model.layers.1.self_attn.v_proj.weight:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 20/57 [00:01<00:02, 14.13w/s, dev=0]      model.layers.1.self_attn.v_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 21/57 [00:01<00:02, 14.75w/s, dev=0]model.layers.2.input_layernorm.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 21/57 [00:01<00:02, 14.75w/s, dev=0] model.layers.2.mlp.down_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:01<00:02, 15.45w/s, dev=0]  model.layers.2.mlp.gate_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 23/57 [00:01<00:02, 14.53w/s, dev=0]model.layers.2.mlp.up_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/57 [00:01<00:02, 13.76w/s, dev=0]  model.layers.2.mlp.up_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/57 [00:01<00:02, 13.07w/s, dev=0]model.layers.2.post_attention_layernorm.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/57 [00:01<00:02, 13.07w/s, dev=0]model.layers.2.self_attn.k_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 26/57 [00:01<00:02, 13.59w/s, dev=0]        model.layers.2.self_attn.o_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 27/57 [00:01<00:02, 14.04w/s, dev=0]model.layers.2.self_attn.q_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:01<00:02, 14.21w/s, dev=0]model.layers.2.self_attn.q_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 29/57 [00:02<00:01, 14.39w/s, dev=0]model.layers.2.self_attn.rotary_emb.inv_freq:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 29/57 [00:02<00:01, 14.39w/s, dev=0]model.layers.2.self_attn.v_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 30/57 [00:02<00:01, 14.88w/s, dev=0]      model.layers.3.input_layernorm.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 31/57 [00:02<00:01, 15.33w/s, dev=0] model.layers.3.mlp.down_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 32/57 [00:02<00:01, 15.83w/s, dev=0]  model.layers.3.mlp.down_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 33/57 [00:02<00:01, 15.16w/s, dev=0]model.layers.3.mlp.gate_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 33/57 [00:02<00:01, 15.16w/s, dev=0]model.layers.3.mlp.up_proj.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:02<00:01, 14.55w/s, dev=0]  model.layers.3.post_attention_layernorm.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/57 [00:02<00:01, 14.01w/s, dev=0]model.layers.3.self_attn.k_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 36/57 [00:02<00:01, 14.41w/s, dev=0]        model.layers.3.self_attn.k_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 37/57 [00:02<00:01, 14.77w/s, dev=0]model.layers.3.self_attn.o_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 37/57 [00:02<00:01, 14.77w/s, dev=0]model.layers.3.self_attn.q_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 38/57 [00:02<00:01, 14.89w/s, dev=0]model.layers.3.self_attn.rotary_emb.inv_freq:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 39/57 [00:02<00:01, 15.01w/s, dev=0]model.layers.3.self_attn.v_proj.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:02<00:01, 15.39w/s, dev=0]      model.layers.4.input_layernorm.weight:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/57 [00:02<00:01, 15.74w/s, dev=0] model.layers.4.mlp.down_proj.weight:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 42/57 [00:02<00:00, 16.12w/s, dev=0]  model.layers.4.mlp.down_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 43/57 [00:02<00:00, 15.49w/s, dev=0]model.layers.4.mlp.gate_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 43/57 [00:02<00:00, 15.49w/s, dev=0]model.layers.4.mlp.up_proj.weight:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 44/57 [00:02<00:00, 14.91w/s, dev=0]  model.layers.4.post_attention_layernorm.weight:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 45/57 [00:03<00:00, 14.45w/s, dev=0]model.layers.4.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:03<00:00, 14.77w/s, dev=0]        model.layers.4.self_attn.o_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 47/57 [00:03<00:00, 15.06w/s, dev=0]model.layers.4.self_attn.q_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/57 [00:03<00:00, 15.16w/s, dev=0]model.layers.4.self_attn.q_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 49/57 [00:03<00:00, 15.25w/s, dev=0]model.layers.4.self_attn.rotary_emb.inv_freq:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 49/57 [00:03<00:00, 15.25w/s, dev=0]model.layers.4.self_attn.v_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 50/57 [00:03<00:00, 15.56w/s, dev=0]      model.layers.5.mlp.gate_proj.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 51/57 [00:03<00:00, 15.83w/s, dev=0]   model.layers.5.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:03<00:00, 15.31w/s, dev=0]model.layers.5.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 53/57 [00:03<00:00, 15.57w/s, dev=0]model.layers.5.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/57 [00:03<00:00, 15.62w/s, dev=0]model.layers.5.self_attn.q_proj.weight:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 55/57 [00:03<00:00, 15.68w/s, dev=0]model.layers.5.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 55/57 [00:03<00:00, 15.68w/s, dev=0]model.layers.5.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 56/57 [00:03<00:00, 15.96w/s, dev=0]                                                                                                     0%|          | 0/59 [00:00<?, ?w/s]model.layers.5.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=0]model.layers.5.mlp.down_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 1612.57w/s, dev=0]model.layers.5.mlp.down_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:05, 11.27w/s, dev=0]  model.layers.5.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:05, 11.26w/s, dev=0]  model.layers.5.post_attention_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:06,  8.46w/s, dev=0]model.layers.5.post_attention_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:04, 11.26w/s, dev=0]model.layers.6.input_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:04, 11.26w/s, dev=0]         model.layers.6.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 14.06w/s, dev=0]  model.layers.6.mlp.down_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:04, 11.43w/s, dev=0]model.layers.6.mlp.gate_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:04, 11.42w/s, dev=0]model.layers.6.mlp.up_proj.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:05, 10.19w/s, dev=0]  model.layers.6.mlp.up_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:05,  9.26w/s, dev=0]model.layers.6.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:05,  9.26w/s, dev=0]model.layers.6.self_attn.k_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:04, 10.41w/s, dev=0]        model.layers.6.self_attn.o_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:04, 11.45w/s, dev=0]model.layers.6.self_attn.q_proj.weight:  19%|â–ˆâ–Š        | 11/59 [00:00<00:04, 11.89w/s, dev=0]model.layers.6.self_attn.q_proj.weight:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:03, 12.26w/s, dev=0]model.layers.6.self_attn.rotary_emb.inv_freq:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:03, 12.26w/s, dev=0]model.layers.6.self_attn.v_proj.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:03, 13.28w/s, dev=0]      model.layers.7.input_layernorm.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:03, 14.18w/s, dev=0] model.layers.7.mlp.down_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:02, 15.18w/s, dev=0]  model.layers.7.mlp.down_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:03, 14.10w/s, dev=0]model.layers.7.mlp.gate_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:03, 14.10w/s, dev=0]model.layers.7.mlp.up_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:01<00:03, 13.25w/s, dev=0]  model.layers.7.post_attention_layernorm.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:03, 12.69w/s, dev=0]model.layers.7.self_attn.k_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:02, 13.39w/s, dev=0]        model.layers.7.self_attn.k_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:02, 14.01w/s, dev=0]model.layers.7.self_attn.o_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:02, 14.01w/s, dev=0]model.layers.7.self_attn.q_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:01<00:02, 14.20w/s, dev=0]model.layers.7.self_attn.rotary_emb.inv_freq:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:02, 14.40w/s, dev=0]model.layers.7.self_attn.v_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:02, 15.05w/s, dev=0]      model.layers.7.self_attn.v_proj.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:02, 15.63w/s, dev=0]model.layers.8.input_layernorm.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:02, 15.63w/s, dev=0] model.layers.8.mlp.down_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:02, 16.28w/s, dev=0]  model.layers.8.mlp.gate_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:02, 15.30w/s, dev=0]model.layers.8.mlp.up_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:01<00:02, 14.69w/s, dev=0]  model.layers.8.mlp.up_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:02, 14.19w/s, dev=0]model.layers.8.post_attention_layernorm.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:02, 14.19w/s, dev=0]model.layers.8.self_attn.k_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:02, 14.69w/s, dev=0]        model.layers.8.self_attn.o_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:01<00:01, 15.14w/s, dev=0]model.layers.8.self_attn.q_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:02<00:01, 15.33w/s, dev=0]model.layers.8.self_attn.rotary_emb.inv_freq:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:02<00:01, 15.51w/s, dev=0]model.layers.8.self_attn.v_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:02<00:01, 15.99w/s, dev=0]      model.layers.9.input_layernorm.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:02<00:01, 16.43w/s, dev=0] model.layers.9.mlp.down_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:02<00:01, 16.91w/s, dev=0]  model.layers.9.mlp.down_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:02<00:01, 16.25w/s, dev=0]model.layers.9.mlp.gate_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:02<00:01, 16.25w/s, dev=0]model.layers.9.mlp.up_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:02<00:01, 15.56w/s, dev=0]  model.layers.9.post_attention_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:02<00:01, 15.15w/s, dev=0]model.layers.9.self_attn.k_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:02<00:01, 15.54w/s, dev=0]        model.layers.9.self_attn.o_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:02<00:01, 15.89w/s, dev=0]model.layers.9.self_attn.q_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:02<00:01, 15.99w/s, dev=0]model.layers.9.self_attn.rotary_emb.inv_freq:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:02<00:01, 16.09w/s, dev=0]model.layers.9.self_attn.v_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:02<00:00, 16.47w/s, dev=0]      model.layers.9.self_attn.v_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:02<00:00, 16.80w/s, dev=0]model.layers.10.input_layernorm.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:02<00:00, 16.80w/s, dev=0]model.layers.10.mlp.down_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:02<00:00, 17.18w/s, dev=0]  model.layers.10.mlp.gate_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:02<00:00, 16.60w/s, dev=0]model.layers.10.mlp.up_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:02<00:00, 16.11w/s, dev=0]  model.layers.10.post_attention_layernorm.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:03<00:00, 15.54w/s, dev=0]model.layers.10.self_attn.k_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:03<00:00, 15.87w/s, dev=0]        model.layers.10.self_attn.o_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:03<00:00, 16.15w/s, dev=0]model.layers.10.self_attn.q_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:03<00:00, 16.22w/s, dev=0]model.layers.10.self_attn.q_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:03<00:00, 16.28w/s, dev=0]model.layers.10.self_attn.rotary_emb.inv_freq:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:03<00:00, 16.28w/s, dev=0]model.layers.10.self_attn.v_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:03<00:00, 16.59w/s, dev=0]      model.layers.11.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:03<00:00, 16.87w/s, dev=0]model.layers.11.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:03<00:00, 17.14w/s, dev=0]model.layers.11.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:03<00:00, 17.23w/s, dev=0]model.layers.11.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:03<00:00, 17.30w/s, dev=0]model.layers.11.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:03<00:00, 17.60w/s, dev=0]                                                                                                      0%|          | 0/55 [00:00<?, ?w/s]model.layers.11.input_layernorm.weight:   0%|          | 0/55 [00:00<?, ?w/s, dev=0]model.layers.11.mlp.down_proj.weight:   2%|â–         | 1/55 [00:00<00:00, 1591.16w/s, dev=0]model.layers.11.mlp.down_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:03, 13.64w/s, dev=0]  model.layers.11.mlp.gate_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:03, 13.62w/s, dev=0]model.layers.11.mlp.up_proj.weight:   5%|â–Œ         | 3/55 [00:00<00:05, 10.05w/s, dev=0]  model.layers.11.mlp.up_proj.weight:   7%|â–‹         | 4/55 [00:00<00:05,  8.96w/s, dev=0]model.layers.11.post_attention_layernorm.weight:   7%|â–‹         | 4/55 [00:00<00:05,  8.95w/s, dev=0]model.layers.12.input_layernorm.weight:   9%|â–‰         | 5/55 [00:00<00:04, 11.18w/s, dev=0]         model.layers.12.mlp.down_proj.weight:  11%|â–ˆ         | 6/55 [00:00<00:03, 13.40w/s, dev=0]  model.layers.12.mlp.down_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:04, 11.91w/s, dev=0]model.layers.12.mlp.gate_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:04, 11.90w/s, dev=0]model.layers.12.mlp.up_proj.weight:  15%|â–ˆâ–        | 8/55 [00:00<00:04, 10.72w/s, dev=0]  model.layers.12.post_attention_layernorm.weight:  16%|â–ˆâ–‹        | 9/55 [00:00<00:04,  9.85w/s, dev=0]model.layers.12.post_attention_layernorm.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:04, 10.94w/s, dev=0]model.layers.12.self_attn.k_proj.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:04, 10.94w/s, dev=0]        model.layers.12.self_attn.o_proj.weight:  20%|â–ˆâ–ˆ        | 11/55 [00:00<00:03, 11.95w/s, dev=0]model.layers.12.self_attn.q_proj.weight:  22%|â–ˆâ–ˆâ–       | 12/55 [00:00<00:03, 12.41w/s, dev=0]model.layers.12.self_attn.rotary_emb.inv_freq:  24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:01<00:03, 12.84w/s, dev=0]model.layers.12.self_attn.v_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 14/55 [00:01<00:02, 13.82w/s, dev=0]      model.layers.12.self_attn.v_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 15/55 [00:01<00:02, 14.72w/s, dev=0]model.layers.13.input_layernorm.weight:  27%|â–ˆâ–ˆâ–‹       | 15/55 [00:01<00:02, 14.71w/s, dev=0] model.layers.13.mlp.down_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 16/55 [00:01<00:02, 15.69w/s, dev=0]  model.layers.13.mlp.gate_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:01<00:02, 14.45w/s, dev=0]model.layers.13.mlp.up_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/55 [00:01<00:02, 13.45w/s, dev=0]  model.layers.13.post_attention_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–      | 19/55 [00:01<00:02, 12.79w/s, dev=0]model.layers.13.post_attention_layernorm.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:01<00:02, 13.46w/s, dev=0]model.layers.13.self_attn.k_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:01<00:02, 13.46w/s, dev=0]        model.layers.13.self_attn.o_proj.weight:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/55 [00:01<00:02, 14.08w/s, dev=0]model.layers.13.self_attn.q_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/55 [00:01<00:02, 14.31w/s, dev=0]model.layers.13.self_attn.rotary_emb.inv_freq:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 23/55 [00:01<00:02, 14.55w/s, dev=0]model.layers.13.self_attn.v_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 24/55 [00:01<00:02, 15.18w/s, dev=0]      model.layers.13.self_attn.v_proj.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:01<00:01, 15.75w/s, dev=0]model.layers.14.input_layernorm.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:01<00:01, 15.75w/s, dev=0] model.layers.14.mlp.down_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 26/55 [00:01<00:01, 16.38w/s, dev=0]  model.layers.14.mlp.gate_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 27/55 [00:01<00:01, 15.46w/s, dev=0]model.layers.14.mlp.up_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 28/55 [00:01<00:01, 14.64w/s, dev=0]  model.layers.14.post_attention_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/55 [00:02<00:01, 14.04w/s, dev=0]model.layers.14.post_attention_layernorm.weight:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 30/55 [00:02<00:01, 14.52w/s, dev=0]model.layers.14.self_attn.k_proj.weight:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 30/55 [00:02<00:01, 14.52w/s, dev=0]        model.layers.14.self_attn.o_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/55 [00:02<00:01, 14.95w/s, dev=0]model.layers.14.self_attn.q_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 32/55 [00:02<00:01, 15.10w/s, dev=0]model.layers.14.self_attn.rotary_emb.inv_freq:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/55 [00:02<00:01, 15.26w/s, dev=0]model.layers.14.self_attn.v_proj.weight:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/55 [00:02<00:01, 15.72w/s, dev=0]      model.layers.14.self_attn.v_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:02<00:01, 16.14w/s, dev=0]model.layers.15.input_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:02<00:01, 16.14w/s, dev=0] model.layers.15.mlp.down_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 36/55 [00:02<00:01, 16.59w/s, dev=0]  model.layers.15.mlp.gate_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 37/55 [00:02<00:01, 16.03w/s, dev=0]model.layers.15.mlp.up_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 38/55 [00:02<00:01, 15.58w/s, dev=0]  model.layers.15.post_attention_layernorm.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 39/55 [00:02<00:01, 15.07w/s, dev=0]model.layers.15.post_attention_layernorm.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 40/55 [00:02<00:00, 15.46w/s, dev=0]model.layers.15.self_attn.k_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 40/55 [00:02<00:00, 15.46w/s, dev=0]        model.layers.15.self_attn.o_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/55 [00:02<00:00, 15.76w/s, dev=0]model.layers.15.self_attn.q_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 42/55 [00:02<00:00, 15.87w/s, dev=0]model.layers.15.self_attn.rotary_emb.inv_freq:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 43/55 [00:02<00:00, 15.98w/s, dev=0]model.layers.15.self_attn.v_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 44/55 [00:02<00:00, 16.35w/s, dev=0]      model.layers.15.self_attn.v_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:02<00:00, 16.68w/s, dev=0]model.layers.16.input_layernorm.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:02<00:00, 16.68w/s, dev=0] model.layers.16.mlp.down_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 46/55 [00:02<00:00, 17.05w/s, dev=0]  model.layers.16.mlp.gate_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 47/55 [00:02<00:00, 16.55w/s, dev=0]model.layers.16.mlp.up_proj.weight:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 48/55 [00:02<00:00, 16.07w/s, dev=0]  model.layers.16.post_attention_layernorm.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 49/55 [00:03<00:00, 15.59w/s, dev=0]model.layers.16.post_attention_layernorm.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 50/55 [00:03<00:00, 15.91w/s, dev=0]model.layers.16.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 50/55 [00:03<00:00, 15.91w/s, dev=0]        model.layers.16.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 51/55 [00:03<00:00, 16.19w/s, dev=0]model.layers.16.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 52/55 [00:03<00:00, 16.29w/s, dev=0]model.layers.16.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 53/55 [00:03<00:00, 16.38w/s, dev=0]model.layers.16.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 54/55 [00:03<00:00, 16.69w/s, dev=0]                                                                                                      0%|          | 0/57 [00:00<?, ?w/s]model.layers.17.input_layernorm.weight:   0%|          | 0/57 [00:00<?, ?w/s, dev=0]model.layers.17.mlp.down_proj.weight:   2%|â–         | 1/57 [00:00<00:00, 1519.12w/s, dev=0]model.layers.17.mlp.down_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:04, 13.63w/s, dev=0]  model.layers.17.mlp.gate_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:04, 13.61w/s, dev=0]model.layers.17.mlp.up_proj.weight:   5%|â–Œ         | 3/57 [00:00<00:05, 10.25w/s, dev=0]  model.layers.17.mlp.up_proj.weight:   7%|â–‹         | 4/57 [00:00<00:05,  9.13w/s, dev=0]model.layers.17.post_attention_layernorm.weight:   7%|â–‹         | 4/57 [00:00<00:05,  9.12w/s, dev=0]model.layers.17.self_attn.k_proj.weight:   9%|â–‰         | 5/57 [00:00<00:04, 11.39w/s, dev=0]        model.layers.17.self_attn.o_proj.weight:  11%|â–ˆ         | 6/57 [00:00<00:03, 13.47w/s, dev=0]model.layers.17.self_attn.q_proj.weight:  12%|â–ˆâ–        | 7/57 [00:00<00:03, 14.15w/s, dev=0]model.layers.17.self_attn.q_proj.weight:  14%|â–ˆâ–        | 8/57 [00:00<00:03, 14.67w/s, dev=0]model.layers.17.self_attn.rotary_emb.inv_freq:  14%|â–ˆâ–        | 8/57 [00:00<00:03, 14.67w/s, dev=0]model.layers.17.self_attn.v_proj.weight:  16%|â–ˆâ–Œ        | 9/57 [00:00<00:02, 16.49w/s, dev=0]      model.layers.18.input_layernorm.weight:  18%|â–ˆâ–Š        | 10/57 [00:00<00:02, 18.09w/s, dev=0]model.layers.18.mlp.down_proj.weight:  19%|â–ˆâ–‰        | 11/57 [00:00<00:02, 19.89w/s, dev=0]  model.layers.18.mlp.down_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:00<00:02, 17.20w/s, dev=0]model.layers.18.mlp.gate_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:00<00:02, 17.19w/s, dev=0]model.layers.18.mlp.up_proj.weight:  23%|â–ˆâ–ˆâ–Ž       | 13/57 [00:00<00:02, 15.33w/s, dev=0]  model.layers.18.post_attention_layernorm.weight:  25%|â–ˆâ–ˆâ–       | 14/57 [00:01<00:03, 13.98w/s, dev=0]model.layers.18.self_attn.k_proj.weight:  26%|â–ˆâ–ˆâ–‹       | 15/57 [00:01<00:02, 14.98w/s, dev=0]        model.layers.18.self_attn.k_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:01<00:02, 15.88w/s, dev=0]model.layers.18.self_attn.o_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:01<00:02, 15.87w/s, dev=0]model.layers.18.self_attn.q_proj.weight:  30%|â–ˆâ–ˆâ–‰       | 17/57 [00:01<00:02, 16.23w/s, dev=0]model.layers.18.self_attn.rotary_emb.inv_freq:  32%|â–ˆâ–ˆâ–ˆâ–      | 18/57 [00:01<00:02, 16.41w/s, dev=0]model.layers.18.self_attn.v_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 19/57 [00:01<00:02, 17.32w/s, dev=0]      model.layers.19.input_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 20/57 [00:01<00:02, 18.11w/s, dev=0] model.layers.19.mlp.down_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 21/57 [00:01<00:01, 19.01w/s, dev=0]  model.layers.19.mlp.down_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:01<00:02, 17.20w/s, dev=0]model.layers.19.mlp.gate_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:01<00:02, 17.19w/s, dev=0]model.layers.19.mlp.up_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 23/57 [00:01<00:02, 15.92w/s, dev=0]  model.layers.19.post_attention_layernorm.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/57 [00:01<00:02, 14.97w/s, dev=0]model.layers.19.self_attn.k_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/57 [00:01<00:02, 15.59w/s, dev=0]        model.layers.19.self_attn.o_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 26/57 [00:01<00:01, 16.15w/s, dev=0]model.layers.19.self_attn.q_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 27/57 [00:01<00:01, 16.31w/s, dev=0]model.layers.19.self_attn.q_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:01<00:01, 16.47w/s, dev=0]model.layers.19.self_attn.rotary_emb.inv_freq:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:01<00:01, 16.47w/s, dev=0]model.layers.19.self_attn.v_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 29/57 [00:01<00:01, 17.05w/s, dev=0]      model.layers.20.input_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 30/57 [00:01<00:01, 15.62w/s, dev=1] model.layers.20.mlp.down_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 31/57 [00:01<00:01, 16.13w/s, dev=1]  model.layers.20.mlp.gate_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 32/57 [00:02<00:01, 15.30w/s, dev=1]model.layers.20.mlp.up_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 33/57 [00:02<00:01, 14.65w/s, dev=1]  model.layers.20.mlp.up_proj.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:02<00:01, 14.02w/s, dev=1]model.layers.20.post_attention_layernorm.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:02<00:01, 14.02w/s, dev=1]model.layers.20.self_attn.k_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/57 [00:02<00:01, 14.43w/s, dev=1]        model.layers.20.self_attn.o_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 36/57 [00:02<00:01, 14.80w/s, dev=1]model.layers.20.self_attn.q_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 37/57 [00:02<00:01, 14.92w/s, dev=1]model.layers.20.self_attn.rotary_emb.inv_freq:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 38/57 [00:02<00:01, 15.03w/s, dev=1]model.layers.20.self_attn.v_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 39/57 [00:02<00:01, 15.42w/s, dev=1]      model.layers.20.self_attn.v_proj.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:02<00:01, 15.77w/s, dev=1]model.layers.21.input_layernorm.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:02<00:01, 15.77w/s, dev=1] model.layers.21.mlp.down_proj.weight:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/57 [00:02<00:00, 16.16w/s, dev=1]  model.layers.21.mlp.gate_proj.weight:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 42/57 [00:02<00:00, 15.49w/s, dev=1]model.layers.21.mlp.up_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 43/57 [00:02<00:00, 14.88w/s, dev=1]  model.layers.21.post_attention_layernorm.weight:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 44/57 [00:03<00:00, 14.43w/s, dev=1]model.layers.21.self_attn.k_proj.weight:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 45/57 [00:03<00:00, 14.76w/s, dev=1]        model.layers.21.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:03<00:00, 15.04w/s, dev=1]model.layers.21.self_attn.o_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:03<00:00, 15.04w/s, dev=1]model.layers.21.self_attn.q_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 47/57 [00:03<00:00, 15.10w/s, dev=1]model.layers.21.self_attn.rotary_emb.inv_freq:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/57 [00:03<00:00, 15.17w/s, dev=1]model.layers.21.self_attn.v_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 49/57 [00:03<00:00, 15.49w/s, dev=1]      model.layers.22.mlp.down_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 50/57 [00:03<00:00, 15.76w/s, dev=1]   model.layers.22.mlp.gate_proj.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 51/57 [00:03<00:00, 15.24w/s, dev=1]model.layers.22.mlp.gate_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:03<00:00, 14.77w/s, dev=1]model.layers.22.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:03<00:00, 14.76w/s, dev=1]model.layers.22.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 53/57 [00:03<00:00, 15.01w/s, dev=1]model.layers.22.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/57 [00:03<00:00, 15.07w/s, dev=1]model.layers.22.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 55/57 [00:03<00:00, 15.13w/s, dev=1]model.layers.22.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 56/57 [00:03<00:00, 15.40w/s, dev=1]                                                                                                      0%|          | 0/59 [00:00<?, ?w/s]model.layers.22.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=1]model.layers.22.mlp.up_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 1244.23w/s, dev=1]model.layers.22.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 12.29w/s, dev=1]  model.layers.22.post_attention_layernorm.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 12.27w/s, dev=1]model.layers.23.input_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:03, 18.36w/s, dev=1]         model.layers.23.mlp.down_proj.weight:   7%|â–‹         | 4/59 [00:00<00:02, 24.45w/s, dev=1]  model.layers.23.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 15.31w/s, dev=1]model.layers.23.mlp.gate_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 15.29w/s, dev=1]model.layers.23.mlp.up_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:04, 12.39w/s, dev=1]  model.layers.23.post_attention_layernorm.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:04, 10.90w/s, dev=1]model.layers.23.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:04, 12.45w/s, dev=1]model.layers.23.self_attn.k_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:04, 12.44w/s, dev=1]        model.layers.23.self_attn.o_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:03, 13.86w/s, dev=1]model.layers.23.self_attn.q_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:03, 14.40w/s, dev=1]model.layers.23.self_attn.rotary_emb.inv_freq:  19%|â–ˆâ–Š        | 11/59 [00:00<00:03, 14.85w/s, dev=1]model.layers.23.self_attn.v_proj.weight:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:02, 16.19w/s, dev=1]      model.layers.23.self_attn.v_proj.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:02, 17.39w/s, dev=1]model.layers.24.input_layernorm.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:02, 17.39w/s, dev=1] model.layers.24.mlp.down_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:02, 18.72w/s, dev=1]  model.layers.24.mlp.gate_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:02, 16.69w/s, dev=1]model.layers.24.mlp.up_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:02, 14.98w/s, dev=1]  model.layers.24.post_attention_layernorm.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:01<00:03, 13.94w/s, dev=1]model.layers.24.post_attention_layernorm.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:02, 14.75w/s, dev=1]model.layers.24.self_attn.k_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:02, 14.75w/s, dev=1]        model.layers.24.self_attn.o_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:02, 15.48w/s, dev=1]model.layers.24.self_attn.q_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:02, 15.67w/s, dev=1]model.layers.24.self_attn.rotary_emb.inv_freq:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:01<00:02, 15.84w/s, dev=1]model.layers.24.self_attn.v_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:02, 16.58w/s, dev=1]      model.layers.24.self_attn.v_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:02, 17.24w/s, dev=1]model.layers.25.input_layernorm.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:02, 17.23w/s, dev=1] model.layers.25.mlp.down_proj.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:01, 17.98w/s, dev=1]  model.layers.25.mlp.gate_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:02, 16.68w/s, dev=1]model.layers.25.mlp.up_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:02, 15.72w/s, dev=1]  model.layers.25.post_attention_layernorm.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:01<00:02, 14.88w/s, dev=1]model.layers.25.post_attention_layernorm.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:02, 15.43w/s, dev=1]model.layers.25.self_attn.k_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:02, 15.43w/s, dev=1]        model.layers.25.self_attn.o_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:01, 15.92w/s, dev=1]model.layers.25.self_attn.q_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:01<00:01, 16.07w/s, dev=1]model.layers.25.self_attn.rotary_emb.inv_freq:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:01<00:01, 16.19w/s, dev=1]model.layers.25.self_attn.v_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:01<00:01, 16.71w/s, dev=1]      model.layers.25.self_attn.v_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:01<00:01, 17.16w/s, dev=1]model.layers.26.input_layernorm.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:01<00:01, 17.16w/s, dev=1] model.layers.26.mlp.down_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:01<00:01, 17.68w/s, dev=1]  model.layers.26.mlp.gate_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:02<00:01, 16.75w/s, dev=1]model.layers.26.mlp.up_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:02<00:01, 15.99w/s, dev=1]  model.layers.26.post_attention_layernorm.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:02<00:01, 15.34w/s, dev=1]model.layers.26.post_attention_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:02<00:01, 15.75w/s, dev=1]model.layers.26.self_attn.k_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:02<00:01, 15.75w/s, dev=1]        model.layers.26.self_attn.o_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:02<00:01, 16.11w/s, dev=1]model.layers.26.self_attn.q_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:02<00:01, 16.20w/s, dev=1]model.layers.26.self_attn.rotary_emb.inv_freq:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:02<00:01, 16.29w/s, dev=1]model.layers.26.self_attn.v_proj.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:02<00:01, 16.68w/s, dev=1]      model.layers.26.self_attn.v_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:02<00:00, 17.03w/s, dev=1]model.layers.27.input_layernorm.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:02<00:00, 17.03w/s, dev=1] model.layers.27.mlp.down_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:02<00:00, 17.43w/s, dev=1]  model.layers.27.mlp.gate_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:02<00:00, 16.73w/s, dev=1]model.layers.27.mlp.up_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:02<00:00, 16.22w/s, dev=1]  model.layers.27.post_attention_layernorm.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:02<00:00, 15.69w/s, dev=1]model.layers.27.post_attention_layernorm.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:02<00:00, 16.02w/s, dev=1]model.layers.27.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:02<00:00, 16.02w/s, dev=1]        model.layers.27.self_attn.o_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:03<00:00, 16.32w/s, dev=1]model.layers.27.self_attn.q_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:03<00:00, 16.41w/s, dev=1]model.layers.27.self_attn.rotary_emb.inv_freq:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:03<00:00, 16.48w/s, dev=1]model.layers.27.self_attn.v_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:03<00:00, 16.80w/s, dev=1]      model.layers.27.self_attn.v_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:03<00:00, 17.09w/s, dev=1]model.layers.28.mlp.gate_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:03<00:00, 17.09w/s, dev=1]   model.layers.28.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:03<00:00, 16.52w/s, dev=1]model.layers.28.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:03<00:00, 16.79w/s, dev=1]model.layers.28.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:03<00:00, 16.84w/s, dev=1]model.layers.28.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:03<00:00, 16.87w/s, dev=1]model.layers.28.self_attn.rotary_emb.inv_freq:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:03<00:00, 17.17w/s, dev=1]model.layers.28.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:03<00:00, 17.17w/s, dev=1]                                                                                                      0%|          | 0/59 [00:00<?, ?w/s]model.layers.28.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=1]model.layers.28.mlp.down_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 1297.74w/s, dev=1]model.layers.28.mlp.down_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 11.95w/s, dev=1]  model.layers.28.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 11.93w/s, dev=1]  model.layers.28.post_attention_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:06,  9.10w/s, dev=1]model.layers.28.post_attention_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:04, 12.11w/s, dev=1]model.layers.29.input_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:04, 12.11w/s, dev=1]         model.layers.29.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 15.12w/s, dev=1]  model.layers.29.mlp.down_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:04, 12.62w/s, dev=1]model.layers.29.mlp.gate_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:04, 12.62w/s, dev=1]model.layers.29.mlp.up_proj.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:04, 11.09w/s, dev=1]  model.layers.29.mlp.up_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:05, 10.15w/s, dev=1]model.layers.29.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:05, 10.15w/s, dev=1]model.layers.29.self_attn.k_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:04, 11.41w/s, dev=1]        model.layers.29.self_attn.o_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:03, 12.57w/s, dev=1]model.layers.29.self_attn.q_proj.weight:  19%|â–ˆâ–Š        | 11/59 [00:00<00:03, 13.15w/s, dev=1]model.layers.29.self_attn.rotary_emb.inv_freq:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:03, 13.63w/s, dev=1]model.layers.29.self_attn.v_proj.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:03, 14.76w/s, dev=1]      model.layers.30.input_layernorm.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:02, 15.78w/s, dev=1] model.layers.30.mlp.down_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:02, 16.91w/s, dev=1]  model.layers.30.mlp.down_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:02, 15.50w/s, dev=1]model.layers.30.mlp.gate_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:02, 15.50w/s, dev=1]model.layers.30.mlp.up_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:01<00:02, 14.35w/s, dev=1]  model.layers.30.post_attention_layernorm.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:03, 13.47w/s, dev=1]model.layers.30.self_attn.k_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:02, 14.21w/s, dev=1]        model.layers.30.self_attn.o_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:02, 14.89w/s, dev=1]model.layers.30.self_attn.q_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:01<00:02, 15.16w/s, dev=1]model.layers.30.self_attn.rotary_emb.inv_freq:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:02, 15.39w/s, dev=1]model.layers.30.self_attn.v_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:02, 16.09w/s, dev=1]      model.layers.30.self_attn.v_proj.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:02, 16.71w/s, dev=1]model.layers.31.input_layernorm.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:02, 16.71w/s, dev=1] model.layers.31.mlp.down_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:01, 17.41w/s, dev=1]  model.layers.31.mlp.gate_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:02, 16.26w/s, dev=1]model.layers.31.mlp.up_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:01<00:02, 15.37w/s, dev=1]  model.layers.31.post_attention_layernorm.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:02, 14.54w/s, dev=1]model.layers.31.self_attn.k_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:01, 15.06w/s, dev=1]        model.layers.31.self_attn.o_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:01<00:01, 15.53w/s, dev=1]model.layers.31.self_attn.q_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:01<00:01, 15.69w/s, dev=1]model.layers.31.self_attn.q_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:02<00:01, 15.84w/s, dev=1]model.layers.31.self_attn.rotary_emb.inv_freq:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:02<00:01, 15.84w/s, dev=1]model.layers.31.self_attn.v_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:02<00:01, 16.33w/s, dev=1]      model.layers.32.input_layernorm.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:02<00:01, 16.76w/s, dev=1] model.layers.32.mlp.down_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:02<00:01, 17.26w/s, dev=1]  model.layers.32.mlp.gate_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:02<00:01, 16.39w/s, dev=1]model.layers.32.mlp.up_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:02<00:01, 15.76w/s, dev=1]  model.layers.32.post_attention_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:02<00:01, 15.15w/s, dev=1]model.layers.32.self_attn.k_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:02<00:01, 15.55w/s, dev=1]        model.layers.32.self_attn.k_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:02<00:01, 15.90w/s, dev=1]model.layers.32.self_attn.o_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:02<00:01, 15.90w/s, dev=1]model.layers.32.self_attn.q_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:02<00:01, 15.98w/s, dev=1]model.layers.32.self_attn.rotary_emb.inv_freq:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:02<00:01, 16.07w/s, dev=1]model.layers.32.self_attn.v_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:02<00:00, 16.45w/s, dev=1]      model.layers.33.input_layernorm.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:02<00:00, 15.80w/s, dev=1] model.layers.33.mlp.down_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:02<00:00, 16.15w/s, dev=1]  model.layers.33.mlp.gate_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:02<00:00, 15.60w/s, dev=1]model.layers.33.mlp.up_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:03<00:00, 15.12w/s, dev=1]  model.layers.33.mlp.up_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:03<00:00, 14.01w/s, dev=1]model.layers.33.post_attention_layernorm.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:03<00:00, 14.00w/s, dev=1]model.layers.33.self_attn.k_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:03<00:00, 14.29w/s, dev=1]        model.layers.33.self_attn.o_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:03<00:00, 14.53w/s, dev=1]model.layers.33.self_attn.q_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:03<00:00, 14.62w/s, dev=1]model.layers.33.self_attn.rotary_emb.inv_freq:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:03<00:00, 14.71w/s, dev=1]model.layers.33.self_attn.v_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:03<00:00, 14.99w/s, dev=1]      model.layers.34.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:03<00:00, 15.23w/s, dev=1]model.layers.34.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:03<00:00, 15.44w/s, dev=1]model.layers.34.self_attn.o_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:03<00:00, 15.51w/s, dev=1]model.layers.34.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:03<00:00, 15.51w/s, dev=1]model.layers.34.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:03<00:00, 15.57w/s, dev=1]model.layers.34.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:03<00:00, 15.84w/s, dev=1]                                                                                                      0%|          | 0/55 [00:00<?, ?w/s]model.layers.34.input_layernorm.weight:   0%|          | 0/55 [00:00<?, ?w/s, dev=1]model.layers.34.mlp.down_proj.weight:   2%|â–         | 1/55 [00:00<00:00, 1204.57w/s, dev=1]model.layers.34.mlp.down_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:04, 12.62w/s, dev=1]  model.layers.34.mlp.gate_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:04, 12.60w/s, dev=1]model.layers.34.mlp.up_proj.weight:   5%|â–Œ         | 3/55 [00:00<00:05,  9.17w/s, dev=1]  model.layers.34.mlp.up_proj.weight:   7%|â–‹         | 4/55 [00:00<00:09,  5.44w/s, dev=1]model.layers.34.post_attention_layernorm.weight:   7%|â–‹         | 4/55 [00:00<00:09,  5.44w/s, dev=1]model.layers.35.input_layernorm.weight:   9%|â–‰         | 5/55 [00:00<00:07,  6.80w/s, dev=1]         model.layers.35.mlp.down_proj.weight:  11%|â–ˆ         | 6/55 [00:00<00:06,  8.16w/s, dev=1]  model.layers.35.mlp.down_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:06,  7.69w/s, dev=1]model.layers.35.mlp.gate_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:06,  7.68w/s, dev=1]model.layers.35.mlp.up_proj.weight:  15%|â–ˆâ–        | 8/55 [00:01<00:06,  7.53w/s, dev=1]  model.layers.35.post_attention_layernorm.weight:  16%|â–ˆâ–‹        | 9/55 [00:01<00:06,  6.62w/s, dev=1]model.layers.35.post_attention_layernorm.weight:  18%|â–ˆâ–Š        | 10/55 [00:01<00:06,  7.35w/s, dev=1]model.layers.35.self_attn.k_proj.weight:  18%|â–ˆâ–Š        | 10/55 [00:01<00:06,  7.35w/s, dev=1]        model.layers.35.self_attn.o_proj.weight:  20%|â–ˆâ–ˆ        | 11/55 [00:01<00:05,  8.03w/s, dev=1]model.layers.35.self_attn.q_proj.weight:  22%|â–ˆâ–ˆâ–       | 12/55 [00:01<00:05,  8.43w/s, dev=1]model.layers.35.self_attn.q_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:01<00:04,  8.81w/s, dev=1]model.layers.35.self_attn.rotary_emb.inv_freq:  24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:01<00:04,  8.81w/s, dev=1]model.layers.35.self_attn.v_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 14/55 [00:01<00:04,  9.49w/s, dev=1]      model.layers.36.input_layernorm.weight:  27%|â–ˆâ–ˆâ–‹       | 15/55 [00:01<00:03, 10.12w/s, dev=1] model.layers.36.mlp.down_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 16/55 [00:01<00:03, 10.79w/s, dev=1]  model.layers.36.mlp.down_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:01<00:04,  8.90w/s, dev=1]model.layers.36.mlp.gate_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:01<00:04,  8.90w/s, dev=1]model.layers.36.mlp.up_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/55 [00:02<00:04,  8.72w/s, dev=1]  model.layers.36.post_attention_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–      | 19/55 [00:02<00:04,  8.52w/s, dev=1]model.layers.36.self_attn.k_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:02<00:03,  8.97w/s, dev=1]        model.layers.36.self_attn.k_proj.weight:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/55 [00:02<00:03,  8.79w/s, dev=1]model.layers.36.self_attn.o_proj.weight:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/55 [00:02<00:03,  8.79w/s, dev=1]model.layers.36.self_attn.q_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/55 [00:02<00:03,  9.02w/s, dev=1]model.layers.36.self_attn.rotary_emb.inv_freq:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 23/55 [00:02<00:03,  9.24w/s, dev=1]model.layers.36.self_attn.v_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 24/55 [00:02<00:03,  9.64w/s, dev=1]      model.layers.36.self_attn.v_proj.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:02<00:02, 10.01w/s, dev=1]model.layers.37.input_layernorm.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:02<00:02, 10.01w/s, dev=1] model.layers.37.mlp.down_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 26/55 [00:02<00:02, 10.41w/s, dev=1]  model.layers.37.mlp.gate_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 27/55 [00:02<00:02, 10.15w/s, dev=1]model.layers.37.mlp.up_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 28/55 [00:02<00:02,  9.91w/s, dev=1]  model.layers.37.mlp.up_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/55 [00:02<00:02,  9.71w/s, dev=1]model.layers.37.post_attention_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/55 [00:02<00:02,  9.70w/s, dev=1]model.layers.37.self_attn.k_proj.weight:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 30/55 [00:02<00:02, 10.04w/s, dev=1]        model.layers.37.self_attn.o_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/55 [00:02<00:02, 10.35w/s, dev=1]model.layers.37.self_attn.q_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 32/55 [00:03<00:02, 10.53w/s, dev=1]model.layers.37.self_attn.rotary_emb.inv_freq:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/55 [00:03<00:02, 10.69w/s, dev=1]model.layers.37.self_attn.v_proj.weight:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/55 [00:03<00:01, 11.01w/s, dev=1]      model.layers.37.self_attn.v_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:03<00:01, 11.31w/s, dev=1]model.layers.38.input_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:03<00:01, 11.31w/s, dev=1] model.layers.38.mlp.down_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 36/55 [00:03<00:01, 11.63w/s, dev=1]  model.layers.38.mlp.gate_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 37/55 [00:03<00:01, 11.38w/s, dev=1]model.layers.38.mlp.up_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 38/55 [00:03<00:01, 11.11w/s, dev=1]  model.layers.38.post_attention_layernorm.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 39/55 [00:03<00:01, 10.90w/s, dev=1]model.layers.38.self_attn.k_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 40/55 [00:03<00:01, 11.17w/s, dev=1]        model.layers.38.self_attn.k_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/55 [00:03<00:01, 11.43w/s, dev=1]model.layers.38.self_attn.o_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/55 [00:03<00:01, 11.43w/s, dev=1]model.layers.38.self_attn.q_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 42/55 [00:03<00:01, 11.56w/s, dev=1]model.layers.38.self_attn.rotary_emb.inv_freq:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 43/55 [00:03<00:01, 11.70w/s, dev=1]model.layers.38.self_attn.v_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 44/55 [00:03<00:00, 11.97w/s, dev=1]      model.layers.39.input_layernorm.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:03<00:00, 12.22w/s, dev=1] model.layers.39.mlp.down_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 46/55 [00:03<00:00, 12.49w/s, dev=1]  model.layers.39.mlp.down_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 47/55 [00:03<00:00, 12.22w/s, dev=1]model.layers.39.mlp.gate_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 47/55 [00:03<00:00, 12.22w/s, dev=1]model.layers.39.mlp.up_proj.weight:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 48/55 [00:04<00:00, 11.93w/s, dev=1]  model.layers.39.post_attention_layernorm.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 49/55 [00:04<00:00, 11.73w/s, dev=1]model.layers.39.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 50/55 [00:04<00:00, 11.97w/s, dev=1]        model.layers.39.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 51/55 [00:04<00:00, 12.19w/s, dev=1]model.layers.39.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 52/55 [00:04<00:00, 12.30w/s, dev=1]model.layers.39.self_attn.q_proj.weight:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 53/55 [00:04<00:00, 12.40w/s, dev=1]model.layers.39.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 53/55 [00:04<00:00, 12.40w/s, dev=1]model.layers.39.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 54/55 [00:04<00:00, 12.63w/s, dev=1]                                                                                                      0%|          | 0/57 [00:00<?, ?w/s]model.layers.40.input_layernorm.weight:   0%|          | 0/57 [00:00<?, ?w/s, dev=1]model.layers.40.mlp.down_proj.weight:   2%|â–         | 1/57 [00:00<00:00, 1189.54w/s, dev=1]model.layers.40.mlp.down_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:04, 11.95w/s, dev=1]  model.layers.40.mlp.gate_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:04, 11.93w/s, dev=1]model.layers.40.mlp.up_proj.weight:   5%|â–Œ         | 3/57 [00:00<00:05,  9.48w/s, dev=1]  model.layers.40.mlp.up_proj.weight:   7%|â–‹         | 4/57 [00:00<00:06,  8.61w/s, dev=1]model.layers.40.post_attention_layernorm.weight:   7%|â–‹         | 4/57 [00:00<00:06,  8.60w/s, dev=1]model.layers.40.self_attn.k_proj.weight:   9%|â–‰         | 5/57 [00:00<00:04, 10.75w/s, dev=1]        model.layers.40.self_attn.o_proj.weight:  11%|â–ˆ         | 6/57 [00:00<00:04, 12.55w/s, dev=1]model.layers.40.self_attn.q_proj.weight:  12%|â–ˆâ–        | 7/57 [00:00<00:03, 13.37w/s, dev=1]model.layers.40.self_attn.q_proj.weight:  14%|â–ˆâ–        | 8/57 [00:00<00:03, 14.10w/s, dev=1]model.layers.40.self_attn.rotary_emb.inv_freq:  14%|â–ˆâ–        | 8/57 [00:00<00:03, 14.10w/s, dev=1]model.layers.40.self_attn.v_proj.weight:  16%|â–ˆâ–Œ        | 9/57 [00:00<00:03, 15.85w/s, dev=1]      model.layers.41.input_layernorm.weight:  18%|â–ˆâ–Š        | 10/57 [00:00<00:03, 15.62w/s, dev=2]model.layers.41.mlp.down_proj.weight:  19%|â–ˆâ–‰        | 11/57 [00:00<00:02, 17.16w/s, dev=2]  model.layers.41.mlp.down_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:00<00:03, 14.74w/s, dev=2]model.layers.41.mlp.gate_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:00<00:03, 14.74w/s, dev=2]model.layers.41.mlp.up_proj.weight:  23%|â–ˆâ–ˆâ–Ž       | 13/57 [00:00<00:03, 13.26w/s, dev=2]  model.layers.41.post_attention_layernorm.weight:  25%|â–ˆâ–ˆâ–       | 14/57 [00:01<00:03, 12.28w/s, dev=2]model.layers.41.self_attn.k_proj.weight:  26%|â–ˆâ–ˆâ–‹       | 15/57 [00:01<00:03, 13.15w/s, dev=2]        model.layers.41.self_attn.k_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:01<00:02, 13.95w/s, dev=2]model.layers.41.self_attn.o_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:01<00:02, 13.95w/s, dev=2]model.layers.41.self_attn.q_proj.weight:  30%|â–ˆâ–ˆâ–‰       | 17/57 [00:01<00:02, 14.23w/s, dev=2]model.layers.41.self_attn.rotary_emb.inv_freq:  32%|â–ˆâ–ˆâ–ˆâ–      | 18/57 [00:01<00:02, 14.50w/s, dev=2]model.layers.41.self_attn.v_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 19/57 [00:01<00:02, 15.30w/s, dev=2]      model.layers.41.self_attn.v_proj.weight:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 20/57 [00:01<00:02, 16.02w/s, dev=2]model.layers.42.input_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 20/57 [00:01<00:02, 16.02w/s, dev=2] model.layers.42.mlp.down_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 21/57 [00:01<00:02, 16.82w/s, dev=2]  model.layers.42.mlp.gate_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:01<00:02, 15.57w/s, dev=2]model.layers.42.mlp.up_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 23/57 [00:01<00:02, 14.54w/s, dev=2]  model.layers.42.mlp.up_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/57 [00:01<00:02, 13.75w/s, dev=2]model.layers.42.post_attention_layernorm.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/57 [00:01<00:02, 13.74w/s, dev=2]model.layers.42.self_attn.k_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/57 [00:01<00:02, 14.31w/s, dev=2]        model.layers.42.self_attn.o_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 26/57 [00:01<00:02, 14.83w/s, dev=2]model.layers.42.self_attn.q_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 27/57 [00:01<00:01, 15.01w/s, dev=2]model.layers.42.self_attn.rotary_emb.inv_freq:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:01<00:01, 15.17w/s, dev=2]model.layers.42.self_attn.rotary_emb.inv_freq:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 29/57 [00:01<00:01, 15.71w/s, dev=2]model.layers.42.self_attn.v_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 29/57 [00:01<00:01, 15.71w/s, dev=2]      model.layers.43.input_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 30/57 [00:01<00:01, 16.18w/s, dev=2] model.layers.43.mlp.down_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 31/57 [00:01<00:01, 16.72w/s, dev=2]  model.layers.43.mlp.gate_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 32/57 [00:02<00:01, 15.92w/s, dev=2]model.layers.43.mlp.up_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 33/57 [00:02<00:01, 15.13w/s, dev=2]  model.layers.43.mlp.up_proj.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:02<00:01, 14.41w/s, dev=2]model.layers.43.post_attention_layernorm.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:02<00:01, 14.40w/s, dev=2]model.layers.43.self_attn.k_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/57 [00:02<00:01, 14.82w/s, dev=2]        model.layers.43.self_attn.o_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 36/57 [00:02<00:01, 15.19w/s, dev=2]model.layers.43.self_attn.q_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 37/57 [00:02<00:01, 15.27w/s, dev=2]model.layers.43.self_attn.rotary_emb.inv_freq:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 38/57 [00:02<00:01, 15.34w/s, dev=2]model.layers.43.self_attn.rotary_emb.inv_freq:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 39/57 [00:02<00:01, 15.74w/s, dev=2]model.layers.43.self_attn.v_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 39/57 [00:02<00:01, 15.74w/s, dev=2]      model.layers.44.input_layernorm.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:02<00:01, 16.10w/s, dev=2] model.layers.44.mlp.down_proj.weight:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/57 [00:02<00:00, 16.50w/s, dev=2]  model.layers.44.mlp.gate_proj.weight:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 42/57 [00:02<00:00, 15.79w/s, dev=2]model.layers.44.mlp.up_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 43/57 [00:02<00:00, 15.24w/s, dev=2]  model.layers.44.mlp.up_proj.weight:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 44/57 [00:02<00:00, 14.77w/s, dev=2]model.layers.44.post_attention_layernorm.weight:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 44/57 [00:02<00:00, 14.77w/s, dev=2]model.layers.44.self_attn.k_proj.weight:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 45/57 [00:02<00:00, 15.10w/s, dev=2]        model.layers.44.self_attn.o_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:02<00:00, 15.41w/s, dev=2]model.layers.44.self_attn.q_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 47/57 [00:03<00:00, 15.50w/s, dev=2]model.layers.44.self_attn.rotary_emb.inv_freq:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/57 [00:03<00:00, 15.59w/s, dev=2]model.layers.44.self_attn.rotary_emb.inv_freq:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 49/57 [00:03<00:00, 15.91w/s, dev=2]model.layers.44.self_attn.v_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 49/57 [00:03<00:00, 15.91w/s, dev=2]      model.layers.45.mlp.down_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 50/57 [00:03<00:00, 16.20w/s, dev=2]   model.layers.45.mlp.gate_proj.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 51/57 [00:03<00:00, 15.77w/s, dev=2]model.layers.45.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:03<00:00, 15.32w/s, dev=2]model.layers.45.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 53/57 [00:03<00:00, 15.58w/s, dev=2]model.layers.45.self_attn.o_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/57 [00:03<00:00, 15.62w/s, dev=2]model.layers.45.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/57 [00:03<00:00, 15.61w/s, dev=2]model.layers.45.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 55/57 [00:03<00:00, 15.68w/s, dev=2]model.layers.45.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 56/57 [00:03<00:00, 15.96w/s, dev=2]                                                                                                      0%|          | 0/59 [00:00<?, ?w/s]model.layers.45.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=2]model.layers.45.mlp.up_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 1064.27w/s, dev=2]model.layers.45.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 12.83w/s, dev=2]  model.layers.45.post_attention_layernorm.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 12.81w/s, dev=2]model.layers.46.input_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:02, 19.16w/s, dev=2]         model.layers.46.mlp.down_proj.weight:   7%|â–‹         | 4/59 [00:00<00:02, 25.51w/s, dev=2]  model.layers.46.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 15.79w/s, dev=2]model.layers.46.mlp.gate_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 15.78w/s, dev=2]model.layers.46.mlp.up_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:04, 12.61w/s, dev=2]  model.layers.46.post_attention_layernorm.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:04, 11.09w/s, dev=2]model.layers.46.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:04, 12.66w/s, dev=2]model.layers.46.self_attn.k_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:04, 12.66w/s, dev=2]        model.layers.46.self_attn.o_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:03, 14.10w/s, dev=2]model.layers.46.self_attn.q_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:03, 14.60w/s, dev=2]model.layers.46.self_attn.rotary_emb.inv_freq:  19%|â–ˆâ–Š        | 11/59 [00:00<00:03, 15.05w/s, dev=2]model.layers.46.self_attn.v_proj.weight:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:02, 16.41w/s, dev=2]      model.layers.46.self_attn.v_proj.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:02, 17.61w/s, dev=2]model.layers.47.input_layernorm.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:02, 17.60w/s, dev=2] model.layers.47.mlp.down_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:02, 18.95w/s, dev=2]  model.layers.47.mlp.gate_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:02, 16.70w/s, dev=2]model.layers.47.mlp.up_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:02, 15.16w/s, dev=2]  model.layers.47.post_attention_layernorm.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:01<00:03, 13.99w/s, dev=2]model.layers.47.post_attention_layernorm.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:02, 14.81w/s, dev=2]model.layers.47.self_attn.k_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:02, 14.81w/s, dev=2]        model.layers.47.self_attn.o_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:02, 15.54w/s, dev=2]model.layers.47.self_attn.q_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:02, 15.76w/s, dev=2]model.layers.47.self_attn.rotary_emb.inv_freq:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:01<00:02, 15.94w/s, dev=2]model.layers.47.self_attn.v_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:02, 16.69w/s, dev=2]      model.layers.47.self_attn.v_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:02, 17.35w/s, dev=2]model.layers.48.input_layernorm.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:02, 17.34w/s, dev=2] model.layers.48.mlp.down_proj.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:01, 18.10w/s, dev=2]  model.layers.48.mlp.gate_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:02, 16.84w/s, dev=2]model.layers.48.mlp.up_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:02, 15.90w/s, dev=2]  model.layers.48.post_attention_layernorm.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:01<00:02, 15.05w/s, dev=2]model.layers.48.post_attention_layernorm.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:01, 15.60w/s, dev=2]model.layers.48.self_attn.k_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:01, 15.60w/s, dev=2]        model.layers.48.self_attn.o_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:01, 16.08w/s, dev=2]model.layers.48.self_attn.q_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:01<00:01, 16.20w/s, dev=2]model.layers.48.self_attn.rotary_emb.inv_freq:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:01<00:01, 16.29w/s, dev=2]model.layers.48.self_attn.v_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:01<00:01, 16.81w/s, dev=2]      model.layers.48.self_attn.v_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:01<00:01, 17.27w/s, dev=2]model.layers.49.input_layernorm.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:01<00:01, 17.27w/s, dev=2] model.layers.49.mlp.down_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:01<00:01, 17.79w/s, dev=2]  model.layers.49.mlp.gate_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:02<00:01, 16.89w/s, dev=2]model.layers.49.mlp.up_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:02<00:01, 16.16w/s, dev=2]  model.layers.49.post_attention_layernorm.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:02<00:01, 15.55w/s, dev=2]model.layers.49.post_attention_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:02<00:01, 15.97w/s, dev=2]model.layers.49.self_attn.k_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:02<00:01, 15.96w/s, dev=2]        model.layers.49.self_attn.o_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:02<00:01, 16.34w/s, dev=2]model.layers.49.self_attn.q_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:02<00:01, 16.44w/s, dev=2]model.layers.49.self_attn.rotary_emb.inv_freq:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:02<00:01, 16.53w/s, dev=2]model.layers.49.self_attn.v_proj.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:02<00:01, 16.93w/s, dev=2]      model.layers.49.self_attn.v_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:02<00:00, 17.29w/s, dev=2]model.layers.50.input_layernorm.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:02<00:00, 17.29w/s, dev=2] model.layers.50.mlp.down_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:02<00:00, 17.69w/s, dev=2]  model.layers.50.mlp.gate_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:02<00:00, 17.01w/s, dev=2]model.layers.50.mlp.up_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:02<00:00, 16.43w/s, dev=2]  model.layers.50.post_attention_layernorm.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:02<00:00, 15.91w/s, dev=2]model.layers.50.post_attention_layernorm.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:02<00:00, 16.24w/s, dev=2]model.layers.50.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:02<00:00, 16.24w/s, dev=2]        model.layers.50.self_attn.o_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:02<00:00, 16.54w/s, dev=2]model.layers.50.self_attn.q_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:03<00:00, 16.64w/s, dev=2]model.layers.50.self_attn.rotary_emb.inv_freq:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:03<00:00, 16.70w/s, dev=2]model.layers.50.self_attn.v_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:03<00:00, 17.03w/s, dev=2]      model.layers.50.self_attn.v_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:03<00:00, 17.32w/s, dev=2]model.layers.51.mlp.gate_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:03<00:00, 17.32w/s, dev=2]   model.layers.51.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:03<00:00, 16.84w/s, dev=2]model.layers.51.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:03<00:00, 17.11w/s, dev=2]model.layers.51.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:03<00:00, 17.19w/s, dev=2]model.layers.51.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:03<00:00, 17.23w/s, dev=2]model.layers.51.self_attn.rotary_emb.inv_freq:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:03<00:00, 17.53w/s, dev=2]model.layers.51.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:03<00:00, 17.53w/s, dev=2]                                                                                                      0%|          | 0/59 [00:00<?, ?w/s]model.layers.51.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=2]model.layers.51.mlp.down_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 1045.70w/s, dev=2]model.layers.51.mlp.down_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 12.31w/s, dev=2]  model.layers.51.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 12.29w/s, dev=2]  model.layers.51.post_attention_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:05,  9.46w/s, dev=2]model.layers.51.post_attention_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:04, 12.59w/s, dev=2]model.layers.52.input_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:04, 12.58w/s, dev=2]         model.layers.52.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 15.72w/s, dev=2]  model.layers.52.mlp.down_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:04, 12.89w/s, dev=2]model.layers.52.mlp.gate_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:04, 12.89w/s, dev=2]model.layers.52.mlp.up_proj.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:04, 11.33w/s, dev=2]  model.layers.52.mlp.up_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:04, 10.40w/s, dev=2]model.layers.52.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:04, 10.40w/s, dev=2]model.layers.52.self_attn.k_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:04, 11.69w/s, dev=2]        model.layers.52.self_attn.o_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:03, 12.88w/s, dev=2]model.layers.52.self_attn.q_proj.weight:  19%|â–ˆâ–Š        | 11/59 [00:00<00:03, 13.37w/s, dev=2]model.layers.52.self_attn.rotary_emb.inv_freq:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:03, 13.82w/s, dev=2]model.layers.52.self_attn.v_proj.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:03, 14.96w/s, dev=2]      model.layers.52.self_attn.v_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:02, 15.96w/s, dev=2]model.layers.53.input_layernorm.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:02, 15.95w/s, dev=2] model.layers.53.mlp.down_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:02, 17.09w/s, dev=2]  model.layers.53.mlp.gate_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:02, 15.37w/s, dev=2]model.layers.53.mlp.up_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:01<00:02, 14.22w/s, dev=2]  model.layers.53.post_attention_layernorm.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:03, 13.26w/s, dev=2]model.layers.53.self_attn.k_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:02, 14.00w/s, dev=2]        model.layers.53.self_attn.k_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:02, 14.66w/s, dev=2]model.layers.53.self_attn.o_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:02, 14.65w/s, dev=2]model.layers.53.self_attn.q_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:01<00:02, 14.87w/s, dev=2]model.layers.53.self_attn.rotary_emb.inv_freq:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:02, 15.07w/s, dev=2]model.layers.53.self_attn.v_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:02, 15.75w/s, dev=2]      model.layers.54.input_layernorm.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:02, 16.36w/s, dev=2] model.layers.54.mlp.down_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:01, 17.04w/s, dev=2]  model.layers.54.mlp.down_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:02, 15.97w/s, dev=2]model.layers.54.mlp.gate_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:02, 15.97w/s, dev=2]model.layers.54.mlp.up_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:01<00:02, 15.14w/s, dev=2]  model.layers.54.post_attention_layernorm.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:02, 14.45w/s, dev=2]model.layers.54.self_attn.k_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:02, 14.97w/s, dev=2]        model.layers.54.self_attn.o_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:01<00:01, 15.42w/s, dev=2]model.layers.54.self_attn.q_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:01<00:01, 15.58w/s, dev=2]model.layers.54.self_attn.q_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:02<00:01, 15.75w/s, dev=2]model.layers.54.self_attn.rotary_emb.inv_freq:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:02<00:01, 15.75w/s, dev=2]model.layers.54.self_attn.v_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:02<00:01, 16.24w/s, dev=2]      model.layers.55.input_layernorm.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:02<00:01, 16.68w/s, dev=2] model.layers.55.mlp.down_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:02<00:01, 17.17w/s, dev=2]  model.layers.55.mlp.gate_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:02<00:01, 16.48w/s, dev=2]model.layers.55.mlp.up_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:02<00:01, 15.80w/s, dev=2]  model.layers.55.mlp.up_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:02<00:01, 15.19w/s, dev=2]model.layers.55.post_attention_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:02<00:01, 15.18w/s, dev=2]model.layers.55.self_attn.k_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:02<00:01, 15.58w/s, dev=2]        model.layers.55.self_attn.o_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:02<00:01, 15.94w/s, dev=2]model.layers.55.self_attn.q_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:02<00:01, 16.04w/s, dev=2]model.layers.55.self_attn.rotary_emb.inv_freq:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:02<00:01, 16.16w/s, dev=2]model.layers.55.self_attn.v_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:02<00:00, 16.54w/s, dev=2]      model.layers.55.self_attn.v_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:02<00:00, 16.89w/s, dev=2]model.layers.56.input_layernorm.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:02<00:00, 16.88w/s, dev=2] model.layers.56.mlp.down_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:02<00:00, 17.27w/s, dev=2]  model.layers.56.mlp.gate_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:02<00:00, 16.66w/s, dev=2]model.layers.56.mlp.up_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:02<00:00, 16.14w/s, dev=2]  model.layers.56.post_attention_layernorm.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:03<00:00, 15.67w/s, dev=2]model.layers.56.self_attn.k_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:03<00:00, 16.00w/s, dev=2]        model.layers.56.self_attn.k_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:03<00:00, 16.29w/s, dev=2]model.layers.56.self_attn.o_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:03<00:00, 16.29w/s, dev=2]model.layers.56.self_attn.q_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:03<00:00, 16.37w/s, dev=2]model.layers.56.self_attn.rotary_emb.inv_freq:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:03<00:00, 16.44w/s, dev=2]model.layers.56.self_attn.v_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:03<00:00, 16.75w/s, dev=2]      model.layers.57.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:03<00:00, 17.03w/s, dev=2]model.layers.57.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:03<00:00, 17.30w/s, dev=2]model.layers.57.self_attn.o_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:03<00:00, 17.36w/s, dev=2]model.layers.57.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:03<00:00, 17.36w/s, dev=2]model.layers.57.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:03<00:00, 17.40w/s, dev=2]model.layers.57.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:03<00:00, 17.70w/s, dev=2]                                                                                                      0%|          | 0/55 [00:00<?, ?w/s]model.layers.57.input_layernorm.weight:   0%|          | 0/55 [00:00<?, ?w/s, dev=2]model.layers.57.mlp.down_proj.weight:   2%|â–         | 1/55 [00:00<00:00, 1042.32w/s, dev=2]model.layers.57.mlp.down_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:04, 12.67w/s, dev=2]  model.layers.57.mlp.gate_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:04, 12.65w/s, dev=2]model.layers.57.mlp.up_proj.weight:   5%|â–Œ         | 3/55 [00:00<00:05,  9.64w/s, dev=2]  model.layers.57.mlp.up_proj.weight:   7%|â–‹         | 4/55 [00:00<00:06,  8.48w/s, dev=2]model.layers.57.post_attention_layernorm.weight:   7%|â–‹         | 4/55 [00:00<00:06,  8.47w/s, dev=2]model.layers.58.input_layernorm.weight:   9%|â–‰         | 5/55 [00:00<00:04, 10.58w/s, dev=2]         model.layers.58.mlp.down_proj.weight:  11%|â–ˆ         | 6/55 [00:00<00:03, 12.69w/s, dev=2]  model.layers.58.mlp.down_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:04, 11.06w/s, dev=2]model.layers.58.mlp.gate_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:04, 11.06w/s, dev=2]model.layers.58.mlp.up_proj.weight:  15%|â–ˆâ–        | 8/55 [00:00<00:04, 10.16w/s, dev=2]  model.layers.58.post_attention_layernorm.weight:  16%|â–ˆâ–‹        | 9/55 [00:00<00:04,  9.51w/s, dev=2]model.layers.58.post_attention_layernorm.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:04, 10.56w/s, dev=2]model.layers.58.self_attn.k_proj.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:04, 10.56w/s, dev=2]        model.layers.58.self_attn.o_proj.weight:  20%|â–ˆâ–ˆ        | 11/55 [00:00<00:03, 11.54w/s, dev=2]model.layers.58.self_attn.q_proj.weight:  22%|â–ˆâ–ˆâ–       | 12/55 [00:00<00:03, 12.01w/s, dev=2]model.layers.58.self_attn.rotary_emb.inv_freq:  24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:01<00:03, 12.44w/s, dev=2]model.layers.58.self_attn.v_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 14/55 [00:01<00:03, 13.39w/s, dev=2]      model.layers.58.self_attn.v_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 15/55 [00:01<00:02, 14.26w/s, dev=2]model.layers.59.input_layernorm.weight:  27%|â–ˆâ–ˆâ–‹       | 15/55 [00:01<00:02, 14.26w/s, dev=2] model.layers.59.mlp.down_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 16/55 [00:01<00:02, 15.21w/s, dev=2]  model.layers.59.mlp.gate_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:01<00:02, 14.06w/s, dev=2]model.layers.59.mlp.up_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/55 [00:01<00:02, 13.29w/s, dev=2]  model.layers.59.post_attention_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–      | 19/55 [00:01<00:02, 12.61w/s, dev=2]model.layers.59.post_attention_layernorm.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:01<00:02, 13.27w/s, dev=2]model.layers.59.self_attn.k_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:01<00:02, 13.27w/s, dev=2]        model.layers.59.self_attn.o_proj.weight:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/55 [00:01<00:02, 13.87w/s, dev=2]model.layers.59.self_attn.q_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/55 [00:01<00:02, 14.12w/s, dev=2]model.layers.59.self_attn.rotary_emb.inv_freq:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 23/55 [00:01<00:02, 14.35w/s, dev=2]model.layers.59.self_attn.v_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 24/55 [00:01<00:02, 14.97w/s, dev=2]      model.layers.59.self_attn.v_proj.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:01<00:01, 15.53w/s, dev=2]model.layers.60.input_layernorm.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:01<00:01, 15.53w/s, dev=2] model.layers.60.mlp.down_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 26/55 [00:01<00:01, 16.15w/s, dev=2]  model.layers.60.mlp.gate_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 27/55 [00:01<00:01, 15.34w/s, dev=2]model.layers.60.mlp.up_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 28/55 [00:01<00:01, 14.66w/s, dev=2]  model.layers.60.post_attention_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/55 [00:02<00:01, 14.07w/s, dev=2]model.layers.60.post_attention_layernorm.weight:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 30/55 [00:02<00:01, 14.56w/s, dev=2]model.layers.60.self_attn.k_proj.weight:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 30/55 [00:02<00:01, 14.56w/s, dev=2]        model.layers.60.self_attn.o_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/55 [00:02<00:01, 15.00w/s, dev=2]model.layers.60.self_attn.q_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 32/55 [00:02<00:01, 15.16w/s, dev=2]model.layers.60.self_attn.rotary_emb.inv_freq:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/55 [00:02<00:01, 15.32w/s, dev=2]model.layers.60.self_attn.v_proj.weight:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/55 [00:02<00:01, 15.78w/s, dev=2]      model.layers.61.input_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:02<00:01, 16.20w/s, dev=2] model.layers.61.mlp.down_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 36/55 [00:02<00:01, 16.66w/s, dev=2]  model.layers.61.mlp.down_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 37/55 [00:02<00:01, 15.99w/s, dev=2]model.layers.61.mlp.gate_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 37/55 [00:02<00:01, 15.99w/s, dev=2]model.layers.61.mlp.up_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 38/55 [00:02<00:01, 15.36w/s, dev=2]  model.layers.61.post_attention_layernorm.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 39/55 [00:02<00:01, 14.81w/s, dev=2]model.layers.61.self_attn.k_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 40/55 [00:02<00:00, 15.19w/s, dev=2]        model.layers.61.self_attn.o_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/55 [00:02<00:00, 15.53w/s, dev=2]model.layers.61.self_attn.q_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 42/55 [00:02<00:00, 15.62w/s, dev=2]model.layers.61.self_attn.rotary_emb.inv_freq:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 43/55 [00:02<00:00, 15.73w/s, dev=2]model.layers.61.self_attn.rotary_emb.inv_freq:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 44/55 [00:02<00:00, 16.09w/s, dev=2]model.layers.61.self_attn.v_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 44/55 [00:02<00:00, 16.09w/s, dev=2]      model.layers.62.input_layernorm.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:03<00:00, 14.70w/s, dev=3] model.layers.62.mlp.down_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 46/55 [00:03<00:00, 14.73w/s, dev=3]  model.layers.62.mlp.gate_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 47/55 [00:03<00:00, 14.32w/s, dev=3]model.layers.62.mlp.up_proj.weight:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 48/55 [00:03<00:00, 13.61w/s, dev=3]  model.layers.62.post_attention_layernorm.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 49/55 [00:03<00:00, 13.04w/s, dev=3]model.layers.62.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 50/55 [00:03<00:00, 13.30w/s, dev=3]        model.layers.62.self_attn.k_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 51/55 [00:03<00:00, 13.25w/s, dev=3]model.layers.62.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 51/55 [00:03<00:00, 13.25w/s, dev=3]model.layers.62.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 52/55 [00:03<00:00, 13.32w/s, dev=3]model.layers.62.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 53/55 [00:03<00:00, 13.40w/s, dev=3]model.layers.62.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 54/55 [00:03<00:00, 13.65w/s, dev=3]                                                                                                      0%|          | 0/57 [00:00<?, ?w/s]model.layers.63.input_layernorm.weight:   0%|          | 0/57 [00:00<?, ?w/s, dev=3]model.layers.63.mlp.down_proj.weight:   2%|â–         | 1/57 [00:00<00:00, 798.00w/s, dev=3]model.layers.63.mlp.down_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:05,  9.81w/s, dev=3] model.layers.63.mlp.gate_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:05,  9.80w/s, dev=3]model.layers.63.mlp.up_proj.weight:   5%|â–Œ         | 3/57 [00:00<00:09,  5.69w/s, dev=3]  model.layers.63.mlp.up_proj.weight:   7%|â–‹         | 4/57 [00:00<00:11,  4.55w/s, dev=3]model.layers.63.post_attention_layernorm.weight:   7%|â–‹         | 4/57 [00:00<00:11,  4.55w/s, dev=3]model.layers.63.self_attn.k_proj.weight:   9%|â–‰         | 5/57 [00:00<00:09,  5.68w/s, dev=3]        model.layers.63.self_attn.o_proj.weight:  11%|â–ˆ         | 6/57 [00:00<00:07,  6.76w/s, dev=3]model.layers.63.self_attn.q_proj.weight:  12%|â–ˆâ–        | 7/57 [00:00<00:06,  7.47w/s, dev=3]model.layers.63.self_attn.q_proj.weight:  14%|â–ˆâ–        | 8/57 [00:00<00:06,  8.11w/s, dev=3]model.layers.63.self_attn.rotary_emb.inv_freq:  14%|â–ˆâ–        | 8/57 [00:00<00:06,  8.11w/s, dev=3]model.layers.63.self_attn.v_proj.weight:  16%|â–ˆâ–Œ        | 9/57 [00:00<00:05,  9.12w/s, dev=3]      model.layers.64.input_layernorm.weight:  18%|â–ˆâ–Š        | 10/57 [00:00<00:04, 10.07w/s, dev=3]model.layers.64.mlp.down_proj.weight:  19%|â–ˆâ–‰        | 11/57 [00:00<00:04, 11.07w/s, dev=3]  model.layers.64.mlp.down_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:01<00:04, 10.43w/s, dev=3]model.layers.64.mlp.gate_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:01<00:04, 10.43w/s, dev=3]model.layers.64.mlp.up_proj.weight:  23%|â–ˆâ–ˆâ–Ž       | 13/57 [00:01<00:04,  9.92w/s, dev=3]  model.layers.64.post_attention_layernorm.weight:  25%|â–ˆâ–ˆâ–       | 14/57 [00:01<00:04,  9.05w/s, dev=3]model.layers.64.self_attn.k_proj.weight:  26%|â–ˆâ–ˆâ–‹       | 15/57 [00:01<00:04,  9.69w/s, dev=3]        model.layers.64.self_attn.k_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:01<00:03, 10.28w/s, dev=3]model.layers.64.self_attn.o_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:01<00:03, 10.28w/s, dev=3]model.layers.64.self_attn.q_proj.weight:  30%|â–ˆâ–ˆâ–‰       | 17/57 [00:01<00:03, 10.60w/s, dev=3]model.layers.64.self_attn.rotary_emb.inv_freq:  32%|â–ˆâ–ˆâ–ˆâ–      | 18/57 [00:01<00:03, 10.90w/s, dev=3]model.layers.64.self_attn.v_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 19/57 [00:01<00:03, 11.50w/s, dev=3]      model.layers.64.self_attn.v_proj.weight:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 20/57 [00:01<00:03, 12.06w/s, dev=3]model.layers.65.input_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 20/57 [00:01<00:03, 12.06w/s, dev=3] model.layers.65.mlp.down_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 21/57 [00:01<00:02, 12.66w/s, dev=3]  model.layers.65.mlp.gate_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:01<00:03, 11.05w/s, dev=3]model.layers.65.mlp.up_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 23/57 [00:02<00:03, 10.70w/s, dev=3]  model.layers.65.mlp.up_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/57 [00:02<00:03, 10.19w/s, dev=3]model.layers.65.post_attention_layernorm.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/57 [00:02<00:03, 10.19w/s, dev=3]model.layers.65.self_attn.k_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/57 [00:02<00:03, 10.61w/s, dev=3]        model.layers.65.self_attn.o_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 26/57 [00:02<00:02, 10.99w/s, dev=3]model.layers.65.self_attn.q_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 27/57 [00:02<00:02, 11.20w/s, dev=3]model.layers.65.self_attn.q_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:02<00:02, 11.41w/s, dev=3]model.layers.65.self_attn.rotary_emb.inv_freq:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:02<00:02, 11.40w/s, dev=3]model.layers.65.self_attn.v_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 29/57 [00:02<00:02, 11.81w/s, dev=3]      model.layers.66.input_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 30/57 [00:02<00:02, 12.10w/s, dev=3] model.layers.66.mlp.down_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 31/57 [00:02<00:02, 12.50w/s, dev=3]  model.layers.66.mlp.down_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 32/57 [00:02<00:02, 11.39w/s, dev=3]model.layers.66.mlp.gate_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 32/57 [00:02<00:02, 11.39w/s, dev=3]model.layers.66.mlp.up_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 33/57 [00:02<00:02, 11.12w/s, dev=3]  model.layers.66.post_attention_layernorm.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:03<00:02, 10.29w/s, dev=3]model.layers.66.self_attn.k_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/57 [00:03<00:02, 10.59w/s, dev=3]        model.layers.66.self_attn.k_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 36/57 [00:03<00:01, 10.87w/s, dev=3]model.layers.66.self_attn.o_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 36/57 [00:03<00:01, 10.87w/s, dev=3]model.layers.66.self_attn.q_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 37/57 [00:03<00:01, 10.99w/s, dev=3]model.layers.66.self_attn.rotary_emb.inv_freq:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 38/57 [00:03<00:01, 11.14w/s, dev=3]model.layers.66.self_attn.v_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 39/57 [00:03<00:01, 11.43w/s, dev=3]      model.layers.66.self_attn.v_proj.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:03<00:01, 11.69w/s, dev=3]model.layers.67.input_layernorm.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:03<00:01, 11.69w/s, dev=3] model.layers.67.mlp.down_proj.weight:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/57 [00:03<00:01, 11.98w/s, dev=3]  model.layers.67.mlp.gate_proj.weight:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 42/57 [00:03<00:01, 11.71w/s, dev=3]model.layers.67.mlp.up_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 43/57 [00:03<00:01, 11.24w/s, dev=3]  model.layers.67.mlp.up_proj.weight:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 44/57 [00:03<00:01, 11.05w/s, dev=3]model.layers.67.post_attention_layernorm.weight:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 44/57 [00:03<00:01, 11.05w/s, dev=3]model.layers.67.self_attn.k_proj.weight:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 45/57 [00:03<00:01, 11.30w/s, dev=3]        model.layers.67.self_attn.o_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:04<00:01, 10.89w/s, dev=3]model.layers.67.self_attn.q_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 47/57 [00:04<00:00, 11.00w/s, dev=3]model.layers.67.self_attn.q_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/57 [00:04<00:00, 10.68w/s, dev=3]model.layers.67.self_attn.rotary_emb.inv_freq:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/57 [00:04<00:00, 10.68w/s, dev=3]model.layers.67.self_attn.v_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 49/57 [00:04<00:00, 10.90w/s, dev=3]      model.layers.68.mlp.down_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 50/57 [00:04<00:00, 11.10w/s, dev=3]   model.layers.68.mlp.gate_proj.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 51/57 [00:04<00:00, 10.93w/s, dev=3]model.layers.68.mlp.gate_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:04<00:00, 10.79w/s, dev=3]model.layers.68.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:04<00:00, 10.79w/s, dev=3]model.layers.68.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 53/57 [00:04<00:00, 10.66w/s, dev=3]model.layers.68.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/57 [00:05<00:00, 10.76w/s, dev=3]model.layers.68.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 55/57 [00:05<00:00, 10.85w/s, dev=3]model.layers.68.self_attn.rotary_emb.inv_freq:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 56/57 [00:05<00:00, 11.04w/s, dev=3]model.layers.68.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 56/57 [00:05<00:00, 11.04w/s, dev=3]                                                                                                      0%|          | 0/59 [00:00<?, ?w/s]model.layers.68.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=3]model.layers.68.mlp.up_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 980.89w/s, dev=3]model.layers.68.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 13.86w/s, dev=3] model.layers.68.post_attention_layernorm.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 13.84w/s, dev=3]model.layers.69.input_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:02, 20.71w/s, dev=3]         model.layers.69.mlp.down_proj.weight:   7%|â–‹         | 4/59 [00:00<00:01, 27.56w/s, dev=3]  model.layers.69.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:04, 12.29w/s, dev=3]model.layers.69.mlp.gate_proj.weight:   8%|â–Š         | 5/59 [00:00<00:04, 12.29w/s, dev=3]model.layers.69.mlp.up_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:04, 10.67w/s, dev=3]  model.layers.69.post_attention_layernorm.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:05,  9.67w/s, dev=3]model.layers.69.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:04, 11.04w/s, dev=3]model.layers.69.self_attn.k_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:04, 11.04w/s, dev=3]        model.layers.69.self_attn.o_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:04, 10.52w/s, dev=3]model.layers.69.self_attn.q_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:04, 11.04w/s, dev=3]model.layers.69.self_attn.q_proj.weight:  19%|â–ˆâ–Š        | 11/59 [00:00<00:04, 11.50w/s, dev=3]model.layers.69.self_attn.rotary_emb.inv_freq:  19%|â–ˆâ–Š        | 11/59 [00:00<00:04, 11.50w/s, dev=3]model.layers.69.self_attn.v_proj.weight:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:03, 12.54w/s, dev=3]      model.layers.70.input_layernorm.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:03, 13.47w/s, dev=3] model.layers.70.mlp.down_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:03, 14.50w/s, dev=3]  model.layers.70.mlp.down_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:01<00:03, 13.32w/s, dev=3]model.layers.70.mlp.gate_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:01<00:03, 13.32w/s, dev=3]model.layers.70.mlp.up_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:03, 12.59w/s, dev=3]  model.layers.70.post_attention_layernorm.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:01<00:03, 11.00w/s, dev=3]model.layers.70.self_attn.k_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:03, 11.64w/s, dev=3]        model.layers.70.self_attn.k_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:03, 12.19w/s, dev=3]model.layers.70.self_attn.o_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:03, 12.19w/s, dev=3]model.layers.70.self_attn.q_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:03, 12.40w/s, dev=3]model.layers.70.self_attn.rotary_emb.inv_freq:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:01<00:03, 12.65w/s, dev=3]model.layers.70.self_attn.v_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:02, 13.25w/s, dev=3]      model.layers.70.self_attn.v_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:02, 13.72w/s, dev=3]model.layers.71.input_layernorm.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:02, 13.72w/s, dev=3] model.layers.71.mlp.down_proj.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:02, 14.31w/s, dev=3]  model.layers.71.mlp.gate_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:02, 13.74w/s, dev=3]model.layers.71.mlp.up_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:02<00:02, 12.21w/s, dev=3]  model.layers.71.mlp.up_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:02<00:02, 11.88w/s, dev=3]model.layers.71.post_attention_layernorm.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:02<00:02, 11.88w/s, dev=3]model.layers.71.self_attn.k_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:02<00:02, 12.32w/s, dev=3]        model.layers.71.self_attn.o_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:02<00:02, 12.70w/s, dev=3]model.layers.71.self_attn.q_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:02<00:02, 12.90w/s, dev=3]model.layers.71.self_attn.rotary_emb.inv_freq:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:02<00:02, 13.08w/s, dev=3]model.layers.71.self_attn.v_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:02<00:02, 13.49w/s, dev=3]      model.layers.71.self_attn.v_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:02<00:02, 12.71w/s, dev=3]model.layers.72.input_layernorm.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:02<00:02, 12.71w/s, dev=3] model.layers.72.mlp.down_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:02<00:01, 13.09w/s, dev=3]  model.layers.72.mlp.gate_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:02<00:01, 12.76w/s, dev=3]model.layers.72.mlp.up_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:02<00:01, 12.47w/s, dev=3]  model.layers.72.post_attention_layernorm.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:03<00:01, 12.22w/s, dev=3]model.layers.72.self_attn.k_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:03<00:01, 12.55w/s, dev=3]        model.layers.72.self_attn.k_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:03<00:01, 12.20w/s, dev=3]model.layers.72.self_attn.o_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:03<00:01, 12.20w/s, dev=3]model.layers.72.self_attn.q_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:03<00:01, 12.35w/s, dev=3]model.layers.72.self_attn.rotary_emb.inv_freq:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:03<00:01, 12.47w/s, dev=3]model.layers.72.self_attn.v_proj.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:03<00:01, 12.78w/s, dev=3]      model.layers.73.input_layernorm.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:03<00:01, 13.03w/s, dev=3] model.layers.73.mlp.down_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:03<00:01, 13.33w/s, dev=3]  model.layers.73.mlp.down_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:03<00:01, 13.06w/s, dev=3]model.layers.73.mlp.gate_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:03<00:01, 13.05w/s, dev=3]model.layers.73.mlp.up_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:03<00:01, 12.80w/s, dev=3]  model.layers.73.post_attention_layernorm.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:03<00:00, 12.58w/s, dev=3]model.layers.73.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:03<00:00, 12.85w/s, dev=3]        model.layers.73.self_attn.o_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:03<00:00, 13.09w/s, dev=3]model.layers.73.self_attn.q_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:03<00:00, 13.21w/s, dev=3]model.layers.73.self_attn.q_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:03<00:00, 13.33w/s, dev=3]model.layers.73.self_attn.rotary_emb.inv_freq:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:03<00:00, 13.33w/s, dev=3]model.layers.73.self_attn.v_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:03<00:00, 13.59w/s, dev=3]      model.layers.74.mlp.gate_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:03<00:00, 13.83w/s, dev=3]   model.layers.74.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:03<00:00, 13.58w/s, dev=3]model.layers.74.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:03<00:00, 13.81w/s, dev=3]model.layers.74.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:04<00:00, 13.92w/s, dev=3]model.layers.74.self_attn.q_proj.weight:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:04<00:00, 14.02w/s, dev=3]model.layers.74.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:04<00:00, 14.02w/s, dev=3]model.layers.74.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:04<00:00, 14.26w/s, dev=3]                                                                                                      0%|          | 0/55 [00:00<?, ?w/s]model.layers.74.input_layernorm.weight:   0%|          | 0/55 [00:00<?, ?w/s, dev=3]model.layers.74.mlp.down_proj.weight:   2%|â–         | 1/55 [00:00<00:00, 916.79w/s, dev=3]model.layers.74.mlp.down_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:04, 12.30w/s, dev=3] model.layers.74.mlp.up_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:04, 12.28w/s, dev=3]  model.layers.74.post_attention_layernorm.weight:   5%|â–Œ         | 3/55 [00:00<00:05,  9.53w/s, dev=3]model.layers.74.post_attention_layernorm.weight:   7%|â–‹         | 4/55 [00:00<00:04, 12.69w/s, dev=3]model.layers.75.input_layernorm.weight:   7%|â–‹         | 4/55 [00:00<00:04, 12.68w/s, dev=3]         model.layers.75.mlp.down_proj.weight:   9%|â–‰         | 5/55 [00:00<00:03, 15.84w/s, dev=3]  model.layers.75.mlp.down_proj.weight:  11%|â–ˆ         | 6/55 [00:00<00:03, 12.96w/s, dev=3]model.layers.75.mlp.gate_proj.weight:  11%|â–ˆ         | 6/55 [00:00<00:03, 12.95w/s, dev=3]model.layers.75.mlp.up_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:04, 11.44w/s, dev=3]  model.layers.75.mlp.up_proj.weight:  15%|â–ˆâ–        | 8/55 [00:00<00:04, 10.38w/s, dev=3]model.layers.75.post_attention_layernorm.weight:  15%|â–ˆâ–        | 8/55 [00:00<00:04, 10.37w/s, dev=3]model.layers.75.self_attn.k_proj.weight:  16%|â–ˆâ–‹        | 9/55 [00:00<00:03, 11.66w/s, dev=3]        model.layers.75.self_attn.o_proj.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:03, 12.84w/s, dev=3]model.layers.75.self_attn.q_proj.weight:  20%|â–ˆâ–ˆ        | 11/55 [00:00<00:03, 13.34w/s, dev=3]model.layers.75.self_attn.rotary_emb.inv_freq:  22%|â–ˆâ–ˆâ–       | 12/55 [00:00<00:03, 13.83w/s, dev=3]model.layers.75.self_attn.v_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:00<00:02, 14.97w/s, dev=3]      model.layers.75.self_attn.v_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 14/55 [00:00<00:02, 16.01w/s, dev=3]model.layers.76.input_layernorm.weight:  25%|â–ˆâ–ˆâ–Œ       | 14/55 [00:00<00:02, 16.00w/s, dev=3] model.layers.76.mlp.down_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 15/55 [00:00<00:02, 17.14w/s, dev=3]  model.layers.76.mlp.gate_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 16/55 [00:01<00:02, 15.50w/s, dev=3]model.layers.76.mlp.up_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:01<00:02, 14.30w/s, dev=3]  model.layers.76.post_attention_layernorm.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/55 [00:01<00:02, 13.43w/s, dev=3]model.layers.76.self_attn.k_proj.weight:  35%|â–ˆâ–ˆâ–ˆâ–      | 19/55 [00:01<00:02, 14.17w/s, dev=3]        model.layers.76.self_attn.k_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:01<00:02, 14.85w/s, dev=3]model.layers.76.self_attn.o_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:01<00:02, 14.85w/s, dev=3]model.layers.76.self_attn.q_proj.weight:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/55 [00:01<00:02, 15.12w/s, dev=3]model.layers.76.self_attn.rotary_emb.inv_freq:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/55 [00:01<00:02, 15.37w/s, dev=3]model.layers.76.self_attn.v_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 23/55 [00:01<00:01, 16.07w/s, dev=3]      model.layers.77.input_layernorm.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 24/55 [00:01<00:01, 16.69w/s, dev=3] model.layers.77.mlp.down_proj.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:01<00:01, 17.39w/s, dev=3]  model.layers.77.mlp.down_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 26/55 [00:01<00:01, 16.42w/s, dev=3]model.layers.77.mlp.gate_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 26/55 [00:01<00:01, 16.42w/s, dev=3]model.layers.77.mlp.up_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 27/55 [00:01<00:01, 15.36w/s, dev=3]  model.layers.77.post_attention_layernorm.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 28/55 [00:01<00:01, 14.57w/s, dev=3]model.layers.77.self_attn.k_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/55 [00:01<00:01, 15.09w/s, dev=3]        model.layers.77.self_attn.o_proj.weight:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 30/55 [00:01<00:01, 15.56w/s, dev=3]model.layers.77.self_attn.q_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/55 [00:01<00:01, 15.72w/s, dev=3]model.layers.77.self_attn.q_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 32/55 [00:02<00:01, 15.88w/s, dev=3]model.layers.77.self_attn.rotary_emb.inv_freq:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 32/55 [00:02<00:01, 15.88w/s, dev=3]model.layers.77.self_attn.v_proj.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/55 [00:02<00:01, 16.38w/s, dev=3]      model.layers.78.input_layernorm.weight:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/55 [00:02<00:01, 16.82w/s, dev=3] model.layers.78.mlp.down_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:02<00:01, 17.31w/s, dev=3]  model.layers.78.mlp.gate_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 36/55 [00:02<00:01, 16.47w/s, dev=3]model.layers.78.mlp.up_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 37/55 [00:02<00:01, 15.86w/s, dev=3]  model.layers.78.mlp.up_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 38/55 [00:02<00:01, 15.33w/s, dev=3]model.layers.78.post_attention_layernorm.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 38/55 [00:02<00:01, 15.33w/s, dev=3]model.layers.78.self_attn.k_proj.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 39/55 [00:02<00:01, 15.73w/s, dev=3]        model.layers.78.self_attn.o_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 40/55 [00:02<00:00, 16.09w/s, dev=3]model.layers.78.self_attn.q_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/55 [00:02<00:00, 16.19w/s, dev=3]model.layers.78.self_attn.rotary_emb.inv_freq:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 42/55 [00:02<00:00, 16.29w/s, dev=3]model.layers.78.self_attn.v_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 43/55 [00:02<00:00, 16.68w/s, dev=3]      model.layers.78.self_attn.v_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 44/55 [00:02<00:00, 17.02w/s, dev=3]model.layers.79.input_layernorm.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 44/55 [00:02<00:00, 17.02w/s, dev=3] model.layers.79.mlp.down_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:02<00:00, 17.40w/s, dev=3]  model.layers.79.mlp.gate_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 46/55 [00:02<00:00, 16.75w/s, dev=3]model.layers.79.mlp.up_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 47/55 [00:02<00:00, 16.24w/s, dev=3]  model.layers.79.post_attention_layernorm.weight:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 48/55 [00:03<00:00, 15.80w/s, dev=3]model.layers.79.self_attn.k_proj.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 49/55 [00:03<00:00, 16.12w/s, dev=3]        model.layers.79.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 50/55 [00:03<00:00, 16.42w/s, dev=3]model.layers.79.self_attn.o_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 50/55 [00:03<00:00, 16.42w/s, dev=3]model.layers.79.self_attn.q_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 51/55 [00:03<00:00, 16.52w/s, dev=3]model.layers.79.self_attn.rotary_emb.inv_freq:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 52/55 [00:03<00:00, 16.59w/s, dev=3]model.layers.79.self_attn.v_proj.weight:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 53/55 [00:03<00:00, 16.90w/s, dev=3]      model.norm.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 54/55 [00:03<00:00, 17.19w/s, dev=3]                                                                                                0%|          | 0/1 [00:00<?, ?w/s]lm_head.weight:   0%|          | 0/1 [00:00<?, ?w/s, dev=3]lm_head.weight: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  6.21w/s, dev=3]                                                                   self.device: cuda:0
Loading BASE model stabilityai/stablelm-base-alpha-3b-v2...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:01<00:01,  1.50s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.24it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.10it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[2] MOVING BASE MODEL TO GPU.../home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for iamgroot42/mimir contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/iamgroot42/mimir
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
Loading dataset the_pile...
Loading from HuggingFace!
Downloading data:   0%|          | 0.00/1.21M [00:00<?, ?B/s]Downloading data:  27%|â–ˆâ–ˆâ–‹       | 329k/1.21M [00:00<00:00, 3.28MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.21M/1.21M [00:00<00:00, 7.70MB/s]
Downloading data:   0%|          | 0.00/1.24M [00:00<?, ?B/s]Downloading data:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 992k/1.24M [00:00<00:00, 9.90MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.24M/1.24M [00:00<00:00, 11.8MB/s]
Downloading data:   0%|          | 0.00/31.5M [00:00<?, ?B/s]Downloading data:  25%|â–ˆâ–ˆâ–       | 7.84M/31.5M [00:00<00:00, 78.4MB/s]Downloading data:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 18.0M/31.5M [00:00<00:00, 91.9MB/s]Downloading data:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 27.7M/31.5M [00:00<00:00, 94.2MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31.5M/31.5M [00:00<00:00, 93.2MB/s]
Downloading data:   0%|          | 0.00/32.3M [00:00<?, ?B/s]Downloading data:   4%|â–         | 1.36M/32.3M [00:00<00:02, 13.6MB/s]Downloading data:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 10.8M/32.3M [00:00<00:00, 61.4MB/s]Downloading data:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 21.1M/32.3M [00:00<00:00, 80.0MB/s]Downloading data:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 31.3M/32.3M [00:00<00:00, 89.0MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32.3M/32.3M [00:00<00:00, 78.8MB/s]
Downloading data:   0%|          | 0.00/1.21M [00:00<?, ?B/s]Downloading data:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 558k/1.21M [00:00<00:00, 5.46MB/s]Downloading data:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1.13M/1.21M [00:00<00:00, 5.34MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.21M/1.21M [00:00<00:00, 5.59MB/s]
Downloading data:   0%|          | 0.00/1.21M [00:00<?, ?B/s]Downloading data:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 624k/1.21M [00:00<00:00, 6.22MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.21M/1.21M [00:00<00:00, 9.41MB/s]
Downloading data:   0%|          | 0.00/31.5M [00:00<?, ?B/s]Downloading data:  20%|â–ˆâ–‰        | 6.17M/31.5M [00:00<00:00, 61.7MB/s]Downloading data:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 15.1M/31.5M [00:00<00:00, 78.1MB/s]Downloading data:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 25.2M/31.5M [00:00<00:00, 88.3MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31.5M/31.5M [00:00<00:00, 86.5MB/s]
Downloading data:   0%|          | 0.00/31.7M [00:00<?, ?B/s]Downloading data:  26%|â–ˆâ–ˆâ–‹       | 8.38M/31.7M [00:00<00:00, 83.8MB/s]Downloading data:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 18.1M/31.7M [00:00<00:00, 91.6MB/s]Downloading data:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 28.3M/31.7M [00:00<00:00, 96.2MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31.7M/31.7M [00:00<00:00, 94.9MB/s]
Downloading data:   0%|          | 0.00/1.21M [00:00<?, ?B/s]Downloading data:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 738k/1.21M [00:00<00:00, 7.37MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.21M/1.21M [00:00<00:00, 9.60MB/s]
Downloading data:   0%|          | 0.00/1.20M [00:00<?, ?B/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.20M/1.20M [00:00<00:00, 19.2MB/s]
Downloading data:   0%|          | 0.00/31.5M [00:00<?, ?B/s]Downloading data:   5%|â–         | 1.54M/31.5M [00:00<00:01, 15.4MB/s]Downloading data:  29%|â–ˆâ–ˆâ–‰       | 9.19M/31.5M [00:00<00:00, 51.3MB/s]Downloading data:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 15.3M/31.5M [00:00<00:00, 54.5MB/s]Downloading data:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 23.0M/31.5M [00:00<00:00, 63.2MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31.5M/31.5M [00:00<00:00, 62.9MB/s]
Downloading data:   0%|          | 0.00/31.3M [00:00<?, ?B/s]Downloading data:  24%|â–ˆâ–ˆâ–       | 7.61M/31.3M [00:00<00:00, 76.1MB/s]Downloading data:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 16.9M/31.3M [00:00<00:00, 86.2MB/s]Downloading data:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26.2M/31.3M [00:00<00:00, 89.3MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31.3M/31.3M [00:00<00:00, 89.2MB/s]
Generating ngram_7_0.2 split: 0 examples [00:00, ? examples/s]Generating ngram_7_0.2 split: 72 examples [00:00, 710.54 examples/s]Generating ngram_7_0.2 split: 147 examples [00:00, 728.45 examples/s]Generating ngram_7_0.2 split: 223 examples [00:00, 739.62 examples/s]Generating ngram_7_0.2 split: 304 examples [00:00, 765.47 examples/s]Generating ngram_7_0.2 split: 384 examples [00:00, 774.11 examples/s]Generating ngram_7_0.2 split: 462 examples [00:00, 772.64 examples/s]Generating ngram_7_0.2 split: 544 examples [00:00, 784.16 examples/s]Generating ngram_7_0.2 split: 627 examples [00:00, 797.04 examples/s]Generating ngram_7_0.2 split: 710 examples [00:00, 803.58 examples/s]Generating ngram_7_0.2 split: 829 examples [00:01, 796.00 examples/s]Generating ngram_7_0.2 split: 946 examples [00:01, 788.46 examples/s]Generating ngram_7_0.2 split: 1000 examples [00:01, 627.87 examples/s]
Generating ngram_13_0.2 split: 0 examples [00:00, ? examples/s]Generating ngram_13_0.2 split: 81 examples [00:00, 798.53 examples/s]Generating ngram_13_0.2 split: 164 examples [00:00, 814.02 examples/s]Generating ngram_13_0.2 split: 286 examples [00:00, 811.34 examples/s]Generating ngram_13_0.2 split: 369 examples [00:00, 816.11 examples/s]Generating ngram_13_0.2 split: 451 examples [00:00, 812.37 examples/s]Generating ngram_13_0.2 split: 571 examples [00:00, 802.50 examples/s]Generating ngram_13_0.2 split: 653 examples [00:00, 804.58 examples/s]Generating ngram_13_0.2 split: 770 examples [00:00, 792.05 examples/s]Generating ngram_13_0.2 split: 890 examples [00:01, 791.70 examples/s]Generating ngram_13_0.2 split: 974 examples [00:01, 800.07 examples/s]Generating ngram_13_0.2 split: 1000 examples [00:01, 644.36 examples/s]
Generating ngram_13_0.8 split: 0 examples [00:00, ? examples/s]Generating ngram_13_0.8 split: 79 examples [00:00, 777.24 examples/s]Generating ngram_13_0.8 split: 159 examples [00:00, 783.79 examples/s]Generating ngram_13_0.8 split: 240 examples [00:00, 794.24 examples/s]Generating ngram_13_0.8 split: 322 examples [00:00, 803.38 examples/s]Generating ngram_13_0.8 split: 407 examples [00:00, 817.04 examples/s]Generating ngram_13_0.8 split: 490 examples [00:00, 817.80 examples/s]Generating ngram_13_0.8 split: 574 examples [00:00, 823.38 examples/s]Generating ngram_13_0.8 split: 657 examples [00:00, 824.01 examples/s]Generating ngram_13_0.8 split: 778 examples [00:00, 813.38 examples/s]Generating ngram_13_0.8 split: 863 examples [00:01, 819.72 examples/s]Generating ngram_13_0.8 split: 986 examples [00:01, 818.59 examples/s]Generating ngram_13_0.8 split: 1000 examples [00:01, 641.87 examples/s]
Loading dataset the_pile...
Loading from HuggingFace!
Generating samples:   0%|          | 0/21 [00:00<?, ?it/s]Generating samples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:00<00:00, 884.79it/s]
NEW N_SAMPLES IS  1000
Writing raw data to tmp_results/neo125_github_experiment/meta-llama_Llama-2-70b-hf/wikipedia_(en)_ngram_13_<0.8_truncated/raw_data.json
Writing raw data to tmp_results/neo125_github_experiment/meta-llama_Llama-2-70b-hf/wikipedia_(en)_ngram_13_<0.8_truncated/raw_data_lens.json
Computing criterion:   0%|          | 0/20 [00:00<?, ?it/s]Computing criterion:   5%|â–Œ         | 1/20 [00:21<06:51, 21.67s/it]Computing criterion:  10%|â–ˆ         | 2/20 [00:37<05:31, 18.43s/it]Computing criterion:  15%|â–ˆâ–Œ        | 3/20 [00:55<05:03, 17.88s/it]Computing criterion:  20%|â–ˆâ–ˆ        | 4/20 [01:11<04:37, 17.36s/it]Computing criterion:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [01:28<04:15, 17.04s/it]Computing criterion:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [01:44<03:57, 16.98s/it]Computing criterion:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [02:02<03:41, 17.01s/it]Computing criterion:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [02:18<03:21, 16.77s/it]Computing criterion:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [02:34<03:03, 16.68s/it]Computing criterion:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [02:51<02:47, 16.75s/it]Computing criterion:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [03:08<02:31, 16.78s/it]Computing criterion:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [03:25<02:13, 16.74s/it]Computing criterion:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [03:41<01:56, 16.64s/it]Computing criterion:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [03:58<01:40, 16.78s/it]Computing criterion:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [04:15<01:24, 16.80s/it]Computing criterion:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [04:32<01:07, 16.94s/it]Computing criterion:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [04:49<00:50, 16.81s/it]Computing criterion:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [05:05<00:33, 16.63s/it]Computing criterion:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [05:22<00:16, 16.84s/it]Computing criterion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [05:39<00:00, 16.89s/it]Computing criterion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [05:39<00:00, 16.99s/it]
Ref scores:   0%|          | 0/1000 [00:00<?, ?it/s]Ref scores:   0%|          | 1/1000 [00:10<2:51:38, 10.31s/it]Ref scores:   0%|          | 2/1000 [00:10<1:12:11,  4.34s/it]Ref scores:   0%|          | 3/1000 [00:10<40:19,  2.43s/it]  Ref scores:   0%|          | 4/1000 [00:10<25:22,  1.53s/it]Ref scores:   0%|          | 5/1000 [00:10<16:52,  1.02s/it]Ref scores:   1%|          | 6/1000 [00:11<11:52,  1.40it/s]Ref scores:   1%|          | 7/1000 [00:11<08:59,  1.84it/s]Ref scores:   1%|          | 8/1000 [00:11<06:41,  2.47it/s]Ref scores:   1%|          | 9/1000 [00:11<05:07,  3.22it/s]Ref scores:   1%|          | 11/1000 [00:11<03:27,  4.76it/s]Ref scores:   1%|          | 12/1000 [00:11<03:13,  5.11it/s]Ref scores:   1%|â–         | 13/1000 [00:11<02:57,  5.57it/s]Ref scores:   1%|â–         | 14/1000 [00:12<02:44,  5.99it/s]Ref scores:   2%|â–         | 15/1000 [00:12<02:35,  6.34it/s]Ref scores:   2%|â–         | 16/1000 [00:12<02:19,  7.08it/s]Ref scores:   2%|â–         | 17/1000 [00:12<02:17,  7.15it/s]Ref scores:   2%|â–         | 18/1000 [00:12<02:14,  7.28it/s]Ref scores:   2%|â–         | 19/1000 [00:12<02:14,  7.27it/s]Ref scores:   2%|â–         | 20/1000 [00:12<02:19,  7.02it/s]Ref scores:   2%|â–         | 21/1000 [00:12<02:08,  7.64it/s]Ref scores:   2%|â–         | 22/1000 [00:13<02:08,  7.62it/s]Ref scores:   2%|â–         | 23/1000 [00:13<02:08,  7.59it/s]Ref scores:   2%|â–         | 24/1000 [00:13<02:09,  7.53it/s]Ref scores:   2%|â–Ž         | 25/1000 [00:13<02:09,  7.51it/s]Ref scores:   3%|â–Ž         | 26/1000 [00:13<02:03,  7.91it/s]Ref scores:   3%|â–Ž         | 27/1000 [00:13<02:05,  7.78it/s]Ref scores:   3%|â–Ž         | 28/1000 [00:13<02:06,  7.69it/s]Ref scores:   3%|â–Ž         | 29/1000 [00:13<02:12,  7.32it/s]Ref scores:   3%|â–Ž         | 30/1000 [00:14<02:11,  7.38it/s]Ref scores:   3%|â–Ž         | 31/1000 [00:14<02:11,  7.38it/s]Ref scores:   3%|â–Ž         | 32/1000 [00:14<02:02,  7.92it/s]Ref scores:   3%|â–Ž         | 33/1000 [00:14<02:04,  7.75it/s]Ref scores:   3%|â–Ž         | 34/1000 [00:14<01:59,  8.10it/s]Ref scores:   4%|â–Ž         | 36/1000 [00:14<01:52,  8.61it/s]Ref scores:   4%|â–Ž         | 37/1000 [00:14<01:56,  8.30it/s]Ref scores:   4%|â–         | 38/1000 [00:15<01:58,  8.12it/s]Ref scores:   4%|â–         | 39/1000 [00:15<01:54,  8.40it/s]Ref scores:   4%|â–         | 40/1000 [00:15<01:58,  8.13it/s]Ref scores:   4%|â–         | 41/1000 [00:15<02:00,  7.93it/s]Ref scores:   4%|â–         | 42/1000 [00:15<01:54,  8.38it/s]Ref scores:   4%|â–         | 43/1000 [00:15<01:58,  8.06it/s]Ref scores:   4%|â–         | 44/1000 [00:15<02:01,  7.89it/s]Ref scores:   4%|â–         | 45/1000 [00:15<02:07,  7.50it/s]Ref scores:   5%|â–         | 46/1000 [00:16<02:07,  7.47it/s]Ref scores:   5%|â–         | 47/1000 [00:16<02:06,  7.54it/s]Ref scores:   5%|â–         | 48/1000 [00:16<01:59,  7.98it/s]Ref scores:   5%|â–         | 49/1000 [00:16<02:00,  7.88it/s]Ref scores:   5%|â–Œ         | 50/1000 [00:16<02:02,  7.77it/s]Ref scores:   5%|â–Œ         | 51/1000 [00:16<02:02,  7.73it/s]Ref scores:   5%|â–Œ         | 53/1000 [00:16<01:56,  8.15it/s]Ref scores:   6%|â–Œ         | 55/1000 [00:17<01:53,  8.36it/s]Ref scores:   6%|â–Œ         | 56/1000 [00:17<02:00,  7.82it/s]Ref scores:   6%|â–Œ         | 57/1000 [00:17<01:56,  8.11it/s]Ref scores:   6%|â–Œ         | 58/1000 [00:17<01:57,  8.00it/s]Ref scores:   6%|â–Œ         | 59/1000 [00:17<01:58,  7.91it/s]Ref scores:   6%|â–Œ         | 60/1000 [00:17<02:00,  7.80it/s]Ref scores:   6%|â–Œ         | 61/1000 [00:17<01:56,  8.07it/s]Ref scores:   6%|â–Œ         | 62/1000 [00:18<01:51,  8.38it/s]Ref scores:   6%|â–‹         | 63/1000 [00:18<01:54,  8.18it/s]Ref scores:   6%|â–‹         | 64/1000 [00:18<01:57,  7.94it/s]Ref scores:   6%|â–‹         | 65/1000 [00:18<01:58,  7.87it/s]Ref scores:   7%|â–‹         | 66/1000 [00:18<01:53,  8.26it/s]Ref scores:   7%|â–‹         | 68/1000 [00:18<01:46,  8.76it/s]Ref scores:   7%|â–‹         | 69/1000 [00:18<01:44,  8.89it/s]Ref scores:   7%|â–‹         | 70/1000 [00:19<01:43,  8.98it/s]Ref scores:   7%|â–‹         | 71/1000 [00:19<01:47,  8.62it/s]Ref scores:   7%|â–‹         | 72/1000 [00:19<01:51,  8.30it/s]Ref scores:   7%|â–‹         | 73/1000 [00:19<01:55,  8.04it/s]Ref scores:   7%|â–‹         | 74/1000 [00:19<01:57,  7.90it/s]Ref scores:   8%|â–Š         | 75/1000 [00:19<02:02,  7.52it/s]Ref scores:   8%|â–Š         | 76/1000 [00:19<02:06,  7.28it/s]Ref scores:   8%|â–Š         | 77/1000 [00:19<02:04,  7.40it/s]Ref scores:   8%|â–Š         | 78/1000 [00:20<02:02,  7.50it/s]Ref scores:   8%|â–Š         | 79/1000 [00:20<02:03,  7.48it/s]Ref scores:   8%|â–Š         | 80/1000 [00:20<01:56,  7.93it/s]Ref scores:   8%|â–Š         | 81/1000 [00:20<02:02,  7.51it/s]Ref scores:   8%|â–Š         | 82/1000 [00:20<02:01,  7.56it/s]Ref scores:   8%|â–Š         | 83/1000 [00:20<02:01,  7.53it/s]Ref scores:   8%|â–Š         | 84/1000 [00:20<02:01,  7.54it/s]Ref scores:   8%|â–Š         | 85/1000 [00:21<02:21,  6.46it/s]Ref scores:   9%|â–Š         | 86/1000 [00:21<02:19,  6.53it/s]Ref scores:   9%|â–Š         | 87/1000 [00:21<02:13,  6.85it/s]Ref scores:   9%|â–‰         | 88/1000 [00:21<02:08,  7.08it/s]Ref scores:   9%|â–‰         | 90/1000 [00:21<01:52,  8.08it/s]Ref scores:   9%|â–‰         | 91/1000 [00:21<01:54,  7.95it/s]Ref scores:   9%|â–‰         | 92/1000 [00:21<01:55,  7.86it/s]Ref scores:   9%|â–‰         | 94/1000 [00:22<01:47,  8.43it/s]Ref scores:  10%|â–‰         | 95/1000 [00:22<01:55,  7.82it/s]Ref scores:  10%|â–‰         | 96/1000 [00:22<01:56,  7.76it/s]Ref scores:  10%|â–‰         | 97/1000 [00:22<01:56,  7.75it/s]Ref scores:  10%|â–‰         | 98/1000 [00:22<01:56,  7.75it/s]Ref scores:  10%|â–ˆ         | 100/1000 [00:22<01:40,  8.94it/s]Ref scores:  10%|â–ˆ         | 101/1000 [00:23<01:44,  8.57it/s]Ref scores:  10%|â–ˆ         | 102/1000 [00:23<01:47,  8.36it/s]Ref scores:  10%|â–ˆ         | 103/1000 [00:23<01:49,  8.16it/s]Ref scores:  10%|â–ˆ         | 104/1000 [00:23<01:52,  7.97it/s]Ref scores:  10%|â–ˆ         | 105/1000 [00:23<01:47,  8.30it/s]Ref scores:  11%|â–ˆ         | 106/1000 [00:23<01:51,  8.04it/s]Ref scores:  11%|â–ˆ         | 107/1000 [00:23<02:07,  7.00it/s]Ref scores:  11%|â–ˆ         | 108/1000 [00:24<02:04,  7.17it/s]Ref scores:  11%|â–ˆ         | 109/1000 [00:24<02:01,  7.33it/s]Ref scores:  11%|â–ˆ         | 110/1000 [00:24<02:01,  7.35it/s]Ref scores:  11%|â–ˆ         | 111/1000 [00:24<02:04,  7.15it/s]Ref scores:  11%|â–ˆ         | 112/1000 [00:24<02:02,  7.28it/s]Ref scores:  11%|â–ˆâ–        | 113/1000 [00:24<01:59,  7.39it/s]Ref scores:  11%|â–ˆâ–        | 114/1000 [00:24<01:51,  7.96it/s]Ref scores:  12%|â–ˆâ–        | 115/1000 [00:24<01:58,  7.50it/s]Ref scores:  12%|â–ˆâ–        | 116/1000 [00:25<01:57,  7.50it/s]Ref scores:  12%|â–ˆâ–        | 117/1000 [00:25<01:57,  7.50it/s]Ref scores:  12%|â–ˆâ–        | 118/1000 [00:25<01:56,  7.58it/s]Ref scores:  12%|â–ˆâ–        | 119/1000 [00:25<02:00,  7.30it/s]Ref scores:  12%|â–ˆâ–        | 120/1000 [00:25<01:59,  7.38it/s]Ref scores:  12%|â–ˆâ–        | 121/1000 [00:25<01:57,  7.45it/s]Ref scores:  12%|â–ˆâ–        | 122/1000 [00:25<01:56,  7.53it/s]Ref scores:  12%|â–ˆâ–        | 123/1000 [00:26<01:56,  7.55it/s]Ref scores:  12%|â–ˆâ–        | 124/1000 [00:26<01:55,  7.57it/s]Ref scores:  12%|â–ˆâ–Ž        | 125/1000 [00:26<01:55,  7.59it/s]Ref scores:  13%|â–ˆâ–Ž        | 126/1000 [00:26<01:56,  7.53it/s]Ref scores:  13%|â–ˆâ–Ž        | 127/1000 [00:26<02:02,  7.15it/s]Ref scores:  13%|â–ˆâ–Ž        | 128/1000 [00:26<02:04,  6.99it/s]Ref scores:  13%|â–ˆâ–Ž        | 129/1000 [00:26<02:21,  6.15it/s]Ref scores:  13%|â–ˆâ–Ž        | 130/1000 [00:27<02:13,  6.53it/s]Ref scores:  13%|â–ˆâ–Ž        | 131/1000 [00:27<02:01,  7.15it/s]Ref scores:  13%|â–ˆâ–Ž        | 132/1000 [00:27<02:00,  7.20it/s]Ref scores:  13%|â–ˆâ–Ž        | 133/1000 [00:27<01:59,  7.24it/s]Ref scores:  13%|â–ˆâ–Ž        | 134/1000 [00:27<01:58,  7.32it/s]Ref scores:  14%|â–ˆâ–Ž        | 135/1000 [00:27<01:57,  7.35it/s]Ref scores:  14%|â–ˆâ–Ž        | 136/1000 [00:27<01:50,  7.84it/s]Ref scores:  14%|â–ˆâ–Ž        | 137/1000 [00:27<01:51,  7.77it/s]Ref scores:  14%|â–ˆâ–        | 138/1000 [00:28<02:04,  6.92it/s]Ref scores:  14%|â–ˆâ–        | 139/1000 [00:28<01:55,  7.46it/s]Ref scores:  14%|â–ˆâ–        | 140/1000 [00:28<01:54,  7.49it/s]Ref scores:  14%|â–ˆâ–        | 141/1000 [00:28<01:48,  7.89it/s]Ref scores:  14%|â–ˆâ–        | 142/1000 [00:28<01:51,  7.72it/s]Ref scores:  14%|â–ˆâ–        | 143/1000 [00:28<01:52,  7.63it/s]Ref scores:  14%|â–ˆâ–        | 144/1000 [00:28<01:53,  7.55it/s]Ref scores:  14%|â–ˆâ–        | 145/1000 [00:29<02:07,  6.71it/s]Ref scores:  15%|â–ˆâ–        | 146/1000 [00:29<02:02,  6.95it/s]Ref scores:  15%|â–ˆâ–        | 147/1000 [00:29<02:00,  7.07it/s]Ref scores:  15%|â–ˆâ–        | 148/1000 [00:29<01:58,  7.20it/s]Ref scores:  15%|â–ˆâ–        | 149/1000 [00:29<01:50,  7.69it/s]Ref scores:  15%|â–ˆâ–Œ        | 150/1000 [00:29<01:45,  8.09it/s]Ref scores:  15%|â–ˆâ–Œ        | 151/1000 [00:29<01:46,  7.97it/s]Ref scores:  15%|â–ˆâ–Œ        | 152/1000 [00:29<01:48,  7.80it/s]Ref scores:  15%|â–ˆâ–Œ        | 153/1000 [00:30<01:49,  7.71it/s]Ref scores:  15%|â–ˆâ–Œ        | 154/1000 [00:30<01:50,  7.69it/s]Ref scores:  16%|â–ˆâ–Œ        | 155/1000 [00:30<01:51,  7.61it/s]Ref scores:  16%|â–ˆâ–Œ        | 156/1000 [00:30<01:43,  8.19it/s]Ref scores:  16%|â–ˆâ–Œ        | 157/1000 [00:30<01:50,  7.61it/s]Ref scores:  16%|â–ˆâ–Œ        | 158/1000 [00:30<01:43,  8.10it/s]Ref scores:  16%|â–ˆâ–Œ        | 159/1000 [00:30<01:40,  8.38it/s]Ref scores:  16%|â–ˆâ–Œ        | 160/1000 [00:30<01:43,  8.08it/s]Ref scores:  16%|â–ˆâ–Œ        | 161/1000 [00:31<01:50,  7.59it/s]Ref scores:  16%|â–ˆâ–Œ        | 162/1000 [00:31<01:56,  7.16it/s]Ref scores:  16%|â–ˆâ–‹        | 163/1000 [00:31<02:00,  6.97it/s]Ref scores:  16%|â–ˆâ–‹        | 165/1000 [00:31<01:41,  8.22it/s]Ref scores:  17%|â–ˆâ–‹        | 166/1000 [00:31<01:39,  8.42it/s]Ref scores:  17%|â–ˆâ–‹        | 167/1000 [00:31<01:42,  8.16it/s]Ref scores:  17%|â–ˆâ–‹        | 168/1000 [00:31<01:44,  7.94it/s]Ref scores:  17%|â–ˆâ–‹        | 169/1000 [00:32<01:46,  7.82it/s]Ref scores:  17%|â–ˆâ–‹        | 170/1000 [00:32<01:47,  7.70it/s]Ref scores:  17%|â–ˆâ–‹        | 171/1000 [00:32<01:48,  7.64it/s]Ref scores:  17%|â–ˆâ–‹        | 172/1000 [00:32<01:49,  7.59it/s]Ref scores:  17%|â–ˆâ–‹        | 173/1000 [00:32<01:54,  7.24it/s]Ref scores:  18%|â–ˆâ–Š        | 175/1000 [00:32<01:37,  8.46it/s]Ref scores:  18%|â–ˆâ–Š        | 176/1000 [00:33<01:43,  7.94it/s]Ref scores:  18%|â–ˆâ–Š        | 177/1000 [00:33<01:49,  7.55it/s]Ref scores:  18%|â–ˆâ–Š        | 178/1000 [00:33<01:49,  7.50it/s]Ref scores:  18%|â–ˆâ–Š        | 179/1000 [00:33<01:49,  7.50it/s]Ref scores:  18%|â–ˆâ–Š        | 180/1000 [00:33<01:53,  7.23it/s]Ref scores:  18%|â–ˆâ–Š        | 181/1000 [00:33<01:52,  7.30it/s]Ref scores:  18%|â–ˆâ–Š        | 182/1000 [00:33<01:51,  7.36it/s]Ref scores:  18%|â–ˆâ–Š        | 183/1000 [00:33<01:50,  7.37it/s]Ref scores:  18%|â–ˆâ–Š        | 184/1000 [00:34<01:50,  7.41it/s]Ref scores:  18%|â–ˆâ–Š        | 185/1000 [00:34<01:54,  7.15it/s]Ref scores:  19%|â–ˆâ–Š        | 186/1000 [00:34<01:46,  7.62it/s]Ref scores:  19%|â–ˆâ–‰        | 188/1000 [00:34<01:38,  8.28it/s]Ref scores:  19%|â–ˆâ–‰        | 189/1000 [00:34<01:35,  8.46it/s]Ref scores:  19%|â–ˆâ–‰        | 190/1000 [00:34<01:38,  8.21it/s]Ref scores:  19%|â–ˆâ–‰        | 191/1000 [00:34<01:41,  8.01it/s]Ref scores:  19%|â–ˆâ–‰        | 192/1000 [00:35<01:43,  7.81it/s]Ref scores:  19%|â–ˆâ–‰        | 193/1000 [00:35<01:38,  8.16it/s]Ref scores:  19%|â–ˆâ–‰        | 194/1000 [00:35<01:41,  7.93it/s]Ref scores:  20%|â–ˆâ–‰        | 195/1000 [00:35<01:43,  7.81it/s]Ref scores:  20%|â–ˆâ–‰        | 196/1000 [00:35<01:48,  7.43it/s]Ref scores:  20%|â–ˆâ–‰        | 197/1000 [00:35<01:47,  7.44it/s]Ref scores:  20%|â–ˆâ–‰        | 198/1000 [00:35<01:42,  7.86it/s]Ref scores:  20%|â–ˆâ–‰        | 199/1000 [00:36<01:42,  7.78it/s]Ref scores:  20%|â–ˆâ–ˆ        | 200/1000 [00:36<01:43,  7.69it/s]Ref scores:  20%|â–ˆâ–ˆ        | 201/1000 [00:36<01:39,  8.07it/s]Ref scores:  20%|â–ˆâ–ˆ        | 202/1000 [00:36<01:40,  7.91it/s]Ref scores:  20%|â–ˆâ–ˆ        | 203/1000 [00:36<01:41,  7.85it/s]Ref scores:  20%|â–ˆâ–ˆ        | 204/1000 [00:36<01:41,  7.81it/s]Ref scores:  20%|â–ˆâ–ˆ        | 205/1000 [00:36<01:42,  7.75it/s]Ref scores:  21%|â–ˆâ–ˆ        | 207/1000 [00:36<01:30,  8.77it/s]Ref scores:  21%|â–ˆâ–ˆ        | 209/1000 [00:37<01:28,  8.95it/s]Ref scores:  21%|â–ˆâ–ˆ        | 210/1000 [00:37<01:41,  7.77it/s]Ref scores:  21%|â–ˆâ–ˆ        | 211/1000 [00:37<01:46,  7.43it/s]Ref scores:  21%|â–ˆâ–ˆâ–       | 213/1000 [00:37<01:37,  8.08it/s]Ref scores:  21%|â–ˆâ–ˆâ–       | 214/1000 [00:37<01:38,  7.96it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 215/1000 [00:37<01:35,  8.22it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 216/1000 [00:38<01:49,  7.19it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 217/1000 [00:38<01:47,  7.26it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 218/1000 [00:38<01:50,  7.09it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 219/1000 [00:38<01:43,  7.53it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 220/1000 [00:38<01:43,  7.57it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 221/1000 [00:38<01:43,  7.55it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 222/1000 [00:38<01:43,  7.53it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 223/1000 [00:39<01:43,  7.51it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 224/1000 [00:39<01:38,  7.91it/s]Ref scores:  22%|â–ˆâ–ˆâ–Ž       | 225/1000 [00:39<01:51,  6.95it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 226/1000 [00:39<01:49,  7.09it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 227/1000 [00:39<01:47,  7.18it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 228/1000 [00:39<01:46,  7.25it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 229/1000 [00:39<01:44,  7.35it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 230/1000 [00:40<01:44,  7.40it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 231/1000 [00:40<01:56,  6.61it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 232/1000 [00:40<01:52,  6.84it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 233/1000 [00:40<01:49,  7.02it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 234/1000 [00:40<01:41,  7.57it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 235/1000 [00:40<01:41,  7.54it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 236/1000 [00:40<01:47,  7.10it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 237/1000 [00:41<01:45,  7.22it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 238/1000 [00:41<01:44,  7.30it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 239/1000 [00:41<01:37,  7.78it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 240/1000 [00:41<01:38,  7.69it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 241/1000 [00:41<01:45,  7.20it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 242/1000 [00:41<01:43,  7.31it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 243/1000 [00:41<01:37,  7.79it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 244/1000 [00:41<01:37,  7.72it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 245/1000 [00:42<01:38,  7.64it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 246/1000 [00:42<01:39,  7.56it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 247/1000 [00:42<01:39,  7.58it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 248/1000 [00:42<01:39,  7.56it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 249/1000 [00:42<01:43,  7.28it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 250/1000 [00:42<01:42,  7.32it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 251/1000 [00:42<01:45,  7.11it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 252/1000 [00:43<01:42,  7.26it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 253/1000 [00:43<01:41,  7.34it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 254/1000 [00:43<01:41,  7.38it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 255/1000 [00:43<01:44,  7.12it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 256/1000 [00:43<01:43,  7.21it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 257/1000 [00:43<01:42,  7.28it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 258/1000 [00:43<01:44,  7.09it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 259/1000 [00:44<01:42,  7.24it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 260/1000 [00:44<01:40,  7.34it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 261/1000 [00:44<01:40,  7.39it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 262/1000 [00:44<01:39,  7.44it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 264/1000 [00:44<02:12,  5.57it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 265/1000 [00:45<02:03,  5.97it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 266/1000 [00:45<01:56,  6.32it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 267/1000 [00:45<01:56,  6.30it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 268/1000 [00:45<01:44,  6.97it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 269/1000 [00:45<01:37,  7.49it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 270/1000 [00:45<01:37,  7.52it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 271/1000 [00:45<01:36,  7.55it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 272/1000 [00:45<01:36,  7.54it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 273/1000 [00:46<01:31,  7.94it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 274/1000 [00:46<01:32,  7.82it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 275/1000 [00:46<01:34,  7.70it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 276/1000 [00:46<01:29,  8.05it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 277/1000 [00:46<01:26,  8.36it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 278/1000 [00:46<01:29,  8.10it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 279/1000 [00:46<01:30,  7.95it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 280/1000 [00:46<01:26,  8.37it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 281/1000 [00:47<01:28,  8.09it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 282/1000 [00:47<01:31,  7.86it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 283/1000 [00:47<01:32,  7.75it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 284/1000 [00:47<01:32,  7.71it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 285/1000 [00:47<01:33,  7.63it/s]Ref scores:  29%|â–ˆâ–ˆâ–Š       | 286/1000 [00:47<01:33,  7.61it/s]Ref scores:  29%|â–ˆâ–ˆâ–Š       | 287/1000 [00:47<01:34,  7.57it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 289/1000 [00:48<01:25,  8.29it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 290/1000 [00:48<01:27,  8.07it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 291/1000 [00:48<01:29,  7.92it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 292/1000 [00:48<01:30,  7.81it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 293/1000 [00:48<01:26,  8.15it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 294/1000 [00:48<01:28,  7.98it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 295/1000 [00:48<01:25,  8.26it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 296/1000 [00:48<01:27,  8.02it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 297/1000 [00:49<01:29,  7.83it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 299/1000 [00:49<01:23,  8.37it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 300/1000 [00:49<01:49,  6.42it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 301/1000 [00:49<01:44,  6.68it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 302/1000 [00:49<01:44,  6.66it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 303/1000 [00:49<01:44,  6.66it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 304/1000 [00:50<01:41,  6.87it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 305/1000 [00:50<01:38,  7.04it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 306/1000 [00:50<01:37,  7.14it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 307/1000 [00:50<01:46,  6.48it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 308/1000 [00:50<01:42,  6.74it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 310/1000 [00:50<01:29,  7.74it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 311/1000 [00:51<01:25,  8.05it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 312/1000 [00:51<01:27,  7.88it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆâ–      | 313/1000 [00:51<01:22,  8.31it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆâ–      | 314/1000 [00:51<01:28,  7.71it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 315/1000 [00:51<01:29,  7.62it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 316/1000 [00:51<01:30,  7.55it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 317/1000 [00:51<01:29,  7.59it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 318/1000 [00:51<01:30,  7.57it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 319/1000 [00:52<01:33,  7.29it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 320/1000 [00:52<01:32,  7.32it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 322/1000 [00:52<01:33,  7.26it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 323/1000 [00:52<01:32,  7.32it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 324/1000 [00:52<01:31,  7.37it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 325/1000 [00:52<01:30,  7.42it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 326/1000 [00:53<01:34,  7.10it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 327/1000 [00:53<01:32,  7.25it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 328/1000 [00:53<01:31,  7.32it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 329/1000 [00:53<01:31,  7.37it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 330/1000 [00:53<01:30,  7.40it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 331/1000 [00:53<01:29,  7.47it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 332/1000 [00:53<01:29,  7.47it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 333/1000 [00:53<01:28,  7.51it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 334/1000 [00:54<01:29,  7.48it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 335/1000 [00:54<01:28,  7.48it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 336/1000 [00:54<01:23,  7.98it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 337/1000 [00:54<01:24,  7.82it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 338/1000 [00:54<01:25,  7.73it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 339/1000 [00:54<01:26,  7.64it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 340/1000 [00:54<01:26,  7.63it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 341/1000 [00:55<01:26,  7.63it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 342/1000 [00:55<01:26,  7.59it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 343/1000 [00:55<01:26,  7.56it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 344/1000 [00:55<01:22,  7.98it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 345/1000 [00:55<01:23,  7.89it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 346/1000 [00:55<01:24,  7.75it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 347/1000 [00:55<01:28,  7.34it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 348/1000 [00:55<01:28,  7.39it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 349/1000 [00:56<01:23,  7.81it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 350/1000 [00:56<01:23,  7.75it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 351/1000 [00:56<01:24,  7.66it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 352/1000 [00:56<01:25,  7.60it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 353/1000 [00:56<01:30,  7.16it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 354/1000 [00:56<01:29,  7.22it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 355/1000 [00:56<01:27,  7.34it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 356/1000 [00:56<01:20,  7.98it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 358/1000 [00:57<01:14,  8.57it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 359/1000 [00:57<01:17,  8.31it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 360/1000 [00:57<01:19,  8.08it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 361/1000 [00:57<01:23,  7.61it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 362/1000 [00:57<01:24,  7.54it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 363/1000 [00:57<01:20,  7.92it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 364/1000 [00:58<01:25,  7.47it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 365/1000 [00:58<01:28,  7.19it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 366/1000 [00:58<01:27,  7.26it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 367/1000 [00:58<01:30,  7.03it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 368/1000 [00:58<01:28,  7.16it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 369/1000 [00:58<01:22,  7.63it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 370/1000 [00:58<01:22,  7.63it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 371/1000 [00:58<01:16,  8.20it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 372/1000 [00:59<01:13,  8.57it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 373/1000 [00:59<01:16,  8.18it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 374/1000 [00:59<01:21,  7.66it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 375/1000 [00:59<01:22,  7.61it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 376/1000 [00:59<01:27,  7.17it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 377/1000 [00:59<01:26,  7.24it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 378/1000 [00:59<01:25,  7.30it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 379/1000 [01:00<01:28,  7.06it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 380/1000 [01:00<01:26,  7.18it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 381/1000 [01:00<01:24,  7.33it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 382/1000 [01:00<01:17,  7.96it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 383/1000 [01:00<01:28,  6.96it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 384/1000 [01:00<01:26,  7.10it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 386/1000 [01:00<01:14,  8.28it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 387/1000 [01:01<01:16,  8.05it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 388/1000 [01:01<01:17,  7.94it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 389/1000 [01:01<01:18,  7.82it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 390/1000 [01:01<01:18,  7.77it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 391/1000 [01:01<01:14,  8.21it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 392/1000 [01:01<01:15,  8.01it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 393/1000 [01:01<01:17,  7.82it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 395/1000 [01:02<01:19,  7.61it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 397/1000 [01:02<01:15,  8.00it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 398/1000 [01:02<01:16,  7.87it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 399/1000 [01:02<01:17,  7.80it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 400/1000 [01:02<01:14,  8.08it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 401/1000 [01:02<01:15,  7.91it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 402/1000 [01:02<01:17,  7.76it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 403/1000 [01:03<01:17,  7.69it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 404/1000 [01:03<01:21,  7.32it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 406/1000 [01:03<01:13,  8.08it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 407/1000 [01:03<01:17,  7.68it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 408/1000 [01:03<01:14,  7.99it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 409/1000 [01:03<01:10,  8.43it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 410/1000 [01:03<01:07,  8.79it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 411/1000 [01:04<01:10,  8.33it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 412/1000 [01:04<01:12,  8.08it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 413/1000 [01:04<01:08,  8.54it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 414/1000 [01:04<01:11,  8.15it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 415/1000 [01:04<01:13,  7.95it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 416/1000 [01:04<01:15,  7.78it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 417/1000 [01:04<01:15,  7.68it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 418/1000 [01:04<01:11,  8.14it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 419/1000 [01:05<01:13,  7.95it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 420/1000 [01:05<01:14,  7.81it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 421/1000 [01:05<01:15,  7.69it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 422/1000 [01:05<01:18,  7.32it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 423/1000 [01:05<01:18,  7.37it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 424/1000 [01:05<01:13,  7.82it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 425/1000 [01:05<01:18,  7.29it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 426/1000 [01:06<01:13,  7.76it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 427/1000 [01:06<01:14,  7.68it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 428/1000 [01:06<01:19,  7.21it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 429/1000 [01:06<01:17,  7.33it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 430/1000 [01:06<01:26,  6.62it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 431/1000 [01:06<01:22,  6.87it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 432/1000 [01:06<01:20,  7.08it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 433/1000 [01:07<01:18,  7.18it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 434/1000 [01:07<01:17,  7.31it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 435/1000 [01:07<01:16,  7.39it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 436/1000 [01:07<01:20,  7.02it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 437/1000 [01:07<01:18,  7.18it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 438/1000 [01:07<01:20,  6.98it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 439/1000 [01:07<01:18,  7.15it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 440/1000 [01:07<01:17,  7.24it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 441/1000 [01:08<01:16,  7.34it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 442/1000 [01:08<01:15,  7.38it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 443/1000 [01:08<01:14,  7.45it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 444/1000 [01:08<01:17,  7.17it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 445/1000 [01:08<01:16,  7.25it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 446/1000 [01:08<01:11,  7.72it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 447/1000 [01:08<01:12,  7.63it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 448/1000 [01:09<01:07,  8.14it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 449/1000 [01:09<01:18,  6.99it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 450/1000 [01:09<01:13,  7.52it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 451/1000 [01:09<01:13,  7.51it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 452/1000 [01:09<01:13,  7.49it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 454/1000 [01:09<01:08,  7.98it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 455/1000 [01:09<01:09,  7.84it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 456/1000 [01:10<01:09,  7.79it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 457/1000 [01:10<01:13,  7.39it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 458/1000 [01:10<01:13,  7.42it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 459/1000 [01:10<01:09,  7.82it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 460/1000 [01:10<01:10,  7.71it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 461/1000 [01:10<01:10,  7.63it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 462/1000 [01:10<01:11,  7.58it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 463/1000 [01:11<01:13,  7.27it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 464/1000 [01:11<01:13,  7.32it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 465/1000 [01:11<01:12,  7.40it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 466/1000 [01:11<01:11,  7.46it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 467/1000 [01:11<01:14,  7.19it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 468/1000 [01:11<01:13,  7.26it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 469/1000 [01:11<01:12,  7.33it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 470/1000 [01:12<01:14,  7.13it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 471/1000 [01:12<01:12,  7.26it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 472/1000 [01:12<01:12,  7.29it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 473/1000 [01:12<01:11,  7.40it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 474/1000 [01:12<01:11,  7.40it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 475/1000 [01:12<01:07,  7.82it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 476/1000 [01:12<01:07,  7.73it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 477/1000 [01:12<01:08,  7.66it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 478/1000 [01:13<01:04,  8.14it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 479/1000 [01:13<01:05,  7.92it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 480/1000 [01:13<01:06,  7.79it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 481/1000 [01:13<01:07,  7.72it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 482/1000 [01:13<01:07,  7.68it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 483/1000 [01:13<01:08,  7.57it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 484/1000 [01:13<01:08,  7.55it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 485/1000 [01:13<01:08,  7.54it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 486/1000 [01:14<01:08,  7.50it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 487/1000 [01:14<01:08,  7.49it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 488/1000 [01:14<01:08,  7.52it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 489/1000 [01:14<01:03,  8.04it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 490/1000 [01:14<01:07,  7.52it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 491/1000 [01:14<01:07,  7.52it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 492/1000 [01:14<01:07,  7.49it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 493/1000 [01:15<01:07,  7.50it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 494/1000 [01:15<01:07,  7.50it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 495/1000 [01:15<01:07,  7.48it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 496/1000 [01:15<01:07,  7.51it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 497/1000 [01:15<01:07,  7.49it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 498/1000 [01:15<01:07,  7.48it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 499/1000 [01:15<01:06,  7.50it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 500/1000 [01:15<01:06,  7.48it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 501/1000 [01:16<01:06,  7.50it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 502/1000 [01:16<01:06,  7.49it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 503/1000 [01:16<01:02,  7.90it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 504/1000 [01:16<01:03,  7.77it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 506/1000 [01:16<00:58,  8.38it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 507/1000 [01:16<00:57,  8.57it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 508/1000 [01:16<00:56,  8.71it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 509/1000 [01:17<00:58,  8.32it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 510/1000 [01:17<01:00,  8.08it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 511/1000 [01:17<01:01,  7.93it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 512/1000 [01:17<01:06,  7.38it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 513/1000 [01:17<01:09,  7.00it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 514/1000 [01:17<01:04,  7.50it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 515/1000 [01:17<01:04,  7.50it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 516/1000 [01:18<01:07,  7.22it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 517/1000 [01:18<01:06,  7.29it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 518/1000 [01:18<01:05,  7.39it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 519/1000 [01:18<01:05,  7.40it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 520/1000 [01:18<01:04,  7.42it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 521/1000 [01:18<01:03,  7.49it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 522/1000 [01:18<01:03,  7.48it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 523/1000 [01:18<01:03,  7.45it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 524/1000 [01:19<01:03,  7.48it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 525/1000 [01:19<01:00,  7.91it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 526/1000 [01:19<00:57,  8.23it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 527/1000 [01:19<00:59,  7.96it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 528/1000 [01:19<01:00,  7.85it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 529/1000 [01:19<01:00,  7.75it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 530/1000 [01:19<01:01,  7.65it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 531/1000 [01:19<01:01,  7.58it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 532/1000 [01:20<00:57,  8.09it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 533/1000 [01:20<00:58,  7.94it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 534/1000 [01:20<01:03,  7.38it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 535/1000 [01:20<01:06,  7.04it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 536/1000 [01:20<01:04,  7.19it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 537/1000 [01:20<01:03,  7.24it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 538/1000 [01:20<01:06,  6.94it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 539/1000 [01:21<01:05,  7.06it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 541/1000 [01:21<00:57,  7.94it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 542/1000 [01:21<00:55,  8.20it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 543/1000 [01:21<00:57,  7.98it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 544/1000 [01:21<00:58,  7.85it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 545/1000 [01:21<00:58,  7.73it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 546/1000 [01:21<00:56,  8.10it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 547/1000 [01:22<00:57,  7.93it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 548/1000 [01:22<00:57,  7.81it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 550/1000 [01:22<00:53,  8.37it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 551/1000 [01:22<00:51,  8.65it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 552/1000 [01:22<00:56,  7.98it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 553/1000 [01:22<00:57,  7.84it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 554/1000 [01:22<01:00,  7.34it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 555/1000 [01:23<01:00,  7.41it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 556/1000 [01:23<00:59,  7.43it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 557/1000 [01:23<00:59,  7.45it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 559/1000 [01:23<00:53,  8.22it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 560/1000 [01:23<00:54,  8.01it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 561/1000 [01:23<00:53,  8.27it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 562/1000 [01:23<00:56,  7.73it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 563/1000 [01:24<00:56,  7.71it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 564/1000 [01:24<00:56,  7.71it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 565/1000 [01:24<00:56,  7.68it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 566/1000 [01:24<00:56,  7.63it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 567/1000 [01:24<00:53,  8.08it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 568/1000 [01:24<01:00,  7.12it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 569/1000 [01:24<01:02,  6.90it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 570/1000 [01:25<01:00,  7.07it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 571/1000 [01:25<00:59,  7.24it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 572/1000 [01:25<00:58,  7.32it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 573/1000 [01:25<00:53,  7.95it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 574/1000 [01:25<00:54,  7.88it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 576/1000 [01:25<00:46,  9.16it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 577/1000 [01:25<00:48,  8.70it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 578/1000 [01:25<00:49,  8.46it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 580/1000 [01:26<00:49,  8.54it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 581/1000 [01:26<00:50,  8.28it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 582/1000 [01:26<00:51,  8.14it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 583/1000 [01:26<00:54,  7.61it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 584/1000 [01:26<01:00,  6.87it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 585/1000 [01:27<01:08,  6.09it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 586/1000 [01:27<01:04,  6.47it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 587/1000 [01:27<01:00,  6.80it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 588/1000 [01:27<00:58,  7.03it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 589/1000 [01:27<00:58,  6.97it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 590/1000 [01:27<00:54,  7.54it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 591/1000 [01:27<00:54,  7.56it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 592/1000 [01:27<00:53,  7.62it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 593/1000 [01:28<00:55,  7.34it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 594/1000 [01:28<00:54,  7.39it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 595/1000 [01:28<00:53,  7.51it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 596/1000 [01:28<00:53,  7.58it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 597/1000 [01:28<00:52,  7.61it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 598/1000 [01:28<00:52,  7.62it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 599/1000 [01:28<00:49,  8.06it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 600/1000 [01:28<00:46,  8.53it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 601/1000 [01:29<00:48,  8.24it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 602/1000 [01:29<00:49,  8.04it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 603/1000 [01:29<00:49,  7.95it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 604/1000 [01:29<00:57,  6.89it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 605/1000 [01:29<00:55,  7.14it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 607/1000 [01:29<00:46,  8.42it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 608/1000 [01:29<00:47,  8.22it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 609/1000 [01:30<00:45,  8.57it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 610/1000 [01:30<00:43,  8.92it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 611/1000 [01:30<00:45,  8.56it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 612/1000 [01:30<00:44,  8.76it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 613/1000 [01:30<00:46,  8.41it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 614/1000 [01:30<00:47,  8.18it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 615/1000 [01:30<00:47,  8.04it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 616/1000 [01:30<00:48,  7.91it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 617/1000 [01:31<00:48,  7.86it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 618/1000 [01:31<00:48,  7.84it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 619/1000 [01:31<00:46,  8.21it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 620/1000 [01:31<00:49,  7.72it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 621/1000 [01:31<00:45,  8.27it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 622/1000 [01:31<00:46,  8.05it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 623/1000 [01:31<00:47,  7.96it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 624/1000 [01:31<00:47,  7.90it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 625/1000 [01:32<00:47,  7.83it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 626/1000 [01:32<00:48,  7.74it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 627/1000 [01:32<00:48,  7.72it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 628/1000 [01:32<00:48,  7.69it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 629/1000 [01:32<00:51,  7.25it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 630/1000 [01:32<00:46,  7.90it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 631/1000 [01:32<00:47,  7.80it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 632/1000 [01:32<00:47,  7.80it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 633/1000 [01:33<00:50,  7.33it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 634/1000 [01:33<00:49,  7.40it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 635/1000 [01:33<00:50,  7.22it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 636/1000 [01:33<00:49,  7.33it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 637/1000 [01:33<00:48,  7.43it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 638/1000 [01:33<00:48,  7.47it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 639/1000 [01:33<00:45,  7.96it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 640/1000 [01:34<00:47,  7.59it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 641/1000 [01:34<00:44,  8.16it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 642/1000 [01:34<00:41,  8.57it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 643/1000 [01:34<00:40,  8.92it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 644/1000 [01:34<00:44,  8.01it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 645/1000 [01:34<00:44,  7.90it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 646/1000 [01:34<00:50,  6.96it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 647/1000 [01:34<00:49,  7.17it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 648/1000 [01:35<00:47,  7.35it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 649/1000 [01:35<00:47,  7.42it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 650/1000 [01:35<00:46,  7.52it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 651/1000 [01:35<00:46,  7.56it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 652/1000 [01:35<00:45,  7.60it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 653/1000 [01:35<00:45,  7.61it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 654/1000 [01:35<00:51,  6.73it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 655/1000 [01:36<00:49,  7.02it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 656/1000 [01:36<00:44,  7.71it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 657/1000 [01:36<00:50,  6.77it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 658/1000 [01:36<00:48,  6.98it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 659/1000 [01:36<00:50,  6.78it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 660/1000 [01:36<00:51,  6.65it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 661/1000 [01:36<00:54,  6.18it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 662/1000 [01:37<00:51,  6.55it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 663/1000 [01:37<00:49,  6.83it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 664/1000 [01:37<00:47,  7.01it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 665/1000 [01:37<00:46,  7.19it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 666/1000 [01:37<00:45,  7.26it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 667/1000 [01:37<00:45,  7.35it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 668/1000 [01:37<00:44,  7.43it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 670/1000 [01:38<00:36,  9.11it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 671/1000 [01:38<00:36,  9.11it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 672/1000 [01:38<00:38,  8.63it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 673/1000 [01:38<00:39,  8.31it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 674/1000 [01:38<00:38,  8.52it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 675/1000 [01:38<00:37,  8.73it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 676/1000 [01:38<00:39,  8.29it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 677/1000 [01:38<00:39,  8.08it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 678/1000 [01:39<00:38,  8.46it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 679/1000 [01:39<00:39,  8.18it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 680/1000 [01:39<00:40,  7.95it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 681/1000 [01:39<00:42,  7.53it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 682/1000 [01:39<00:42,  7.51it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 683/1000 [01:39<00:42,  7.50it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 684/1000 [01:39<00:42,  7.51it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 685/1000 [01:39<00:43,  7.25it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 686/1000 [01:40<00:42,  7.35it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 687/1000 [01:40<00:42,  7.43it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 688/1000 [01:40<00:41,  7.50it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 689/1000 [01:40<00:43,  7.20it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 690/1000 [01:40<00:42,  7.33it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 691/1000 [01:40<00:39,  7.78it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 692/1000 [01:40<00:39,  7.75it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 693/1000 [01:41<00:39,  7.73it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 694/1000 [01:41<00:39,  7.71it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 695/1000 [01:41<00:40,  7.62it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 696/1000 [01:41<00:39,  7.63it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 697/1000 [01:41<00:39,  7.62it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 698/1000 [01:41<00:39,  7.64it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 699/1000 [01:41<00:39,  7.63it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 700/1000 [01:41<00:39,  7.61it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 701/1000 [01:42<00:39,  7.61it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 702/1000 [01:42<00:39,  7.55it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 704/1000 [01:42<00:37,  7.93it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 705/1000 [01:42<00:37,  7.84it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 706/1000 [01:42<00:37,  7.80it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 707/1000 [01:42<00:36,  8.12it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 708/1000 [01:42<00:38,  7.67it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 709/1000 [01:43<00:35,  8.21it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 710/1000 [01:43<00:38,  7.56it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 711/1000 [01:43<00:39,  7.27it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 712/1000 [01:43<00:40,  7.07it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 713/1000 [01:43<00:39,  7.22it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 714/1000 [01:43<00:39,  7.32it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 715/1000 [01:43<00:40,  7.08it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 716/1000 [01:44<00:40,  6.97it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 717/1000 [01:44<00:39,  7.10it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 718/1000 [01:44<00:38,  7.25it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 719/1000 [01:44<00:40,  6.93it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 721/1000 [01:44<00:36,  7.60it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 722/1000 [01:44<00:36,  7.60it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 723/1000 [01:45<00:36,  7.62it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 724/1000 [01:45<00:38,  7.21it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 725/1000 [01:45<00:37,  7.34it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 726/1000 [01:45<00:38,  7.14it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 728/1000 [01:45<00:34,  7.88it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 729/1000 [01:45<00:34,  7.80it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 730/1000 [01:45<00:34,  7.74it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 731/1000 [01:46<00:35,  7.66it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 732/1000 [01:46<00:35,  7.63it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 733/1000 [01:46<00:34,  7.63it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 734/1000 [01:46<00:34,  7.61it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 735/1000 [01:46<00:34,  7.61it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 736/1000 [01:46<00:33,  7.99it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 737/1000 [01:46<00:33,  7.92it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 738/1000 [01:46<00:31,  8.37it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 739/1000 [01:47<00:32,  8.08it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 740/1000 [01:47<00:33,  7.86it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 741/1000 [01:47<00:33,  7.72it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 742/1000 [01:47<00:31,  8.16it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 743/1000 [01:47<00:32,  7.96it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 744/1000 [01:47<00:32,  7.81it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 746/1000 [01:47<00:28,  9.07it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 747/1000 [01:48<00:29,  8.68it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 748/1000 [01:48<00:30,  8.33it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 749/1000 [01:48<00:30,  8.10it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 750/1000 [01:48<00:35,  7.12it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 751/1000 [01:48<00:32,  7.62it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 752/1000 [01:48<00:32,  7.60it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 753/1000 [01:48<00:32,  7.61it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 754/1000 [01:49<00:32,  7.56it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 755/1000 [01:49<00:32,  7.58it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 756/1000 [01:49<00:33,  7.28it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 757/1000 [01:49<00:32,  7.37it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 758/1000 [01:49<00:30,  7.84it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 759/1000 [01:49<00:31,  7.76it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 760/1000 [01:49<00:31,  7.74it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 761/1000 [01:49<00:31,  7.68it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 762/1000 [01:50<00:31,  7.63it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 763/1000 [01:50<00:31,  7.62it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 764/1000 [01:50<00:30,  7.61it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 765/1000 [01:50<00:30,  7.65it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 766/1000 [01:50<00:31,  7.33it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 767/1000 [01:50<00:32,  7.13it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 768/1000 [01:50<00:32,  7.22it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 769/1000 [01:50<00:29,  7.74it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 770/1000 [01:51<00:29,  7.72it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 771/1000 [01:51<00:28,  8.09it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 772/1000 [01:51<00:28,  7.94it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 773/1000 [01:51<00:29,  7.80it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 774/1000 [01:51<00:30,  7.46it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 775/1000 [01:51<00:29,  7.53it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 776/1000 [01:51<00:29,  7.53it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 777/1000 [01:52<00:28,  7.96it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 778/1000 [01:52<00:28,  7.81it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 779/1000 [01:52<00:28,  7.70it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 780/1000 [01:52<00:29,  7.38it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 781/1000 [01:52<00:38,  5.73it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 782/1000 [01:52<00:36,  5.91it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 783/1000 [01:52<00:34,  6.35it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 784/1000 [01:53<00:32,  6.64it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 785/1000 [01:53<00:32,  6.64it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 786/1000 [01:53<00:31,  6.88it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 787/1000 [01:53<00:28,  7.45it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 788/1000 [01:53<00:28,  7.46it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 789/1000 [01:53<00:28,  7.51it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 790/1000 [01:53<00:28,  7.49it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 791/1000 [01:54<00:25,  8.07it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 792/1000 [01:54<00:26,  7.88it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 793/1000 [01:54<00:29,  6.90it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 794/1000 [01:54<00:28,  7.13it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 795/1000 [01:54<00:26,  7.63it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 796/1000 [01:54<00:26,  7.64it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 797/1000 [01:54<00:26,  7.60it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 798/1000 [01:54<00:26,  7.60it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 799/1000 [01:55<00:26,  7.57it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 800/1000 [01:55<00:26,  7.55it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 801/1000 [01:55<00:26,  7.55it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 803/1000 [01:55<00:24,  8.05it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 804/1000 [01:55<00:24,  7.94it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 805/1000 [01:55<00:25,  7.80it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 806/1000 [01:55<00:23,  8.14it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 807/1000 [01:56<00:26,  7.17it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 808/1000 [01:56<00:26,  7.28it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 809/1000 [01:56<00:25,  7.35it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 810/1000 [01:56<00:25,  7.39it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 811/1000 [01:56<00:25,  7.42it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 812/1000 [01:56<00:26,  7.09it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 813/1000 [01:56<00:25,  7.26it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 814/1000 [01:57<00:25,  7.34it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 815/1000 [01:57<00:23,  7.92it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 817/1000 [01:57<00:21,  8.50it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 818/1000 [01:57<00:23,  7.83it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 819/1000 [01:57<00:23,  7.75it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 820/1000 [01:57<00:23,  7.71it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 821/1000 [01:57<00:23,  7.64it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 822/1000 [01:58<00:23,  7.62it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 823/1000 [01:58<00:22,  8.02it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 824/1000 [01:58<00:22,  7.90it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 825/1000 [01:58<00:22,  7.82it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 826/1000 [01:58<00:22,  7.73it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 827/1000 [01:58<00:20,  8.26it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 828/1000 [01:58<00:22,  7.66it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 829/1000 [01:58<00:21,  8.09it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 830/1000 [01:59<00:21,  7.88it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 831/1000 [01:59<00:21,  7.77it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 832/1000 [01:59<00:21,  7.77it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 833/1000 [01:59<00:21,  7.73it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 834/1000 [01:59<00:21,  7.71it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 835/1000 [01:59<00:21,  7.66it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 836/1000 [01:59<00:21,  7.62it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 837/1000 [02:00<00:21,  7.64it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 838/1000 [02:00<00:20,  8.04it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 839/1000 [02:00<00:18,  8.49it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 840/1000 [02:00<00:19,  8.12it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 841/1000 [02:00<00:20,  7.93it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 842/1000 [02:00<00:20,  7.52it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 844/1000 [02:00<00:18,  8.41it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 845/1000 [02:00<00:18,  8.16it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 846/1000 [02:01<00:19,  8.01it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 847/1000 [02:01<00:19,  7.86it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 848/1000 [02:01<00:18,  8.19it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 849/1000 [02:01<00:18,  8.03it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 850/1000 [02:01<00:20,  7.47it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 851/1000 [02:01<00:19,  7.50it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 853/1000 [02:01<00:17,  8.25it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 854/1000 [02:02<00:18,  7.79it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 855/1000 [02:02<00:19,  7.47it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 856/1000 [02:02<00:21,  6.73it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 857/1000 [02:02<00:20,  6.92it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 858/1000 [02:02<00:19,  7.11it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 859/1000 [02:02<00:20,  6.91it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 860/1000 [02:03<00:19,  7.11it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 861/1000 [02:03<00:19,  7.28it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 862/1000 [02:03<00:17,  7.88it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 863/1000 [02:03<00:16,  8.24it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 864/1000 [02:03<00:15,  8.52it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 865/1000 [02:03<00:16,  8.23it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 866/1000 [02:03<00:16,  8.08it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 867/1000 [02:03<00:16,  7.94it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 868/1000 [02:03<00:16,  8.24it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 869/1000 [02:04<00:15,  8.50it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 870/1000 [02:04<00:15,  8.25it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 871/1000 [02:04<00:16,  8.03it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 872/1000 [02:04<00:15,  8.34it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 873/1000 [02:04<00:15,  8.04it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 874/1000 [02:04<00:16,  7.56it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 875/1000 [02:04<00:16,  7.57it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 876/1000 [02:05<00:17,  7.26it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 877/1000 [02:05<00:15,  7.73it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 879/1000 [02:05<00:13,  8.79it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 880/1000 [02:05<00:14,  8.48it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 881/1000 [02:05<00:14,  8.26it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 882/1000 [02:05<00:13,  8.61it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 883/1000 [02:05<00:14,  8.31it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 884/1000 [02:05<00:14,  8.11it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 885/1000 [02:06<00:13,  8.40it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 886/1000 [02:06<00:14,  8.09it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 887/1000 [02:06<00:13,  8.51it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 888/1000 [02:06<00:13,  8.24it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 889/1000 [02:06<00:13,  8.02it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 890/1000 [02:06<00:13,  7.88it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 892/1000 [02:06<00:12,  8.95it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 893/1000 [02:06<00:11,  9.11it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 894/1000 [02:07<00:12,  8.31it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 895/1000 [02:07<00:13,  8.07it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 897/1000 [02:07<00:11,  8.65it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 898/1000 [02:07<00:12,  8.06it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 899/1000 [02:07<00:12,  8.35it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 901/1000 [02:07<00:11,  8.32it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 902/1000 [02:08<00:12,  7.89it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 903/1000 [02:08<00:12,  7.52it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 904/1000 [02:08<00:12,  7.49it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 905/1000 [02:08<00:12,  7.53it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 906/1000 [02:08<00:12,  7.52it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 907/1000 [02:08<00:12,  7.54it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 908/1000 [02:08<00:12,  7.59it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 909/1000 [02:09<00:12,  7.58it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 910/1000 [02:09<00:11,  7.61it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 911/1000 [02:09<00:11,  8.02it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 912/1000 [02:09<00:11,  7.57it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 913/1000 [02:09<00:11,  7.56it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 914/1000 [02:09<00:10,  7.97it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 915/1000 [02:09<00:10,  7.82it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 916/1000 [02:09<00:10,  7.71it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 917/1000 [02:10<00:10,  7.67it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 918/1000 [02:10<00:10,  7.63it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 919/1000 [02:10<00:10,  7.62it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 920/1000 [02:10<00:10,  7.30it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 921/1000 [02:10<00:10,  7.40it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 922/1000 [02:10<00:09,  7.84it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 923/1000 [02:10<00:09,  7.72it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 924/1000 [02:11<00:09,  7.63it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 925/1000 [02:11<00:09,  7.63it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 926/1000 [02:11<00:09,  7.63it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 927/1000 [02:11<00:08,  8.17it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 928/1000 [02:11<00:08,  8.03it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 929/1000 [02:11<00:09,  7.58it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 930/1000 [02:11<00:09,  7.57it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 931/1000 [02:11<00:09,  7.53it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 932/1000 [02:12<00:09,  7.54it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 933/1000 [02:12<00:09,  7.28it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 934/1000 [02:12<00:08,  7.37it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 935/1000 [02:12<00:09,  7.13it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 936/1000 [02:12<00:08,  7.29it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 937/1000 [02:12<00:08,  7.40it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 938/1000 [02:12<00:08,  7.46it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 939/1000 [02:13<00:08,  7.52it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 940/1000 [02:13<00:07,  7.56it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 941/1000 [02:13<00:07,  8.15it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 942/1000 [02:13<00:07,  8.01it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 943/1000 [02:13<00:07,  7.47it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 944/1000 [02:13<00:08,  6.73it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 945/1000 [02:13<00:07,  7.31it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 946/1000 [02:13<00:07,  7.11it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 947/1000 [02:14<00:07,  7.25it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 948/1000 [02:14<00:06,  7.88it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 949/1000 [02:14<00:07,  6.52it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 950/1000 [02:14<00:07,  6.81it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 951/1000 [02:14<00:07,  6.99it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 952/1000 [02:14<00:06,  7.20it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 954/1000 [02:15<00:05,  8.14it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 955/1000 [02:15<00:05,  7.99it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 956/1000 [02:15<00:05,  8.27it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 957/1000 [02:15<00:05,  8.06it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 958/1000 [02:15<00:05,  7.88it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 959/1000 [02:15<00:04,  8.25it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 960/1000 [02:15<00:04,  8.59it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 961/1000 [02:15<00:04,  8.31it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 962/1000 [02:16<00:04,  7.65it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 963/1000 [02:16<00:04,  8.04it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 964/1000 [02:16<00:05,  6.63it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 965/1000 [02:16<00:05,  6.86it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 966/1000 [02:16<00:04,  7.03it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 967/1000 [02:16<00:04,  6.92it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 968/1000 [02:16<00:04,  7.10it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 969/1000 [02:17<00:04,  7.20it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 970/1000 [02:17<00:04,  7.32it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 971/1000 [02:17<00:03,  7.79it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 972/1000 [02:17<00:03,  7.44it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 973/1000 [02:17<00:03,  7.44it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 974/1000 [02:17<00:03,  7.45it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 975/1000 [02:17<00:03,  7.91it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 976/1000 [02:17<00:02,  8.26it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 977/1000 [02:18<00:02,  7.74it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 978/1000 [02:18<00:02,  7.70it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 979/1000 [02:18<00:02,  7.35it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 980/1000 [02:18<00:02,  7.02it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 981/1000 [02:18<00:02,  7.16it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 982/1000 [02:18<00:02,  7.65it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 983/1000 [02:18<00:02,  7.60it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 984/1000 [02:19<00:02,  7.56it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 985/1000 [02:19<00:01,  7.57it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 986/1000 [02:19<00:01,  7.50it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 987/1000 [02:19<00:01,  7.53it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 988/1000 [02:19<00:01,  7.26it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 989/1000 [02:19<00:01,  7.35it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 990/1000 [02:19<00:01,  7.40it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 991/1000 [02:19<00:01,  7.46it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 992/1000 [02:20<00:01,  7.24it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 993/1000 [02:20<00:00,  7.34it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 994/1000 [02:20<00:00,  6.99it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 995/1000 [02:20<00:00,  6.88it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 996/1000 [02:20<00:00,  7.06it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 997/1000 [02:20<00:00,  7.24it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 998/1000 [02:20<00:00,  7.07it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 999/1000 [02:21<00:00,  6.93it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [02:21<00:00,  7.12it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [02:21<00:00,  7.08it/s]
DONE (10.11s)
DONE (7.76s)
Computing criterion:   0%|          | 0/20 [00:00<?, ?it/s]Computing criterion:   5%|â–Œ         | 1/20 [00:16<05:16, 16.64s/it]Computing criterion:  10%|â–ˆ         | 2/20 [00:33<05:04, 16.90s/it]Computing criterion:  15%|â–ˆâ–Œ        | 3/20 [00:51<04:51, 17.17s/it]Computing criterion:  20%|â–ˆâ–ˆ        | 4/20 [01:08<04:36, 17.25s/it]Computing criterion:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [01:25<04:16, 17.09s/it]Computing criterion:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [01:42<03:59, 17.12s/it]Computing criterion:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [02:00<03:44, 17.29s/it]Computing criterion:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [02:17<03:27, 17.30s/it]Computing criterion:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [02:35<03:12, 17.51s/it]Computing criterion:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [02:53<02:56, 17.66s/it]Computing criterion:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [03:11<02:38, 17.65s/it]Computing criterion:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [03:28<02:21, 17.65s/it]Computing criterion:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [03:46<02:03, 17.60s/it]Computing criterion:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [04:02<01:43, 17.33s/it]Computing criterion:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [04:20<01:26, 17.26s/it]Computing criterion:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [04:37<01:09, 17.35s/it]Computing criterion:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [04:54<00:51, 17.30s/it]Computing criterion:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [05:11<00:34, 17.18s/it]Computing criterion:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [05:28<00:17, 17.18s/it]Computing criterion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [05:46<00:00, 17.28s/it]Computing criterion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [05:46<00:00, 17.32s/it]
Ref scores:   0%|          | 0/1000 [00:00<?, ?it/s]Ref scores:   0%|          | 1/1000 [00:02<38:45,  2.33s/it]Ref scores:   0%|          | 2/1000 [00:02<17:16,  1.04s/it]Ref scores:   0%|          | 3/1000 [00:02<10:11,  1.63it/s]Ref scores:   0%|          | 4/1000 [00:02<07:07,  2.33it/s]Ref scores:   0%|          | 5/1000 [00:02<05:19,  3.11it/s]Ref scores:   1%|          | 6/1000 [00:02<04:15,  3.89it/s]Ref scores:   1%|          | 7/1000 [00:03<03:34,  4.64it/s]Ref scores:   1%|          | 8/1000 [00:03<03:07,  5.29it/s]Ref scores:   1%|          | 9/1000 [00:03<02:49,  5.86it/s]Ref scores:   1%|          | 10/1000 [00:03<02:36,  6.31it/s]Ref scores:   1%|          | 11/1000 [00:03<02:27,  6.70it/s]Ref scores:   1%|          | 12/1000 [00:03<02:41,  6.11it/s]Ref scores:   1%|â–         | 14/1000 [00:04<02:14,  7.31it/s]Ref scores:   2%|â–         | 15/1000 [00:04<02:12,  7.43it/s]Ref scores:   2%|â–         | 17/1000 [00:04<01:59,  8.25it/s]Ref scores:   2%|â–         | 18/1000 [00:04<02:01,  8.11it/s]Ref scores:   2%|â–         | 19/1000 [00:04<02:03,  7.96it/s]Ref scores:   2%|â–         | 20/1000 [00:04<01:58,  8.28it/s]Ref scores:   2%|â–         | 21/1000 [00:04<02:01,  8.06it/s]Ref scores:   2%|â–         | 22/1000 [00:05<02:02,  7.99it/s]Ref scores:   2%|â–         | 23/1000 [00:05<02:04,  7.84it/s]Ref scores:   2%|â–         | 24/1000 [00:05<02:04,  7.84it/s]Ref scores:   2%|â–Ž         | 25/1000 [00:05<02:05,  7.77it/s]Ref scores:   3%|â–Ž         | 26/1000 [00:05<02:05,  7.77it/s]Ref scores:   3%|â–Ž         | 27/1000 [00:05<02:05,  7.75it/s]Ref scores:   3%|â–Ž         | 28/1000 [00:05<02:05,  7.74it/s]Ref scores:   3%|â–Ž         | 29/1000 [00:05<02:06,  7.67it/s]Ref scores:   3%|â–Ž         | 30/1000 [00:06<01:59,  8.10it/s]Ref scores:   3%|â–Ž         | 31/1000 [00:06<02:02,  7.93it/s]Ref scores:   3%|â–Ž         | 32/1000 [00:06<02:03,  7.81it/s]Ref scores:   3%|â–Ž         | 33/1000 [00:06<02:03,  7.81it/s]Ref scores:   3%|â–Ž         | 34/1000 [00:06<01:55,  8.36it/s]Ref scores:   4%|â–Ž         | 35/1000 [00:06<01:58,  8.14it/s]Ref scores:   4%|â–Ž         | 37/1000 [00:06<01:39,  9.72it/s]Ref scores:   4%|â–         | 38/1000 [00:06<01:45,  9.09it/s]Ref scores:   4%|â–         | 39/1000 [00:07<01:51,  8.63it/s]Ref scores:   4%|â–         | 41/1000 [00:07<01:42,  9.34it/s]Ref scores:   4%|â–         | 42/1000 [00:07<01:53,  8.47it/s]Ref scores:   4%|â–         | 43/1000 [00:07<01:56,  8.25it/s]Ref scores:   4%|â–         | 44/1000 [00:07<01:59,  8.03it/s]Ref scores:   4%|â–         | 45/1000 [00:07<01:54,  8.35it/s]Ref scores:   5%|â–         | 46/1000 [00:07<01:57,  8.12it/s]Ref scores:   5%|â–         | 47/1000 [00:08<01:59,  7.96it/s]Ref scores:   5%|â–         | 48/1000 [00:08<02:01,  7.85it/s]Ref scores:   5%|â–         | 49/1000 [00:08<02:01,  7.81it/s]Ref scores:   5%|â–Œ         | 50/1000 [00:08<02:01,  7.79it/s]Ref scores:   5%|â–Œ         | 51/1000 [00:08<01:56,  8.17it/s]Ref scores:   5%|â–Œ         | 52/1000 [00:08<01:58,  8.00it/s]Ref scores:   5%|â–Œ         | 53/1000 [00:08<02:00,  7.86it/s]Ref scores:   5%|â–Œ         | 54/1000 [00:08<02:06,  7.47it/s]Ref scores:   6%|â–Œ         | 55/1000 [00:09<02:10,  7.24it/s]Ref scores:   6%|â–Œ         | 56/1000 [00:09<02:08,  7.35it/s]Ref scores:   6%|â–Œ         | 57/1000 [00:09<02:27,  6.39it/s]Ref scores:   6%|â–Œ         | 58/1000 [00:09<02:19,  6.73it/s]Ref scores:   6%|â–Œ         | 59/1000 [00:09<02:14,  7.00it/s]Ref scores:   6%|â–Œ         | 60/1000 [00:09<02:11,  7.16it/s]Ref scores:   6%|â–Œ         | 61/1000 [00:09<02:08,  7.32it/s]Ref scores:   6%|â–Œ         | 62/1000 [00:10<02:07,  7.36it/s]Ref scores:   6%|â–‹         | 63/1000 [00:10<02:05,  7.46it/s]Ref scores:   6%|â–‹         | 64/1000 [00:10<01:58,  7.91it/s]Ref scores:   6%|â–‹         | 65/1000 [00:10<01:59,  7.86it/s]Ref scores:   7%|â–‹         | 66/1000 [00:10<01:59,  7.81it/s]Ref scores:   7%|â–‹         | 67/1000 [00:10<01:59,  7.78it/s]Ref scores:   7%|â–‹         | 68/1000 [00:10<01:53,  8.20it/s]Ref scores:   7%|â–‹         | 70/1000 [00:11<01:46,  8.73it/s]Ref scores:   7%|â–‹         | 71/1000 [00:11<01:49,  8.45it/s]Ref scores:   7%|â–‹         | 72/1000 [00:11<01:53,  8.21it/s]Ref scores:   7%|â–‹         | 73/1000 [00:11<01:54,  8.09it/s]Ref scores:   7%|â–‹         | 74/1000 [00:11<01:55,  7.99it/s]Ref scores:   8%|â–Š         | 75/1000 [00:11<01:57,  7.89it/s]Ref scores:   8%|â–Š         | 76/1000 [00:11<01:58,  7.77it/s]Ref scores:   8%|â–Š         | 77/1000 [00:11<01:59,  7.74it/s]Ref scores:   8%|â–Š         | 78/1000 [00:12<02:00,  7.68it/s]Ref scores:   8%|â–Š         | 79/1000 [00:12<02:00,  7.67it/s]Ref scores:   8%|â–Š         | 80/1000 [00:12<02:34,  5.96it/s]Ref scores:   8%|â–Š         | 81/1000 [00:12<02:24,  6.37it/s]Ref scores:   8%|â–Š         | 82/1000 [00:12<02:08,  7.12it/s]Ref scores:   8%|â–Š         | 83/1000 [00:12<02:06,  7.24it/s]Ref scores:   8%|â–Š         | 85/1000 [00:13<01:53,  8.09it/s]Ref scores:   9%|â–Š         | 86/1000 [00:13<01:54,  7.96it/s]Ref scores:   9%|â–Š         | 87/1000 [00:13<01:55,  7.90it/s]Ref scores:   9%|â–‰         | 89/1000 [00:13<01:47,  8.49it/s]Ref scores:   9%|â–‰         | 90/1000 [00:13<01:55,  7.90it/s]Ref scores:   9%|â–‰         | 91/1000 [00:13<01:55,  7.85it/s]Ref scores:   9%|â–‰         | 92/1000 [00:13<01:55,  7.83it/s]Ref scores:   9%|â–‰         | 93/1000 [00:14<01:56,  7.81it/s]Ref scores:   9%|â–‰         | 94/1000 [00:14<01:56,  7.76it/s]Ref scores:  10%|â–‰         | 96/1000 [00:14<01:47,  8.44it/s]Ref scores:  10%|â–‰         | 97/1000 [00:14<01:54,  7.89it/s]Ref scores:  10%|â–‰         | 98/1000 [00:14<01:55,  7.83it/s]Ref scores:  10%|â–‰         | 99/1000 [00:14<01:48,  8.27it/s]Ref scores:  10%|â–ˆ         | 100/1000 [00:14<01:51,  8.08it/s]Ref scores:  10%|â–ˆ         | 101/1000 [00:15<01:47,  8.38it/s]Ref scores:  10%|â–ˆ         | 102/1000 [00:15<01:50,  8.12it/s]Ref scores:  10%|â–ˆ         | 103/1000 [00:15<01:46,  8.42it/s]Ref scores:  10%|â–ˆ         | 104/1000 [00:15<01:49,  8.20it/s]Ref scores:  10%|â–ˆ         | 105/1000 [00:15<02:10,  6.87it/s]Ref scores:  11%|â–ˆ         | 106/1000 [00:15<02:11,  6.81it/s]Ref scores:  11%|â–ˆ         | 107/1000 [00:15<02:06,  7.03it/s]Ref scores:  11%|â–ˆ         | 108/1000 [00:16<02:03,  7.23it/s]Ref scores:  11%|â–ˆ         | 109/1000 [00:16<02:00,  7.38it/s]Ref scores:  11%|â–ˆ         | 110/1000 [00:16<02:04,  7.16it/s]Ref scores:  11%|â–ˆ         | 111/1000 [00:16<02:02,  7.28it/s]Ref scores:  11%|â–ˆ         | 112/1000 [00:16<02:17,  6.45it/s]Ref scores:  11%|â–ˆâ–        | 114/1000 [00:16<01:56,  7.61it/s]Ref scores:  12%|â–ˆâ–        | 115/1000 [00:16<01:56,  7.59it/s]Ref scores:  12%|â–ˆâ–        | 116/1000 [00:17<01:56,  7.59it/s]Ref scores:  12%|â–ˆâ–        | 117/1000 [00:17<02:01,  7.26it/s]Ref scores:  12%|â–ˆâ–        | 118/1000 [00:17<01:59,  7.35it/s]Ref scores:  12%|â–ˆâ–        | 119/1000 [00:17<01:58,  7.46it/s]Ref scores:  12%|â–ˆâ–        | 120/1000 [00:17<01:57,  7.50it/s]Ref scores:  12%|â–ˆâ–        | 121/1000 [00:17<01:56,  7.57it/s]Ref scores:  12%|â–ˆâ–        | 122/1000 [00:17<01:56,  7.57it/s]Ref scores:  12%|â–ˆâ–        | 123/1000 [00:18<01:55,  7.61it/s]Ref scores:  12%|â–ˆâ–        | 124/1000 [00:18<01:47,  8.13it/s]Ref scores:  12%|â–ˆâ–Ž        | 125/1000 [00:18<01:49,  7.98it/s]Ref scores:  13%|â–ˆâ–Ž        | 126/1000 [00:18<01:51,  7.87it/s]Ref scores:  13%|â–ˆâ–Ž        | 127/1000 [00:18<01:51,  7.80it/s]Ref scores:  13%|â–ˆâ–Ž        | 128/1000 [00:18<01:52,  7.74it/s]Ref scores:  13%|â–ˆâ–Ž        | 129/1000 [00:18<01:46,  8.17it/s]Ref scores:  13%|â–ˆâ–Ž        | 130/1000 [00:18<01:54,  7.57it/s]Ref scores:  13%|â–ˆâ–Ž        | 131/1000 [00:19<01:54,  7.58it/s]Ref scores:  13%|â–ˆâ–Ž        | 132/1000 [00:19<01:54,  7.61it/s]Ref scores:  13%|â–ˆâ–Ž        | 133/1000 [00:19<01:48,  8.02it/s]Ref scores:  13%|â–ˆâ–Ž        | 134/1000 [00:19<01:49,  7.89it/s]Ref scores:  14%|â–ˆâ–Ž        | 135/1000 [00:19<02:15,  6.38it/s]Ref scores:  14%|â–ˆâ–Ž        | 136/1000 [00:19<02:02,  7.03it/s]Ref scores:  14%|â–ˆâ–Ž        | 137/1000 [00:19<02:00,  7.18it/s]Ref scores:  14%|â–ˆâ–        | 138/1000 [00:20<01:51,  7.70it/s]Ref scores:  14%|â–ˆâ–        | 139/1000 [00:20<01:52,  7.68it/s]Ref scores:  14%|â–ˆâ–        | 140/1000 [00:20<01:52,  7.67it/s]Ref scores:  14%|â–ˆâ–        | 141/1000 [00:20<01:52,  7.65it/s]Ref scores:  14%|â–ˆâ–        | 142/1000 [00:20<01:52,  7.65it/s]Ref scores:  14%|â–ˆâ–        | 143/1000 [00:20<01:51,  7.67it/s]Ref scores:  14%|â–ˆâ–        | 145/1000 [00:20<01:35,  8.93it/s]Ref scores:  15%|â–ˆâ–        | 146/1000 [00:20<01:43,  8.23it/s]Ref scores:  15%|â–ˆâ–        | 147/1000 [00:21<01:45,  8.06it/s]Ref scores:  15%|â–ˆâ–        | 148/1000 [00:21<01:47,  7.93it/s]Ref scores:  15%|â–ˆâ–        | 149/1000 [00:21<01:48,  7.85it/s]Ref scores:  15%|â–ˆâ–Œ        | 150/1000 [00:21<01:49,  7.76it/s]Ref scores:  15%|â–ˆâ–Œ        | 151/1000 [00:21<01:53,  7.45it/s]Ref scores:  15%|â–ˆâ–Œ        | 152/1000 [00:21<01:47,  7.93it/s]Ref scores:  15%|â–ˆâ–Œ        | 153/1000 [00:21<01:54,  7.40it/s]Ref scores:  15%|â–ˆâ–Œ        | 154/1000 [00:22<01:53,  7.43it/s]Ref scores:  16%|â–ˆâ–Œ        | 155/1000 [00:22<01:52,  7.52it/s]Ref scores:  16%|â–ˆâ–Œ        | 156/1000 [00:22<02:08,  6.58it/s]Ref scores:  16%|â–ˆâ–Œ        | 157/1000 [00:22<02:02,  6.89it/s]Ref scores:  16%|â–ˆâ–Œ        | 158/1000 [00:22<02:02,  6.85it/s]Ref scores:  16%|â–ˆâ–Œ        | 159/1000 [00:22<01:59,  7.06it/s]Ref scores:  16%|â–ˆâ–Œ        | 160/1000 [00:22<01:50,  7.59it/s]Ref scores:  16%|â–ˆâ–Œ        | 161/1000 [00:23<01:44,  8.02it/s]Ref scores:  16%|â–ˆâ–Œ        | 162/1000 [00:23<01:45,  7.92it/s]Ref scores:  16%|â–ˆâ–‹        | 163/1000 [00:23<01:47,  7.79it/s]Ref scores:  16%|â–ˆâ–‹        | 164/1000 [00:23<01:52,  7.41it/s]Ref scores:  16%|â–ˆâ–‹        | 165/1000 [00:23<01:51,  7.51it/s]Ref scores:  17%|â–ˆâ–‹        | 166/1000 [00:23<01:50,  7.53it/s]Ref scores:  17%|â–ˆâ–‹        | 167/1000 [00:23<01:49,  7.59it/s]Ref scores:  17%|â–ˆâ–‹        | 168/1000 [00:23<01:49,  7.60it/s]Ref scores:  17%|â–ˆâ–‹        | 169/1000 [00:24<01:48,  7.63it/s]Ref scores:  17%|â–ˆâ–‹        | 170/1000 [00:24<01:48,  7.62it/s]Ref scores:  17%|â–ˆâ–‹        | 171/1000 [00:24<01:48,  7.63it/s]Ref scores:  17%|â–ˆâ–‹        | 173/1000 [00:24<01:34,  8.75it/s]Ref scores:  17%|â–ˆâ–‹        | 174/1000 [00:24<01:37,  8.43it/s]Ref scores:  18%|â–ˆâ–Š        | 175/1000 [00:24<01:40,  8.22it/s]Ref scores:  18%|â–ˆâ–Š        | 176/1000 [00:24<01:42,  8.05it/s]Ref scores:  18%|â–ˆâ–Š        | 177/1000 [00:25<01:37,  8.45it/s]Ref scores:  18%|â–ˆâ–Š        | 178/1000 [00:25<01:40,  8.20it/s]Ref scores:  18%|â–ˆâ–Š        | 179/1000 [00:25<01:42,  8.03it/s]Ref scores:  18%|â–ˆâ–Š        | 180/1000 [00:25<01:37,  8.38it/s]Ref scores:  18%|â–ˆâ–Š        | 181/1000 [00:25<01:40,  8.15it/s]Ref scores:  18%|â–ˆâ–Š        | 182/1000 [00:25<01:42,  7.98it/s]Ref scores:  18%|â–ˆâ–Š        | 183/1000 [00:25<01:43,  7.88it/s]Ref scores:  18%|â–ˆâ–Š        | 184/1000 [00:25<01:48,  7.51it/s]Ref scores:  19%|â–ˆâ–Š        | 186/1000 [00:26<01:52,  7.26it/s]Ref scores:  19%|â–ˆâ–Š        | 187/1000 [00:26<01:50,  7.37it/s]Ref scores:  19%|â–ˆâ–‰        | 188/1000 [00:26<01:48,  7.47it/s]Ref scores:  19%|â–ˆâ–‰        | 189/1000 [00:26<01:47,  7.51it/s]Ref scores:  19%|â–ˆâ–‰        | 190/1000 [00:26<01:47,  7.52it/s]Ref scores:  19%|â–ˆâ–‰        | 191/1000 [00:26<01:47,  7.51it/s]Ref scores:  19%|â–ˆâ–‰        | 192/1000 [00:27<01:47,  7.52it/s]Ref scores:  19%|â–ˆâ–‰        | 193/1000 [00:27<01:46,  7.58it/s]Ref scores:  19%|â–ˆâ–‰        | 194/1000 [00:27<01:52,  7.19it/s]Ref scores:  20%|â–ˆâ–‰        | 195/1000 [00:27<01:50,  7.27it/s]Ref scores:  20%|â–ˆâ–‰        | 196/1000 [00:27<01:49,  7.36it/s]Ref scores:  20%|â–ˆâ–‰        | 198/1000 [00:27<01:38,  8.13it/s]Ref scores:  20%|â–ˆâ–ˆ        | 200/1000 [00:27<01:32,  8.61it/s]Ref scores:  20%|â–ˆâ–ˆ        | 201/1000 [00:28<01:35,  8.35it/s]Ref scores:  20%|â–ˆâ–ˆ        | 202/1000 [00:28<01:37,  8.17it/s]Ref scores:  20%|â–ˆâ–ˆ        | 203/1000 [00:28<01:39,  8.04it/s]Ref scores:  20%|â–ˆâ–ˆ        | 204/1000 [00:28<01:39,  7.97it/s]Ref scores:  20%|â–ˆâ–ˆ        | 205/1000 [00:28<01:41,  7.84it/s]Ref scores:  21%|â–ˆâ–ˆ        | 207/1000 [00:28<01:33,  8.50it/s]Ref scores:  21%|â–ˆâ–ˆ        | 208/1000 [00:29<01:38,  8.00it/s]Ref scores:  21%|â–ˆâ–ˆ        | 209/1000 [00:29<01:59,  6.60it/s]Ref scores:  21%|â–ˆâ–ˆ        | 210/1000 [00:29<01:55,  6.84it/s]Ref scores:  21%|â–ˆâ–ˆ        | 211/1000 [00:29<01:52,  7.01it/s]Ref scores:  21%|â–ˆâ–ˆ        | 212/1000 [00:29<01:44,  7.52it/s]Ref scores:  21%|â–ˆâ–ˆâ–       | 213/1000 [00:29<01:44,  7.54it/s]Ref scores:  21%|â–ˆâ–ˆâ–       | 214/1000 [00:29<01:49,  7.18it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 215/1000 [00:30<01:47,  7.32it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 216/1000 [00:30<01:46,  7.37it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 217/1000 [00:30<01:40,  7.83it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 218/1000 [00:30<01:44,  7.49it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 219/1000 [00:30<01:43,  7.52it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 220/1000 [00:30<01:43,  7.54it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 221/1000 [00:30<01:42,  7.60it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 222/1000 [00:30<01:42,  7.57it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 223/1000 [00:31<01:42,  7.57it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 224/1000 [00:31<01:42,  7.58it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 226/1000 [00:31<01:32,  8.38it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 227/1000 [00:31<01:28,  8.70it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 228/1000 [00:31<01:32,  8.34it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 229/1000 [00:31<01:39,  7.76it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 230/1000 [00:31<01:39,  7.71it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 231/1000 [00:32<01:33,  8.22it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 232/1000 [00:32<01:30,  8.49it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 233/1000 [00:32<01:33,  8.19it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 234/1000 [00:32<01:35,  8.03it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 235/1000 [00:32<01:37,  7.85it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 236/1000 [00:32<01:32,  8.22it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 237/1000 [00:32<01:35,  8.01it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 238/1000 [00:32<01:31,  8.34it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 240/1000 [00:33<01:20,  9.42it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 241/1000 [00:33<01:25,  8.92it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 242/1000 [00:33<01:28,  8.56it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 243/1000 [00:33<01:31,  8.32it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 244/1000 [00:33<01:32,  8.14it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 245/1000 [00:33<01:34,  7.98it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 246/1000 [00:33<01:30,  8.34it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 247/1000 [00:33<01:36,  7.82it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 248/1000 [00:34<01:36,  7.82it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 249/1000 [00:34<01:36,  7.82it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 250/1000 [00:34<01:51,  6.75it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 251/1000 [00:34<01:41,  7.36it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 252/1000 [00:34<01:40,  7.44it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 253/1000 [00:34<01:34,  7.89it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 254/1000 [00:34<01:35,  7.84it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 255/1000 [00:35<01:36,  7.75it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 256/1000 [00:35<01:36,  7.72it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 257/1000 [00:35<01:36,  7.71it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 258/1000 [00:35<01:30,  8.21it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 259/1000 [00:35<01:27,  8.49it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 260/1000 [00:35<01:30,  8.19it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 261/1000 [00:35<01:32,  8.00it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 262/1000 [00:35<01:33,  7.88it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 263/1000 [00:36<01:34,  7.84it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 264/1000 [00:36<01:35,  7.74it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 265/1000 [00:36<01:39,  7.39it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 266/1000 [00:36<01:38,  7.46it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 267/1000 [00:36<01:36,  7.57it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 268/1000 [00:36<01:36,  7.61it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 269/1000 [00:36<01:30,  8.07it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 270/1000 [00:36<01:26,  8.44it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 271/1000 [00:37<01:29,  8.16it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 272/1000 [00:37<01:34,  7.71it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 273/1000 [00:37<01:34,  7.73it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 274/1000 [00:37<01:33,  7.74it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 275/1000 [00:37<01:28,  8.18it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 276/1000 [00:37<01:24,  8.54it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 277/1000 [00:37<01:32,  7.80it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 278/1000 [00:37<01:28,  8.17it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 279/1000 [00:38<01:29,  8.06it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 280/1000 [00:38<01:30,  7.97it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 281/1000 [00:38<01:31,  7.88it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 282/1000 [00:38<01:36,  7.45it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 283/1000 [00:38<01:35,  7.51it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 285/1000 [00:38<01:22,  8.67it/s]Ref scores:  29%|â–ˆâ–ˆâ–Š       | 286/1000 [00:38<01:24,  8.43it/s]Ref scores:  29%|â–ˆâ–ˆâ–Š       | 287/1000 [00:39<01:26,  8.21it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 288/1000 [00:39<01:27,  8.10it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 289/1000 [00:39<01:29,  7.97it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 290/1000 [00:39<01:30,  7.89it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 291/1000 [00:39<01:30,  7.83it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 292/1000 [00:39<01:30,  7.80it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 293/1000 [00:39<01:31,  7.72it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 294/1000 [00:39<01:31,  7.72it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 295/1000 [00:40<01:31,  7.71it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 296/1000 [00:40<01:31,  7.66it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 298/1000 [00:40<01:18,  8.93it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 299/1000 [00:40<01:22,  8.54it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 300/1000 [00:40<01:24,  8.25it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 301/1000 [00:40<01:30,  7.75it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 302/1000 [00:40<01:30,  7.74it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 303/1000 [00:41<01:30,  7.69it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 304/1000 [00:41<01:30,  7.68it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 305/1000 [00:41<01:30,  7.70it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 306/1000 [00:41<01:30,  7.66it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 307/1000 [00:41<01:30,  7.63it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 308/1000 [00:41<01:30,  7.64it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 309/1000 [00:41<01:25,  8.05it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 310/1000 [00:41<01:27,  7.89it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 311/1000 [00:42<01:22,  8.39it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 312/1000 [00:42<01:19,  8.61it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆâ–      | 313/1000 [00:42<01:22,  8.29it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 315/1000 [00:42<01:19,  8.57it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 316/1000 [00:42<01:18,  8.73it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 317/1000 [00:42<01:16,  8.92it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 318/1000 [00:42<01:23,  8.21it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 319/1000 [00:43<01:24,  8.05it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 321/1000 [00:43<01:18,  8.67it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 322/1000 [00:43<01:20,  8.42it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 323/1000 [00:43<01:25,  7.96it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 324/1000 [00:43<01:25,  7.89it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 325/1000 [00:43<01:26,  7.82it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 326/1000 [00:43<01:26,  7.79it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 327/1000 [00:44<01:26,  7.75it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 328/1000 [00:44<01:27,  7.69it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 329/1000 [00:44<01:27,  7.67it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 330/1000 [00:44<01:27,  7.69it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 331/1000 [00:44<01:22,  8.11it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 332/1000 [00:44<01:23,  8.00it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 333/1000 [00:44<01:24,  7.86it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 334/1000 [00:44<01:24,  7.84it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 335/1000 [00:45<01:28,  7.49it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 336/1000 [00:45<01:27,  7.55it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 337/1000 [00:45<01:26,  7.64it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 338/1000 [00:45<01:26,  7.62it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 339/1000 [00:45<01:27,  7.59it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 340/1000 [00:45<01:26,  7.62it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 341/1000 [00:45<01:22,  7.99it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 342/1000 [00:45<01:23,  7.90it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 343/1000 [00:46<01:24,  7.79it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 344/1000 [00:46<01:24,  7.72it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 345/1000 [00:46<01:24,  7.74it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 346/1000 [00:46<01:25,  7.69it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 347/1000 [00:46<01:25,  7.66it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 348/1000 [00:46<01:25,  7.59it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 349/1000 [00:46<01:29,  7.30it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 350/1000 [00:47<01:24,  7.72it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 351/1000 [00:47<01:38,  6.61it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 352/1000 [00:47<01:29,  7.24it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 353/1000 [00:47<01:27,  7.36it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 354/1000 [00:47<01:31,  7.04it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 355/1000 [00:47<01:29,  7.17it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 356/1000 [00:47<01:28,  7.30it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 357/1000 [00:48<01:26,  7.40it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 358/1000 [00:48<01:25,  7.47it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 359/1000 [00:48<01:25,  7.48it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 360/1000 [00:48<01:25,  7.51it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 361/1000 [00:48<01:25,  7.51it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 362/1000 [00:48<01:28,  7.25it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 363/1000 [00:48<01:26,  7.34it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 364/1000 [00:48<01:25,  7.44it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 365/1000 [00:49<01:24,  7.48it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 366/1000 [00:49<01:24,  7.51it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 367/1000 [00:49<01:24,  7.50it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 368/1000 [00:49<01:23,  7.54it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 369/1000 [00:49<01:18,  8.08it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 370/1000 [00:49<01:19,  7.94it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 371/1000 [00:49<01:20,  7.80it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 372/1000 [00:49<01:24,  7.43it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 373/1000 [00:50<01:23,  7.52it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 375/1000 [00:50<01:15,  8.28it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 376/1000 [00:50<01:17,  8.10it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 377/1000 [00:50<01:18,  7.94it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 378/1000 [00:50<01:19,  7.85it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 379/1000 [00:50<01:19,  7.77it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 380/1000 [00:50<01:20,  7.74it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 381/1000 [00:51<01:20,  7.72it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 382/1000 [00:51<01:20,  7.72it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 383/1000 [00:51<01:20,  7.71it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 384/1000 [00:51<01:30,  6.84it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 386/1000 [00:51<01:18,  7.87it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 387/1000 [00:51<01:18,  7.81it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 388/1000 [00:52<01:14,  8.22it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 389/1000 [00:52<01:15,  8.06it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 390/1000 [00:52<01:17,  7.88it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 391/1000 [00:52<01:14,  8.21it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 392/1000 [00:52<01:10,  8.61it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 393/1000 [00:52<01:13,  8.24it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 394/1000 [00:52<01:11,  8.52it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 395/1000 [00:52<01:14,  8.17it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 396/1000 [00:53<01:18,  7.66it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 397/1000 [00:53<01:22,  7.34it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 398/1000 [00:53<01:21,  7.42it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 399/1000 [00:53<01:15,  7.97it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 400/1000 [00:53<01:12,  8.31it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 401/1000 [00:53<01:26,  6.90it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 402/1000 [00:53<01:35,  6.26it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 403/1000 [00:54<01:33,  6.35it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 404/1000 [00:54<01:38,  6.06it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 405/1000 [00:54<01:43,  5.77it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 406/1000 [00:54<01:35,  6.25it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 408/1000 [00:54<01:20,  7.40it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 409/1000 [00:54<01:15,  7.79it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 410/1000 [00:55<01:15,  7.77it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 411/1000 [00:55<01:15,  7.77it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 412/1000 [00:55<01:19,  7.44it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 413/1000 [00:55<01:18,  7.49it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 414/1000 [00:55<01:12,  8.04it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 415/1000 [00:55<01:13,  7.94it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 416/1000 [00:55<01:17,  7.57it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 417/1000 [00:55<01:16,  7.60it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 418/1000 [00:56<01:16,  7.59it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 419/1000 [00:56<01:16,  7.61it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 420/1000 [00:56<01:20,  7.20it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 421/1000 [00:56<01:18,  7.37it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 422/1000 [00:56<01:21,  7.09it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 423/1000 [00:56<01:22,  6.99it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 424/1000 [00:56<01:20,  7.19it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 425/1000 [00:57<01:18,  7.30it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 426/1000 [00:57<01:26,  6.60it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 427/1000 [00:57<01:23,  6.89it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 428/1000 [00:57<01:20,  7.13it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 429/1000 [00:57<01:18,  7.29it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 430/1000 [00:57<01:17,  7.40it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 431/1000 [00:57<01:16,  7.47it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 432/1000 [00:58<01:15,  7.53it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 433/1000 [00:58<01:14,  7.57it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 434/1000 [00:58<01:14,  7.56it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 436/1000 [00:58<01:10,  8.01it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 437/1000 [00:58<01:11,  7.88it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 438/1000 [00:58<01:12,  7.79it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 439/1000 [00:58<01:15,  7.47it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 440/1000 [00:59<01:14,  7.48it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 441/1000 [00:59<01:13,  7.55it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 442/1000 [00:59<01:10,  7.97it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 443/1000 [00:59<01:13,  7.54it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 444/1000 [00:59<01:13,  7.58it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 445/1000 [00:59<01:09,  7.98it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 446/1000 [00:59<01:10,  7.87it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 447/1000 [00:59<01:11,  7.75it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 448/1000 [01:00<01:11,  7.72it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 449/1000 [01:00<01:06,  8.24it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 451/1000 [01:00<01:05,  8.43it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 452/1000 [01:00<01:03,  8.59it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 453/1000 [01:00<01:05,  8.29it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 454/1000 [01:00<01:07,  8.05it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 455/1000 [01:00<01:08,  7.95it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 456/1000 [01:01<01:05,  8.30it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 457/1000 [01:01<01:10,  7.67it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 458/1000 [01:01<01:07,  8.04it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 459/1000 [01:01<01:08,  7.95it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 460/1000 [01:01<01:08,  7.88it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 461/1000 [01:01<01:08,  7.81it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 462/1000 [01:01<01:09,  7.79it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 463/1000 [01:01<01:09,  7.73it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 464/1000 [01:02<01:11,  7.45it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 465/1000 [01:02<01:11,  7.53it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 466/1000 [01:02<01:10,  7.61it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 467/1000 [01:02<01:20,  6.59it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 468/1000 [01:02<01:17,  6.84it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 469/1000 [01:02<01:15,  7.04it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 470/1000 [01:02<01:14,  7.16it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 471/1000 [01:03<01:12,  7.27it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 472/1000 [01:03<01:08,  7.74it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 473/1000 [01:03<01:08,  7.71it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 474/1000 [01:03<01:08,  7.71it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 475/1000 [01:03<01:08,  7.67it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 476/1000 [01:03<01:08,  7.61it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 477/1000 [01:03<01:09,  7.58it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 478/1000 [01:03<01:08,  7.60it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 479/1000 [01:04<01:11,  7.30it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 480/1000 [01:04<01:13,  7.08it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 481/1000 [01:04<01:14,  6.94it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 482/1000 [01:04<01:12,  7.16it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 483/1000 [01:04<01:07,  7.65it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 484/1000 [01:04<01:03,  8.18it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 485/1000 [01:04<00:59,  8.61it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 486/1000 [01:05<01:13,  7.04it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 487/1000 [01:05<01:06,  7.68it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 488/1000 [01:05<01:06,  7.69it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 489/1000 [01:05<01:06,  7.68it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 490/1000 [01:05<01:06,  7.66it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 491/1000 [01:05<01:09,  7.36it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 492/1000 [01:05<01:12,  7.03it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 493/1000 [01:05<01:07,  7.56it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 494/1000 [01:06<01:10,  7.18it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 495/1000 [01:06<01:09,  7.28it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 496/1000 [01:06<01:23,  6.03it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 497/1000 [01:06<01:18,  6.43it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 498/1000 [01:06<01:14,  6.75it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 499/1000 [01:06<01:11,  6.99it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 500/1000 [01:07<01:12,  6.92it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 501/1000 [01:07<01:10,  7.10it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 502/1000 [01:07<01:08,  7.25it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 503/1000 [01:07<01:07,  7.41it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 504/1000 [01:07<01:02,  7.90it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 505/1000 [01:07<01:06,  7.47it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 506/1000 [01:07<01:02,  7.93it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 507/1000 [01:07<01:02,  7.84it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 508/1000 [01:08<01:06,  7.43it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 509/1000 [01:08<01:16,  6.45it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 510/1000 [01:08<01:12,  6.76it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 511/1000 [01:08<01:12,  6.77it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 512/1000 [01:08<01:09,  6.98it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 513/1000 [01:08<01:08,  7.15it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 514/1000 [01:08<01:07,  7.25it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 515/1000 [01:09<01:05,  7.38it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 516/1000 [01:09<01:05,  7.44it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 517/1000 [01:09<01:06,  7.22it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 518/1000 [01:09<01:05,  7.34it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 519/1000 [01:09<01:05,  7.38it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 520/1000 [01:09<01:04,  7.43it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 521/1000 [01:09<01:04,  7.45it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 522/1000 [01:10<01:03,  7.48it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 523/1000 [01:10<00:59,  8.03it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 525/1000 [01:10<00:53,  8.95it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 526/1000 [01:10<00:57,  8.29it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 527/1000 [01:10<00:58,  8.09it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 528/1000 [01:10<01:01,  7.68it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 529/1000 [01:10<01:01,  7.68it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 530/1000 [01:11<01:01,  7.64it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 531/1000 [01:11<01:01,  7.60it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 532/1000 [01:11<01:02,  7.55it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 533/1000 [01:11<01:01,  7.56it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 534/1000 [01:11<01:03,  7.30it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 535/1000 [01:11<01:03,  7.38it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 536/1000 [01:11<00:59,  7.82it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 537/1000 [01:11<00:59,  7.79it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 538/1000 [01:12<00:56,  8.17it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 539/1000 [01:12<00:53,  8.60it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 540/1000 [01:12<00:55,  8.25it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 541/1000 [01:12<00:57,  8.04it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 543/1000 [01:12<00:50,  9.02it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 544/1000 [01:12<00:49,  9.22it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 545/1000 [01:12<00:49,  9.23it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 546/1000 [01:12<00:52,  8.70it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 547/1000 [01:13<00:50,  8.99it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 548/1000 [01:13<00:53,  8.50it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 549/1000 [01:13<00:51,  8.68it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 550/1000 [01:13<00:53,  8.34it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 551/1000 [01:13<01:11,  6.30it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 552/1000 [01:13<01:07,  6.63it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 553/1000 [01:13<01:04,  6.91it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 554/1000 [01:14<01:02,  7.12it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 555/1000 [01:14<01:01,  7.28it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 556/1000 [01:14<01:02,  7.10it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 557/1000 [01:14<01:01,  7.24it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 558/1000 [01:14<01:00,  7.33it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 559/1000 [01:14<00:59,  7.43it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 560/1000 [01:14<00:58,  7.47it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 561/1000 [01:14<00:58,  7.51it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 562/1000 [01:15<00:58,  7.53it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 563/1000 [01:15<00:58,  7.53it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 565/1000 [01:15<00:54,  8.04it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 566/1000 [01:15<00:54,  7.94it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 567/1000 [01:15<00:52,  8.27it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 568/1000 [01:15<00:50,  8.52it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 569/1000 [01:15<00:52,  8.23it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 570/1000 [01:16<00:53,  8.06it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 571/1000 [01:16<00:53,  7.96it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 572/1000 [01:16<00:50,  8.39it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 574/1000 [01:16<00:48,  8.86it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 575/1000 [01:16<00:56,  7.46it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 576/1000 [01:16<00:56,  7.47it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 577/1000 [01:17<00:56,  7.50it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 578/1000 [01:17<01:11,  5.93it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 579/1000 [01:17<01:08,  6.12it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 580/1000 [01:17<01:04,  6.47it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 581/1000 [01:17<01:02,  6.76it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 582/1000 [01:17<00:59,  7.00it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 584/1000 [01:18<00:52,  7.91it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 585/1000 [01:18<00:53,  7.80it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 586/1000 [01:18<00:53,  7.77it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 587/1000 [01:18<00:53,  7.68it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 588/1000 [01:18<00:53,  7.68it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 589/1000 [01:18<00:56,  7.26it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 590/1000 [01:18<00:55,  7.35it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 591/1000 [01:18<00:55,  7.41it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 592/1000 [01:19<00:54,  7.55it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 593/1000 [01:19<00:53,  7.61it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 594/1000 [01:19<00:50,  7.98it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 595/1000 [01:19<00:51,  7.83it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 596/1000 [01:19<00:49,  8.21it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 597/1000 [01:19<00:50,  7.99it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 598/1000 [01:19<00:51,  7.87it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 599/1000 [01:19<00:51,  7.77it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 600/1000 [01:20<00:53,  7.43it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 601/1000 [01:20<00:53,  7.46it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 602/1000 [01:20<00:55,  7.12it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 604/1000 [01:20<00:47,  8.42it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 605/1000 [01:20<00:50,  7.89it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 606/1000 [01:20<00:50,  7.79it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 607/1000 [01:21<00:50,  7.76it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 608/1000 [01:21<00:50,  7.73it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 609/1000 [01:21<00:50,  7.68it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 610/1000 [01:21<00:48,  8.06it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 611/1000 [01:21<00:46,  8.39it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 612/1000 [01:21<00:49,  7.82it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 613/1000 [01:21<01:07,  5.75it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 614/1000 [01:22<01:13,  5.25it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 615/1000 [01:22<01:06,  5.77it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 617/1000 [01:22<00:54,  6.97it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 618/1000 [01:22<00:53,  7.11it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 619/1000 [01:22<00:52,  7.25it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 620/1000 [01:22<00:49,  7.72it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 621/1000 [01:23<00:49,  7.70it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 622/1000 [01:23<00:49,  7.64it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 623/1000 [01:23<00:46,  8.14it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 624/1000 [01:23<00:47,  7.93it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 625/1000 [01:23<00:45,  8.27it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 626/1000 [01:23<00:46,  8.06it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 627/1000 [01:23<00:47,  7.92it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 628/1000 [01:23<00:47,  7.81it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 629/1000 [01:24<00:47,  7.75it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 631/1000 [01:24<00:41,  8.97it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 632/1000 [01:24<00:40,  9.01it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 633/1000 [01:24<00:42,  8.59it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 634/1000 [01:24<00:43,  8.32it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 635/1000 [01:24<00:45,  8.09it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 636/1000 [01:24<00:45,  7.92it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 637/1000 [01:24<00:43,  8.28it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 639/1000 [01:25<00:42,  8.49it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 640/1000 [01:25<00:43,  8.23it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 641/1000 [01:25<00:44,  8.07it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 642/1000 [01:25<00:44,  7.97it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 643/1000 [01:25<00:45,  7.86it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 644/1000 [01:25<00:45,  7.77it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 645/1000 [01:25<00:43,  8.18it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 646/1000 [01:26<00:44,  7.95it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 647/1000 [01:26<00:47,  7.46it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 648/1000 [01:26<00:47,  7.48it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 649/1000 [01:26<00:43,  8.03it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 650/1000 [01:26<00:41,  8.34it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 652/1000 [01:26<00:39,  8.84it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 653/1000 [01:26<00:38,  9.05it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 654/1000 [01:27<00:40,  8.57it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 656/1000 [01:27<00:39,  8.67it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 657/1000 [01:27<00:40,  8.41it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 658/1000 [01:27<00:43,  7.80it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 659/1000 [01:27<00:44,  7.74it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 660/1000 [01:27<00:41,  8.22it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 661/1000 [01:27<00:43,  7.74it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 662/1000 [01:28<00:43,  7.69it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 663/1000 [01:28<00:44,  7.62it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 664/1000 [01:28<00:43,  7.64it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 665/1000 [01:28<00:50,  6.64it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 666/1000 [01:28<00:46,  7.24it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 667/1000 [01:28<00:47,  7.06it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 668/1000 [01:28<00:46,  7.17it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 669/1000 [01:29<00:45,  7.29it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 670/1000 [01:29<00:46,  7.08it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 671/1000 [01:29<00:43,  7.58it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 672/1000 [01:29<00:43,  7.60it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 673/1000 [01:29<00:43,  7.60it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 674/1000 [01:29<00:40,  8.06it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 675/1000 [01:29<00:38,  8.50it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 676/1000 [01:29<00:39,  8.16it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 677/1000 [01:30<00:40,  7.97it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 678/1000 [01:30<00:40,  7.87it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 679/1000 [01:30<00:41,  7.74it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 680/1000 [01:30<00:41,  7.69it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 681/1000 [01:30<00:43,  7.37it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 682/1000 [01:30<00:42,  7.47it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 683/1000 [01:30<00:40,  7.92it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 684/1000 [01:30<00:40,  7.79it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 685/1000 [01:31<00:38,  8.20it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 686/1000 [01:31<00:39,  8.01it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 687/1000 [01:31<00:39,  7.90it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 688/1000 [01:31<00:39,  7.81it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 689/1000 [01:31<00:40,  7.72it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 690/1000 [01:31<00:40,  7.68it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 691/1000 [01:31<00:37,  8.25it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 693/1000 [01:32<00:35,  8.71it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 694/1000 [01:32<00:36,  8.38it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 695/1000 [01:32<00:37,  8.14it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 696/1000 [01:32<00:38,  7.96it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 697/1000 [01:32<00:38,  7.85it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 698/1000 [01:32<00:38,  7.78it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 699/1000 [01:32<00:40,  7.42it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 700/1000 [01:32<00:40,  7.49it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 701/1000 [01:33<00:39,  7.52it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 702/1000 [01:33<00:40,  7.37it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 703/1000 [01:33<00:40,  7.42it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 704/1000 [01:33<00:39,  7.48it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 705/1000 [01:33<00:40,  7.22it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 706/1000 [01:33<00:41,  7.03it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 707/1000 [01:33<00:38,  7.69it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 708/1000 [01:34<00:38,  7.65it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 709/1000 [01:34<00:37,  7.66it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 710/1000 [01:34<00:35,  8.06it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 711/1000 [01:34<00:33,  8.53it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 712/1000 [01:34<00:36,  7.86it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 713/1000 [01:34<00:38,  7.48it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 714/1000 [01:34<00:37,  7.54it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 715/1000 [01:34<00:39,  7.27it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 716/1000 [01:35<00:38,  7.32it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 718/1000 [01:35<00:35,  7.88it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 719/1000 [01:35<00:35,  7.84it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 720/1000 [01:35<00:36,  7.74it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 721/1000 [01:35<00:36,  7.74it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 722/1000 [01:35<00:36,  7.69it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 723/1000 [01:35<00:36,  7.63it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 724/1000 [01:36<00:36,  7.64it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 725/1000 [01:36<00:33,  8.18it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 726/1000 [01:36<00:34,  7.98it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 727/1000 [01:36<00:36,  7.52it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 728/1000 [01:36<00:35,  7.58it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 729/1000 [01:36<00:35,  7.58it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 730/1000 [01:36<00:35,  7.63it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 731/1000 [01:36<00:33,  8.13it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 732/1000 [01:37<00:33,  7.94it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 733/1000 [01:37<00:35,  7.51it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 734/1000 [01:37<00:35,  7.52it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 735/1000 [01:37<00:35,  7.52it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 736/1000 [01:37<00:33,  7.95it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 737/1000 [01:37<00:34,  7.55it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 738/1000 [01:37<00:32,  8.11it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 739/1000 [01:38<00:33,  7.87it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 740/1000 [01:38<00:35,  7.38it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 741/1000 [01:38<00:34,  7.42it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 743/1000 [01:38<00:30,  8.29it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 744/1000 [01:38<00:32,  7.84it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 745/1000 [01:38<00:32,  7.79it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 746/1000 [01:38<00:32,  7.73it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 747/1000 [01:39<00:32,  7.68it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 748/1000 [01:39<00:32,  7.66it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 749/1000 [01:39<00:32,  7.64it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 750/1000 [01:39<00:30,  8.20it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 751/1000 [01:39<00:31,  7.94it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 752/1000 [01:39<00:31,  7.83it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 753/1000 [01:39<00:31,  7.74it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 754/1000 [01:39<00:31,  7.69it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 755/1000 [01:40<00:32,  7.63it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 756/1000 [01:40<00:33,  7.35it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 757/1000 [01:40<00:32,  7.41it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 758/1000 [01:40<00:32,  7.45it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 759/1000 [01:40<00:32,  7.50it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 760/1000 [01:40<00:31,  7.53it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 761/1000 [01:41<00:38,  6.16it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 762/1000 [01:41<00:36,  6.56it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 763/1000 [01:41<00:34,  6.83it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 764/1000 [01:41<00:34,  6.80it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 765/1000 [01:41<00:33,  7.01it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 766/1000 [01:41<00:34,  6.80it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 767/1000 [01:41<00:37,  6.19it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 768/1000 [01:42<00:35,  6.56it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 770/1000 [01:42<00:29,  7.68it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 771/1000 [01:42<00:29,  7.67it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 772/1000 [01:42<00:30,  7.43it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 773/1000 [01:42<00:33,  6.76it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 774/1000 [01:42<00:32,  6.99it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 775/1000 [01:42<00:31,  7.13it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 776/1000 [01:43<00:30,  7.25it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 777/1000 [01:43<00:30,  7.31it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 778/1000 [01:43<00:28,  7.79it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 779/1000 [01:43<00:28,  7.77it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 780/1000 [01:43<00:28,  7.66it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 781/1000 [01:43<00:28,  7.65it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 782/1000 [01:43<00:28,  7.57it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 783/1000 [01:43<00:27,  7.98it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 784/1000 [01:44<00:25,  8.35it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 785/1000 [01:44<00:26,  8.12it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 786/1000 [01:44<00:27,  7.88it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 787/1000 [01:44<00:25,  8.37it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 788/1000 [01:44<00:26,  8.06it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 789/1000 [01:44<00:27,  7.60it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 790/1000 [01:44<00:27,  7.58it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 791/1000 [01:45<00:27,  7.56it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 792/1000 [01:45<00:25,  8.14it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 793/1000 [01:45<00:27,  7.62it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 794/1000 [01:45<00:27,  7.57it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 795/1000 [01:45<00:27,  7.54it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 796/1000 [01:45<00:26,  7.58it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 797/1000 [01:45<00:26,  7.61it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 798/1000 [01:45<00:26,  7.54it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 799/1000 [01:46<00:26,  7.54it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 800/1000 [01:46<00:24,  8.06it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 801/1000 [01:46<00:23,  8.38it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 802/1000 [01:46<00:22,  8.66it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 803/1000 [01:46<00:24,  8.00it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 805/1000 [01:46<00:22,  8.54it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 806/1000 [01:46<00:22,  8.79it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 807/1000 [01:46<00:22,  8.47it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 808/1000 [01:47<00:23,  8.20it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 809/1000 [01:47<00:23,  8.03it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 810/1000 [01:47<00:24,  7.88it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 811/1000 [01:47<00:24,  7.79it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 813/1000 [01:47<00:21,  8.54it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 814/1000 [01:47<00:22,  8.27it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 815/1000 [01:47<00:22,  8.07it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 816/1000 [01:48<00:24,  7.66it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 817/1000 [01:48<00:23,  7.63it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 818/1000 [01:48<00:23,  7.60it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 820/1000 [01:48<00:24,  7.29it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 821/1000 [01:48<00:24,  7.32it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 822/1000 [01:48<00:24,  7.38it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 823/1000 [01:49<00:23,  7.45it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 824/1000 [01:49<00:22,  7.85it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 825/1000 [01:49<00:21,  8.20it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 826/1000 [01:49<00:21,  8.01it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 827/1000 [01:49<00:22,  7.84it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 828/1000 [01:49<00:22,  7.72it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 829/1000 [01:49<00:22,  7.73it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 830/1000 [01:49<00:22,  7.67it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 831/1000 [01:50<00:22,  7.36it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 832/1000 [01:50<00:22,  7.46it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 833/1000 [01:50<00:23,  7.13it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 834/1000 [01:50<00:22,  7.28it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 836/1000 [01:50<00:18,  9.03it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 837/1000 [01:50<00:17,  9.19it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 838/1000 [01:50<00:18,  8.70it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 839/1000 [01:51<00:19,  8.37it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 840/1000 [01:51<00:20,  7.71it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 841/1000 [01:51<00:20,  7.70it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 842/1000 [01:51<00:20,  7.71it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 843/1000 [01:51<00:20,  7.69it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 844/1000 [01:51<00:18,  8.22it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 845/1000 [01:51<00:19,  7.93it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 846/1000 [01:51<00:19,  7.76it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 847/1000 [01:52<00:19,  7.71it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 848/1000 [01:52<00:18,  8.07it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 849/1000 [01:52<00:20,  7.54it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 850/1000 [01:52<00:19,  7.56it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 851/1000 [01:52<00:19,  7.56it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 852/1000 [01:52<00:19,  7.51it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 853/1000 [01:52<00:19,  7.44it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 854/1000 [01:53<00:19,  7.43it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 855/1000 [01:53<00:18,  7.83it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 856/1000 [01:53<00:17,  8.14it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 858/1000 [01:53<00:16,  8.63it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 859/1000 [01:53<00:15,  8.85it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 860/1000 [01:53<00:15,  8.91it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 861/1000 [01:53<00:16,  8.44it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 862/1000 [01:53<00:16,  8.59it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 863/1000 [01:54<00:15,  8.70it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 864/1000 [01:54<00:15,  8.81it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 865/1000 [01:54<00:16,  8.17it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 866/1000 [01:54<00:16,  7.98it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 867/1000 [01:54<00:16,  8.25it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 868/1000 [01:54<00:17,  7.55it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 869/1000 [01:54<00:17,  7.52it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 870/1000 [01:54<00:17,  7.51it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 871/1000 [01:55<00:17,  7.50it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 873/1000 [01:55<00:15,  8.31it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 875/1000 [01:55<00:13,  9.09it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 876/1000 [01:55<00:13,  9.06it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 877/1000 [01:55<00:14,  8.60it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 878/1000 [01:55<00:13,  8.84it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 879/1000 [01:55<00:14,  8.45it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 880/1000 [01:56<00:14,  8.18it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 882/1000 [01:56<00:14,  8.28it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 883/1000 [01:56<00:15,  7.78it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 884/1000 [01:56<00:15,  7.68it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 885/1000 [01:56<00:15,  7.61it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 886/1000 [01:56<00:15,  7.60it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 887/1000 [01:57<00:15,  7.28it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 888/1000 [01:57<00:15,  7.03it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 889/1000 [01:57<00:15,  7.14it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 890/1000 [01:57<00:15,  7.22it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 892/1000 [01:57<00:13,  8.05it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 893/1000 [01:57<00:13,  7.87it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 894/1000 [01:57<00:13,  7.78it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 895/1000 [01:58<00:13,  7.66it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 896/1000 [01:58<00:13,  7.61it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 897/1000 [01:58<00:14,  7.16it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 898/1000 [01:58<00:14,  7.19it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 899/1000 [01:58<00:13,  7.27it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 900/1000 [01:58<00:13,  7.32it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 901/1000 [01:58<00:14,  7.07it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 902/1000 [01:59<00:13,  7.15it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 903/1000 [01:59<00:13,  6.96it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 904/1000 [01:59<00:13,  7.07it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 905/1000 [01:59<00:12,  7.72it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 906/1000 [01:59<00:12,  7.58it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 907/1000 [01:59<00:11,  7.98it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 908/1000 [01:59<00:11,  7.81it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 909/1000 [01:59<00:11,  7.65it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 910/1000 [02:00<00:11,  7.61it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 911/1000 [02:00<00:11,  7.52it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 912/1000 [02:00<00:11,  7.50it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 913/1000 [02:00<00:11,  7.48it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 914/1000 [02:00<00:11,  7.47it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 915/1000 [02:00<00:11,  7.44it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 916/1000 [02:00<00:11,  7.43it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 917/1000 [02:01<00:11,  7.41it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 918/1000 [02:01<00:11,  7.37it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 919/1000 [02:01<00:10,  7.39it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 920/1000 [02:01<00:10,  7.41it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 921/1000 [02:01<00:10,  7.40it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 922/1000 [02:01<00:10,  7.40it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 923/1000 [02:01<00:10,  7.13it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 924/1000 [02:02<00:12,  6.31it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 925/1000 [02:02<00:10,  6.93it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 926/1000 [02:02<00:13,  5.56it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 928/1000 [02:02<00:10,  6.85it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 929/1000 [02:02<00:10,  7.01it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 930/1000 [02:02<00:10,  6.89it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 931/1000 [02:03<00:09,  7.03it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 932/1000 [02:03<00:09,  7.52it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 934/1000 [02:03<00:07,  8.63it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 935/1000 [02:03<00:07,  8.73it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 936/1000 [02:03<00:07,  8.38it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 937/1000 [02:03<00:07,  8.09it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 938/1000 [02:03<00:07,  7.86it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 939/1000 [02:04<00:07,  7.78it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 940/1000 [02:04<00:08,  7.36it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 941/1000 [02:04<00:08,  7.36it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 942/1000 [02:04<00:07,  7.41it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 943/1000 [02:04<00:07,  7.40it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 944/1000 [02:04<00:07,  7.43it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 945/1000 [02:04<00:07,  7.41it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 946/1000 [02:05<00:07,  7.41it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 947/1000 [02:05<00:07,  7.40it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 948/1000 [02:05<00:07,  7.03it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 949/1000 [02:05<00:07,  7.13it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 950/1000 [02:05<00:07,  6.96it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 951/1000 [02:05<00:06,  7.60it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 952/1000 [02:05<00:06,  7.95it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 953/1000 [02:05<00:06,  7.81it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 954/1000 [02:06<00:06,  7.27it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 955/1000 [02:06<00:06,  7.36it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 956/1000 [02:06<00:05,  7.82it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 957/1000 [02:06<00:05,  7.40it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 958/1000 [02:06<00:05,  7.43it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 959/1000 [02:06<00:05,  7.46it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 960/1000 [02:06<00:05,  7.44it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 961/1000 [02:07<00:06,  6.42it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 962/1000 [02:07<00:05,  6.69it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 963/1000 [02:07<00:05,  6.91it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 964/1000 [02:07<00:04,  7.43it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 965/1000 [02:07<00:04,  7.42it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 966/1000 [02:07<00:04,  7.42it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 967/1000 [02:07<00:04,  7.46it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 968/1000 [02:08<00:04,  7.42it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 969/1000 [02:08<00:04,  7.06it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 970/1000 [02:08<00:04,  7.18it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 971/1000 [02:08<00:03,  7.29it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 972/1000 [02:08<00:04,  6.42it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 973/1000 [02:08<00:03,  7.02it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 974/1000 [02:08<00:03,  7.14it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 975/1000 [02:09<00:03,  6.47it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 976/1000 [02:09<00:03,  6.73it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 977/1000 [02:09<00:03,  7.38it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 978/1000 [02:09<00:02,  7.34it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 979/1000 [02:09<00:03,  6.07it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 980/1000 [02:09<00:03,  5.68it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 981/1000 [02:10<00:03,  6.11it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 982/1000 [02:10<00:02,  6.11it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 983/1000 [02:10<00:02,  6.45it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 985/1000 [02:10<00:01,  7.55it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 986/1000 [02:10<00:01,  7.54it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 987/1000 [02:10<00:01,  7.51it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 988/1000 [02:10<00:01,  7.49it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 989/1000 [02:11<00:01,  7.45it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 991/1000 [02:11<00:01,  8.10it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 992/1000 [02:11<00:01,  7.97it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 993/1000 [02:11<00:00,  7.81it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 994/1000 [02:11<00:00,  7.70it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 995/1000 [02:11<00:00,  7.62it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 996/1000 [02:11<00:00,  7.57it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 997/1000 [02:12<00:00,  8.03it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 998/1000 [02:12<00:00,  8.28it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 999/1000 [02:12<00:00,  7.99it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [02:12<00:00,  7.84it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [02:12<00:00,  7.55it/s]
DONE (2.25s)
DONE (7.56s)
loss_threshold ROC AUC: 0.49547700000000006, PR AUC: 0.49837822323365727, tpr_at_low_fpr: {0.001: 0.001, 0.01: 0.014}
min_k_threshold ROC AUC: 0.495435, PR AUC: 0.49595680790649077, tpr_at_low_fpr: {0.001: 0.001, 0.01: 0.014}
zlib_threshold ROC AUC: 0.501189, PR AUC: 0.4998934828203725, tpr_at_low_fpr: {0.001: 0.005, 0.01: 0.014}
ref-stablelm-base-alpha-3b-v2_threshold ROC AUC: 0.49009899999999995, PR AUC: 0.4970096669824833, tpr_at_low_fpr: {0.001: 0.005, 0.01: 0.019}
loss_threshold roc_auc: 0.495
min_k_threshold roc_auc: 0.495
zlib_threshold roc_auc: 0.501
ref-stablelm-base-alpha-3b-v2_threshold roc_auc: 0.490
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
results_new/neo125_github_experiment/meta-llama_Llama-2-70b-hf/pile_cc_ngram_13_<0.8_truncated
Saving results to absolute path: /home/chawins/mimir/tmp_results/neo125_github_experiment/meta-llama_Llama-2-70b-hf/pile_cc_ngram_13_<0.8_truncated
LOG: cache_dir is /home/chawins/.cache
Using cache dir /home/chawins/.cache
Loading BASE model meta-llama/Llama-2-70b-hf...
Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.embed_tokens.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:   7%|â–‹         | 1/15 [00:00<00:03,  4.60it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  13%|â–ˆâ–Ž        | 2/15 [00:00<00:02,  4.63it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 3/15 [00:00<00:02,  4.58it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:00<00:02,  4.63it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:01<00:02,  4.63it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:01<00:01,  4.69it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:01<00:01,  4.75it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:01<00:01,  4.77it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:01<00:01,  4.73it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:02<00:01,  4.69it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:02<00:00,  4.73it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:02<00:00,  4.70it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:02<00:00,  4.60it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:03<00:00,  4.56it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for lm_head.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:03<00:00,  4.63it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:03<00:00,  4.66it/s]
  0%|          | 0/57 [00:00<?, ?w/s]model.embed_tokens.weight:   0%|          | 0/57 [00:00<?, ?w/s, dev=0]model.embed_tokens.weight:   2%|â–         | 1/57 [00:00<00:10,  5.32w/s, dev=0]model.layers.0.input_layernorm.weight:   2%|â–         | 1/57 [00:00<00:10,  5.32w/s, dev=0]model.layers.0.mlp.down_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:05, 10.61w/s, dev=0]  model.layers.0.mlp.down_proj.weight:   5%|â–Œ         | 3/57 [00:00<00:06,  8.79w/s, dev=0]model.layers.0.mlp.gate_proj.weight:   5%|â–Œ         | 3/57 [00:00<00:06,  8.78w/s, dev=0]model.layers.0.mlp.up_proj.weight:   7%|â–‹         | 4/57 [00:00<00:06,  7.95w/s, dev=0]  model.layers.0.mlp.up_proj.weight:   9%|â–‰         | 5/57 [00:00<00:06,  7.64w/s, dev=0]model.layers.0.post_attention_layernorm.weight:   9%|â–‰         | 5/57 [00:00<00:06,  7.63w/s, dev=0]model.layers.0.self_attn.k_proj.weight:  11%|â–ˆ         | 6/57 [00:00<00:05,  9.15w/s, dev=0]        model.layers.0.self_attn.o_proj.weight:  12%|â–ˆâ–        | 7/57 [00:00<00:04, 10.57w/s, dev=0]model.layers.0.self_attn.q_proj.weight:  14%|â–ˆâ–        | 8/57 [00:00<00:04, 11.23w/s, dev=0]model.layers.0.self_attn.q_proj.weight:  16%|â–ˆâ–Œ        | 9/57 [00:00<00:04, 11.83w/s, dev=0]model.layers.0.self_attn.rotary_emb.inv_freq:  16%|â–ˆâ–Œ        | 9/57 [00:00<00:04, 11.83w/s, dev=0]model.layers.0.self_attn.v_proj.weight:  18%|â–ˆâ–Š        | 10/57 [00:00<00:03, 13.14w/s, dev=0]     model.layers.1.input_layernorm.weight:  19%|â–ˆâ–‰        | 11/57 [00:00<00:03, 14.31w/s, dev=0] model.layers.1.mlp.down_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:00<00:02, 15.60w/s, dev=0]  model.layers.1.mlp.down_proj.weight:  23%|â–ˆâ–ˆâ–Ž       | 13/57 [00:00<00:03, 13.79w/s, dev=0]model.layers.1.mlp.gate_proj.weight:  23%|â–ˆâ–ˆâ–Ž       | 13/57 [00:00<00:03, 13.79w/s, dev=0]model.layers.1.mlp.up_proj.weight:  25%|â–ˆâ–ˆâ–       | 14/57 [00:01<00:03, 12.53w/s, dev=0]  model.layers.1.post_attention_layernorm.weight:  26%|â–ˆâ–ˆâ–‹       | 15/57 [00:01<00:03, 11.63w/s, dev=0]model.layers.1.self_attn.k_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:01<00:03, 12.40w/s, dev=0]        model.layers.1.self_attn.k_proj.weight:  30%|â–ˆâ–ˆâ–‰       | 17/57 [00:01<00:03, 13.10w/s, dev=0]model.layers.1.self_attn.o_proj.weight:  30%|â–ˆâ–ˆâ–‰       | 17/57 [00:01<00:03, 13.10w/s, dev=0]model.layers.1.self_attn.q_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 18/57 [00:01<00:02, 13.39w/s, dev=0]model.layers.1.self_attn.rotary_emb.inv_freq:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 19/57 [00:01<00:02, 13.65w/s, dev=0]model.layers.1.self_attn.v_proj.weight:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 20/57 [00:01<00:02, 14.37w/s, dev=0]      model.layers.1.self_attn.v_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 21/57 [00:01<00:02, 15.02w/s, dev=0]model.layers.2.input_layernorm.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 21/57 [00:01<00:02, 15.02w/s, dev=0] model.layers.2.mlp.down_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:01<00:02, 15.73w/s, dev=0]  model.layers.2.mlp.gate_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 23/57 [00:01<00:02, 14.79w/s, dev=0]model.layers.2.mlp.up_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/57 [00:01<00:02, 14.07w/s, dev=0]  model.layers.2.mlp.up_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/57 [00:01<00:02, 13.35w/s, dev=0]model.layers.2.post_attention_layernorm.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/57 [00:01<00:02, 13.35w/s, dev=0]model.layers.2.self_attn.k_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 26/57 [00:01<00:02, 13.88w/s, dev=0]        model.layers.2.self_attn.o_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 27/57 [00:01<00:02, 14.36w/s, dev=0]model.layers.2.self_attn.q_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:01<00:01, 14.54w/s, dev=0]model.layers.2.self_attn.rotary_emb.inv_freq:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 29/57 [00:01<00:01, 14.71w/s, dev=0]model.layers.2.self_attn.v_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 30/57 [00:01<00:01, 15.22w/s, dev=0]      model.layers.2.self_attn.v_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 31/57 [00:01<00:01, 15.68w/s, dev=0]model.layers.3.input_layernorm.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 31/57 [00:01<00:01, 15.68w/s, dev=0] model.layers.3.mlp.down_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 32/57 [00:01<00:01, 16.18w/s, dev=0]  model.layers.3.mlp.gate_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 33/57 [00:02<00:01, 15.47w/s, dev=0]model.layers.3.mlp.up_proj.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:02<00:01, 14.82w/s, dev=0]  model.layers.3.post_attention_layernorm.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/57 [00:02<00:01, 14.25w/s, dev=0]model.layers.3.self_attn.k_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 36/57 [00:02<00:01, 14.66w/s, dev=0]        model.layers.3.self_attn.k_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 37/57 [00:02<00:01, 15.03w/s, dev=0]model.layers.3.self_attn.o_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 37/57 [00:02<00:01, 15.02w/s, dev=0]model.layers.3.self_attn.q_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 38/57 [00:02<00:01, 15.15w/s, dev=0]model.layers.3.self_attn.rotary_emb.inv_freq:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 39/57 [00:02<00:01, 15.27w/s, dev=0]model.layers.3.self_attn.v_proj.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:02<00:01, 15.66w/s, dev=0]      model.layers.4.input_layernorm.weight:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/57 [00:02<00:00, 16.01w/s, dev=0] model.layers.4.mlp.down_proj.weight:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 42/57 [00:02<00:00, 16.40w/s, dev=0]  model.layers.4.mlp.down_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 43/57 [00:02<00:00, 15.83w/s, dev=0]model.layers.4.mlp.gate_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 43/57 [00:02<00:00, 15.83w/s, dev=0]model.layers.4.mlp.up_proj.weight:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 44/57 [00:02<00:00, 15.30w/s, dev=0]  model.layers.4.post_attention_layernorm.weight:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 45/57 [00:03<00:00, 14.84w/s, dev=0]model.layers.4.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:03<00:00, 15.16w/s, dev=0]        model.layers.4.self_attn.o_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 47/57 [00:03<00:00, 15.46w/s, dev=0]model.layers.4.self_attn.q_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/57 [00:03<00:00, 15.56w/s, dev=0]model.layers.4.self_attn.q_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 49/57 [00:03<00:00, 15.65w/s, dev=0]model.layers.4.self_attn.rotary_emb.inv_freq:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 49/57 [00:03<00:00, 15.64w/s, dev=0]model.layers.4.self_attn.v_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 50/57 [00:03<00:00, 15.96w/s, dev=0]      model.layers.5.mlp.gate_proj.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 51/57 [00:03<00:00, 16.25w/s, dev=0]   model.layers.5.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:03<00:00, 15.70w/s, dev=0]model.layers.5.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 53/57 [00:03<00:00, 15.96w/s, dev=0]model.layers.5.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/57 [00:03<00:00, 16.01w/s, dev=0]model.layers.5.self_attn.q_proj.weight:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 55/57 [00:03<00:00, 16.07w/s, dev=0]model.layers.5.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 55/57 [00:03<00:00, 16.07w/s, dev=0]model.layers.5.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 56/57 [00:03<00:00, 16.36w/s, dev=0]                                                                                                     0%|          | 0/59 [00:00<?, ?w/s]model.layers.5.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=0]model.layers.5.mlp.down_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 1487.34w/s, dev=0]model.layers.5.mlp.down_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 11.54w/s, dev=0]  model.layers.5.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 11.52w/s, dev=0]  model.layers.5.post_attention_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:06,  8.57w/s, dev=0]model.layers.5.post_attention_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:04, 11.41w/s, dev=0]model.layers.6.input_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:04, 11.40w/s, dev=0]         model.layers.6.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 14.25w/s, dev=0]  model.layers.6.mlp.down_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:04, 11.59w/s, dev=0]model.layers.6.mlp.gate_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:04, 11.59w/s, dev=0]model.layers.6.mlp.up_proj.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:05, 10.25w/s, dev=0]  model.layers.6.mlp.up_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:05,  9.28w/s, dev=0]model.layers.6.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:05,  9.28w/s, dev=0]model.layers.6.self_attn.k_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:04, 10.43w/s, dev=0]        model.layers.6.self_attn.o_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:04, 11.48w/s, dev=0]model.layers.6.self_attn.q_proj.weight:  19%|â–ˆâ–Š        | 11/59 [00:00<00:04, 11.86w/s, dev=0]model.layers.6.self_attn.q_proj.weight:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:03, 12.27w/s, dev=0]model.layers.6.self_attn.rotary_emb.inv_freq:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:03, 12.27w/s, dev=0]model.layers.6.self_attn.v_proj.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:03, 13.29w/s, dev=0]      model.layers.7.input_layernorm.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:03, 14.21w/s, dev=0] model.layers.7.mlp.down_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:02, 15.22w/s, dev=0]  model.layers.7.mlp.down_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:03, 13.94w/s, dev=0]model.layers.7.mlp.gate_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:03, 13.93w/s, dev=0]model.layers.7.mlp.up_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:01<00:03, 12.95w/s, dev=0]  model.layers.7.post_attention_layernorm.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:03, 12.34w/s, dev=0]model.layers.7.self_attn.k_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:03, 13.02w/s, dev=0]        model.layers.7.self_attn.k_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:02, 13.65w/s, dev=0]model.layers.7.self_attn.o_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:02, 13.65w/s, dev=0]model.layers.7.self_attn.q_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:01<00:02, 13.88w/s, dev=0]model.layers.7.self_attn.rotary_emb.inv_freq:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:02, 14.10w/s, dev=0]model.layers.7.self_attn.v_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:02, 14.74w/s, dev=0]      model.layers.7.self_attn.v_proj.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:02, 15.31w/s, dev=0]model.layers.8.input_layernorm.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:02, 15.31w/s, dev=0] model.layers.8.mlp.down_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:02, 15.94w/s, dev=0]  model.layers.8.mlp.gate_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:02, 14.98w/s, dev=0]model.layers.8.mlp.up_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:01<00:02, 14.34w/s, dev=0]  model.layers.8.mlp.up_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:02<00:02, 13.82w/s, dev=0]model.layers.8.post_attention_layernorm.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:02<00:02, 13.82w/s, dev=0]model.layers.8.self_attn.k_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:02<00:02, 14.31w/s, dev=0]        model.layers.8.self_attn.o_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:02<00:01, 14.76w/s, dev=0]model.layers.8.self_attn.q_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:02<00:01, 14.95w/s, dev=0]model.layers.8.self_attn.rotary_emb.inv_freq:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:02<00:01, 15.12w/s, dev=0]model.layers.8.self_attn.v_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:02<00:01, 15.59w/s, dev=0]      model.layers.8.self_attn.v_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:02<00:01, 15.99w/s, dev=0]model.layers.9.input_layernorm.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:02<00:01, 15.99w/s, dev=0] model.layers.9.mlp.down_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:02<00:01, 16.46w/s, dev=0]  model.layers.9.mlp.gate_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:02<00:01, 15.78w/s, dev=0]model.layers.9.mlp.up_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:02<00:01, 15.18w/s, dev=0]  model.layers.9.post_attention_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:02<00:01, 14.73w/s, dev=0]model.layers.9.self_attn.k_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:02<00:01, 15.12w/s, dev=0]        model.layers.9.self_attn.k_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:02<00:01, 15.47w/s, dev=0]model.layers.9.self_attn.o_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:02<00:01, 15.46w/s, dev=0]model.layers.9.self_attn.q_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:02<00:01, 15.59w/s, dev=0]model.layers.9.self_attn.rotary_emb.inv_freq:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:02<00:01, 15.67w/s, dev=0]model.layers.9.self_attn.v_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:02<00:00, 16.04w/s, dev=0]      model.layers.10.input_layernorm.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:02<00:00, 16.36w/s, dev=0]model.layers.10.mlp.down_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:02<00:00, 16.73w/s, dev=0]  model.layers.10.mlp.down_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:02<00:00, 16.17w/s, dev=0]model.layers.10.mlp.gate_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:02<00:00, 16.16w/s, dev=0]model.layers.10.mlp.up_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:03<00:00, 15.64w/s, dev=0]  model.layers.10.post_attention_layernorm.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:03<00:00, 15.15w/s, dev=0]model.layers.10.self_attn.k_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:03<00:00, 15.46w/s, dev=0]        model.layers.10.self_attn.o_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:03<00:00, 15.74w/s, dev=0]model.layers.10.self_attn.q_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:03<00:00, 15.81w/s, dev=0]model.layers.10.self_attn.q_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:03<00:00, 15.87w/s, dev=0]model.layers.10.self_attn.rotary_emb.inv_freq:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:03<00:00, 15.87w/s, dev=0]model.layers.10.self_attn.v_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:03<00:00, 16.17w/s, dev=0]      model.layers.11.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:03<00:00, 16.44w/s, dev=0]model.layers.11.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:03<00:00, 16.71w/s, dev=0]model.layers.11.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:03<00:00, 16.79w/s, dev=0]model.layers.11.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:03<00:00, 16.85w/s, dev=0]model.layers.11.self_attn.rotary_emb.inv_freq:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:03<00:00, 17.15w/s, dev=0]model.layers.11.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:03<00:00, 17.14w/s, dev=0]                                                                                                      0%|          | 0/55 [00:00<?, ?w/s]model.layers.11.input_layernorm.weight:   0%|          | 0/55 [00:00<?, ?w/s, dev=0]model.layers.11.mlp.down_proj.weight:   2%|â–         | 1/55 [00:00<00:00, 1298.55w/s, dev=0]model.layers.11.mlp.down_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:04, 12.86w/s, dev=0]  model.layers.11.mlp.gate_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:04, 12.84w/s, dev=0]model.layers.11.mlp.up_proj.weight:   5%|â–Œ         | 3/55 [00:00<00:05,  9.64w/s, dev=0]  model.layers.11.mlp.up_proj.weight:   7%|â–‹         | 4/55 [00:00<00:05,  8.72w/s, dev=0]model.layers.11.post_attention_layernorm.weight:   7%|â–‹         | 4/55 [00:00<00:05,  8.72w/s, dev=0]model.layers.12.input_layernorm.weight:   9%|â–‰         | 5/55 [00:00<00:04, 10.89w/s, dev=0]         model.layers.12.mlp.down_proj.weight:  11%|â–ˆ         | 6/55 [00:00<00:03, 13.06w/s, dev=0]  model.layers.12.mlp.down_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:04, 11.54w/s, dev=0]model.layers.12.mlp.gate_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:04, 11.53w/s, dev=0]model.layers.12.mlp.up_proj.weight:  15%|â–ˆâ–        | 8/55 [00:00<00:04, 10.54w/s, dev=0]  model.layers.12.post_attention_layernorm.weight:  16%|â–ˆâ–‹        | 9/55 [00:00<00:04,  9.84w/s, dev=0]model.layers.12.post_attention_layernorm.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:04, 10.93w/s, dev=0]model.layers.12.self_attn.k_proj.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:04, 10.93w/s, dev=0]        model.layers.12.self_attn.o_proj.weight:  20%|â–ˆâ–ˆ        | 11/55 [00:00<00:03, 11.91w/s, dev=0]model.layers.12.self_attn.q_proj.weight:  22%|â–ˆâ–ˆâ–       | 12/55 [00:00<00:03, 12.34w/s, dev=0]model.layers.12.self_attn.q_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:01<00:03, 12.72w/s, dev=0]model.layers.12.self_attn.rotary_emb.inv_freq:  24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:01<00:03, 12.72w/s, dev=0]model.layers.12.self_attn.v_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 14/55 [00:01<00:02, 13.69w/s, dev=0]      model.layers.13.input_layernorm.weight:  27%|â–ˆâ–ˆâ–‹       | 15/55 [00:01<00:02, 14.58w/s, dev=0] model.layers.13.mlp.down_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 16/55 [00:01<00:02, 15.54w/s, dev=0]  model.layers.13.mlp.down_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:01<00:02, 14.27w/s, dev=0]model.layers.13.mlp.gate_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:01<00:02, 14.27w/s, dev=0]model.layers.13.mlp.up_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/55 [00:01<00:02, 13.27w/s, dev=0]  model.layers.13.post_attention_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–      | 19/55 [00:01<00:02, 12.58w/s, dev=0]model.layers.13.self_attn.k_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:01<00:02, 13.24w/s, dev=0]        model.layers.13.self_attn.k_proj.weight:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/55 [00:01<00:02, 13.85w/s, dev=0]model.layers.13.self_attn.o_proj.weight:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/55 [00:01<00:02, 13.84w/s, dev=0]model.layers.13.self_attn.q_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/55 [00:01<00:02, 14.08w/s, dev=0]model.layers.13.self_attn.rotary_emb.inv_freq:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 23/55 [00:01<00:02, 14.31w/s, dev=0]model.layers.13.self_attn.v_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 24/55 [00:01<00:02, 14.93w/s, dev=0]      model.layers.14.input_layernorm.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:01<00:01, 15.49w/s, dev=0] model.layers.14.mlp.down_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 26/55 [00:01<00:01, 16.11w/s, dev=0]  model.layers.14.mlp.down_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 27/55 [00:01<00:01, 15.31w/s, dev=0]model.layers.14.mlp.gate_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 27/55 [00:01<00:01, 15.31w/s, dev=0]model.layers.14.mlp.up_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 28/55 [00:01<00:01, 14.64w/s, dev=0]  model.layers.14.post_attention_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/55 [00:02<00:01, 14.06w/s, dev=0]model.layers.14.self_attn.k_proj.weight:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 30/55 [00:02<00:01, 14.54w/s, dev=0]        model.layers.14.self_attn.o_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/55 [00:02<00:01, 14.98w/s, dev=0]model.layers.14.self_attn.q_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 32/55 [00:02<00:01, 15.14w/s, dev=0]model.layers.14.self_attn.q_proj.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/55 [00:02<00:01, 15.29w/s, dev=0]model.layers.14.self_attn.rotary_emb.inv_freq:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/55 [00:02<00:01, 15.29w/s, dev=0]model.layers.14.self_attn.v_proj.weight:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/55 [00:02<00:01, 15.75w/s, dev=0]      model.layers.15.input_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:02<00:01, 16.17w/s, dev=0] model.layers.15.mlp.down_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 36/55 [00:02<00:01, 16.63w/s, dev=0]  model.layers.15.mlp.gate_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 37/55 [00:02<00:01, 16.01w/s, dev=0]model.layers.15.mlp.up_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 38/55 [00:02<00:01, 15.50w/s, dev=0]  model.layers.15.mlp.up_proj.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 39/55 [00:02<00:01, 14.97w/s, dev=0]model.layers.15.post_attention_layernorm.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 39/55 [00:02<00:01, 14.97w/s, dev=0]model.layers.15.self_attn.k_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 40/55 [00:02<00:00, 15.35w/s, dev=0]        model.layers.15.self_attn.o_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/55 [00:02<00:00, 15.69w/s, dev=0]model.layers.15.self_attn.q_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 42/55 [00:02<00:00, 15.79w/s, dev=0]model.layers.15.self_attn.rotary_emb.inv_freq:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 43/55 [00:02<00:00, 15.89w/s, dev=0]model.layers.15.self_attn.v_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 44/55 [00:02<00:00, 16.25w/s, dev=0]      model.layers.15.self_attn.v_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:02<00:00, 16.58w/s, dev=0]model.layers.16.input_layernorm.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:02<00:00, 16.58w/s, dev=0] model.layers.16.mlp.down_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 46/55 [00:02<00:00, 16.95w/s, dev=0]  model.layers.16.mlp.gate_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 47/55 [00:02<00:00, 16.35w/s, dev=0]model.layers.16.mlp.up_proj.weight:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 48/55 [00:03<00:00, 15.87w/s, dev=0]  model.layers.16.post_attention_layernorm.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 49/55 [00:03<00:00, 15.40w/s, dev=0]model.layers.16.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 50/55 [00:03<00:00, 15.72w/s, dev=0]        model.layers.16.self_attn.k_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 51/55 [00:03<00:00, 16.00w/s, dev=0]model.layers.16.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 51/55 [00:03<00:00, 16.00w/s, dev=0]model.layers.16.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 52/55 [00:03<00:00, 16.09w/s, dev=0]model.layers.16.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 53/55 [00:03<00:00, 16.16w/s, dev=0]model.layers.16.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 54/55 [00:03<00:00, 16.46w/s, dev=0]                                                                                                      0%|          | 0/57 [00:00<?, ?w/s]model.layers.17.input_layernorm.weight:   0%|          | 0/57 [00:00<?, ?w/s, dev=0]model.layers.17.mlp.down_proj.weight:   2%|â–         | 1/57 [00:00<00:00, 1389.76w/s, dev=0]model.layers.17.mlp.down_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:09,  6.07w/s, dev=0]  model.layers.17.mlp.gate_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:09,  6.06w/s, dev=0]model.layers.17.mlp.up_proj.weight:   5%|â–Œ         | 3/57 [00:00<00:10,  5.27w/s, dev=0]  model.layers.17.mlp.up_proj.weight:   7%|â–‹         | 4/57 [00:00<00:10,  4.93w/s, dev=0]model.layers.17.post_attention_layernorm.weight:   7%|â–‹         | 4/57 [00:00<00:10,  4.93w/s, dev=0]model.layers.17.self_attn.k_proj.weight:   9%|â–‰         | 5/57 [00:00<00:08,  6.16w/s, dev=0]        model.layers.17.self_attn.k_proj.weight:  11%|â–ˆ         | 6/57 [00:00<00:08,  6.33w/s, dev=0]model.layers.17.self_attn.o_proj.weight:  11%|â–ˆ         | 6/57 [00:00<00:08,  6.33w/s, dev=0]model.layers.17.self_attn.q_proj.weight:  12%|â–ˆâ–        | 7/57 [00:00<00:07,  7.03w/s, dev=0]model.layers.17.self_attn.q_proj.weight:  14%|â–ˆâ–        | 8/57 [00:01<00:07,  6.13w/s, dev=0]model.layers.17.self_attn.rotary_emb.inv_freq:  14%|â–ˆâ–        | 8/57 [00:01<00:07,  6.13w/s, dev=0]model.layers.17.self_attn.v_proj.weight:  16%|â–ˆâ–Œ        | 9/57 [00:01<00:06,  6.89w/s, dev=0]      model.layers.18.input_layernorm.weight:  18%|â–ˆâ–Š        | 10/57 [00:01<00:06,  7.61w/s, dev=0]model.layers.18.mlp.down_proj.weight:  19%|â–ˆâ–‰        | 11/57 [00:01<00:05,  8.37w/s, dev=0]  model.layers.18.mlp.down_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:01<00:05,  8.19w/s, dev=0]model.layers.18.mlp.gate_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:01<00:05,  8.19w/s, dev=0]model.layers.18.mlp.up_proj.weight:  23%|â–ˆâ–ˆâ–Ž       | 13/57 [00:01<00:05,  7.48w/s, dev=0]  model.layers.18.post_attention_layernorm.weight:  25%|â–ˆâ–ˆâ–       | 14/57 [00:02<00:06,  6.69w/s, dev=0]model.layers.18.self_attn.k_proj.weight:  26%|â–ˆâ–ˆâ–‹       | 15/57 [00:02<00:05,  7.17w/s, dev=0]        model.layers.18.self_attn.k_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:02<00:05,  7.62w/s, dev=0]model.layers.18.self_attn.o_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:02<00:05,  7.62w/s, dev=0]model.layers.18.self_attn.q_proj.weight:  30%|â–ˆâ–ˆâ–‰       | 17/57 [00:02<00:05,  7.94w/s, dev=0]model.layers.18.self_attn.rotary_emb.inv_freq:  32%|â–ˆâ–ˆâ–ˆâ–      | 18/57 [00:02<00:04,  8.23w/s, dev=0]model.layers.18.self_attn.v_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 19/57 [00:02<00:04,  8.69w/s, dev=0]      model.layers.19.input_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 20/57 [00:02<00:04,  9.12w/s, dev=0] model.layers.19.mlp.down_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 21/57 [00:02<00:03,  9.58w/s, dev=0]  model.layers.19.mlp.down_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:02<00:03,  9.38w/s, dev=0]model.layers.19.mlp.gate_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:02<00:03,  9.38w/s, dev=0]model.layers.19.mlp.up_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 23/57 [00:02<00:03,  9.18w/s, dev=0]  model.layers.19.post_attention_layernorm.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/57 [00:02<00:03,  8.92w/s, dev=0]model.layers.19.self_attn.k_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/57 [00:02<00:03,  9.29w/s, dev=0]        model.layers.19.self_attn.o_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 26/57 [00:02<00:03,  9.64w/s, dev=0]model.layers.19.self_attn.q_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 27/57 [00:02<00:03,  9.84w/s, dev=0]model.layers.19.self_attn.q_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:02<00:02, 10.04w/s, dev=0]model.layers.19.self_attn.rotary_emb.inv_freq:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:02<00:02, 10.04w/s, dev=0]model.layers.19.self_attn.v_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 29/57 [00:02<00:02, 10.40w/s, dev=0]      model.layers.20.input_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 30/57 [00:03<00:02,  9.44w/s, dev=1] model.layers.20.mlp.down_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 31/57 [00:03<00:02,  9.28w/s, dev=1]  model.layers.20.mlp.gate_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 32/57 [00:03<00:02,  9.14w/s, dev=1]model.layers.20.mlp.up_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 33/57 [00:03<00:02,  9.00w/s, dev=1]  model.layers.20.mlp.up_proj.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:03<00:02,  8.62w/s, dev=1]model.layers.20.post_attention_layernorm.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:03<00:02,  8.62w/s, dev=1]model.layers.20.self_attn.k_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/57 [00:03<00:02,  8.87w/s, dev=1]        model.layers.20.self_attn.o_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 36/57 [00:03<00:02,  9.11w/s, dev=1]model.layers.20.self_attn.q_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 37/57 [00:03<00:02,  9.26w/s, dev=1]model.layers.20.self_attn.rotary_emb.inv_freq:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 38/57 [00:04<00:02,  9.35w/s, dev=1]model.layers.20.self_attn.v_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 39/57 [00:04<00:01,  9.60w/s, dev=1]      model.layers.20.self_attn.v_proj.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:04<00:01,  9.33w/s, dev=1]model.layers.21.input_layernorm.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:04<00:01,  9.33w/s, dev=1] model.layers.21.mlp.down_proj.weight:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/57 [00:04<00:01,  9.57w/s, dev=1]  model.layers.21.mlp.gate_proj.weight:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 42/57 [00:04<00:01,  9.45w/s, dev=1]model.layers.21.mlp.up_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 43/57 [00:04<00:01,  9.35w/s, dev=1]  model.layers.21.post_attention_layernorm.weight:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 44/57 [00:04<00:01,  9.24w/s, dev=1]model.layers.21.self_attn.k_proj.weight:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 45/57 [00:04<00:01,  9.45w/s, dev=1]        model.layers.21.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:04<00:01,  9.48w/s, dev=1]model.layers.21.self_attn.o_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:04<00:01,  9.48w/s, dev=1]model.layers.21.self_attn.q_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 47/57 [00:04<00:01,  9.59w/s, dev=1]model.layers.21.self_attn.rotary_emb.inv_freq:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/57 [00:04<00:00,  9.70w/s, dev=1]model.layers.21.self_attn.v_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 49/57 [00:04<00:00,  9.90w/s, dev=1]      model.layers.22.mlp.down_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 50/57 [00:04<00:00, 10.09w/s, dev=1]   model.layers.22.mlp.gate_proj.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 51/57 [00:05<00:00,  9.98w/s, dev=1]model.layers.22.mlp.gate_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:05<00:00,  9.82w/s, dev=1]model.layers.22.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:05<00:00,  9.82w/s, dev=1]model.layers.22.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 53/57 [00:05<00:00,  9.81w/s, dev=1]model.layers.22.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/57 [00:05<00:00,  9.90w/s, dev=1]model.layers.22.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 55/57 [00:05<00:00,  9.98w/s, dev=1]model.layers.22.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 56/57 [00:05<00:00, 10.16w/s, dev=1]                                                                                                      0%|          | 0/59 [00:00<?, ?w/s]model.layers.22.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=1]model.layers.22.mlp.up_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 1242.76w/s, dev=1]model.layers.22.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:11,  5.01w/s, dev=1]  model.layers.22.post_attention_layernorm.weight:   3%|â–Ž         | 2/59 [00:00<00:11,  5.01w/s, dev=1]model.layers.23.input_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:07,  7.50w/s, dev=1]         model.layers.23.mlp.down_proj.weight:   7%|â–‹         | 4/59 [00:00<00:05,  9.99w/s, dev=1]  model.layers.23.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:05,  9.10w/s, dev=1]model.layers.23.mlp.gate_proj.weight:   8%|â–Š         | 5/59 [00:00<00:05,  9.10w/s, dev=1]model.layers.23.mlp.up_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:06,  8.51w/s, dev=1]  model.layers.23.post_attention_layernorm.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:07,  7.20w/s, dev=1]model.layers.23.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:06,  8.23w/s, dev=1]model.layers.23.self_attn.k_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:06,  8.23w/s, dev=1]        model.layers.23.self_attn.o_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:05,  9.13w/s, dev=1]model.layers.23.self_attn.q_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:01<00:05,  9.66w/s, dev=1]model.layers.23.self_attn.q_proj.weight:  19%|â–ˆâ–Š        | 11/59 [00:01<00:04, 10.17w/s, dev=1]model.layers.23.self_attn.rotary_emb.inv_freq:  19%|â–ˆâ–Š        | 11/59 [00:01<00:04, 10.17w/s, dev=1]model.layers.23.self_attn.v_proj.weight:  20%|â–ˆâ–ˆ        | 12/59 [00:01<00:04, 11.09w/s, dev=1]      model.layers.24.input_layernorm.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:01<00:03, 11.93w/s, dev=1] model.layers.24.mlp.down_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:01<00:03, 12.85w/s, dev=1]  model.layers.24.mlp.down_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:01<00:03, 12.00w/s, dev=1]model.layers.24.mlp.gate_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:01<00:03, 12.00w/s, dev=1]model.layers.24.mlp.up_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:04, 10.58w/s, dev=1]  model.layers.24.post_attention_layernorm.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:01<00:04, 10.18w/s, dev=1]model.layers.24.self_attn.k_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:03, 10.77w/s, dev=1]        model.layers.24.self_attn.k_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:03, 11.32w/s, dev=1]model.layers.24.self_attn.o_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:03, 11.32w/s, dev=1]model.layers.24.self_attn.q_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:03, 11.61w/s, dev=1]model.layers.24.self_attn.rotary_emb.inv_freq:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:01<00:03, 10.86w/s, dev=1]model.layers.24.self_attn.v_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:03, 11.37w/s, dev=1]      model.layers.24.self_attn.v_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:03, 11.83w/s, dev=1]model.layers.25.input_layernorm.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:03, 11.83w/s, dev=1] model.layers.25.mlp.down_proj.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:02, 12.34w/s, dev=1]  model.layers.25.mlp.gate_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:02<00:02, 11.88w/s, dev=1]model.layers.25.mlp.up_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:02<00:02, 11.46w/s, dev=1]  model.layers.25.mlp.up_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:02<00:03, 10.42w/s, dev=1]model.layers.25.post_attention_layernorm.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:02<00:03, 10.41w/s, dev=1]model.layers.25.self_attn.k_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:02<00:02, 10.80w/s, dev=1]        model.layers.25.self_attn.o_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:02<00:02, 11.13w/s, dev=1]model.layers.25.self_attn.q_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:02<00:02, 11.31w/s, dev=1]model.layers.25.self_attn.q_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:02<00:02, 11.49w/s, dev=1]model.layers.25.self_attn.rotary_emb.inv_freq:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:02<00:02, 11.49w/s, dev=1]model.layers.25.self_attn.v_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:02<00:02, 11.86w/s, dev=1]      model.layers.26.input_layernorm.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:02<00:02, 12.19w/s, dev=1] model.layers.26.mlp.down_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:02<00:01, 12.56w/s, dev=1]  model.layers.26.mlp.down_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:02<00:01, 12.24w/s, dev=1]model.layers.26.mlp.gate_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:02<00:01, 12.24w/s, dev=1]model.layers.26.mlp.up_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:03<00:02, 11.18w/s, dev=1]  model.layers.26.post_attention_layernorm.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:03<00:02, 10.95w/s, dev=1]model.layers.26.self_attn.k_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:03<00:01, 11.24w/s, dev=1]        model.layers.26.self_attn.k_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:03<00:01, 11.51w/s, dev=1]model.layers.26.self_attn.o_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:03<00:01, 11.51w/s, dev=1]model.layers.26.self_attn.q_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:03<00:01, 11.66w/s, dev=1]model.layers.26.self_attn.rotary_emb.inv_freq:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:03<00:01, 11.01w/s, dev=1]model.layers.26.self_attn.v_proj.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:03<00:01, 11.28w/s, dev=1]      model.layers.26.self_attn.v_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:03<00:01, 11.52w/s, dev=1]model.layers.27.input_layernorm.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:03<00:01, 11.51w/s, dev=1] model.layers.27.mlp.down_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:03<00:01, 11.78w/s, dev=1]  model.layers.27.mlp.gate_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:03<00:01, 11.58w/s, dev=1]model.layers.27.mlp.up_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:04<00:01, 11.34w/s, dev=1]  model.layers.27.mlp.up_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:04<00:01, 11.14w/s, dev=1]model.layers.27.post_attention_layernorm.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:04<00:01, 11.14w/s, dev=1]model.layers.27.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:04<00:00, 11.37w/s, dev=1]        model.layers.27.self_attn.o_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:04<00:00, 11.34w/s, dev=1]model.layers.27.self_attn.q_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:04<00:00, 11.44w/s, dev=1]model.layers.27.self_attn.q_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:04<00:00, 11.54w/s, dev=1]model.layers.27.self_attn.rotary_emb.inv_freq:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:04<00:00, 11.54w/s, dev=1]model.layers.27.self_attn.v_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:04<00:00, 11.76w/s, dev=1]      model.layers.28.mlp.gate_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:04<00:00, 11.97w/s, dev=1]   model.layers.28.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:04<00:00, 11.79w/s, dev=1]model.layers.28.self_attn.k_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:04<00:00, 11.97w/s, dev=1]model.layers.28.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:04<00:00, 11.97w/s, dev=1]model.layers.28.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:04<00:00, 12.07w/s, dev=1]model.layers.28.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:04<00:00, 12.17w/s, dev=1]model.layers.28.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:04<00:00, 12.38w/s, dev=1]                                                                                                      0%|          | 0/59 [00:00<?, ?w/s]model.layers.28.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=1]model.layers.28.mlp.down_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 1204.57w/s, dev=1]model.layers.28.mlp.down_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 12.95w/s, dev=1]  model.layers.28.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 12.92w/s, dev=1]  model.layers.28.post_attention_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:05,  9.67w/s, dev=1]model.layers.28.post_attention_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:04, 12.87w/s, dev=1]model.layers.29.input_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:04, 12.87w/s, dev=1]         model.layers.29.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 16.08w/s, dev=1]  model.layers.29.mlp.down_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:04, 12.65w/s, dev=1]model.layers.29.mlp.gate_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:04, 12.65w/s, dev=1]model.layers.29.mlp.up_proj.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:04, 11.05w/s, dev=1]  model.layers.29.mlp.up_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:05, 10.04w/s, dev=1]model.layers.29.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:05, 10.04w/s, dev=1]model.layers.29.self_attn.k_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:04, 11.29w/s, dev=1]        model.layers.29.self_attn.o_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:03, 12.44w/s, dev=1]model.layers.29.self_attn.q_proj.weight:  19%|â–ˆâ–Š        | 11/59 [00:00<00:03, 12.93w/s, dev=1]model.layers.29.self_attn.q_proj.weight:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:03, 13.36w/s, dev=1]model.layers.29.self_attn.rotary_emb.inv_freq:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:03, 13.35w/s, dev=1]model.layers.29.self_attn.v_proj.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:03, 14.46w/s, dev=1]      model.layers.30.input_layernorm.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:02, 15.43w/s, dev=1] model.layers.30.mlp.down_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:02, 16.52w/s, dev=1]  model.layers.30.mlp.down_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:02, 14.89w/s, dev=1]model.layers.30.mlp.gate_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:02, 14.89w/s, dev=1]model.layers.30.mlp.up_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:01<00:03, 13.79w/s, dev=1]  model.layers.30.post_attention_layernorm.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:03, 12.92w/s, dev=1]model.layers.30.self_attn.k_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:02, 13.63w/s, dev=1]        model.layers.30.self_attn.k_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:02, 14.27w/s, dev=1]model.layers.30.self_attn.o_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:02, 14.27w/s, dev=1]model.layers.30.self_attn.q_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:01<00:02, 14.51w/s, dev=1]model.layers.30.self_attn.rotary_emb.inv_freq:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:02, 14.73w/s, dev=1]model.layers.30.self_attn.v_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:02, 15.40w/s, dev=1]      model.layers.31.input_layernorm.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:02, 15.99w/s, dev=1] model.layers.31.mlp.down_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:02, 16.66w/s, dev=1]  model.layers.31.mlp.down_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:02, 15.72w/s, dev=1]model.layers.31.mlp.gate_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:02, 15.72w/s, dev=1]model.layers.31.mlp.up_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:01<00:02, 14.90w/s, dev=1]  model.layers.31.post_attention_layernorm.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:02<00:02, 13.25w/s, dev=1]model.layers.31.self_attn.k_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:02<00:02, 13.72w/s, dev=1]        model.layers.31.self_attn.o_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:02<00:02, 14.14w/s, dev=1]model.layers.31.self_attn.q_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:02<00:01, 14.26w/s, dev=1]model.layers.31.self_attn.q_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:02<00:01, 14.38w/s, dev=1]model.layers.31.self_attn.rotary_emb.inv_freq:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:02<00:01, 14.38w/s, dev=1]model.layers.31.self_attn.v_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:02<00:01, 14.82w/s, dev=1]      model.layers.32.input_layernorm.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:02<00:01, 15.22w/s, dev=1] model.layers.32.mlp.down_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:02<00:01, 15.67w/s, dev=1]  model.layers.32.mlp.gate_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:02<00:01, 15.09w/s, dev=1]model.layers.32.mlp.up_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:02<00:01, 13.71w/s, dev=1]  model.layers.32.mlp.up_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:02<00:01, 13.31w/s, dev=1]model.layers.32.post_attention_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:02<00:01, 13.31w/s, dev=1]model.layers.32.self_attn.k_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:02<00:01, 13.66w/s, dev=1]        model.layers.32.self_attn.o_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:02<00:01, 13.34w/s, dev=1]model.layers.32.self_attn.q_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:03<00:01, 13.46w/s, dev=1]model.layers.32.self_attn.rotary_emb.inv_freq:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:03<00:01, 13.57w/s, dev=1]model.layers.32.self_attn.v_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:03<00:01, 13.89w/s, dev=1]      model.layers.32.self_attn.v_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:03<00:01, 14.17w/s, dev=1]model.layers.33.input_layernorm.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:03<00:01, 14.17w/s, dev=1] model.layers.33.mlp.down_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:03<00:00, 14.49w/s, dev=1]  model.layers.33.mlp.gate_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:03<00:00, 14.10w/s, dev=1]model.layers.33.mlp.up_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:03<00:00, 13.74w/s, dev=1]  model.layers.33.post_attention_layernorm.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:03<00:00, 13.20w/s, dev=1]model.layers.33.self_attn.k_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:03<00:00, 13.47w/s, dev=1]        model.layers.33.self_attn.k_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:03<00:00, 13.71w/s, dev=1]model.layers.33.self_attn.o_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:03<00:00, 13.71w/s, dev=1]model.layers.33.self_attn.q_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:03<00:00, 13.77w/s, dev=1]model.layers.33.self_attn.rotary_emb.inv_freq:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:03<00:00, 13.86w/s, dev=1]model.layers.33.self_attn.v_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:03<00:00, 14.13w/s, dev=1]      model.layers.34.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:03<00:00, 14.35w/s, dev=1]model.layers.34.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:03<00:00, 14.58w/s, dev=1]model.layers.34.self_attn.o_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:03<00:00, 14.66w/s, dev=1]model.layers.34.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:03<00:00, 14.66w/s, dev=1]model.layers.34.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:03<00:00, 14.73w/s, dev=1]model.layers.34.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:03<00:00, 14.99w/s, dev=1]                                                                                                      0%|          | 0/55 [00:00<?, ?w/s]model.layers.34.input_layernorm.weight:   0%|          | 0/55 [00:00<?, ?w/s, dev=1]model.layers.34.mlp.down_proj.weight:   2%|â–         | 1/55 [00:00<00:00, 1206.30w/s, dev=1]model.layers.34.mlp.down_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:04, 12.57w/s, dev=1]  model.layers.34.mlp.gate_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:04, 12.55w/s, dev=1]model.layers.34.mlp.up_proj.weight:   5%|â–Œ         | 3/55 [00:00<00:05,  9.30w/s, dev=1]  model.layers.34.mlp.up_proj.weight:   7%|â–‹         | 4/55 [00:00<00:09,  5.55w/s, dev=1]model.layers.34.post_attention_layernorm.weight:   7%|â–‹         | 4/55 [00:00<00:09,  5.54w/s, dev=1]model.layers.35.input_layernorm.weight:   9%|â–‰         | 5/55 [00:00<00:07,  6.92w/s, dev=1]         model.layers.35.mlp.down_proj.weight:  11%|â–ˆ         | 6/55 [00:00<00:05,  8.31w/s, dev=1]  model.layers.35.mlp.down_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:06,  7.98w/s, dev=1]model.layers.35.mlp.gate_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:06,  7.98w/s, dev=1]model.layers.35.mlp.up_proj.weight:  15%|â–ˆâ–        | 8/55 [00:01<00:06,  7.68w/s, dev=1]  model.layers.35.post_attention_layernorm.weight:  16%|â–ˆâ–‹        | 9/55 [00:01<00:06,  6.73w/s, dev=1]model.layers.35.post_attention_layernorm.weight:  18%|â–ˆâ–Š        | 10/55 [00:01<00:06,  7.47w/s, dev=1]model.layers.35.self_attn.k_proj.weight:  18%|â–ˆâ–Š        | 10/55 [00:01<00:06,  7.47w/s, dev=1]        model.layers.35.self_attn.o_proj.weight:  20%|â–ˆâ–ˆ        | 11/55 [00:01<00:05,  8.16w/s, dev=1]model.layers.35.self_attn.q_proj.weight:  22%|â–ˆâ–ˆâ–       | 12/55 [00:01<00:05,  8.57w/s, dev=1]model.layers.35.self_attn.q_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:01<00:04,  8.99w/s, dev=1]model.layers.35.self_attn.rotary_emb.inv_freq:  24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:01<00:04,  8.99w/s, dev=1]model.layers.35.self_attn.v_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 14/55 [00:01<00:04,  9.68w/s, dev=1]      model.layers.36.input_layernorm.weight:  27%|â–ˆâ–ˆâ–‹       | 15/55 [00:01<00:03, 10.27w/s, dev=1] model.layers.36.mlp.down_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 16/55 [00:01<00:03, 10.95w/s, dev=1]  model.layers.36.mlp.down_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:01<00:03, 10.48w/s, dev=1]model.layers.36.mlp.gate_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:01<00:03, 10.48w/s, dev=1]model.layers.36.mlp.up_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/55 [00:01<00:03, 10.11w/s, dev=1]  model.layers.36.post_attention_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–      | 19/55 [00:01<00:03,  9.79w/s, dev=1]model.layers.36.self_attn.k_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:01<00:03, 10.30w/s, dev=1]        model.layers.36.self_attn.k_proj.weight:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/55 [00:01<00:03, 10.77w/s, dev=1]model.layers.36.self_attn.o_proj.weight:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/55 [00:01<00:03, 10.77w/s, dev=1]model.layers.36.self_attn.q_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/55 [00:01<00:02, 11.02w/s, dev=1]model.layers.36.self_attn.rotary_emb.inv_freq:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 23/55 [00:02<00:02, 11.26w/s, dev=1]model.layers.36.self_attn.v_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 24/55 [00:02<00:02, 11.74w/s, dev=1]      model.layers.36.self_attn.v_proj.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:02<00:02, 12.19w/s, dev=1]model.layers.37.input_layernorm.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:02<00:02, 12.19w/s, dev=1] model.layers.37.mlp.down_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 26/55 [00:02<00:02, 12.68w/s, dev=1]  model.layers.37.mlp.gate_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 27/55 [00:02<00:02, 12.19w/s, dev=1]model.layers.37.mlp.up_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 28/55 [00:02<00:02, 11.81w/s, dev=1]  model.layers.37.mlp.up_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/55 [00:02<00:02, 11.48w/s, dev=1]model.layers.37.post_attention_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/55 [00:02<00:02, 11.48w/s, dev=1]model.layers.37.self_attn.k_proj.weight:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 30/55 [00:02<00:02, 11.88w/s, dev=1]        model.layers.37.self_attn.o_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/55 [00:02<00:01, 12.24w/s, dev=1]model.layers.37.self_attn.q_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 32/55 [00:02<00:01, 12.41w/s, dev=1]model.layers.37.self_attn.rotary_emb.inv_freq:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/55 [00:02<00:01, 12.57w/s, dev=1]model.layers.37.self_attn.v_proj.weight:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/55 [00:02<00:01, 12.95w/s, dev=1]      model.layers.37.self_attn.v_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:02<00:01, 13.30w/s, dev=1]model.layers.38.input_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:02<00:01, 13.30w/s, dev=1] model.layers.38.mlp.down_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 36/55 [00:02<00:01, 13.68w/s, dev=1]  model.layers.38.mlp.gate_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 37/55 [00:02<00:01, 13.27w/s, dev=1]model.layers.38.mlp.up_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 38/55 [00:02<00:01, 12.91w/s, dev=1]  model.layers.38.post_attention_layernorm.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 39/55 [00:03<00:01, 12.58w/s, dev=1]model.layers.38.self_attn.k_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 40/55 [00:03<00:01, 12.90w/s, dev=1]        model.layers.38.self_attn.k_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/55 [00:03<00:01, 13.20w/s, dev=1]model.layers.38.self_attn.o_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/55 [00:03<00:01, 13.19w/s, dev=1]model.layers.38.self_attn.q_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 42/55 [00:03<00:00, 13.32w/s, dev=1]model.layers.38.self_attn.rotary_emb.inv_freq:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 43/55 [00:03<00:00, 13.43w/s, dev=1]model.layers.38.self_attn.v_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 44/55 [00:03<00:00, 13.74w/s, dev=1]      model.layers.39.input_layernorm.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:03<00:00, 14.02w/s, dev=1] model.layers.39.mlp.down_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 46/55 [00:03<00:00, 14.33w/s, dev=1]  model.layers.39.mlp.down_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 47/55 [00:03<00:00, 13.96w/s, dev=1]model.layers.39.mlp.gate_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 47/55 [00:03<00:00, 13.96w/s, dev=1]model.layers.39.mlp.up_proj.weight:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 48/55 [00:03<00:00, 13.63w/s, dev=1]  model.layers.39.post_attention_layernorm.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 49/55 [00:03<00:00, 13.33w/s, dev=1]model.layers.39.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 50/55 [00:03<00:00, 13.60w/s, dev=1]        model.layers.39.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 51/55 [00:03<00:00, 13.83w/s, dev=1]model.layers.39.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 52/55 [00:03<00:00, 13.91w/s, dev=1]model.layers.39.self_attn.q_proj.weight:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 53/55 [00:03<00:00, 14.00w/s, dev=1]model.layers.39.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 53/55 [00:03<00:00, 14.00w/s, dev=1]model.layers.39.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 54/55 [00:03<00:00, 14.26w/s, dev=1]                                                                                                      0%|          | 0/57 [00:00<?, ?w/s]model.layers.40.input_layernorm.weight:   0%|          | 0/57 [00:00<?, ?w/s, dev=1]model.layers.40.mlp.down_proj.weight:   2%|â–         | 1/57 [00:00<00:00, 1154.82w/s, dev=1]model.layers.40.mlp.down_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:04, 11.91w/s, dev=1]  model.layers.40.mlp.gate_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:04, 11.89w/s, dev=1]model.layers.40.mlp.up_proj.weight:   5%|â–Œ         | 3/57 [00:00<00:06,  8.92w/s, dev=1]  model.layers.40.mlp.up_proj.weight:   7%|â–‹         | 4/57 [00:00<00:06,  7.95w/s, dev=1]model.layers.40.post_attention_layernorm.weight:   7%|â–‹         | 4/57 [00:00<00:06,  7.95w/s, dev=1]model.layers.40.self_attn.k_proj.weight:   9%|â–‰         | 5/57 [00:00<00:05,  9.93w/s, dev=1]        model.layers.40.self_attn.o_proj.weight:  11%|â–ˆ         | 6/57 [00:00<00:04, 11.73w/s, dev=1]model.layers.40.self_attn.q_proj.weight:  12%|â–ˆâ–        | 7/57 [00:00<00:04, 12.48w/s, dev=1]model.layers.40.self_attn.q_proj.weight:  14%|â–ˆâ–        | 8/57 [00:00<00:03, 13.09w/s, dev=1]model.layers.40.self_attn.rotary_emb.inv_freq:  14%|â–ˆâ–        | 8/57 [00:00<00:03, 13.08w/s, dev=1]model.layers.40.self_attn.v_proj.weight:  16%|â–ˆâ–Œ        | 9/57 [00:00<00:03, 14.71w/s, dev=1]      model.layers.41.input_layernorm.weight:  18%|â–ˆâ–Š        | 10/57 [00:00<00:03, 14.18w/s, dev=2]model.layers.41.mlp.down_proj.weight:  19%|â–ˆâ–‰        | 11/57 [00:00<00:02, 15.57w/s, dev=2]  model.layers.41.mlp.down_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:00<00:03, 13.74w/s, dev=2]model.layers.41.mlp.gate_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:00<00:03, 13.73w/s, dev=2]model.layers.41.mlp.up_proj.weight:  23%|â–ˆâ–ˆâ–Ž       | 13/57 [00:01<00:03, 12.49w/s, dev=2]  model.layers.41.post_attention_layernorm.weight:  25%|â–ˆâ–ˆâ–       | 14/57 [00:01<00:03, 11.59w/s, dev=2]model.layers.41.self_attn.k_proj.weight:  26%|â–ˆâ–ˆâ–‹       | 15/57 [00:01<00:03, 12.41w/s, dev=2]        model.layers.41.self_attn.k_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:01<00:03, 13.17w/s, dev=2]model.layers.41.self_attn.o_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:01<00:03, 13.17w/s, dev=2]model.layers.41.self_attn.q_proj.weight:  30%|â–ˆâ–ˆâ–‰       | 17/57 [00:01<00:02, 13.48w/s, dev=2]model.layers.41.self_attn.rotary_emb.inv_freq:  32%|â–ˆâ–ˆâ–ˆâ–      | 18/57 [00:01<00:02, 13.74w/s, dev=2]model.layers.41.self_attn.v_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 19/57 [00:01<00:02, 14.50w/s, dev=2]      model.layers.41.self_attn.v_proj.weight:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 20/57 [00:01<00:02, 15.19w/s, dev=2]model.layers.42.input_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 20/57 [00:01<00:02, 15.18w/s, dev=2] model.layers.42.mlp.down_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 21/57 [00:01<00:02, 15.94w/s, dev=2]  model.layers.42.mlp.gate_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:01<00:02, 14.90w/s, dev=2]model.layers.42.mlp.up_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 23/57 [00:01<00:02, 14.09w/s, dev=2]  model.layers.42.mlp.up_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/57 [00:01<00:02, 13.43w/s, dev=2]model.layers.42.post_attention_layernorm.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/57 [00:01<00:02, 13.43w/s, dev=2]model.layers.42.self_attn.k_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/57 [00:01<00:02, 13.99w/s, dev=2]        model.layers.42.self_attn.o_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 26/57 [00:01<00:02, 14.49w/s, dev=2]model.layers.42.self_attn.q_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 27/57 [00:01<00:02, 14.66w/s, dev=2]model.layers.42.self_attn.q_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:01<00:01, 14.83w/s, dev=2]model.layers.42.self_attn.rotary_emb.inv_freq:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:01<00:01, 14.82w/s, dev=2]model.layers.42.self_attn.v_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 29/57 [00:01<00:01, 15.35w/s, dev=2]      model.layers.43.input_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 30/57 [00:01<00:01, 15.81w/s, dev=2] model.layers.43.mlp.down_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 31/57 [00:01<00:01, 16.33w/s, dev=2]  model.layers.43.mlp.down_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 32/57 [00:02<00:01, 15.54w/s, dev=2]model.layers.43.mlp.gate_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 32/57 [00:02<00:01, 15.54w/s, dev=2]model.layers.43.mlp.up_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 33/57 [00:02<00:01, 14.91w/s, dev=2]  model.layers.43.post_attention_layernorm.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:02<00:01, 14.37w/s, dev=2]model.layers.43.self_attn.k_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/57 [00:02<00:01, 14.79w/s, dev=2]        model.layers.43.self_attn.k_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 36/57 [00:02<00:01, 15.17w/s, dev=2]model.layers.43.self_attn.o_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 36/57 [00:02<00:01, 15.17w/s, dev=2]model.layers.43.self_attn.q_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 37/57 [00:02<00:01, 15.31w/s, dev=2]model.layers.43.self_attn.rotary_emb.inv_freq:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 38/57 [00:02<00:01, 15.44w/s, dev=2]model.layers.43.self_attn.v_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 39/57 [00:02<00:01, 15.84w/s, dev=2]      model.layers.44.input_layernorm.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:02<00:01, 16.20w/s, dev=2] model.layers.44.mlp.down_proj.weight:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/57 [00:02<00:00, 16.61w/s, dev=2]  model.layers.44.mlp.down_proj.weight:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 42/57 [00:02<00:00, 16.00w/s, dev=2]model.layers.44.mlp.gate_proj.weight:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 42/57 [00:02<00:00, 16.00w/s, dev=2]model.layers.44.mlp.up_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 43/57 [00:02<00:00, 15.43w/s, dev=2]  model.layers.44.post_attention_layernorm.weight:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 44/57 [00:02<00:00, 14.93w/s, dev=2]model.layers.44.self_attn.k_proj.weight:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 45/57 [00:02<00:00, 15.27w/s, dev=2]        model.layers.44.self_attn.o_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:02<00:00, 15.57w/s, dev=2]model.layers.44.self_attn.q_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 47/57 [00:03<00:00, 15.66w/s, dev=2]model.layers.44.self_attn.q_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/57 [00:03<00:00, 15.75w/s, dev=2]model.layers.44.self_attn.rotary_emb.inv_freq:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/57 [00:03<00:00, 15.75w/s, dev=2]model.layers.44.self_attn.v_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 49/57 [00:03<00:00, 16.07w/s, dev=2]      model.layers.45.mlp.down_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 50/57 [00:03<00:00, 16.37w/s, dev=2]   model.layers.45.mlp.gate_proj.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 51/57 [00:03<00:00, 15.84w/s, dev=2]model.layers.45.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:03<00:00, 15.37w/s, dev=2]model.layers.45.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 53/57 [00:03<00:00, 15.63w/s, dev=2]model.layers.45.self_attn.o_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/57 [00:03<00:00, 15.71w/s, dev=2]model.layers.45.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/57 [00:03<00:00, 15.71w/s, dev=2]model.layers.45.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 55/57 [00:03<00:00, 15.80w/s, dev=2]model.layers.45.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 56/57 [00:03<00:00, 16.08w/s, dev=2]                                                                                                      0%|          | 0/59 [00:00<?, ?w/s]model.layers.45.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=2]model.layers.45.mlp.up_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 1054.64w/s, dev=2]model.layers.45.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 13.79w/s, dev=2]  model.layers.45.post_attention_layernorm.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 13.76w/s, dev=2]model.layers.46.input_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:02, 20.59w/s, dev=2]         model.layers.46.mlp.down_proj.weight:   7%|â–‹         | 4/59 [00:00<00:02, 27.40w/s, dev=2]  model.layers.46.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 17.48w/s, dev=2]model.layers.46.mlp.gate_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 17.46w/s, dev=2]model.layers.46.mlp.up_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:03, 13.94w/s, dev=2]  model.layers.46.post_attention_layernorm.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:04, 12.05w/s, dev=2]model.layers.46.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:03, 13.75w/s, dev=2]model.layers.46.self_attn.k_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:03, 13.75w/s, dev=2]        model.layers.46.self_attn.o_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:03, 15.32w/s, dev=2]model.layers.46.self_attn.q_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:03, 15.91w/s, dev=2]model.layers.46.self_attn.rotary_emb.inv_freq:  19%|â–ˆâ–Š        | 11/59 [00:00<00:02, 16.43w/s, dev=2]model.layers.46.self_attn.v_proj.weight:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:02, 17.91w/s, dev=2]      model.layers.47.input_layernorm.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:02, 19.19w/s, dev=2] model.layers.47.mlp.down_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:02, 20.66w/s, dev=2]  model.layers.47.mlp.down_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:02, 18.20w/s, dev=2]model.layers.47.mlp.gate_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:02, 18.20w/s, dev=2]model.layers.47.mlp.up_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:00<00:02, 16.53w/s, dev=2]  model.layers.47.post_attention_layernorm.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:01<00:02, 15.36w/s, dev=2]model.layers.47.self_attn.k_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:02, 16.25w/s, dev=2]        model.layers.47.self_attn.o_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:02, 17.05w/s, dev=2]model.layers.47.self_attn.q_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:02, 17.30w/s, dev=2]model.layers.47.self_attn.rotary_emb.inv_freq:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:01<00:02, 17.52w/s, dev=2]model.layers.47.self_attn.rotary_emb.inv_freq:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:02, 18.34w/s, dev=2]model.layers.47.self_attn.v_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:02, 18.34w/s, dev=2]      model.layers.48.input_layernorm.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:01, 19.05w/s, dev=2] model.layers.48.mlp.down_proj.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:01, 19.87w/s, dev=2]  model.layers.48.mlp.gate_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:01, 18.43w/s, dev=2]model.layers.48.mlp.up_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:01, 17.25w/s, dev=2]  model.layers.48.post_attention_layernorm.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:01<00:01, 16.30w/s, dev=2]model.layers.48.self_attn.k_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:01, 16.89w/s, dev=2]        model.layers.48.self_attn.k_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:01, 17.42w/s, dev=2]model.layers.48.self_attn.o_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:01, 17.41w/s, dev=2]model.layers.48.self_attn.q_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:01<00:01, 17.57w/s, dev=2]model.layers.48.self_attn.rotary_emb.inv_freq:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:01<00:01, 17.72w/s, dev=2]model.layers.48.self_attn.v_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:01<00:01, 18.28w/s, dev=2]      model.layers.49.input_layernorm.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:01<00:01, 18.77w/s, dev=2] model.layers.49.mlp.down_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:01<00:01, 19.34w/s, dev=2]  model.layers.49.mlp.gate_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:01<00:01, 18.32w/s, dev=2]model.layers.49.mlp.gate_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:02<00:01, 17.41w/s, dev=2]model.layers.49.mlp.up_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:02<00:01, 17.40w/s, dev=2]  model.layers.49.post_attention_layernorm.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:02<00:01, 16.60w/s, dev=2]model.layers.49.self_attn.k_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:02<00:01, 17.04w/s, dev=2]        model.layers.49.self_attn.o_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:02<00:01, 17.41w/s, dev=2]model.layers.49.self_attn.q_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:02<00:01, 17.52w/s, dev=2]model.layers.49.self_attn.rotary_emb.inv_freq:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:02<00:01, 17.64w/s, dev=2]model.layers.49.self_attn.v_proj.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:02<00:00, 18.07w/s, dev=2]      model.layers.49.self_attn.v_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:02<00:00, 18.45w/s, dev=2]model.layers.50.input_layernorm.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:02<00:00, 18.45w/s, dev=2] model.layers.50.mlp.down_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:02<00:00, 18.88w/s, dev=2]  model.layers.50.mlp.gate_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:02<00:00, 18.15w/s, dev=2]model.layers.50.mlp.up_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:02<00:00, 17.45w/s, dev=2]  model.layers.50.post_attention_layernorm.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:02<00:00, 16.87w/s, dev=2]model.layers.50.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:02<00:00, 17.23w/s, dev=2]        model.layers.50.self_attn.o_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:02<00:00, 17.54w/s, dev=2]model.layers.50.self_attn.o_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:02<00:00, 17.60w/s, dev=2]model.layers.50.self_attn.q_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:02<00:00, 17.60w/s, dev=2]model.layers.50.self_attn.rotary_emb.inv_freq:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:02<00:00, 17.68w/s, dev=2]model.layers.50.self_attn.v_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:02<00:00, 18.02w/s, dev=2]      model.layers.51.mlp.gate_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:02<00:00, 18.33w/s, dev=2]   model.layers.51.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:03<00:00, 17.68w/s, dev=2]model.layers.51.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:03<00:00, 17.97w/s, dev=2]model.layers.51.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:03<00:00, 18.02w/s, dev=2]model.layers.51.self_attn.q_proj.weight:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:03<00:00, 18.05w/s, dev=2]model.layers.51.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:03<00:00, 18.05w/s, dev=2]model.layers.51.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:03<00:00, 18.37w/s, dev=2]                                                                                                      0%|          | 0/59 [00:00<?, ?w/s]model.layers.51.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=2]model.layers.51.mlp.down_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 1010.43w/s, dev=2]model.layers.51.mlp.down_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 12.86w/s, dev=2]  model.layers.51.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 12.84w/s, dev=2]  model.layers.51.post_attention_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:05,  9.80w/s, dev=2]model.layers.51.post_attention_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:04, 13.04w/s, dev=2]model.layers.52.input_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:04, 13.03w/s, dev=2]         model.layers.52.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 16.28w/s, dev=2]  model.layers.52.mlp.down_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:04, 12.84w/s, dev=2]model.layers.52.mlp.gate_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:04, 12.83w/s, dev=2]model.layers.52.mlp.up_proj.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:04, 11.05w/s, dev=2]  model.layers.52.mlp.up_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:05, 10.10w/s, dev=2]model.layers.52.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:05, 10.09w/s, dev=2]model.layers.52.self_attn.k_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:04, 11.35w/s, dev=2]        model.layers.52.self_attn.o_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:03, 12.48w/s, dev=2]model.layers.52.self_attn.q_proj.weight:  19%|â–ˆâ–Š        | 11/59 [00:00<00:03, 13.00w/s, dev=2]model.layers.52.self_attn.rotary_emb.inv_freq:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:03, 13.47w/s, dev=2]model.layers.52.self_attn.v_proj.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:03, 14.59w/s, dev=2]      model.layers.52.self_attn.v_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:02, 15.59w/s, dev=2]model.layers.53.input_layernorm.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:02, 15.59w/s, dev=2] model.layers.53.mlp.down_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:02, 16.70w/s, dev=2]  model.layers.53.mlp.gate_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:02, 15.17w/s, dev=2]model.layers.53.mlp.up_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:01<00:02, 14.07w/s, dev=2]  model.layers.53.post_attention_layernorm.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:03, 13.26w/s, dev=2]model.layers.53.self_attn.k_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:02, 13.99w/s, dev=2]        model.layers.53.self_attn.k_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:02, 14.66w/s, dev=2]model.layers.53.self_attn.o_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:02, 14.66w/s, dev=2]model.layers.53.self_attn.q_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:01<00:02, 14.94w/s, dev=2]model.layers.53.self_attn.rotary_emb.inv_freq:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:02, 15.22w/s, dev=2]model.layers.53.self_attn.v_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:02, 15.90w/s, dev=2]      model.layers.54.input_layernorm.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:02, 16.53w/s, dev=2] model.layers.54.mlp.down_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:01, 17.21w/s, dev=2]  model.layers.54.mlp.down_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:02, 16.33w/s, dev=2]model.layers.54.mlp.gate_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:02, 16.33w/s, dev=2]model.layers.54.mlp.up_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:01<00:02, 15.44w/s, dev=2]  model.layers.54.post_attention_layernorm.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:02, 14.65w/s, dev=2]model.layers.54.self_attn.k_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:01, 15.17w/s, dev=2]        model.layers.54.self_attn.o_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:01<00:01, 15.64w/s, dev=2]model.layers.54.self_attn.q_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:01<00:01, 15.77w/s, dev=2]model.layers.54.self_attn.q_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:02<00:01, 15.89w/s, dev=2]model.layers.54.self_attn.rotary_emb.inv_freq:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:02<00:01, 15.89w/s, dev=2]model.layers.54.self_attn.v_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:02<00:01, 16.38w/s, dev=2]      model.layers.55.input_layernorm.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:02<00:01, 16.82w/s, dev=2] model.layers.55.mlp.down_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:02<00:01, 17.32w/s, dev=2]  model.layers.55.mlp.gate_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:02<00:01, 16.48w/s, dev=2]model.layers.55.mlp.up_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:02<00:01, 15.86w/s, dev=2]  model.layers.55.mlp.up_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:02<00:01, 15.33w/s, dev=2]model.layers.55.post_attention_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:02<00:01, 15.33w/s, dev=2]model.layers.55.self_attn.k_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:02<00:01, 15.73w/s, dev=2]        model.layers.55.self_attn.o_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:02<00:01, 16.08w/s, dev=2]model.layers.55.self_attn.q_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:02<00:01, 16.18w/s, dev=2]model.layers.55.self_attn.rotary_emb.inv_freq:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:02<00:01, 16.26w/s, dev=2]model.layers.55.self_attn.v_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:02<00:00, 16.64w/s, dev=2]      model.layers.55.self_attn.v_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:02<00:00, 16.97w/s, dev=2]model.layers.56.input_layernorm.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:02<00:00, 16.97w/s, dev=2] model.layers.56.mlp.down_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:02<00:00, 17.35w/s, dev=2]  model.layers.56.mlp.gate_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:02<00:00, 16.75w/s, dev=2]model.layers.56.mlp.up_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:02<00:00, 16.17w/s, dev=2]  model.layers.56.post_attention_layernorm.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:03<00:00, 15.66w/s, dev=2]model.layers.56.self_attn.k_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:03<00:00, 15.98w/s, dev=2]        model.layers.56.self_attn.k_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:03<00:00, 16.27w/s, dev=2]model.layers.56.self_attn.o_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:03<00:00, 16.26w/s, dev=2]model.layers.56.self_attn.q_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:03<00:00, 16.34w/s, dev=2]model.layers.56.self_attn.rotary_emb.inv_freq:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:03<00:00, 16.43w/s, dev=2]model.layers.56.self_attn.v_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:03<00:00, 16.75w/s, dev=2]      model.layers.57.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:03<00:00, 17.03w/s, dev=2]model.layers.57.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:03<00:00, 17.31w/s, dev=2]model.layers.57.self_attn.o_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:03<00:00, 17.41w/s, dev=2]model.layers.57.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:03<00:00, 17.40w/s, dev=2]model.layers.57.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:03<00:00, 17.47w/s, dev=2]model.layers.57.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:03<00:00, 17.77w/s, dev=2]                                                                                                      0%|          | 0/55 [00:00<?, ?w/s]model.layers.57.input_layernorm.weight:   0%|          | 0/55 [00:00<?, ?w/s, dev=2]model.layers.57.mlp.down_proj.weight:   2%|â–         | 1/55 [00:00<00:00, 997.22w/s, dev=2]model.layers.57.mlp.down_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:04, 12.63w/s, dev=2] model.layers.57.mlp.gate_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:04, 12.61w/s, dev=2]model.layers.57.mlp.up_proj.weight:   5%|â–Œ         | 3/55 [00:00<00:05,  9.70w/s, dev=2]  model.layers.57.mlp.up_proj.weight:   7%|â–‹         | 4/55 [00:00<00:05,  8.57w/s, dev=2]model.layers.57.post_attention_layernorm.weight:   7%|â–‹         | 4/55 [00:00<00:05,  8.57w/s, dev=2]model.layers.58.input_layernorm.weight:   9%|â–‰         | 5/55 [00:00<00:04, 10.70w/s, dev=2]         model.layers.58.mlp.down_proj.weight:  11%|â–ˆ         | 6/55 [00:00<00:03, 12.83w/s, dev=2]  model.layers.58.mlp.down_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:04, 11.24w/s, dev=2]model.layers.58.mlp.gate_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:04, 11.24w/s, dev=2]model.layers.58.mlp.up_proj.weight:  15%|â–ˆâ–        | 8/55 [00:00<00:04, 10.32w/s, dev=2]  model.layers.58.post_attention_layernorm.weight:  16%|â–ˆâ–‹        | 9/55 [00:00<00:04,  9.66w/s, dev=2]model.layers.58.post_attention_layernorm.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:04, 10.73w/s, dev=2]model.layers.58.self_attn.k_proj.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:04, 10.72w/s, dev=2]        model.layers.58.self_attn.o_proj.weight:  20%|â–ˆâ–ˆ        | 11/55 [00:00<00:03, 11.70w/s, dev=2]model.layers.58.self_attn.q_proj.weight:  22%|â–ˆâ–ˆâ–       | 12/55 [00:00<00:03, 12.17w/s, dev=2]model.layers.58.self_attn.rotary_emb.inv_freq:  24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:01<00:03, 12.60w/s, dev=2]model.layers.58.self_attn.v_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 14/55 [00:01<00:03, 13.56w/s, dev=2]      model.layers.58.self_attn.v_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 15/55 [00:01<00:02, 14.41w/s, dev=2]model.layers.59.input_layernorm.weight:  27%|â–ˆâ–ˆâ–‹       | 15/55 [00:01<00:02, 14.41w/s, dev=2] model.layers.59.mlp.down_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 16/55 [00:01<00:02, 15.36w/s, dev=2]  model.layers.59.mlp.gate_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:01<00:02, 14.19w/s, dev=2]model.layers.59.mlp.up_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/55 [00:01<00:02, 13.22w/s, dev=2]  model.layers.59.post_attention_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–      | 19/55 [00:01<00:02, 12.57w/s, dev=2]model.layers.59.post_attention_layernorm.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:01<00:02, 13.23w/s, dev=2]model.layers.59.self_attn.k_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:01<00:02, 13.23w/s, dev=2]        model.layers.59.self_attn.o_proj.weight:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/55 [00:01<00:02, 13.82w/s, dev=2]model.layers.59.self_attn.q_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/55 [00:01<00:02, 14.06w/s, dev=2]model.layers.59.self_attn.rotary_emb.inv_freq:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 23/55 [00:01<00:02, 14.29w/s, dev=2]model.layers.59.self_attn.v_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 24/55 [00:01<00:02, 14.90w/s, dev=2]      model.layers.59.self_attn.v_proj.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:01<00:01, 15.47w/s, dev=2]model.layers.60.input_layernorm.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:01<00:01, 15.47w/s, dev=2] model.layers.60.mlp.down_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 26/55 [00:01<00:01, 16.08w/s, dev=2]  model.layers.60.mlp.gate_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 27/55 [00:01<00:01, 15.30w/s, dev=2]model.layers.60.mlp.up_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 28/55 [00:01<00:01, 14.63w/s, dev=2]  model.layers.60.post_attention_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/55 [00:02<00:01, 14.05w/s, dev=2]model.layers.60.post_attention_layernorm.weight:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 30/55 [00:02<00:01, 14.54w/s, dev=2]model.layers.60.self_attn.k_proj.weight:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 30/55 [00:02<00:01, 14.53w/s, dev=2]        model.layers.60.self_attn.o_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/55 [00:02<00:01, 14.97w/s, dev=2]model.layers.60.self_attn.q_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 32/55 [00:02<00:01, 15.14w/s, dev=2]model.layers.60.self_attn.rotary_emb.inv_freq:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/55 [00:02<00:01, 15.30w/s, dev=2]model.layers.60.self_attn.v_proj.weight:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/55 [00:02<00:01, 15.76w/s, dev=2]      model.layers.61.input_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:02<00:01, 16.17w/s, dev=2] model.layers.61.input_layernorm.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 36/55 [00:02<00:01, 16.63w/s, dev=2]model.layers.61.mlp.down_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 36/55 [00:02<00:01, 16.63w/s, dev=2]  model.layers.61.mlp.gate_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 37/55 [00:02<00:01, 15.97w/s, dev=2]model.layers.61.mlp.up_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 38/55 [00:02<00:01, 15.37w/s, dev=2]  model.layers.61.post_attention_layernorm.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 39/55 [00:02<00:01, 14.84w/s, dev=2]model.layers.61.self_attn.k_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 40/55 [00:02<00:00, 15.22w/s, dev=2]        model.layers.61.self_attn.o_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/55 [00:02<00:00, 15.56w/s, dev=2]model.layers.61.self_attn.o_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 42/55 [00:02<00:00, 15.68w/s, dev=2]model.layers.61.self_attn.q_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 42/55 [00:02<00:00, 15.68w/s, dev=2]model.layers.61.self_attn.rotary_emb.inv_freq:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 43/55 [00:02<00:00, 15.78w/s, dev=2]model.layers.61.self_attn.v_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 44/55 [00:02<00:00, 16.15w/s, dev=2]      model.layers.62.input_layernorm.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:03<00:00, 14.72w/s, dev=3] model.layers.62.mlp.down_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 46/55 [00:03<00:00, 15.04w/s, dev=3]  model.layers.62.mlp.gate_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 47/55 [00:03<00:00, 14.64w/s, dev=3]model.layers.62.mlp.gate_proj.weight:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 48/55 [00:03<00:00, 14.25w/s, dev=3]model.layers.62.mlp.up_proj.weight:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 48/55 [00:03<00:00, 14.25w/s, dev=3]  model.layers.62.post_attention_layernorm.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 49/55 [00:03<00:00, 13.90w/s, dev=3]model.layers.62.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 50/55 [00:03<00:00, 14.18w/s, dev=3]        model.layers.62.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 51/55 [00:03<00:00, 14.43w/s, dev=3]model.layers.62.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 52/55 [00:03<00:00, 14.52w/s, dev=3]model.layers.62.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 53/55 [00:03<00:00, 14.61w/s, dev=3]model.layers.62.self_attn.rotary_emb.inv_freq:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 54/55 [00:03<00:00, 14.89w/s, dev=3]model.layers.62.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 54/55 [00:03<00:00, 14.88w/s, dev=3]                                                                                                      0%|          | 0/57 [00:00<?, ?w/s]model.layers.63.input_layernorm.weight:   0%|          | 0/57 [00:00<?, ?w/s, dev=3]model.layers.63.mlp.down_proj.weight:   2%|â–         | 1/57 [00:00<00:00, 916.19w/s, dev=3]model.layers.63.mlp.down_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:04, 12.82w/s, dev=3] model.layers.63.mlp.gate_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:04, 12.79w/s, dev=3]model.layers.63.mlp.up_proj.weight:   5%|â–Œ         | 3/57 [00:00<00:05,  9.48w/s, dev=3]  model.layers.63.mlp.up_proj.weight:   7%|â–‹         | 4/57 [00:00<00:06,  8.35w/s, dev=3]model.layers.63.post_attention_layernorm.weight:   7%|â–‹         | 4/57 [00:00<00:06,  8.34w/s, dev=3]model.layers.63.self_attn.k_proj.weight:   9%|â–‰         | 5/57 [00:00<00:04, 10.42w/s, dev=3]        model.layers.63.self_attn.o_proj.weight:  11%|â–ˆ         | 6/57 [00:00<00:04, 12.33w/s, dev=3]model.layers.63.self_attn.q_proj.weight:  12%|â–ˆâ–        | 7/57 [00:00<00:03, 13.08w/s, dev=3]model.layers.63.self_attn.q_proj.weight:  14%|â–ˆâ–        | 8/57 [00:00<00:03, 13.74w/s, dev=3]model.layers.63.self_attn.rotary_emb.inv_freq:  14%|â–ˆâ–        | 8/57 [00:00<00:03, 13.73w/s, dev=3]model.layers.63.self_attn.v_proj.weight:  16%|â–ˆâ–Œ        | 9/57 [00:00<00:03, 15.44w/s, dev=3]      model.layers.64.input_layernorm.weight:  18%|â–ˆâ–Š        | 10/57 [00:00<00:02, 16.96w/s, dev=3]model.layers.64.mlp.down_proj.weight:  19%|â–ˆâ–‰        | 11/57 [00:00<00:02, 18.65w/s, dev=3]  model.layers.64.mlp.down_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:00<00:02, 15.93w/s, dev=3]model.layers.64.mlp.gate_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:00<00:02, 15.93w/s, dev=3]model.layers.64.mlp.up_proj.weight:  23%|â–ˆâ–ˆâ–Ž       | 13/57 [00:00<00:03, 14.24w/s, dev=3]  model.layers.64.post_attention_layernorm.weight:  25%|â–ˆâ–ˆâ–       | 14/57 [00:01<00:03, 13.12w/s, dev=3]model.layers.64.self_attn.k_proj.weight:  26%|â–ˆâ–ˆâ–‹       | 15/57 [00:01<00:02, 14.04w/s, dev=3]        model.layers.64.self_attn.k_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:01<00:02, 14.90w/s, dev=3]model.layers.64.self_attn.o_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:01<00:02, 14.89w/s, dev=3]model.layers.64.self_attn.q_proj.weight:  30%|â–ˆâ–ˆâ–‰       | 17/57 [00:01<00:02, 15.23w/s, dev=3]model.layers.64.self_attn.rotary_emb.inv_freq:  32%|â–ˆâ–ˆâ–ˆâ–      | 18/57 [00:01<00:02, 15.52w/s, dev=3]model.layers.64.self_attn.v_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 19/57 [00:01<00:02, 16.38w/s, dev=3]      model.layers.65.input_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 20/57 [00:01<00:02, 17.14w/s, dev=3] model.layers.65.mlp.down_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 21/57 [00:01<00:02, 17.99w/s, dev=3]  model.layers.65.mlp.down_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:01<00:02, 16.68w/s, dev=3]model.layers.65.mlp.gate_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:01<00:02, 16.68w/s, dev=3]model.layers.65.mlp.up_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 23/57 [00:01<00:02, 15.60w/s, dev=3]  model.layers.65.post_attention_layernorm.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/57 [00:01<00:02, 14.63w/s, dev=3]model.layers.65.self_attn.k_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/57 [00:01<00:02, 15.24w/s, dev=3]        model.layers.65.self_attn.o_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 26/57 [00:01<00:01, 15.78w/s, dev=3]model.layers.65.self_attn.q_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 27/57 [00:01<00:01, 15.91w/s, dev=3]model.layers.65.self_attn.q_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:01<00:01, 16.07w/s, dev=3]model.layers.65.self_attn.rotary_emb.inv_freq:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:01<00:01, 16.06w/s, dev=3]model.layers.65.self_attn.v_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 29/57 [00:01<00:01, 16.63w/s, dev=3]      model.layers.66.input_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 30/57 [00:01<00:01, 17.13w/s, dev=3] model.layers.66.mlp.down_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 31/57 [00:01<00:01, 17.70w/s, dev=3]  model.layers.66.mlp.gate_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 32/57 [00:01<00:01, 16.80w/s, dev=3]model.layers.66.mlp.up_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 33/57 [00:02<00:01, 15.97w/s, dev=3]  model.layers.66.mlp.up_proj.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:02<00:01, 15.38w/s, dev=3]model.layers.66.post_attention_layernorm.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:02<00:01, 15.38w/s, dev=3]model.layers.66.self_attn.k_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/57 [00:02<00:01, 15.83w/s, dev=3]        model.layers.66.self_attn.o_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 36/57 [00:02<00:01, 16.22w/s, dev=3]model.layers.66.self_attn.q_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 37/57 [00:02<00:01, 16.36w/s, dev=3]model.layers.66.self_attn.rotary_emb.inv_freq:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 38/57 [00:02<00:01, 16.49w/s, dev=3]model.layers.66.self_attn.v_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 39/57 [00:02<00:01, 16.92w/s, dev=3]      model.layers.66.self_attn.v_proj.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:02<00:00, 17.30w/s, dev=3]model.layers.67.input_layernorm.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:02<00:00, 17.30w/s, dev=3] model.layers.67.mlp.down_proj.weight:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/57 [00:02<00:00, 17.73w/s, dev=3]  model.layers.67.mlp.gate_proj.weight:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 42/57 [00:02<00:00, 16.97w/s, dev=3]model.layers.67.mlp.up_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 43/57 [00:02<00:00, 16.26w/s, dev=3]  model.layers.67.post_attention_layernorm.weight:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 44/57 [00:02<00:00, 15.66w/s, dev=3]model.layers.67.self_attn.k_proj.weight:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 45/57 [00:02<00:00, 16.01w/s, dev=3]        model.layers.67.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:02<00:00, 16.33w/s, dev=3]model.layers.67.self_attn.o_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:02<00:00, 16.33w/s, dev=3]model.layers.67.self_attn.q_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 47/57 [00:02<00:00, 16.42w/s, dev=3]model.layers.67.self_attn.rotary_emb.inv_freq:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/57 [00:02<00:00, 16.50w/s, dev=3]model.layers.67.self_attn.v_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 49/57 [00:02<00:00, 16.84w/s, dev=3]      model.layers.68.mlp.down_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 50/57 [00:02<00:00, 17.14w/s, dev=3]   model.layers.68.mlp.gate_proj.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 51/57 [00:03<00:00, 16.61w/s, dev=3]model.layers.68.mlp.gate_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:03<00:00, 16.06w/s, dev=3]model.layers.68.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:03<00:00, 16.06w/s, dev=3]model.layers.68.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 53/57 [00:03<00:00, 16.32w/s, dev=3]model.layers.68.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/57 [00:03<00:00, 16.37w/s, dev=3]model.layers.68.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 55/57 [00:03<00:00, 16.42w/s, dev=3]model.layers.68.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 56/57 [00:03<00:00, 16.71w/s, dev=3]                                                                                                      0%|          | 0/59 [00:00<?, ?w/s]model.layers.68.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=3]model.layers.68.mlp.up_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 899.87w/s, dev=3]model.layers.68.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 12.02w/s, dev=3] model.layers.68.post_attention_layernorm.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 12.00w/s, dev=3]model.layers.69.input_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:03, 17.95w/s, dev=3]         model.layers.69.mlp.down_proj.weight:   7%|â–‹         | 4/59 [00:00<00:02, 23.89w/s, dev=3]  model.layers.69.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 14.99w/s, dev=3]model.layers.69.mlp.gate_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 14.98w/s, dev=3]model.layers.69.mlp.up_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:04, 12.18w/s, dev=3]  model.layers.69.post_attention_layernorm.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:04, 11.01w/s, dev=3]model.layers.69.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:04, 12.57w/s, dev=3]model.layers.69.self_attn.k_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:04, 12.57w/s, dev=3]        model.layers.69.self_attn.o_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:03, 14.01w/s, dev=3]model.layers.69.self_attn.q_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:03, 14.64w/s, dev=3]model.layers.69.self_attn.rotary_emb.inv_freq:  19%|â–ˆâ–Š        | 11/59 [00:00<00:03, 15.20w/s, dev=3]model.layers.69.self_attn.v_proj.weight:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:02, 16.57w/s, dev=3]      model.layers.70.input_layernorm.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:02, 17.80w/s, dev=3] model.layers.70.mlp.down_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:02, 19.16w/s, dev=3]  model.layers.70.mlp.down_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:02, 17.07w/s, dev=3]model.layers.70.mlp.gate_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:02, 17.07w/s, dev=3]model.layers.70.mlp.up_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:02, 15.79w/s, dev=3]  model.layers.70.post_attention_layernorm.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:01<00:02, 14.79w/s, dev=3]model.layers.70.self_attn.k_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:02, 15.65w/s, dev=3]        model.layers.70.self_attn.o_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:02, 16.44w/s, dev=3]model.layers.70.self_attn.q_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:02, 16.74w/s, dev=3]model.layers.70.self_attn.rotary_emb.inv_freq:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:01<00:02, 17.00w/s, dev=3]model.layers.70.self_attn.rotary_emb.inv_freq:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:02, 17.80w/s, dev=3]model.layers.70.self_attn.v_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:02, 17.80w/s, dev=3]      model.layers.71.input_layernorm.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:01, 18.53w/s, dev=3] model.layers.71.mlp.down_proj.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:01, 19.33w/s, dev=3]  model.layers.71.mlp.gate_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:01, 18.18w/s, dev=3]model.layers.71.mlp.up_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:01, 17.22w/s, dev=3]  model.layers.71.post_attention_layernorm.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:01<00:01, 16.40w/s, dev=3]model.layers.71.self_attn.k_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:01, 17.00w/s, dev=3]        model.layers.71.self_attn.k_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:01, 17.54w/s, dev=3]model.layers.71.self_attn.o_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:01, 17.54w/s, dev=3]model.layers.71.self_attn.q_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:01<00:01, 17.66w/s, dev=3]model.layers.71.self_attn.rotary_emb.inv_freq:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:01<00:01, 17.84w/s, dev=3]model.layers.71.self_attn.v_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:01<00:01, 18.41w/s, dev=3]      model.layers.72.input_layernorm.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:01<00:01, 18.92w/s, dev=3] model.layers.72.mlp.down_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:01<00:01, 19.49w/s, dev=3]  model.layers.72.mlp.gate_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:01<00:01, 18.63w/s, dev=3]model.layers.72.mlp.gate_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:02<00:01, 17.90w/s, dev=3]model.layers.72.mlp.up_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:02<00:01, 17.90w/s, dev=3]  model.layers.72.post_attention_layernorm.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:02<00:01, 17.23w/s, dev=3]model.layers.72.self_attn.k_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:02<00:01, 17.69w/s, dev=3]        model.layers.72.self_attn.o_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:02<00:01, 18.11w/s, dev=3]model.layers.72.self_attn.q_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:02<00:01, 18.25w/s, dev=3]model.layers.72.self_attn.rotary_emb.inv_freq:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:02<00:00, 18.37w/s, dev=3]model.layers.72.self_attn.v_proj.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:02<00:00, 18.82w/s, dev=3]      model.layers.72.self_attn.v_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:02<00:00, 19.22w/s, dev=3]model.layers.73.input_layernorm.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:02<00:00, 19.22w/s, dev=3] model.layers.73.mlp.down_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:02<00:00, 19.66w/s, dev=3]  model.layers.73.mlp.gate_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:02<00:00, 18.98w/s, dev=3]model.layers.73.mlp.up_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:02<00:00, 18.35w/s, dev=3]  model.layers.73.post_attention_layernorm.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:02<00:00, 17.80w/s, dev=3]model.layers.73.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:02<00:00, 18.18w/s, dev=3]        model.layers.73.self_attn.o_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:02<00:00, 18.52w/s, dev=3]model.layers.73.self_attn.o_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:02<00:00, 18.62w/s, dev=3]model.layers.73.self_attn.q_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:02<00:00, 18.62w/s, dev=3]model.layers.73.self_attn.rotary_emb.inv_freq:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:02<00:00, 18.71w/s, dev=3]model.layers.73.self_attn.v_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:02<00:00, 19.08w/s, dev=3]      model.layers.74.mlp.gate_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:02<00:00, 19.40w/s, dev=3]   model.layers.74.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:02<00:00, 18.83w/s, dev=3]model.layers.74.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:02<00:00, 19.14w/s, dev=3]model.layers.74.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:02<00:00, 19.23w/s, dev=3]model.layers.74.self_attn.q_proj.weight:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:02<00:00, 19.31w/s, dev=3]model.layers.74.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:02<00:00, 19.31w/s, dev=3]model.layers.74.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:02<00:00, 19.65w/s, dev=3]                                                                                                      0%|          | 0/55 [00:00<?, ?w/s]model.layers.74.input_layernorm.weight:   0%|          | 0/55 [00:00<?, ?w/s, dev=3]model.layers.74.mlp.down_proj.weight:   2%|â–         | 1/55 [00:00<00:00, 961.11w/s, dev=3]model.layers.74.mlp.down_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:04, 12.97w/s, dev=3] model.layers.74.mlp.up_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:04, 12.94w/s, dev=3]  model.layers.74.post_attention_layernorm.weight:   5%|â–Œ         | 3/55 [00:00<00:05,  9.82w/s, dev=3]model.layers.74.post_attention_layernorm.weight:   7%|â–‹         | 4/55 [00:00<00:03, 13.07w/s, dev=3]model.layers.75.input_layernorm.weight:   7%|â–‹         | 4/55 [00:00<00:03, 13.06w/s, dev=3]         model.layers.75.mlp.down_proj.weight:   9%|â–‰         | 5/55 [00:00<00:03, 16.31w/s, dev=3]  model.layers.75.mlp.down_proj.weight:  11%|â–ˆ         | 6/55 [00:00<00:03, 13.15w/s, dev=3]model.layers.75.mlp.gate_proj.weight:  11%|â–ˆ         | 6/55 [00:00<00:03, 13.14w/s, dev=3]model.layers.75.mlp.up_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:04, 11.52w/s, dev=3]  model.layers.75.mlp.up_proj.weight:  15%|â–ˆâ–        | 8/55 [00:00<00:04, 10.58w/s, dev=3]model.layers.75.post_attention_layernorm.weight:  15%|â–ˆâ–        | 8/55 [00:00<00:04, 10.57w/s, dev=3]model.layers.75.self_attn.k_proj.weight:  16%|â–ˆâ–‹        | 9/55 [00:00<00:03, 11.89w/s, dev=3]        model.layers.75.self_attn.o_proj.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:03, 13.09w/s, dev=3]model.layers.75.self_attn.q_proj.weight:  20%|â–ˆâ–ˆ        | 11/55 [00:00<00:03, 13.66w/s, dev=3]model.layers.75.self_attn.rotary_emb.inv_freq:  22%|â–ˆâ–ˆâ–       | 12/55 [00:00<00:03, 14.15w/s, dev=3]model.layers.75.self_attn.v_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:00<00:02, 15.32w/s, dev=3]      model.layers.75.self_attn.v_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 14/55 [00:00<00:02, 16.34w/s, dev=3]model.layers.76.input_layernorm.weight:  25%|â–ˆâ–ˆâ–Œ       | 14/55 [00:00<00:02, 16.33w/s, dev=3] model.layers.76.mlp.down_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 15/55 [00:00<00:02, 17.49w/s, dev=3]  model.layers.76.mlp.gate_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 16/55 [00:01<00:02, 15.92w/s, dev=3]model.layers.76.mlp.up_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:01<00:02, 14.79w/s, dev=3]  model.layers.76.post_attention_layernorm.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/55 [00:01<00:02, 14.02w/s, dev=3]model.layers.76.self_attn.k_proj.weight:  35%|â–ˆâ–ˆâ–ˆâ–      | 19/55 [00:01<00:02, 14.80w/s, dev=3]        model.layers.76.self_attn.k_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:01<00:02, 15.48w/s, dev=3]model.layers.76.self_attn.o_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:01<00:02, 15.48w/s, dev=3]model.layers.76.self_attn.q_proj.weight:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/55 [00:01<00:02, 15.75w/s, dev=3]model.layers.76.self_attn.rotary_emb.inv_freq:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/55 [00:01<00:02, 16.02w/s, dev=3]model.layers.76.self_attn.v_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 23/55 [00:01<00:01, 16.74w/s, dev=3]      model.layers.77.input_layernorm.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 24/55 [00:01<00:01, 17.37w/s, dev=3] model.layers.77.mlp.down_proj.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:01<00:01, 18.09w/s, dev=3]  model.layers.77.mlp.down_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 26/55 [00:01<00:01, 17.16w/s, dev=3]model.layers.77.mlp.gate_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 26/55 [00:01<00:01, 17.15w/s, dev=3]model.layers.77.mlp.up_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 27/55 [00:01<00:01, 16.21w/s, dev=3]  model.layers.77.post_attention_layernorm.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 28/55 [00:01<00:01, 15.44w/s, dev=3]model.layers.77.self_attn.k_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/55 [00:01<00:01, 15.99w/s, dev=3]        model.layers.77.self_attn.o_proj.weight:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 30/55 [00:01<00:01, 16.46w/s, dev=3]model.layers.77.self_attn.q_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/55 [00:01<00:01, 16.60w/s, dev=3]model.layers.77.self_attn.q_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 32/55 [00:01<00:01, 16.73w/s, dev=3]model.layers.77.self_attn.rotary_emb.inv_freq:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 32/55 [00:01<00:01, 16.73w/s, dev=3]model.layers.77.self_attn.v_proj.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/55 [00:01<00:01, 17.25w/s, dev=3]      model.layers.78.input_layernorm.weight:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/55 [00:01<00:01, 17.72w/s, dev=3] model.layers.78.mlp.down_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:01<00:01, 18.23w/s, dev=3]  model.layers.78.mlp.gate_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 36/55 [00:02<00:01, 17.39w/s, dev=3]model.layers.78.mlp.up_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 37/55 [00:02<00:01, 16.57w/s, dev=3]  model.layers.78.mlp.up_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 38/55 [00:02<00:01, 15.83w/s, dev=3]model.layers.78.post_attention_layernorm.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 38/55 [00:02<00:01, 15.83w/s, dev=3]model.layers.78.self_attn.k_proj.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 39/55 [00:02<00:00, 16.25w/s, dev=3]        model.layers.78.self_attn.o_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 40/55 [00:02<00:00, 16.62w/s, dev=3]model.layers.78.self_attn.q_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/55 [00:02<00:00, 16.72w/s, dev=3]model.layers.78.self_attn.rotary_emb.inv_freq:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 42/55 [00:02<00:00, 16.81w/s, dev=3]model.layers.78.self_attn.v_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 43/55 [00:02<00:00, 17.20w/s, dev=3]      model.layers.78.self_attn.v_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 44/55 [00:02<00:00, 17.56w/s, dev=3]model.layers.79.input_layernorm.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 44/55 [00:02<00:00, 17.56w/s, dev=3] model.layers.79.mlp.down_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:02<00:00, 17.96w/s, dev=3]  model.layers.79.mlp.gate_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 46/55 [00:02<00:00, 17.31w/s, dev=3]model.layers.79.mlp.up_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 47/55 [00:02<00:00, 16.82w/s, dev=3]  model.layers.79.post_attention_layernorm.weight:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 48/55 [00:02<00:00, 16.41w/s, dev=3]model.layers.79.self_attn.k_proj.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 49/55 [00:02<00:00, 16.75w/s, dev=3]        model.layers.79.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 50/55 [00:02<00:00, 17.05w/s, dev=3]model.layers.79.self_attn.o_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 50/55 [00:02<00:00, 17.05w/s, dev=3]model.layers.79.self_attn.q_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 51/55 [00:02<00:00, 17.16w/s, dev=3]model.layers.79.self_attn.rotary_emb.inv_freq:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 52/55 [00:03<00:00, 17.27w/s, dev=3]model.layers.79.self_attn.v_proj.weight:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 53/55 [00:03<00:00, 17.60w/s, dev=3]      model.norm.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 54/55 [00:03<00:00, 17.89w/s, dev=3]                                                                                                0%|          | 0/1 [00:00<?, ?w/s]lm_head.weight:   0%|          | 0/1 [00:00<?, ?w/s, dev=3]lm_head.weight: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  6.73w/s, dev=3]                                                                   self.device: cuda:0
Loading BASE model stabilityai/stablelm-base-alpha-3b-v2...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:01<00:01,  1.52s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.19it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.06it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[2] MOVING BASE MODEL TO GPU.../home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for iamgroot42/mimir contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/iamgroot42/mimir
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
Loading dataset the_pile...
Loading from HuggingFace!
Loading dataset the_pile...
Loading from HuggingFace!
Generating samples:   0%|          | 0/21 [00:00<?, ?it/s]Generating samples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:00<00:00, 881.38it/s]
NEW N_SAMPLES IS  1000
Writing raw data to tmp_results/neo125_github_experiment/meta-llama_Llama-2-70b-hf/pile_cc_ngram_13_<0.8_truncated/raw_data.json
Writing raw data to tmp_results/neo125_github_experiment/meta-llama_Llama-2-70b-hf/pile_cc_ngram_13_<0.8_truncated/raw_data_lens.json
Computing criterion:   0%|          | 0/20 [00:00<?, ?it/s]Computing criterion:   5%|â–Œ         | 1/20 [00:20<06:34, 20.79s/it]Computing criterion:  10%|â–ˆ         | 2/20 [00:36<05:25, 18.07s/it]Computing criterion:  15%|â–ˆâ–Œ        | 3/20 [00:53<04:55, 17.37s/it]Computing criterion:  20%|â–ˆâ–ˆ        | 4/20 [01:09<04:27, 16.70s/it]Computing criterion:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [01:25<04:07, 16.51s/it]Computing criterion:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [01:41<03:48, 16.33s/it]Computing criterion:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [01:57<03:31, 16.28s/it]Computing criterion:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [02:13<03:12, 16.06s/it]Computing criterion:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [02:28<02:55, 15.96s/it]Computing criterion:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [02:44<02:38, 15.89s/it]Computing criterion:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [03:01<02:24, 16.09s/it]Computing criterion:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [03:16<02:08, 16.02s/it]Computing criterion:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [03:33<01:52, 16.10s/it]Computing criterion:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [03:49<01:37, 16.19s/it]Computing criterion:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [04:05<01:21, 16.22s/it]Computing criterion:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [04:21<01:03, 15.93s/it]Computing criterion:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [04:37<00:48, 16.01s/it]Computing criterion:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [04:53<00:32, 16.14s/it]Computing criterion:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [05:10<00:16, 16.18s/it]Computing criterion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [05:26<00:00, 16.25s/it]Computing criterion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [05:26<00:00, 16.33s/it]
Ref scores:   0%|          | 0/1000 [00:00<?, ?it/s]Ref scores:   0%|          | 1/1000 [00:11<3:03:28, 11.02s/it]Ref scores:   0%|          | 2/1000 [00:11<1:16:54,  4.62s/it]Ref scores:   0%|          | 3/1000 [00:11<42:55,  2.58s/it]  Ref scores:   0%|          | 4/1000 [00:11<26:57,  1.62s/it]Ref scores:   0%|          | 5/1000 [00:11<18:02,  1.09s/it]Ref scores:   1%|          | 6/1000 [00:11<12:36,  1.31it/s]Ref scores:   1%|          | 7/1000 [00:11<09:12,  1.80it/s]Ref scores:   1%|          | 8/1000 [00:12<06:58,  2.37it/s]Ref scores:   1%|          | 9/1000 [00:12<05:30,  3.00it/s]Ref scores:   1%|          | 10/1000 [00:12<04:29,  3.67it/s]Ref scores:   1%|          | 12/1000 [00:12<03:19,  4.96it/s]Ref scores:   1%|â–         | 13/1000 [00:12<02:56,  5.59it/s]Ref scores:   1%|â–         | 14/1000 [00:12<02:39,  6.19it/s]Ref scores:   2%|â–         | 15/1000 [00:12<02:25,  6.76it/s]Ref scores:   2%|â–         | 16/1000 [00:12<02:22,  6.93it/s]Ref scores:   2%|â–         | 17/1000 [00:13<02:19,  7.04it/s]Ref scores:   2%|â–         | 18/1000 [00:13<02:09,  7.60it/s]Ref scores:   2%|â–         | 19/1000 [00:13<02:04,  7.90it/s]Ref scores:   2%|â–         | 20/1000 [00:13<02:08,  7.65it/s]Ref scores:   2%|â–         | 21/1000 [00:13<02:09,  7.57it/s]Ref scores:   2%|â–         | 22/1000 [00:13<02:10,  7.49it/s]Ref scores:   2%|â–         | 24/1000 [00:13<01:53,  8.57it/s]Ref scores:   2%|â–Ž         | 25/1000 [00:14<01:52,  8.63it/s]Ref scores:   3%|â–Ž         | 26/1000 [00:14<01:58,  8.25it/s]Ref scores:   3%|â–Ž         | 27/1000 [00:14<02:02,  7.95it/s]Ref scores:   3%|â–Ž         | 28/1000 [00:14<02:05,  7.78it/s]Ref scores:   3%|â–Ž         | 30/1000 [00:14<01:55,  8.41it/s]Ref scores:   3%|â–Ž         | 31/1000 [00:14<01:58,  8.17it/s]Ref scores:   3%|â–Ž         | 32/1000 [00:14<01:56,  8.33it/s]Ref scores:   3%|â–Ž         | 33/1000 [00:15<01:54,  8.46it/s]Ref scores:   3%|â–Ž         | 34/1000 [00:15<01:52,  8.60it/s]Ref scores:   4%|â–Ž         | 35/1000 [00:15<01:57,  8.24it/s]Ref scores:   4%|â–Ž         | 36/1000 [00:15<02:00,  8.00it/s]Ref scores:   4%|â–Ž         | 37/1000 [00:15<02:03,  7.81it/s]Ref scores:   4%|â–         | 39/1000 [00:15<01:49,  8.79it/s]Ref scores:   4%|â–         | 40/1000 [00:15<01:48,  8.82it/s]Ref scores:   4%|â–         | 41/1000 [00:16<01:55,  8.28it/s]Ref scores:   4%|â–         | 42/1000 [00:16<01:59,  8.04it/s]Ref scores:   4%|â–         | 44/1000 [00:16<01:47,  8.90it/s]Ref scores:   4%|â–         | 45/1000 [00:16<01:52,  8.47it/s]Ref scores:   5%|â–         | 46/1000 [00:16<01:51,  8.56it/s]Ref scores:   5%|â–         | 47/1000 [00:16<01:56,  8.19it/s]Ref scores:   5%|â–         | 48/1000 [00:16<01:53,  8.38it/s]Ref scores:   5%|â–         | 49/1000 [00:16<01:52,  8.49it/s]Ref scores:   5%|â–Œ         | 50/1000 [00:17<01:56,  8.15it/s]Ref scores:   5%|â–Œ         | 51/1000 [00:17<01:59,  7.92it/s]Ref scores:   5%|â–Œ         | 52/1000 [00:17<02:01,  7.79it/s]Ref scores:   5%|â–Œ         | 53/1000 [00:17<02:14,  7.03it/s]Ref scores:   5%|â–Œ         | 54/1000 [00:17<02:05,  7.52it/s]Ref scores:   6%|â–Œ         | 55/1000 [00:17<02:05,  7.51it/s]Ref scores:   6%|â–Œ         | 56/1000 [00:17<02:06,  7.48it/s]Ref scores:   6%|â–Œ         | 57/1000 [00:18<02:00,  7.84it/s]Ref scores:   6%|â–Œ         | 58/1000 [00:18<01:55,  8.13it/s]Ref scores:   6%|â–Œ         | 59/1000 [00:18<01:59,  7.90it/s]Ref scores:   6%|â–Œ         | 60/1000 [00:18<02:00,  7.77it/s]Ref scores:   6%|â–Œ         | 61/1000 [00:18<02:03,  7.62it/s]Ref scores:   6%|â–Œ         | 62/1000 [00:18<02:04,  7.56it/s]Ref scores:   6%|â–‹         | 63/1000 [00:18<02:09,  7.21it/s]Ref scores:   6%|â–‹         | 64/1000 [00:18<02:09,  7.23it/s]Ref scores:   6%|â–‹         | 65/1000 [00:19<02:08,  7.26it/s]Ref scores:   7%|â–‹         | 66/1000 [00:19<02:08,  7.26it/s]Ref scores:   7%|â–‹         | 67/1000 [00:19<02:13,  7.00it/s]Ref scores:   7%|â–‹         | 68/1000 [00:19<02:11,  7.06it/s]Ref scores:   7%|â–‹         | 69/1000 [00:19<02:03,  7.51it/s]Ref scores:   7%|â–‹         | 71/1000 [00:19<01:53,  8.17it/s]Ref scores:   7%|â–‹         | 72/1000 [00:20<01:56,  7.97it/s]Ref scores:   7%|â–‹         | 73/1000 [00:20<01:59,  7.74it/s]Ref scores:   7%|â–‹         | 74/1000 [00:20<02:01,  7.60it/s]Ref scores:   8%|â–Š         | 75/1000 [00:20<01:56,  7.92it/s]Ref scores:   8%|â–Š         | 77/1000 [00:20<01:48,  8.50it/s]Ref scores:   8%|â–Š         | 78/1000 [00:20<01:47,  8.55it/s]Ref scores:   8%|â–Š         | 79/1000 [00:20<01:45,  8.74it/s]Ref scores:   8%|â–Š         | 80/1000 [00:21<02:03,  7.45it/s]Ref scores:   8%|â–Š         | 81/1000 [00:21<01:58,  7.78it/s]Ref scores:   8%|â–Š         | 82/1000 [00:21<01:59,  7.69it/s]Ref scores:   8%|â–Š         | 83/1000 [00:21<02:00,  7.62it/s]Ref scores:   8%|â–Š         | 84/1000 [00:21<02:01,  7.51it/s]Ref scores:   8%|â–Š         | 85/1000 [00:21<02:10,  7.03it/s]Ref scores:   9%|â–Š         | 86/1000 [00:21<02:08,  7.10it/s]Ref scores:   9%|â–Š         | 87/1000 [00:21<02:01,  7.54it/s]Ref scores:   9%|â–‰         | 88/1000 [00:22<01:53,  8.02it/s]Ref scores:   9%|â–‰         | 89/1000 [00:22<01:56,  7.82it/s]Ref scores:   9%|â–‰         | 90/1000 [00:22<01:58,  7.68it/s]Ref scores:   9%|â–‰         | 91/1000 [00:22<01:59,  7.63it/s]Ref scores:   9%|â–‰         | 92/1000 [00:22<01:59,  7.58it/s]Ref scores:   9%|â–‰         | 93/1000 [00:22<02:00,  7.53it/s]Ref scores:   9%|â–‰         | 94/1000 [00:22<01:54,  7.90it/s]Ref scores:  10%|â–‰         | 95/1000 [00:22<01:56,  7.74it/s]Ref scores:  10%|â–‰         | 96/1000 [00:23<01:59,  7.58it/s]Ref scores:  10%|â–‰         | 97/1000 [00:23<01:53,  7.93it/s]Ref scores:  10%|â–‰         | 98/1000 [00:23<01:56,  7.72it/s]Ref scores:  10%|â–‰         | 99/1000 [00:23<01:58,  7.61it/s]Ref scores:  10%|â–ˆ         | 101/1000 [00:23<01:49,  8.19it/s]Ref scores:  10%|â–ˆ         | 102/1000 [00:23<01:59,  7.53it/s]Ref scores:  10%|â–ˆ         | 103/1000 [00:24<01:54,  7.82it/s]Ref scores:  10%|â–ˆ         | 104/1000 [00:24<01:49,  8.19it/s]Ref scores:  10%|â–ˆ         | 105/1000 [00:24<01:52,  7.96it/s]Ref scores:  11%|â–ˆ         | 106/1000 [00:24<01:54,  7.78it/s]Ref scores:  11%|â–ˆ         | 107/1000 [00:24<02:01,  7.32it/s]Ref scores:  11%|â–ˆ         | 108/1000 [00:24<01:55,  7.71it/s]Ref scores:  11%|â–ˆ         | 109/1000 [00:24<01:51,  8.00it/s]Ref scores:  11%|â–ˆ         | 110/1000 [00:24<01:48,  8.21it/s]Ref scores:  11%|â–ˆ         | 111/1000 [00:25<01:51,  7.96it/s]Ref scores:  11%|â–ˆ         | 112/1000 [00:25<01:54,  7.78it/s]Ref scores:  11%|â–ˆâ–        | 113/1000 [00:25<01:56,  7.59it/s]Ref scores:  11%|â–ˆâ–        | 114/1000 [00:25<01:51,  7.93it/s]Ref scores:  12%|â–ˆâ–        | 115/1000 [00:25<01:47,  8.22it/s]Ref scores:  12%|â–ˆâ–        | 116/1000 [00:25<01:58,  7.47it/s]Ref scores:  12%|â–ˆâ–        | 117/1000 [00:25<01:52,  7.84it/s]Ref scores:  12%|â–ˆâ–        | 118/1000 [00:25<01:54,  7.67it/s]Ref scores:  12%|â–ˆâ–        | 119/1000 [00:26<02:00,  7.29it/s]Ref scores:  12%|â–ˆâ–        | 120/1000 [00:26<01:51,  7.88it/s]Ref scores:  12%|â–ˆâ–        | 121/1000 [00:26<01:47,  8.15it/s]Ref scores:  12%|â–ˆâ–        | 122/1000 [00:26<01:52,  7.82it/s]Ref scores:  12%|â–ˆâ–        | 123/1000 [00:26<01:48,  8.08it/s]Ref scores:  12%|â–ˆâ–        | 124/1000 [00:26<01:52,  7.81it/s]Ref scores:  12%|â–ˆâ–Ž        | 125/1000 [00:26<01:54,  7.65it/s]Ref scores:  13%|â–ˆâ–Ž        | 126/1000 [00:26<01:49,  7.98it/s]Ref scores:  13%|â–ˆâ–Ž        | 127/1000 [00:27<01:46,  8.24it/s]Ref scores:  13%|â–ˆâ–Ž        | 128/1000 [00:27<01:43,  8.41it/s]Ref scores:  13%|â–ˆâ–Ž        | 129/1000 [00:27<01:53,  7.71it/s]Ref scores:  13%|â–ˆâ–Ž        | 130/1000 [00:27<01:51,  7.80it/s]Ref scores:  13%|â–ˆâ–Ž        | 131/1000 [00:27<01:53,  7.68it/s]Ref scores:  13%|â–ˆâ–Ž        | 132/1000 [00:27<01:53,  7.62it/s]Ref scores:  13%|â–ˆâ–Ž        | 133/1000 [00:27<01:49,  7.94it/s]Ref scores:  13%|â–ˆâ–Ž        | 134/1000 [00:27<01:45,  8.18it/s]Ref scores:  14%|â–ˆâ–Ž        | 135/1000 [00:28<01:48,  7.96it/s]Ref scores:  14%|â–ˆâ–Ž        | 136/1000 [00:28<01:51,  7.74it/s]Ref scores:  14%|â–ˆâ–Ž        | 137/1000 [00:28<01:47,  8.01it/s]Ref scores:  14%|â–ˆâ–        | 138/1000 [00:28<01:50,  7.78it/s]Ref scores:  14%|â–ˆâ–        | 139/1000 [00:28<01:46,  8.05it/s]Ref scores:  14%|â–ˆâ–        | 140/1000 [00:28<01:49,  7.84it/s]Ref scores:  14%|â–ˆâ–        | 141/1000 [00:28<01:45,  8.11it/s]Ref scores:  14%|â–ˆâ–        | 142/1000 [00:28<01:48,  7.87it/s]Ref scores:  14%|â–ˆâ–        | 143/1000 [00:29<01:50,  7.72it/s]Ref scores:  14%|â–ˆâ–        | 144/1000 [00:29<01:46,  8.03it/s]Ref scores:  14%|â–ˆâ–        | 145/1000 [00:29<01:50,  7.76it/s]Ref scores:  15%|â–ˆâ–        | 146/1000 [00:29<01:52,  7.60it/s]Ref scores:  15%|â–ˆâ–        | 147/1000 [00:29<01:53,  7.53it/s]Ref scores:  15%|â–ˆâ–        | 149/1000 [00:29<01:38,  8.66it/s]Ref scores:  15%|â–ˆâ–Œ        | 150/1000 [00:29<01:42,  8.31it/s]Ref scores:  15%|â–ˆâ–Œ        | 152/1000 [00:30<01:32,  9.19it/s]Ref scores:  15%|â–ˆâ–Œ        | 153/1000 [00:30<01:37,  8.67it/s]Ref scores:  15%|â–ˆâ–Œ        | 154/1000 [00:30<01:42,  8.25it/s]Ref scores:  16%|â–ˆâ–Œ        | 156/1000 [00:30<01:37,  8.67it/s]Ref scores:  16%|â–ˆâ–Œ        | 158/1000 [00:30<01:35,  8.80it/s]Ref scores:  16%|â–ˆâ–Œ        | 159/1000 [00:30<01:40,  8.40it/s]Ref scores:  16%|â–ˆâ–Œ        | 160/1000 [00:31<01:46,  7.86it/s]Ref scores:  16%|â–ˆâ–Œ        | 161/1000 [00:31<01:48,  7.73it/s]Ref scores:  16%|â–ˆâ–‹        | 163/1000 [00:31<01:42,  8.18it/s]Ref scores:  16%|â–ˆâ–‹        | 164/1000 [00:31<01:40,  8.33it/s]Ref scores:  16%|â–ˆâ–‹        | 165/1000 [00:31<01:38,  8.49it/s]Ref scores:  17%|â–ˆâ–‹        | 166/1000 [00:31<01:42,  8.13it/s]Ref scores:  17%|â–ˆâ–‹        | 167/1000 [00:31<01:45,  7.91it/s]Ref scores:  17%|â–ˆâ–‹        | 168/1000 [00:32<01:39,  8.33it/s]Ref scores:  17%|â–ˆâ–‹        | 169/1000 [00:32<01:43,  8.04it/s]Ref scores:  17%|â–ˆâ–‹        | 171/1000 [00:32<01:37,  8.52it/s]Ref scores:  17%|â–ˆâ–‹        | 173/1000 [00:32<01:34,  8.71it/s]Ref scores:  17%|â–ˆâ–‹        | 174/1000 [00:32<01:38,  8.36it/s]Ref scores:  18%|â–ˆâ–Š        | 175/1000 [00:32<01:37,  8.46it/s]Ref scores:  18%|â–ˆâ–Š        | 176/1000 [00:33<01:36,  8.55it/s]Ref scores:  18%|â–ˆâ–Š        | 177/1000 [00:33<01:33,  8.78it/s]Ref scores:  18%|â–ˆâ–Š        | 178/1000 [00:33<01:33,  8.78it/s]Ref scores:  18%|â–ˆâ–Š        | 179/1000 [00:33<01:38,  8.34it/s]Ref scores:  18%|â–ˆâ–Š        | 180/1000 [00:33<01:41,  8.07it/s]Ref scores:  18%|â–ˆâ–Š        | 181/1000 [00:33<01:44,  7.86it/s]Ref scores:  18%|â–ˆâ–Š        | 182/1000 [00:33<01:45,  7.73it/s]Ref scores:  18%|â–ˆâ–Š        | 183/1000 [00:33<01:51,  7.32it/s]Ref scores:  18%|â–ˆâ–Š        | 185/1000 [00:34<01:37,  8.34it/s]Ref scores:  19%|â–ˆâ–Š        | 186/1000 [00:34<01:41,  8.04it/s]Ref scores:  19%|â–ˆâ–Š        | 187/1000 [00:34<01:43,  7.84it/s]Ref scores:  19%|â–ˆâ–‰        | 188/1000 [00:34<01:44,  7.73it/s]Ref scores:  19%|â–ˆâ–‰        | 189/1000 [00:34<01:47,  7.57it/s]Ref scores:  19%|â–ˆâ–‰        | 191/1000 [00:34<01:36,  8.42it/s]Ref scores:  19%|â–ˆâ–‰        | 192/1000 [00:35<01:40,  8.04it/s]Ref scores:  19%|â–ˆâ–‰        | 193/1000 [00:35<01:43,  7.78it/s]Ref scores:  19%|â–ˆâ–‰        | 194/1000 [00:35<01:40,  8.02it/s]Ref scores:  20%|â–ˆâ–‰        | 195/1000 [00:35<01:35,  8.39it/s]Ref scores:  20%|â–ˆâ–‰        | 196/1000 [00:35<01:34,  8.51it/s]Ref scores:  20%|â–ˆâ–‰        | 197/1000 [00:35<01:41,  7.94it/s]Ref scores:  20%|â–ˆâ–‰        | 198/1000 [00:35<01:44,  7.65it/s]Ref scores:  20%|â–ˆâ–‰        | 199/1000 [00:35<01:45,  7.56it/s]Ref scores:  20%|â–ˆâ–ˆ        | 200/1000 [00:36<01:46,  7.48it/s]Ref scores:  20%|â–ˆâ–ˆ        | 201/1000 [00:36<01:40,  7.92it/s]Ref scores:  20%|â–ˆâ–ˆ        | 202/1000 [00:36<01:43,  7.74it/s]Ref scores:  20%|â–ˆâ–ˆ        | 203/1000 [00:36<01:44,  7.61it/s]Ref scores:  20%|â–ˆâ–ˆ        | 204/1000 [00:36<01:45,  7.55it/s]Ref scores:  20%|â–ˆâ–ˆ        | 205/1000 [00:36<01:40,  7.90it/s]Ref scores:  21%|â–ˆâ–ˆ        | 206/1000 [00:36<01:43,  7.68it/s]Ref scores:  21%|â–ˆâ–ˆ        | 207/1000 [00:36<01:44,  7.58it/s]Ref scores:  21%|â–ˆâ–ˆ        | 208/1000 [00:37<01:45,  7.48it/s]Ref scores:  21%|â–ˆâ–ˆ        | 209/1000 [00:37<01:47,  7.39it/s]Ref scores:  21%|â–ˆâ–ˆ        | 210/1000 [00:37<01:47,  7.38it/s]Ref scores:  21%|â–ˆâ–ˆ        | 211/1000 [00:37<01:41,  7.76it/s]Ref scores:  21%|â–ˆâ–ˆ        | 212/1000 [00:37<01:43,  7.61it/s]Ref scores:  21%|â–ˆâ–ˆâ–       | 213/1000 [00:37<01:44,  7.51it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 215/1000 [00:37<01:26,  9.08it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 216/1000 [00:38<01:31,  8.54it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 217/1000 [00:38<01:35,  8.20it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 218/1000 [00:38<01:33,  8.34it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 219/1000 [00:38<01:31,  8.52it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 220/1000 [00:38<01:35,  8.19it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 221/1000 [00:38<01:37,  7.97it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 222/1000 [00:38<01:32,  8.42it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 223/1000 [00:38<01:36,  8.03it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 224/1000 [00:39<01:34,  8.25it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 226/1000 [00:39<01:29,  8.62it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 227/1000 [00:39<01:29,  8.67it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 228/1000 [00:39<01:32,  8.33it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 229/1000 [00:39<01:31,  8.46it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 230/1000 [00:39<01:34,  8.16it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 231/1000 [00:39<01:32,  8.34it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 232/1000 [00:40<01:35,  8.00it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 233/1000 [00:40<01:33,  8.23it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 234/1000 [00:40<01:36,  7.94it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 235/1000 [00:40<01:33,  8.17it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 236/1000 [00:40<01:36,  7.88it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 237/1000 [00:40<01:39,  7.68it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 238/1000 [00:40<01:40,  7.55it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 239/1000 [00:40<01:41,  7.52it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 240/1000 [00:41<01:38,  7.69it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 241/1000 [00:41<01:34,  8.00it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 242/1000 [00:41<01:41,  7.47it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 243/1000 [00:41<01:36,  7.81it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 244/1000 [00:41<01:37,  7.72it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 245/1000 [00:41<01:39,  7.61it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 246/1000 [00:41<01:34,  7.95it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 247/1000 [00:41<01:37,  7.72it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 248/1000 [00:42<01:40,  7.49it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 249/1000 [00:42<01:40,  7.47it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 250/1000 [00:42<01:40,  7.45it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 251/1000 [00:42<01:41,  7.40it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 252/1000 [00:42<01:41,  7.38it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 253/1000 [00:42<01:41,  7.37it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 254/1000 [00:42<01:41,  7.34it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 255/1000 [00:43<01:41,  7.35it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 256/1000 [00:43<01:35,  7.75it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 257/1000 [00:43<01:37,  7.63it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 258/1000 [00:43<01:38,  7.53it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 259/1000 [00:43<01:33,  7.91it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 260/1000 [00:43<01:35,  7.77it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 261/1000 [00:43<01:31,  8.07it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 262/1000 [00:43<01:34,  7.85it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 263/1000 [00:44<01:30,  8.18it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 264/1000 [00:44<01:32,  7.92it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 265/1000 [00:44<01:34,  7.79it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 266/1000 [00:44<01:30,  8.12it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 267/1000 [00:44<01:27,  8.35it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 268/1000 [00:44<01:30,  8.08it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 269/1000 [00:44<01:27,  8.37it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 270/1000 [00:44<01:30,  8.05it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 272/1000 [00:45<01:20,  9.03it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 273/1000 [00:45<01:24,  8.56it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 274/1000 [00:45<01:23,  8.67it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 275/1000 [00:45<01:27,  8.30it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 277/1000 [00:45<01:15,  9.53it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 278/1000 [00:45<01:16,  9.45it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 280/1000 [00:45<01:14,  9.69it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 281/1000 [00:46<01:33,  7.73it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 282/1000 [00:46<01:34,  7.62it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 283/1000 [00:46<01:34,  7.58it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 284/1000 [00:46<01:30,  7.89it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 285/1000 [00:46<01:31,  7.79it/s]Ref scores:  29%|â–ˆâ–ˆâ–Š       | 286/1000 [00:46<01:38,  7.23it/s]Ref scores:  29%|â–ˆâ–ˆâ–Š       | 287/1000 [00:46<01:32,  7.72it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 289/1000 [00:47<01:26,  8.22it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 290/1000 [00:47<01:25,  8.35it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 291/1000 [00:47<01:27,  8.08it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 292/1000 [00:47<01:30,  7.81it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 293/1000 [00:47<01:32,  7.67it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 294/1000 [00:47<01:37,  7.26it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 295/1000 [00:48<01:36,  7.30it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 296/1000 [00:48<01:36,  7.33it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 297/1000 [00:48<01:35,  7.33it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 298/1000 [00:48<01:35,  7.35it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 299/1000 [00:48<01:35,  7.36it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 300/1000 [00:48<01:30,  7.72it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 301/1000 [00:48<01:27,  8.00it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 302/1000 [00:48<01:24,  8.22it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 303/1000 [00:49<01:28,  7.85it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 304/1000 [00:49<01:24,  8.23it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 305/1000 [00:49<01:40,  6.93it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 306/1000 [00:49<01:33,  7.39it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 307/1000 [00:49<01:34,  7.32it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 308/1000 [00:49<01:34,  7.35it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 309/1000 [00:49<01:29,  7.75it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 310/1000 [00:49<01:31,  7.58it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 311/1000 [00:50<01:25,  8.10it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 312/1000 [00:50<01:23,  8.26it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆâ–      | 313/1000 [00:50<01:26,  7.94it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆâ–      | 314/1000 [00:50<01:28,  7.71it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 315/1000 [00:50<01:30,  7.55it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 316/1000 [00:50<01:26,  7.89it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 318/1000 [00:51<01:38,  6.95it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 319/1000 [00:51<01:36,  7.04it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 320/1000 [00:51<01:31,  7.44it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 321/1000 [00:51<01:27,  7.74it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 322/1000 [00:51<01:29,  7.58it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 323/1000 [00:51<01:25,  7.90it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 324/1000 [00:51<01:27,  7.72it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 325/1000 [00:51<01:24,  8.00it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 326/1000 [00:52<01:22,  8.21it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 327/1000 [00:52<01:24,  7.93it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 329/1000 [00:52<01:18,  8.51it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 330/1000 [00:52<01:18,  8.58it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 331/1000 [00:52<01:17,  8.63it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 332/1000 [00:52<01:21,  8.19it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 333/1000 [00:52<01:23,  7.94it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 334/1000 [00:53<01:21,  8.18it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 335/1000 [00:53<01:24,  7.90it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 336/1000 [00:53<01:25,  7.76it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 338/1000 [00:53<01:20,  8.22it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 340/1000 [00:53<01:13,  9.01it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 341/1000 [00:53<01:17,  8.54it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 342/1000 [00:53<01:20,  8.14it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 343/1000 [00:54<01:23,  7.88it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 345/1000 [00:54<01:21,  8.05it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 346/1000 [00:54<01:18,  8.36it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 347/1000 [00:54<01:21,  8.05it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 348/1000 [00:54<01:23,  7.82it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 349/1000 [00:54<01:25,  7.63it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 350/1000 [00:55<01:26,  7.53it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 351/1000 [00:55<01:21,  7.98it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 352/1000 [00:55<01:18,  8.21it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 353/1000 [00:55<01:21,  7.91it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 354/1000 [00:55<01:19,  8.15it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 355/1000 [00:55<01:27,  7.40it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 357/1000 [00:55<01:18,  8.17it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 358/1000 [00:56<01:21,  7.91it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 359/1000 [00:56<01:22,  7.77it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 360/1000 [00:56<01:19,  8.02it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 362/1000 [00:56<01:15,  8.43it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 363/1000 [00:56<01:18,  8.14it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 364/1000 [00:56<01:20,  7.88it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 366/1000 [00:56<01:09,  9.15it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 367/1000 [00:57<01:08,  9.25it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 368/1000 [00:57<01:13,  8.63it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 369/1000 [00:57<01:16,  8.20it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 370/1000 [00:57<01:19,  7.88it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 371/1000 [00:57<01:21,  7.69it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 372/1000 [00:57<01:17,  8.08it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 373/1000 [00:57<01:15,  8.33it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 374/1000 [00:57<01:17,  8.05it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 375/1000 [00:58<01:15,  8.33it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 376/1000 [00:58<01:11,  8.71it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 377/1000 [00:58<01:11,  8.77it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 378/1000 [00:58<01:14,  8.31it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 379/1000 [00:58<01:13,  8.48it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 380/1000 [00:58<01:16,  8.15it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 381/1000 [00:58<01:17,  7.96it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 382/1000 [00:58<01:19,  7.78it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 383/1000 [00:59<01:15,  8.19it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 384/1000 [00:59<01:13,  8.37it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 385/1000 [00:59<01:16,  8.03it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 386/1000 [00:59<01:18,  7.81it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 387/1000 [00:59<01:19,  7.70it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 388/1000 [00:59<01:20,  7.61it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 389/1000 [00:59<01:20,  7.59it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 390/1000 [00:59<01:16,  7.96it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 391/1000 [01:00<01:18,  7.75it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 392/1000 [01:00<01:19,  7.67it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 393/1000 [01:00<01:20,  7.57it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 394/1000 [01:00<01:20,  7.56it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 395/1000 [01:00<01:20,  7.54it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 396/1000 [01:00<01:16,  7.92it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 397/1000 [01:00<01:17,  7.74it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 398/1000 [01:00<01:14,  8.06it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 399/1000 [01:01<01:16,  7.82it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 400/1000 [01:01<01:17,  7.71it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 401/1000 [01:01<01:18,  7.65it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 402/1000 [01:01<01:14,  8.01it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 403/1000 [01:01<01:16,  7.84it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 404/1000 [01:01<01:17,  7.70it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 405/1000 [01:01<01:12,  8.18it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 406/1000 [01:01<01:15,  7.91it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 407/1000 [01:02<01:17,  7.67it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 408/1000 [01:02<01:21,  7.27it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 409/1000 [01:02<01:17,  7.67it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 410/1000 [01:02<01:14,  7.96it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 411/1000 [01:02<01:21,  7.27it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 412/1000 [01:02<01:16,  7.65it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 413/1000 [01:02<01:17,  7.54it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 414/1000 [01:03<01:18,  7.47it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 415/1000 [01:03<01:19,  7.40it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 416/1000 [01:03<01:16,  7.68it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 417/1000 [01:03<01:11,  8.10it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 419/1000 [01:03<01:07,  8.58it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 420/1000 [01:03<01:10,  8.19it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 421/1000 [01:03<01:08,  8.50it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 422/1000 [01:03<01:07,  8.58it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 423/1000 [01:04<01:06,  8.68it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 425/1000 [01:04<01:04,  8.98it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 426/1000 [01:04<01:06,  8.57it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 427/1000 [01:04<01:09,  8.22it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 428/1000 [01:04<01:11,  7.98it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 430/1000 [01:04<01:00,  9.44it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 431/1000 [01:05<01:15,  7.56it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 432/1000 [01:05<01:10,  8.01it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 433/1000 [01:05<01:12,  7.81it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 434/1000 [01:05<01:10,  8.07it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 435/1000 [01:05<01:07,  8.32it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 436/1000 [01:05<01:06,  8.49it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 437/1000 [01:05<01:05,  8.60it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 438/1000 [01:05<01:05,  8.62it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 439/1000 [01:06<01:09,  8.11it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 440/1000 [01:06<01:07,  8.28it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 441/1000 [01:06<01:09,  8.00it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 442/1000 [01:06<01:21,  6.88it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 443/1000 [01:06<01:14,  7.48it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 444/1000 [01:06<01:11,  7.81it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 446/1000 [01:06<01:07,  8.26it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 448/1000 [01:07<01:04,  8.51it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 449/1000 [01:07<01:07,  8.18it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 450/1000 [01:07<01:09,  7.95it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 451/1000 [01:07<01:07,  8.14it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 452/1000 [01:07<01:09,  7.86it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 453/1000 [01:07<01:07,  8.08it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 454/1000 [01:07<01:09,  7.88it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 456/1000 [01:08<01:01,  8.84it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 457/1000 [01:08<01:04,  8.43it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 458/1000 [01:08<01:06,  8.14it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 459/1000 [01:08<01:08,  7.88it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 461/1000 [01:08<01:00,  8.84it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 462/1000 [01:08<01:00,  8.87it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 463/1000 [01:08<01:03,  8.43it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 464/1000 [01:09<01:05,  8.13it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 465/1000 [01:09<01:02,  8.53it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 466/1000 [01:09<01:06,  8.08it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 467/1000 [01:09<01:07,  7.91it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 468/1000 [01:09<01:05,  8.16it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 469/1000 [01:09<01:06,  7.94it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 470/1000 [01:09<01:07,  7.80it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 471/1000 [01:10<01:09,  7.64it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 472/1000 [01:10<01:09,  7.57it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 473/1000 [01:10<01:06,  7.95it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 474/1000 [01:10<01:03,  8.26it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 475/1000 [01:10<01:05,  7.96it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 476/1000 [01:10<01:07,  7.80it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 477/1000 [01:10<01:04,  8.12it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 478/1000 [01:10<01:02,  8.33it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 479/1000 [01:10<01:04,  8.05it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 480/1000 [01:11<01:02,  8.29it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 481/1000 [01:11<01:04,  8.00it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 482/1000 [01:11<01:06,  7.83it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 483/1000 [01:11<01:06,  7.72it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 485/1000 [01:11<00:52,  9.75it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 486/1000 [01:11<00:59,  8.65it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 487/1000 [01:11<01:01,  8.33it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 488/1000 [01:12<00:58,  8.68it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 489/1000 [01:12<01:01,  8.32it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 490/1000 [01:12<00:58,  8.66it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 491/1000 [01:12<01:04,  7.92it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 492/1000 [01:12<01:05,  7.77it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 493/1000 [01:12<01:06,  7.68it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 494/1000 [01:12<01:06,  7.60it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 495/1000 [01:12<01:02,  8.10it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 497/1000 [01:13<00:55,  9.05it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 498/1000 [01:13<00:55,  9.00it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 499/1000 [01:13<00:58,  8.51it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 501/1000 [01:13<00:55,  8.91it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 502/1000 [01:13<00:58,  8.50it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 503/1000 [01:13<01:00,  8.18it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 504/1000 [01:13<01:02,  7.96it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 505/1000 [01:14<01:03,  7.82it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 506/1000 [01:14<01:00,  8.12it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 507/1000 [01:14<01:02,  7.86it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 508/1000 [01:14<01:04,  7.68it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 509/1000 [01:14<01:04,  7.64it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 510/1000 [01:14<01:07,  7.29it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 511/1000 [01:14<01:06,  7.33it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 512/1000 [01:15<01:06,  7.29it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 513/1000 [01:15<01:07,  7.26it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 515/1000 [01:15<00:54,  8.98it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 517/1000 [01:15<00:53,  9.06it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 518/1000 [01:15<00:53,  9.09it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 519/1000 [01:15<00:53,  9.01it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 520/1000 [01:15<00:56,  8.52it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 521/1000 [01:16<00:59,  8.07it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 522/1000 [01:16<01:01,  7.82it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 523/1000 [01:16<01:01,  7.73it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 524/1000 [01:16<00:59,  8.05it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 525/1000 [01:16<00:57,  8.28it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 526/1000 [01:16<00:59,  7.95it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 527/1000 [01:16<00:57,  8.25it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 529/1000 [01:17<00:52,  9.02it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 530/1000 [01:17<00:54,  8.58it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 532/1000 [01:17<00:53,  8.81it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 533/1000 [01:17<00:55,  8.38it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 534/1000 [01:17<00:57,  8.14it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 535/1000 [01:17<00:55,  8.37it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 536/1000 [01:17<00:57,  8.05it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 537/1000 [01:18<00:58,  7.87it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 538/1000 [01:18<00:59,  7.73it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 539/1000 [01:18<00:57,  8.01it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 540/1000 [01:18<00:59,  7.74it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 541/1000 [01:18<01:00,  7.61it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 542/1000 [01:18<01:01,  7.46it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 543/1000 [01:18<00:58,  7.81it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 544/1000 [01:18<00:59,  7.62it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 545/1000 [01:19<01:11,  6.34it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 546/1000 [01:19<01:05,  6.89it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 547/1000 [01:19<01:04,  6.99it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 548/1000 [01:19<01:04,  7.03it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 549/1000 [01:19<01:00,  7.49it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 550/1000 [01:19<01:00,  7.42it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 551/1000 [01:19<01:00,  7.38it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 552/1000 [01:20<01:00,  7.36it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 553/1000 [01:20<01:00,  7.36it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 554/1000 [01:20<00:57,  7.71it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 555/1000 [01:20<00:58,  7.55it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 557/1000 [01:20<00:54,  8.16it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 558/1000 [01:20<00:55,  7.95it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 560/1000 [01:21<00:52,  8.31it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 561/1000 [01:21<00:51,  8.54it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 563/1000 [01:21<00:49,  8.80it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 564/1000 [01:21<00:52,  8.34it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 565/1000 [01:21<00:53,  8.10it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 566/1000 [01:21<00:52,  8.24it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 567/1000 [01:21<00:54,  7.98it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 568/1000 [01:22<00:55,  7.84it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 570/1000 [01:22<00:50,  8.49it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 571/1000 [01:22<00:49,  8.59it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 572/1000 [01:22<00:54,  7.90it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 573/1000 [01:22<00:54,  7.83it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 574/1000 [01:22<00:54,  7.80it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 575/1000 [01:22<00:54,  7.81it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 576/1000 [01:23<00:51,  8.17it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 577/1000 [01:23<00:49,  8.49it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 578/1000 [01:23<00:51,  8.25it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 579/1000 [01:23<00:55,  7.60it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 580/1000 [01:23<00:54,  7.66it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 581/1000 [01:23<00:54,  7.65it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 582/1000 [01:23<00:54,  7.67it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 583/1000 [01:23<00:54,  7.70it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 584/1000 [01:24<00:53,  7.73it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 585/1000 [01:24<00:56,  7.30it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 586/1000 [01:24<00:55,  7.43it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 588/1000 [01:24<00:49,  8.37it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 589/1000 [01:24<00:47,  8.62it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 590/1000 [01:24<00:46,  8.81it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 591/1000 [01:24<00:48,  8.49it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 592/1000 [01:25<00:49,  8.24it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 593/1000 [01:25<00:50,  8.08it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 594/1000 [01:25<00:51,  7.94it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 596/1000 [01:25<00:46,  8.60it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 597/1000 [01:25<00:48,  8.32it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 598/1000 [01:25<00:47,  8.54it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 599/1000 [01:25<00:48,  8.32it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 600/1000 [01:25<00:49,  8.16it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 601/1000 [01:26<00:47,  8.46it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 602/1000 [01:26<00:48,  8.22it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 603/1000 [01:26<00:46,  8.51it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 604/1000 [01:26<00:47,  8.26it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 605/1000 [01:26<00:49,  8.05it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 606/1000 [01:26<00:49,  7.98it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 607/1000 [01:26<00:47,  8.31it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 608/1000 [01:26<00:48,  8.15it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 609/1000 [01:27<00:46,  8.45it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 611/1000 [01:27<00:41,  9.47it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 612/1000 [01:27<00:43,  8.91it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 613/1000 [01:27<00:45,  8.54it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 614/1000 [01:27<00:46,  8.29it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 615/1000 [01:27<00:45,  8.52it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 616/1000 [01:27<00:46,  8.28it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 617/1000 [01:28<00:47,  8.09it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 618/1000 [01:28<00:48,  7.92it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 619/1000 [01:28<00:48,  7.85it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 620/1000 [01:28<00:48,  7.77it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 621/1000 [01:28<00:49,  7.73it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 622/1000 [01:28<00:49,  7.69it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 623/1000 [01:28<00:48,  7.69it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 624/1000 [01:28<00:48,  7.69it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 625/1000 [01:29<00:46,  8.11it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 627/1000 [01:29<00:43,  8.62it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 628/1000 [01:29<00:44,  8.34it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 629/1000 [01:29<00:43,  8.57it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 630/1000 [01:29<00:44,  8.32it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 632/1000 [01:29<00:40,  9.13it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 633/1000 [01:29<00:40,  9.17it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 634/1000 [01:30<00:42,  8.71it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 635/1000 [01:30<00:43,  8.40it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 636/1000 [01:30<00:42,  8.59it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 637/1000 [01:30<00:41,  8.74it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 638/1000 [01:30<00:43,  8.31it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 639/1000 [01:30<00:44,  8.08it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 640/1000 [01:30<00:42,  8.37it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 642/1000 [01:30<00:40,  8.77it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 643/1000 [01:31<00:42,  8.42it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 644/1000 [01:31<00:41,  8.59it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 646/1000 [01:31<00:38,  9.09it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 647/1000 [01:31<00:40,  8.75it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 648/1000 [01:31<00:41,  8.44it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 649/1000 [01:31<00:40,  8.67it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 650/1000 [01:31<00:41,  8.37it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 651/1000 [01:32<00:42,  8.19it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 653/1000 [01:32<00:40,  8.67it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 654/1000 [01:32<00:39,  8.81it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 655/1000 [01:32<00:40,  8.50it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 656/1000 [01:32<00:41,  8.23it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 657/1000 [01:32<00:39,  8.62it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 658/1000 [01:32<00:38,  8.82it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 659/1000 [01:32<00:40,  8.44it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 660/1000 [01:33<00:39,  8.66it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 662/1000 [01:33<00:37,  9.10it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 663/1000 [01:33<00:38,  8.70it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 664/1000 [01:33<00:40,  8.34it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 665/1000 [01:33<00:41,  8.12it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 666/1000 [01:33<00:39,  8.41it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 667/1000 [01:33<00:40,  8.18it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 668/1000 [01:34<00:39,  8.43it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 669/1000 [01:34<00:40,  8.20it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 670/1000 [01:34<00:41,  8.04it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 671/1000 [01:34<00:41,  7.85it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 673/1000 [01:34<00:38,  8.49it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 674/1000 [01:34<00:39,  8.26it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 675/1000 [01:34<00:40,  8.11it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 676/1000 [01:35<00:40,  8.00it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 677/1000 [01:35<00:39,  8.28it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 678/1000 [01:35<00:39,  8.11it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 679/1000 [01:35<00:38,  8.41it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 681/1000 [01:35<00:34,  9.25it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 682/1000 [01:35<00:36,  8.78it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 683/1000 [01:35<00:37,  8.39it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 684/1000 [01:35<00:36,  8.59it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 685/1000 [01:36<00:36,  8.75it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 686/1000 [01:36<00:37,  8.29it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 687/1000 [01:36<00:38,  8.04it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 688/1000 [01:36<00:39,  7.91it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 689/1000 [01:36<00:37,  8.23it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 690/1000 [01:36<00:36,  8.49it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 691/1000 [01:36<00:37,  8.23it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 692/1000 [01:36<00:36,  8.49it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 693/1000 [01:37<00:37,  8.14it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 694/1000 [01:37<00:38,  7.95it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 695/1000 [01:37<00:38,  7.85it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 696/1000 [01:37<00:39,  7.74it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 697/1000 [01:37<00:39,  7.68it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 698/1000 [01:37<00:37,  8.07it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 699/1000 [01:37<00:39,  7.58it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 700/1000 [01:37<00:39,  7.61it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 701/1000 [01:38<00:39,  7.63it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 702/1000 [01:38<00:48,  6.10it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 703/1000 [01:38<00:45,  6.48it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 704/1000 [01:38<00:41,  7.11it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 705/1000 [01:38<00:38,  7.65it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 706/1000 [01:38<00:38,  7.62it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 708/1000 [01:38<00:31,  9.19it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 709/1000 [01:39<00:33,  8.75it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 710/1000 [01:39<00:34,  8.45it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 711/1000 [01:39<00:35,  8.21it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 712/1000 [01:39<00:33,  8.58it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 713/1000 [01:39<00:34,  8.28it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 714/1000 [01:39<00:33,  8.56it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 715/1000 [01:39<00:32,  8.73it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 716/1000 [01:39<00:32,  8.85it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 717/1000 [01:40<00:31,  8.95it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 718/1000 [01:40<00:33,  8.49it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 719/1000 [01:40<00:35,  7.86it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 720/1000 [01:40<00:34,  8.20it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 721/1000 [01:40<00:34,  8.05it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 722/1000 [01:40<00:35,  7.91it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 723/1000 [01:40<00:35,  7.79it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 725/1000 [01:41<00:33,  8.13it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 726/1000 [01:41<00:34,  8.01it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 727/1000 [01:41<00:34,  7.94it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 728/1000 [01:41<00:32,  8.39it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 729/1000 [01:41<00:33,  8.17it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 731/1000 [01:41<00:30,  8.74it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 732/1000 [01:41<00:33,  8.04it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 733/1000 [01:42<00:33,  7.92it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 734/1000 [01:42<00:32,  8.24it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 735/1000 [01:42<00:33,  8.02it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 736/1000 [01:42<00:33,  7.91it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 737/1000 [01:42<00:31,  8.26it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 739/1000 [01:42<00:29,  8.88it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 740/1000 [01:42<00:29,  8.96it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 741/1000 [01:42<00:30,  8.58it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 743/1000 [01:43<00:28,  9.01it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 744/1000 [01:43<00:29,  8.62it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 745/1000 [01:43<00:30,  8.32it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 746/1000 [01:43<00:32,  7.83it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 747/1000 [01:43<00:32,  7.78it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 748/1000 [01:43<00:32,  7.74it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 749/1000 [01:44<00:32,  7.74it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 750/1000 [01:44<00:30,  8.12it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 751/1000 [01:44<00:29,  8.53it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 752/1000 [01:44<00:30,  8.24it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 753/1000 [01:44<00:30,  8.08it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 754/1000 [01:44<00:31,  7.89it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 755/1000 [01:44<00:29,  8.24it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 756/1000 [01:44<00:30,  8.04it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 758/1000 [01:45<00:27,  8.72it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 760/1000 [01:45<00:25,  9.48it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 761/1000 [01:45<00:26,  9.01it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 762/1000 [01:45<00:26,  9.06it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 763/1000 [01:45<00:27,  8.62it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 764/1000 [01:45<00:26,  8.78it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 765/1000 [01:45<00:27,  8.44it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 766/1000 [01:45<00:28,  8.21it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 767/1000 [01:46<00:27,  8.48it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 768/1000 [01:46<00:28,  8.19it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 769/1000 [01:46<00:27,  8.47it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 770/1000 [01:46<00:28,  8.18it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 772/1000 [01:46<00:24,  9.20it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 773/1000 [01:46<00:26,  8.71it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 774/1000 [01:46<00:25,  8.86it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 776/1000 [01:47<00:24,  9.18it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 777/1000 [01:47<00:25,  8.72it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 778/1000 [01:47<00:25,  8.84it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 779/1000 [01:47<00:26,  8.45it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 781/1000 [01:47<00:24,  8.95it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 782/1000 [01:47<00:24,  9.03it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 783/1000 [01:47<00:23,  9.07it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 784/1000 [01:48<00:25,  8.60it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 785/1000 [01:48<00:25,  8.32it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 787/1000 [01:48<00:24,  8.78it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 788/1000 [01:48<00:24,  8.48it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 790/1000 [01:48<00:22,  9.35it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 791/1000 [01:48<00:22,  9.34it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 792/1000 [01:48<00:23,  8.88it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 794/1000 [01:49<00:22,  9.14it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 795/1000 [01:49<00:23,  8.78it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 796/1000 [01:49<00:25,  8.05it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 797/1000 [01:49<00:25,  7.90it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 798/1000 [01:49<00:25,  7.80it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 799/1000 [01:49<00:25,  7.78it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 800/1000 [01:49<00:24,  8.14it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 801/1000 [01:50<00:26,  7.62it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 803/1000 [01:50<00:23,  8.35it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 804/1000 [01:50<00:24,  8.15it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 805/1000 [01:50<00:23,  8.32it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 806/1000 [01:50<00:22,  8.56it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 807/1000 [01:50<00:23,  8.28it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 808/1000 [01:50<00:23,  8.10it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 809/1000 [01:51<00:24,  7.96it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 810/1000 [01:51<00:24,  7.84it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 811/1000 [01:51<00:24,  7.78it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 812/1000 [01:51<00:24,  7.75it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 813/1000 [01:51<00:24,  7.74it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 814/1000 [01:51<00:24,  7.66it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 815/1000 [01:51<00:24,  7.68it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 816/1000 [01:51<00:22,  8.08it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 818/1000 [01:52<00:21,  8.60it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 819/1000 [01:52<00:21,  8.35it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 820/1000 [01:52<00:21,  8.55it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 821/1000 [01:52<00:21,  8.27it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 822/1000 [01:52<00:22,  8.04it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 823/1000 [01:52<00:23,  7.48it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 825/1000 [01:53<00:21,  8.28it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 826/1000 [01:53<00:21,  8.13it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 827/1000 [01:53<00:22,  7.59it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 828/1000 [01:53<00:21,  8.09it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 829/1000 [01:53<00:21,  7.97it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 830/1000 [01:53<00:21,  7.87it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 831/1000 [01:53<00:21,  7.81it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 832/1000 [01:53<00:21,  7.76it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 833/1000 [01:54<00:20,  8.14it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 834/1000 [01:54<00:21,  7.62it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 835/1000 [01:54<00:20,  8.06it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 836/1000 [01:54<00:20,  7.91it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 837/1000 [01:54<00:20,  7.79it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 838/1000 [01:54<00:20,  7.73it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 840/1000 [01:54<00:17,  8.91it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 841/1000 [01:54<00:18,  8.58it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 842/1000 [01:55<00:19,  8.31it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 844/1000 [01:55<00:17,  8.89it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 845/1000 [01:55<00:17,  8.99it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 846/1000 [01:55<00:16,  9.06it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 847/1000 [01:55<00:17,  8.60it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 848/1000 [01:55<00:18,  8.30it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 849/1000 [01:55<00:17,  8.52it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 850/1000 [01:56<00:16,  8.83it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 851/1000 [01:56<00:18,  8.06it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 852/1000 [01:56<00:18,  7.90it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 853/1000 [01:56<00:18,  7.84it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 854/1000 [01:56<00:17,  8.20it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 855/1000 [01:56<00:18,  8.01it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 856/1000 [01:56<00:17,  8.34it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 857/1000 [01:56<00:17,  8.13it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 858/1000 [01:57<00:17,  7.98it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 859/1000 [01:57<00:18,  7.53it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 860/1000 [01:57<00:18,  7.56it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 861/1000 [01:57<00:17,  8.11it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 862/1000 [01:57<00:17,  7.94it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 863/1000 [01:57<00:16,  8.29it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 864/1000 [01:57<00:16,  8.11it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 865/1000 [01:57<00:16,  7.96it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 866/1000 [01:58<00:17,  7.83it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 867/1000 [01:58<00:17,  7.73it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 868/1000 [01:58<00:16,  8.13it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 869/1000 [01:58<00:16,  7.99it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 870/1000 [01:58<00:16,  7.86it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 871/1000 [01:58<00:16,  7.78it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 872/1000 [01:58<00:16,  7.78it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 873/1000 [01:58<00:17,  7.41it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 874/1000 [01:59<00:16,  7.43it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 875/1000 [01:59<00:15,  7.98it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 876/1000 [01:59<00:15,  7.86it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 877/1000 [01:59<00:15,  7.77it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 878/1000 [01:59<00:15,  7.73it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 879/1000 [01:59<00:14,  8.13it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 880/1000 [01:59<00:15,  7.98it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 881/1000 [01:59<00:15,  7.83it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 882/1000 [02:00<00:14,  8.18it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 884/1000 [02:00<00:13,  8.69it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 885/1000 [02:00<00:13,  8.37it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 886/1000 [02:00<00:13,  8.57it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 887/1000 [02:00<00:13,  8.32it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 888/1000 [02:00<00:13,  8.13it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 889/1000 [02:00<00:13,  8.42it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 890/1000 [02:01<00:12,  8.63it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 891/1000 [02:01<00:13,  7.95it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 892/1000 [02:01<00:13,  7.82it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 893/1000 [02:01<00:13,  7.75it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 894/1000 [02:01<00:12,  8.27it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 895/1000 [02:01<00:13,  8.03it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 896/1000 [02:01<00:13,  7.55it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 897/1000 [02:01<00:12,  7.97it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 898/1000 [02:02<00:12,  8.28it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 899/1000 [02:02<00:11,  8.56it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 900/1000 [02:02<00:12,  8.25it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 901/1000 [02:02<00:12,  8.04it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 902/1000 [02:02<00:12,  7.89it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 903/1000 [02:02<00:11,  8.26it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 904/1000 [02:02<00:11,  8.08it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 905/1000 [02:02<00:11,  8.37it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 907/1000 [02:03<00:10,  9.00it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 908/1000 [02:03<00:10,  8.56it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 909/1000 [02:03<00:10,  8.33it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 910/1000 [02:03<00:11,  8.13it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 911/1000 [02:03<00:11,  7.98it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 912/1000 [02:03<00:10,  8.30it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 913/1000 [02:03<00:10,  8.54it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 914/1000 [02:03<00:10,  8.22it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 915/1000 [02:04<00:10,  8.00it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 916/1000 [02:04<00:10,  7.84it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 918/1000 [02:04<00:10,  8.18it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 919/1000 [02:04<00:09,  8.44it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 920/1000 [02:04<00:09,  8.24it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 921/1000 [02:04<00:09,  8.06it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 922/1000 [02:04<00:09,  8.31it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 923/1000 [02:05<00:09,  8.00it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 924/1000 [02:05<00:08,  8.45it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 925/1000 [02:05<00:09,  8.21it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 926/1000 [02:05<00:09,  8.07it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 927/1000 [02:05<00:09,  7.96it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 928/1000 [02:05<00:08,  8.28it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 929/1000 [02:05<00:10,  6.75it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 930/1000 [02:06<00:11,  5.96it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 931/1000 [02:06<00:10,  6.36it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 932/1000 [02:06<00:09,  7.01it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 933/1000 [02:06<00:08,  7.55it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 934/1000 [02:06<00:08,  7.99it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 935/1000 [02:06<00:08,  7.90it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 936/1000 [02:06<00:08,  7.83it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 937/1000 [02:06<00:08,  7.77it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 938/1000 [02:07<00:07,  8.14it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 939/1000 [02:07<00:07,  8.45it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 940/1000 [02:07<00:06,  8.69it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 941/1000 [02:07<00:06,  9.00it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 942/1000 [02:07<00:06,  9.06it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 943/1000 [02:07<00:06,  8.57it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 944/1000 [02:07<00:06,  8.76it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 945/1000 [02:07<00:06,  8.41it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 946/1000 [02:07<00:06,  8.62it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 947/1000 [02:08<00:06,  8.25it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 948/1000 [02:08<00:06,  8.03it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 949/1000 [02:08<00:06,  7.86it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 951/1000 [02:08<00:05,  8.37it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 953/1000 [02:08<00:05,  9.12it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 954/1000 [02:08<00:05,  8.71it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 955/1000 [02:09<00:05,  8.01it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 956/1000 [02:09<00:05,  8.28it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 957/1000 [02:09<00:05,  8.09it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 958/1000 [02:09<00:05,  7.99it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 959/1000 [02:09<00:04,  8.33it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 960/1000 [02:09<00:04,  8.13it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 961/1000 [02:09<00:04,  7.99it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 962/1000 [02:09<00:04,  8.33it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 963/1000 [02:10<00:04,  8.13it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 964/1000 [02:10<00:04,  7.96it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 965/1000 [02:10<00:04,  8.31it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 966/1000 [02:10<00:04,  8.09it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 967/1000 [02:10<00:04,  7.92it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 968/1000 [02:10<00:04,  7.85it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 969/1000 [02:10<00:03,  7.81it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 970/1000 [02:10<00:03,  7.71it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 971/1000 [02:11<00:03,  7.69it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 972/1000 [02:11<00:03,  7.65it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 973/1000 [02:11<00:03,  7.66it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 974/1000 [02:11<00:03,  8.08it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 975/1000 [02:11<00:03,  7.98it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 977/1000 [02:11<00:02,  8.98it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 978/1000 [02:11<00:02,  8.60it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 979/1000 [02:11<00:02,  8.74it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 980/1000 [02:12<00:02,  8.85it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 981/1000 [02:12<00:02,  8.93it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 982/1000 [02:12<00:02,  8.48it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 983/1000 [02:12<00:01,  8.69it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 984/1000 [02:12<00:01,  8.34it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 985/1000 [02:12<00:01,  8.16it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 986/1000 [02:12<00:01,  8.02it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 987/1000 [02:12<00:01,  7.88it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 988/1000 [02:13<00:01,  8.23it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 989/1000 [02:13<00:01,  8.50it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 990/1000 [02:13<00:01,  8.19it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 991/1000 [02:13<00:01,  7.98it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 992/1000 [02:13<00:01,  7.85it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 993/1000 [02:13<00:00,  7.79it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 994/1000 [02:13<00:00,  7.43it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 995/1000 [02:14<00:00,  7.49it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 997/1000 [02:14<00:00,  8.26it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 998/1000 [02:14<00:00,  8.09it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 999/1000 [02:14<00:00,  7.92it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [02:14<00:00,  7.79it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [02:14<00:00,  7.43it/s]
DONE (10.79s)
DONE (7.98s)
Computing criterion:   0%|          | 0/20 [00:00<?, ?it/s]Computing criterion:   5%|â–Œ         | 1/20 [00:16<05:20, 16.88s/it]Computing criterion:  10%|â–ˆ         | 2/20 [00:33<04:58, 16.59s/it]Computing criterion:  15%|â–ˆâ–Œ        | 3/20 [00:50<04:44, 16.72s/it]Computing criterion:  20%|â–ˆâ–ˆ        | 4/20 [01:06<04:26, 16.67s/it]Computing criterion:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [01:23<04:09, 16.66s/it]Computing criterion:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [01:40<03:54, 16.74s/it]Computing criterion:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [01:55<03:32, 16.38s/it]Computing criterion:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [02:12<03:16, 16.38s/it]Computing criterion:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [02:28<02:59, 16.34s/it]Computing criterion:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [02:45<02:45, 16.53s/it]Computing criterion:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [03:02<02:28, 16.55s/it]Computing criterion:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [03:18<02:12, 16.60s/it]Computing criterion:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [03:34<01:54, 16.40s/it]Computing criterion:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [03:51<01:39, 16.58s/it]Computing criterion:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [04:08<01:22, 16.53s/it]Computing criterion:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [04:25<01:06, 16.68s/it]Computing criterion:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [04:41<00:50, 16.69s/it]Computing criterion:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [04:58<00:33, 16.80s/it]Computing criterion:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [05:15<00:16, 16.71s/it]Computing criterion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [05:31<00:00, 16.60s/it]Computing criterion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [05:31<00:00, 16.59s/it]
Ref scores:   0%|          | 0/1000 [00:00<?, ?it/s]Ref scores:   0%|          | 1/1000 [00:03<54:19,  3.26s/it]Ref scores:   0%|          | 2/1000 [00:03<23:30,  1.41s/it]Ref scores:   0%|          | 3/1000 [00:03<13:46,  1.21it/s]Ref scores:   0%|          | 4/1000 [00:03<09:12,  1.80it/s]Ref scores:   0%|          | 5/1000 [00:03<06:40,  2.48it/s]Ref scores:   1%|          | 6/1000 [00:03<05:01,  3.29it/s]Ref scores:   1%|          | 7/1000 [00:04<04:07,  4.01it/s]Ref scores:   1%|          | 8/1000 [00:04<03:31,  4.69it/s]Ref scores:   1%|          | 9/1000 [00:04<02:57,  5.60it/s]Ref scores:   1%|          | 11/1000 [00:04<02:24,  6.84it/s]Ref scores:   1%|          | 12/1000 [00:04<02:21,  6.98it/s]Ref scores:   1%|â–         | 13/1000 [00:04<02:19,  7.09it/s]Ref scores:   1%|â–         | 14/1000 [00:04<02:17,  7.19it/s]Ref scores:   2%|â–         | 15/1000 [00:05<02:17,  7.18it/s]Ref scores:   2%|â–         | 16/1000 [00:05<02:09,  7.60it/s]Ref scores:   2%|â–         | 17/1000 [00:05<02:11,  7.50it/s]Ref scores:   2%|â–         | 18/1000 [00:05<02:05,  7.84it/s]Ref scores:   2%|â–         | 19/1000 [00:05<02:13,  7.36it/s]Ref scores:   2%|â–         | 20/1000 [00:05<02:04,  7.85it/s]Ref scores:   2%|â–         | 21/1000 [00:05<02:07,  7.67it/s]Ref scores:   2%|â–         | 22/1000 [00:05<02:08,  7.62it/s]Ref scores:   2%|â–         | 23/1000 [00:06<02:10,  7.51it/s]Ref scores:   2%|â–         | 24/1000 [00:06<02:04,  7.86it/s]Ref scores:   2%|â–Ž         | 25/1000 [00:06<02:06,  7.69it/s]Ref scores:   3%|â–Ž         | 26/1000 [00:06<02:08,  7.58it/s]Ref scores:   3%|â–Ž         | 27/1000 [00:06<02:09,  7.53it/s]Ref scores:   3%|â–Ž         | 28/1000 [00:06<02:11,  7.42it/s]Ref scores:   3%|â–Ž         | 29/1000 [00:06<02:03,  7.84it/s]Ref scores:   3%|â–Ž         | 30/1000 [00:06<02:04,  7.77it/s]Ref scores:   3%|â–Ž         | 31/1000 [00:07<02:06,  7.67it/s]Ref scores:   3%|â–Ž         | 32/1000 [00:07<02:07,  7.57it/s]Ref scores:   3%|â–Ž         | 33/1000 [00:07<02:07,  7.56it/s]Ref scores:   3%|â–Ž         | 34/1000 [00:07<02:01,  7.95it/s]Ref scores:   4%|â–Ž         | 35/1000 [00:07<01:56,  8.29it/s]Ref scores:   4%|â–Ž         | 36/1000 [00:07<02:05,  7.69it/s]Ref scores:   4%|â–Ž         | 37/1000 [00:07<02:13,  7.19it/s]Ref scores:   4%|â–         | 38/1000 [00:08<02:05,  7.65it/s]Ref scores:   4%|â–         | 39/1000 [00:08<02:06,  7.58it/s]Ref scores:   4%|â–         | 40/1000 [00:08<02:07,  7.55it/s]Ref scores:   4%|â–         | 41/1000 [00:08<02:01,  7.90it/s]Ref scores:   4%|â–         | 42/1000 [00:08<01:56,  8.20it/s]Ref scores:   4%|â–         | 43/1000 [00:08<01:54,  8.39it/s]Ref scores:   4%|â–         | 44/1000 [00:08<01:52,  8.52it/s]Ref scores:   4%|â–         | 45/1000 [00:08<01:50,  8.65it/s]Ref scores:   5%|â–         | 46/1000 [00:08<01:56,  8.22it/s]Ref scores:   5%|â–         | 47/1000 [00:09<02:00,  7.93it/s]Ref scores:   5%|â–         | 48/1000 [00:09<02:02,  7.79it/s]Ref scores:   5%|â–Œ         | 50/1000 [00:09<01:52,  8.42it/s]Ref scores:   5%|â–Œ         | 51/1000 [00:09<01:57,  8.11it/s]Ref scores:   5%|â–Œ         | 52/1000 [00:09<01:54,  8.24it/s]Ref scores:   5%|â–Œ         | 53/1000 [00:09<01:58,  8.00it/s]Ref scores:   5%|â–Œ         | 54/1000 [00:09<01:55,  8.20it/s]Ref scores:   6%|â–Œ         | 55/1000 [00:10<01:59,  7.93it/s]Ref scores:   6%|â–Œ         | 56/1000 [00:10<02:18,  6.80it/s]Ref scores:   6%|â–Œ         | 57/1000 [00:10<02:15,  6.97it/s]Ref scores:   6%|â–Œ         | 58/1000 [00:10<02:07,  7.41it/s]Ref scores:   6%|â–Œ         | 59/1000 [00:10<02:07,  7.39it/s]Ref scores:   6%|â–Œ         | 60/1000 [00:10<02:06,  7.41it/s]Ref scores:   6%|â–Œ         | 61/1000 [00:10<02:06,  7.40it/s]Ref scores:   6%|â–Œ         | 62/1000 [00:11<02:08,  7.32it/s]Ref scores:   6%|â–‹         | 63/1000 [00:11<02:08,  7.31it/s]Ref scores:   6%|â–‹         | 64/1000 [00:11<02:08,  7.28it/s]Ref scores:   6%|â–‹         | 65/1000 [00:11<02:01,  7.70it/s]Ref scores:   7%|â–‹         | 66/1000 [00:11<02:03,  7.55it/s]Ref scores:   7%|â–‹         | 67/1000 [00:11<02:05,  7.44it/s]Ref scores:   7%|â–‹         | 68/1000 [00:11<01:59,  7.81it/s]Ref scores:   7%|â–‹         | 69/1000 [00:11<01:54,  8.11it/s]Ref scores:   7%|â–‹         | 70/1000 [00:12<01:58,  7.82it/s]Ref scores:   7%|â–‹         | 71/1000 [00:12<02:01,  7.62it/s]Ref scores:   7%|â–‹         | 72/1000 [00:12<01:54,  8.10it/s]Ref scores:   7%|â–‹         | 74/1000 [00:12<01:46,  8.67it/s]Ref scores:   8%|â–Š         | 75/1000 [00:12<01:45,  8.76it/s]Ref scores:   8%|â–Š         | 76/1000 [00:12<01:55,  8.01it/s]Ref scores:   8%|â–Š         | 77/1000 [00:12<01:52,  8.20it/s]Ref scores:   8%|â–Š         | 78/1000 [00:13<01:59,  7.74it/s]Ref scores:   8%|â–Š         | 79/1000 [00:13<02:01,  7.61it/s]Ref scores:   8%|â–Š         | 80/1000 [00:13<02:27,  6.25it/s]Ref scores:   8%|â–Š         | 81/1000 [00:13<02:14,  6.84it/s]Ref scores:   8%|â–Š         | 83/1000 [00:13<01:52,  8.17it/s]Ref scores:   8%|â–Š         | 84/1000 [00:13<01:54,  7.98it/s]Ref scores:   8%|â–Š         | 85/1000 [00:14<01:57,  7.79it/s]Ref scores:   9%|â–Š         | 86/1000 [00:14<02:00,  7.61it/s]Ref scores:   9%|â–Š         | 87/1000 [00:14<02:01,  7.54it/s]Ref scores:   9%|â–‰         | 88/1000 [00:14<02:02,  7.47it/s]Ref scores:   9%|â–‰         | 89/1000 [00:14<02:02,  7.41it/s]Ref scores:   9%|â–‰         | 90/1000 [00:14<01:57,  7.76it/s]Ref scores:   9%|â–‰         | 91/1000 [00:14<01:53,  8.04it/s]Ref scores:   9%|â–‰         | 92/1000 [00:14<01:56,  7.80it/s]Ref scores:   9%|â–‰         | 93/1000 [00:15<01:58,  7.64it/s]Ref scores:   9%|â–‰         | 94/1000 [00:15<02:00,  7.52it/s]Ref scores:  10%|â–‰         | 95/1000 [00:15<01:53,  7.98it/s]Ref scores:  10%|â–‰         | 97/1000 [00:15<01:39,  9.05it/s]Ref scores:  10%|â–‰         | 98/1000 [00:15<01:45,  8.55it/s]Ref scores:  10%|â–‰         | 99/1000 [00:15<01:49,  8.24it/s]Ref scores:  10%|â–ˆ         | 100/1000 [00:15<01:53,  7.94it/s]Ref scores:  10%|â–ˆ         | 101/1000 [00:16<01:55,  7.76it/s]Ref scores:  10%|â–ˆ         | 102/1000 [00:16<01:58,  7.58it/s]Ref scores:  10%|â–ˆ         | 103/1000 [00:16<01:59,  7.50it/s]Ref scores:  10%|â–ˆ         | 104/1000 [00:16<02:01,  7.40it/s]Ref scores:  10%|â–ˆ         | 105/1000 [00:16<02:01,  7.38it/s]Ref scores:  11%|â–ˆ         | 106/1000 [00:16<01:54,  7.78it/s]Ref scores:  11%|â–ˆ         | 108/1000 [00:16<01:47,  8.29it/s]Ref scores:  11%|â–ˆ         | 109/1000 [00:17<01:50,  8.03it/s]Ref scores:  11%|â–ˆ         | 110/1000 [00:17<01:48,  8.21it/s]Ref scores:  11%|â–ˆ         | 111/1000 [00:17<01:51,  7.98it/s]Ref scores:  11%|â–ˆ         | 112/1000 [00:17<01:54,  7.77it/s]Ref scores:  11%|â–ˆâ–        | 113/1000 [00:17<01:55,  7.66it/s]Ref scores:  11%|â–ˆâ–        | 114/1000 [00:17<01:50,  8.00it/s]Ref scores:  12%|â–ˆâ–        | 115/1000 [00:17<01:47,  8.21it/s]Ref scores:  12%|â–ˆâ–        | 116/1000 [00:17<01:45,  8.40it/s]Ref scores:  12%|â–ˆâ–        | 117/1000 [00:18<01:43,  8.54it/s]Ref scores:  12%|â–ˆâ–        | 118/1000 [00:18<01:47,  8.19it/s]Ref scores:  12%|â–ˆâ–        | 119/1000 [00:18<01:51,  7.91it/s]Ref scores:  12%|â–ˆâ–        | 120/1000 [00:18<01:45,  8.31it/s]Ref scores:  12%|â–ˆâ–        | 121/1000 [00:18<01:43,  8.48it/s]Ref scores:  12%|â–ˆâ–        | 122/1000 [00:18<01:48,  8.11it/s]Ref scores:  12%|â–ˆâ–        | 123/1000 [00:18<01:52,  7.82it/s]Ref scores:  12%|â–ˆâ–        | 124/1000 [00:18<01:53,  7.69it/s]Ref scores:  12%|â–ˆâ–Ž        | 125/1000 [00:19<01:55,  7.58it/s]Ref scores:  13%|â–ˆâ–Ž        | 126/1000 [00:19<01:56,  7.52it/s]Ref scores:  13%|â–ˆâ–Ž        | 127/1000 [00:19<01:57,  7.43it/s]Ref scores:  13%|â–ˆâ–Ž        | 128/1000 [00:19<01:57,  7.42it/s]Ref scores:  13%|â–ˆâ–Ž        | 129/1000 [00:19<01:57,  7.38it/s]Ref scores:  13%|â–ˆâ–Ž        | 130/1000 [00:19<01:58,  7.34it/s]Ref scores:  13%|â–ˆâ–Ž        | 131/1000 [00:19<01:52,  7.73it/s]Ref scores:  13%|â–ˆâ–Ž        | 132/1000 [00:20<01:47,  8.05it/s]Ref scores:  13%|â–ˆâ–Ž        | 133/1000 [00:20<01:55,  7.49it/s]Ref scores:  13%|â–ˆâ–Ž        | 134/1000 [00:20<01:50,  7.87it/s]Ref scores:  14%|â–ˆâ–Ž        | 135/1000 [00:20<01:46,  8.14it/s]Ref scores:  14%|â–ˆâ–Ž        | 136/1000 [00:20<01:41,  8.48it/s]Ref scores:  14%|â–ˆâ–Ž        | 137/1000 [00:20<01:40,  8.59it/s]Ref scores:  14%|â–ˆâ–        | 138/1000 [00:20<01:39,  8.69it/s]Ref scores:  14%|â–ˆâ–        | 139/1000 [00:20<01:38,  8.72it/s]Ref scores:  14%|â–ˆâ–        | 140/1000 [00:20<01:37,  8.80it/s]Ref scores:  14%|â–ˆâ–        | 141/1000 [00:21<01:43,  8.27it/s]Ref scores:  14%|â–ˆâ–        | 142/1000 [00:21<01:41,  8.46it/s]Ref scores:  14%|â–ˆâ–        | 143/1000 [00:21<01:44,  8.17it/s]Ref scores:  14%|â–ˆâ–        | 144/1000 [00:21<01:47,  7.93it/s]Ref scores:  14%|â–ˆâ–        | 145/1000 [00:21<01:50,  7.77it/s]Ref scores:  15%|â–ˆâ–        | 146/1000 [00:21<01:51,  7.65it/s]Ref scores:  15%|â–ˆâ–        | 147/1000 [00:21<01:46,  8.03it/s]Ref scores:  15%|â–ˆâ–        | 149/1000 [00:22<01:34,  9.03it/s]Ref scores:  15%|â–ˆâ–Œ        | 150/1000 [00:22<01:39,  8.56it/s]Ref scores:  15%|â–ˆâ–Œ        | 151/1000 [00:22<01:43,  8.20it/s]Ref scores:  15%|â–ˆâ–Œ        | 153/1000 [00:22<01:34,  8.93it/s]Ref scores:  15%|â–ˆâ–Œ        | 154/1000 [00:22<01:34,  8.92it/s]Ref scores:  16%|â–ˆâ–Œ        | 155/1000 [00:22<01:39,  8.50it/s]Ref scores:  16%|â–ˆâ–Œ        | 156/1000 [00:22<01:42,  8.21it/s]Ref scores:  16%|â–ˆâ–Œ        | 158/1000 [00:23<01:37,  8.59it/s]Ref scores:  16%|â–ˆâ–Œ        | 159/1000 [00:23<01:36,  8.69it/s]Ref scores:  16%|â–ˆâ–Œ        | 160/1000 [00:23<01:40,  8.36it/s]Ref scores:  16%|â–ˆâ–Œ        | 161/1000 [00:23<01:44,  8.04it/s]Ref scores:  16%|â–ˆâ–Œ        | 162/1000 [00:23<01:46,  7.85it/s]Ref scores:  16%|â–ˆâ–‹        | 164/1000 [00:23<01:39,  8.40it/s]Ref scores:  16%|â–ˆâ–‹        | 165/1000 [00:23<01:38,  8.48it/s]Ref scores:  17%|â–ˆâ–‹        | 166/1000 [00:24<01:37,  8.59it/s]Ref scores:  17%|â–ˆâ–‹        | 167/1000 [00:24<01:41,  8.17it/s]Ref scores:  17%|â–ˆâ–‹        | 168/1000 [00:24<01:45,  7.91it/s]Ref scores:  17%|â–ˆâ–‹        | 169/1000 [00:24<01:40,  8.27it/s]Ref scores:  17%|â–ˆâ–‹        | 170/1000 [00:24<01:44,  7.97it/s]Ref scores:  17%|â–ˆâ–‹        | 171/1000 [00:24<01:47,  7.74it/s]Ref scores:  17%|â–ˆâ–‹        | 173/1000 [00:24<01:37,  8.46it/s]Ref scores:  17%|â–ˆâ–‹        | 174/1000 [00:25<01:40,  8.19it/s]Ref scores:  18%|â–ˆâ–Š        | 175/1000 [00:25<01:43,  7.97it/s]Ref scores:  18%|â–ˆâ–Š        | 176/1000 [00:25<01:40,  8.21it/s]Ref scores:  18%|â–ˆâ–Š        | 177/1000 [00:25<01:44,  7.90it/s]Ref scores:  18%|â–ˆâ–Š        | 178/1000 [00:25<01:45,  7.76it/s]Ref scores:  18%|â–ˆâ–Š        | 179/1000 [00:25<01:48,  7.58it/s]Ref scores:  18%|â–ˆâ–Š        | 180/1000 [00:25<01:48,  7.52it/s]Ref scores:  18%|â–ˆâ–Š        | 181/1000 [00:26<01:49,  7.51it/s]Ref scores:  18%|â–ˆâ–Š        | 182/1000 [00:26<01:49,  7.47it/s]Ref scores:  18%|â–ˆâ–Š        | 183/1000 [00:26<01:50,  7.40it/s]Ref scores:  18%|â–ˆâ–Š        | 184/1000 [00:26<01:42,  7.92it/s]Ref scores:  18%|â–ˆâ–Š        | 185/1000 [00:26<01:45,  7.70it/s]Ref scores:  19%|â–ˆâ–Š        | 186/1000 [00:26<01:40,  8.12it/s]Ref scores:  19%|â–ˆâ–‰        | 188/1000 [00:26<01:33,  8.65it/s]Ref scores:  19%|â–ˆâ–‰        | 189/1000 [00:26<01:38,  8.25it/s]Ref scores:  19%|â–ˆâ–‰        | 190/1000 [00:27<01:41,  7.97it/s]Ref scores:  19%|â–ˆâ–‰        | 191/1000 [00:27<01:44,  7.74it/s]Ref scores:  19%|â–ˆâ–‰        | 192/1000 [00:27<01:45,  7.64it/s]Ref scores:  19%|â–ˆâ–‰        | 193/1000 [00:27<01:41,  7.92it/s]Ref scores:  19%|â–ˆâ–‰        | 194/1000 [00:27<01:38,  8.15it/s]Ref scores:  20%|â–ˆâ–‰        | 195/1000 [00:27<01:36,  8.33it/s]Ref scores:  20%|â–ˆâ–‰        | 196/1000 [00:27<01:33,  8.64it/s]Ref scores:  20%|â–ˆâ–‰        | 197/1000 [00:27<01:37,  8.25it/s]Ref scores:  20%|â–ˆâ–‰        | 198/1000 [00:28<01:40,  7.97it/s]Ref scores:  20%|â–ˆâ–‰        | 199/1000 [00:28<01:43,  7.76it/s]Ref scores:  20%|â–ˆâ–ˆ        | 200/1000 [00:28<01:45,  7.62it/s]Ref scores:  20%|â–ˆâ–ˆ        | 201/1000 [00:28<01:46,  7.52it/s]Ref scores:  20%|â–ˆâ–ˆ        | 202/1000 [00:28<01:47,  7.43it/s]Ref scores:  20%|â–ˆâ–ˆ        | 204/1000 [00:28<01:30,  8.75it/s]Ref scores:  20%|â–ˆâ–ˆ        | 205/1000 [00:28<01:35,  8.31it/s]Ref scores:  21%|â–ˆâ–ˆ        | 206/1000 [00:29<01:34,  8.42it/s]Ref scores:  21%|â–ˆâ–ˆ        | 207/1000 [00:29<01:38,  8.09it/s]Ref scores:  21%|â–ˆâ–ˆ        | 208/1000 [00:29<01:35,  8.27it/s]Ref scores:  21%|â–ˆâ–ˆ        | 209/1000 [00:29<01:39,  7.98it/s]Ref scores:  21%|â–ˆâ–ˆ        | 210/1000 [00:29<01:36,  8.20it/s]Ref scores:  21%|â–ˆâ–ˆ        | 212/1000 [00:29<01:19,  9.93it/s]Ref scores:  21%|â–ˆâ–ˆâ–       | 213/1000 [00:29<01:22,  9.58it/s]Ref scores:  21%|â–ˆâ–ˆâ–       | 214/1000 [00:30<01:33,  8.40it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 215/1000 [00:30<01:32,  8.51it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 216/1000 [00:30<01:36,  8.16it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 217/1000 [00:30<01:43,  7.56it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 219/1000 [00:30<01:35,  8.21it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 220/1000 [00:30<01:37,  7.96it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 221/1000 [00:30<01:43,  7.54it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 223/1000 [00:31<01:36,  8.05it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 224/1000 [00:31<01:38,  7.89it/s]Ref scores:  22%|â–ˆâ–ˆâ–Ž       | 225/1000 [00:31<01:35,  8.10it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 226/1000 [00:31<01:38,  7.87it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 227/1000 [00:31<01:39,  7.77it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 228/1000 [00:31<01:36,  8.02it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 229/1000 [00:31<01:38,  7.85it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 230/1000 [00:32<01:40,  7.63it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 231/1000 [00:32<01:36,  7.93it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 232/1000 [00:32<01:38,  7.77it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 233/1000 [00:32<01:34,  8.08it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 234/1000 [00:32<01:38,  7.79it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 235/1000 [00:32<01:34,  8.10it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 236/1000 [00:32<01:37,  7.84it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 237/1000 [00:32<01:34,  8.09it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 238/1000 [00:33<01:37,  7.82it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 239/1000 [00:33<01:38,  7.73it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 240/1000 [00:33<01:34,  8.04it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 241/1000 [00:33<01:38,  7.73it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 243/1000 [00:33<01:30,  8.36it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 244/1000 [00:33<01:29,  8.48it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 245/1000 [00:33<01:27,  8.60it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 246/1000 [00:34<01:32,  8.19it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 247/1000 [00:34<01:30,  8.34it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 249/1000 [00:34<01:21,  9.24it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 250/1000 [00:34<01:21,  9.16it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 251/1000 [00:34<01:27,  8.57it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 252/1000 [00:34<01:26,  8.67it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 254/1000 [00:34<01:23,  8.92it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 255/1000 [00:35<01:23,  8.89it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 256/1000 [00:35<01:27,  8.50it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 257/1000 [00:35<01:26,  8.59it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 258/1000 [00:35<01:25,  8.70it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 259/1000 [00:35<01:29,  8.31it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 260/1000 [00:35<01:26,  8.58it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 261/1000 [00:35<01:25,  8.62it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 262/1000 [00:35<01:29,  8.21it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 263/1000 [00:35<01:27,  8.40it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 264/1000 [00:36<01:35,  7.73it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 265/1000 [00:36<01:36,  7.63it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 266/1000 [00:36<01:36,  7.58it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 267/1000 [00:36<01:37,  7.53it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 268/1000 [00:36<01:38,  7.47it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 270/1000 [00:36<01:28,  8.26it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 272/1000 [00:37<01:24,  8.63it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 273/1000 [00:37<01:27,  8.29it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 274/1000 [00:37<01:30,  8.02it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 275/1000 [00:37<01:32,  7.83it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 276/1000 [00:37<01:33,  7.73it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 277/1000 [00:37<01:36,  7.49it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 278/1000 [00:37<01:40,  7.18it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 279/1000 [00:38<01:34,  7.60it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 280/1000 [00:38<01:35,  7.57it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 281/1000 [00:38<01:35,  7.53it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 282/1000 [00:38<01:35,  7.50it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 283/1000 [00:38<01:31,  7.86it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 284/1000 [00:38<01:32,  7.70it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 285/1000 [00:38<01:33,  7.61it/s]Ref scores:  29%|â–ˆâ–ˆâ–Š       | 286/1000 [00:38<01:35,  7.48it/s]Ref scores:  29%|â–ˆâ–ˆâ–Š       | 287/1000 [00:39<01:35,  7.45it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 288/1000 [00:39<01:35,  7.45it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 289/1000 [00:39<01:35,  7.47it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 290/1000 [00:39<01:35,  7.45it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 291/1000 [00:39<01:36,  7.38it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 292/1000 [00:39<01:36,  7.37it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 293/1000 [00:39<01:35,  7.37it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 294/1000 [00:40<01:35,  7.39it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 295/1000 [00:40<01:35,  7.39it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 296/1000 [00:40<01:36,  7.33it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 298/1000 [00:40<01:15,  9.26it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 299/1000 [00:40<01:21,  8.61it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 300/1000 [00:40<01:29,  7.86it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 301/1000 [00:40<01:26,  8.10it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 302/1000 [00:41<01:23,  8.33it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 303/1000 [00:41<01:22,  8.45it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 304/1000 [00:41<01:21,  8.53it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 305/1000 [00:41<01:25,  8.09it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 307/1000 [00:41<01:20,  8.63it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 308/1000 [00:41<01:23,  8.29it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 310/1000 [00:41<01:11,  9.71it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 311/1000 [00:42<01:12,  9.53it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 312/1000 [00:42<01:17,  8.86it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆâ–      | 314/1000 [00:42<01:16,  8.98it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 315/1000 [00:42<01:20,  8.52it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 316/1000 [00:42<01:23,  8.20it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 318/1000 [00:42<01:10,  9.66it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 319/1000 [00:42<01:15,  8.99it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 320/1000 [00:43<01:15,  8.98it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 321/1000 [00:43<01:15,  8.97it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 322/1000 [00:43<01:15,  8.93it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 323/1000 [00:43<01:20,  8.45it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 325/1000 [00:43<01:16,  8.77it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 326/1000 [00:43<01:31,  7.41it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 327/1000 [00:43<01:31,  7.39it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 328/1000 [00:44<01:31,  7.35it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 329/1000 [00:44<01:26,  7.80it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 330/1000 [00:44<01:28,  7.58it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 331/1000 [00:44<01:24,  7.91it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 333/1000 [00:44<01:18,  8.52it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 334/1000 [00:44<01:20,  8.24it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 335/1000 [00:44<01:19,  8.38it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 336/1000 [00:45<01:18,  8.50it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 337/1000 [00:45<01:21,  8.14it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 339/1000 [00:45<01:06,  9.87it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 341/1000 [00:45<01:04, 10.30it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 343/1000 [00:45<01:09,  9.39it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 344/1000 [00:45<01:14,  8.82it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 345/1000 [00:46<01:17,  8.40it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 347/1000 [00:46<01:06,  9.82it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 348/1000 [00:46<01:11,  9.10it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 349/1000 [00:46<01:19,  8.22it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 350/1000 [00:46<01:20,  8.03it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 351/1000 [00:46<01:22,  7.84it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 352/1000 [00:46<01:24,  7.64it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 353/1000 [00:46<01:21,  7.97it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 354/1000 [00:47<01:22,  7.80it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 355/1000 [00:47<01:23,  7.68it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 356/1000 [00:47<01:19,  8.12it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 357/1000 [00:47<01:21,  7.86it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 358/1000 [00:47<01:23,  7.71it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 359/1000 [00:47<01:25,  7.53it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 360/1000 [00:47<01:21,  7.88it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 361/1000 [00:48<01:23,  7.67it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 362/1000 [00:48<01:25,  7.50it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 363/1000 [00:48<01:24,  7.51it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 365/1000 [00:48<01:12,  8.73it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 367/1000 [00:48<01:02, 10.13it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 368/1000 [00:48<01:08,  9.28it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 369/1000 [00:48<01:20,  7.88it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 370/1000 [00:49<01:18,  7.98it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 371/1000 [00:49<01:20,  7.82it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 373/1000 [00:49<01:11,  8.72it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 375/1000 [00:49<01:10,  8.88it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 376/1000 [00:49<01:13,  8.53it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 377/1000 [00:49<01:15,  8.26it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 378/1000 [00:50<01:14,  8.37it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 379/1000 [00:50<01:11,  8.64it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 380/1000 [00:50<01:11,  8.70it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 381/1000 [00:50<01:15,  8.22it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 382/1000 [00:50<01:18,  7.91it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 383/1000 [00:50<01:23,  7.41it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 384/1000 [00:50<01:23,  7.40it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 385/1000 [00:50<01:22,  7.42it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 386/1000 [00:51<01:23,  7.37it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 387/1000 [00:51<01:22,  7.40it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 388/1000 [00:51<01:23,  7.36it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 389/1000 [00:51<01:19,  7.71it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 390/1000 [00:51<01:20,  7.56it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 391/1000 [00:51<01:21,  7.51it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 392/1000 [00:51<01:21,  7.43it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 393/1000 [00:51<01:16,  7.89it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 394/1000 [00:52<01:18,  7.70it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 395/1000 [00:52<01:13,  8.20it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 396/1000 [00:52<01:15,  8.01it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 397/1000 [00:52<01:16,  7.89it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 398/1000 [00:52<01:11,  8.37it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 399/1000 [00:52<01:10,  8.57it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 400/1000 [00:52<01:12,  8.27it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 401/1000 [00:52<01:14,  8.04it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 402/1000 [00:53<01:15,  7.88it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 403/1000 [00:53<01:16,  7.78it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 404/1000 [00:53<01:16,  7.77it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 405/1000 [00:53<01:17,  7.72it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 407/1000 [00:53<01:09,  8.49it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 408/1000 [00:53<01:08,  8.67it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 409/1000 [00:53<01:10,  8.36it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 410/1000 [00:54<01:08,  8.58it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 411/1000 [00:54<01:11,  8.29it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 413/1000 [00:54<01:07,  8.68it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 414/1000 [00:54<01:06,  8.80it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 415/1000 [00:54<01:09,  8.43it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 416/1000 [00:54<01:11,  8.17it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 417/1000 [00:54<01:12,  8.00it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 418/1000 [00:55<01:14,  7.84it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 419/1000 [00:55<01:11,  8.18it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 420/1000 [00:55<01:12,  7.97it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 421/1000 [00:55<01:13,  7.84it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 422/1000 [00:55<01:14,  7.77it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 423/1000 [00:55<01:14,  7.71it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 425/1000 [00:55<01:07,  8.49it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 426/1000 [00:55<01:06,  8.64it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 427/1000 [00:56<01:05,  8.73it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 428/1000 [00:56<01:08,  8.41it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 429/1000 [00:56<01:06,  8.60it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 430/1000 [00:56<01:08,  8.28it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 431/1000 [00:56<01:10,  8.10it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 432/1000 [00:56<01:07,  8.38it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 433/1000 [00:56<01:09,  8.19it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 434/1000 [00:56<01:10,  7.98it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 435/1000 [00:57<01:11,  7.89it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 436/1000 [00:57<01:12,  7.81it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 437/1000 [00:57<01:12,  7.75it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 438/1000 [00:57<01:12,  7.74it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 439/1000 [00:57<01:08,  8.15it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 440/1000 [00:57<01:10,  7.99it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 441/1000 [00:57<01:05,  8.49it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 442/1000 [00:57<01:04,  8.69it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 443/1000 [00:58<01:01,  8.99it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 444/1000 [00:58<01:05,  8.52it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 445/1000 [00:58<01:10,  7.90it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 446/1000 [00:58<01:07,  8.24it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 447/1000 [00:58<01:05,  8.50it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 448/1000 [00:58<01:03,  8.68it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 449/1000 [00:58<01:02,  8.83it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 450/1000 [00:58<01:05,  8.46it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 451/1000 [00:59<01:07,  8.16it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 452/1000 [00:59<01:08,  7.97it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 453/1000 [00:59<01:09,  7.87it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 455/1000 [00:59<01:03,  8.62it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 456/1000 [00:59<01:05,  8.32it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 457/1000 [00:59<01:02,  8.70it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 458/1000 [00:59<01:01,  8.81it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 459/1000 [00:59<01:04,  8.43it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 460/1000 [01:00<01:05,  8.19it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 461/1000 [01:00<01:03,  8.45it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 462/1000 [01:00<01:05,  8.20it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 463/1000 [01:00<01:06,  8.03it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 464/1000 [01:00<01:08,  7.86it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 465/1000 [01:00<01:05,  8.21it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 466/1000 [01:00<01:03,  8.46it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 467/1000 [01:00<01:05,  8.18it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 468/1000 [01:01<01:06,  8.01it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 469/1000 [01:01<01:03,  8.36it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 471/1000 [01:01<00:55,  9.46it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 472/1000 [01:01<01:11,  7.43it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 473/1000 [01:01<01:10,  7.49it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 474/1000 [01:01<01:09,  7.56it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 475/1000 [01:01<01:08,  7.61it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 476/1000 [01:02<01:08,  7.61it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 477/1000 [01:02<01:04,  8.17it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 478/1000 [01:02<01:01,  8.48it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 479/1000 [01:02<01:03,  8.23it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 480/1000 [01:02<01:04,  8.02it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 481/1000 [01:02<01:05,  7.92it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 482/1000 [01:02<01:16,  6.74it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 483/1000 [01:03<01:10,  7.35it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 484/1000 [01:03<01:05,  7.83it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 486/1000 [01:03<01:00,  8.50it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 487/1000 [01:03<01:01,  8.31it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 488/1000 [01:03<01:03,  8.12it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 489/1000 [01:03<01:00,  8.41it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 490/1000 [01:03<00:58,  8.66it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 491/1000 [01:03<00:57,  8.87it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 492/1000 [01:04<00:59,  8.51it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 493/1000 [01:04<01:04,  7.92it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 494/1000 [01:04<01:01,  8.27it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 495/1000 [01:04<01:02,  8.07it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 497/1000 [01:04<00:57,  8.81it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 498/1000 [01:04<00:59,  8.45it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 499/1000 [01:04<01:00,  8.26it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 500/1000 [01:05<01:01,  8.07it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 501/1000 [01:05<01:02,  7.99it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 502/1000 [01:05<01:03,  7.85it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 503/1000 [01:05<01:03,  7.81it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 504/1000 [01:05<01:03,  7.80it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 505/1000 [01:05<01:06,  7.46it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 506/1000 [01:05<01:01,  8.03it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 507/1000 [01:05<01:02,  7.87it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 508/1000 [01:06<01:12,  6.78it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 509/1000 [01:06<01:09,  7.05it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 511/1000 [01:06<00:57,  8.50it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 512/1000 [01:06<00:59,  8.25it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 513/1000 [01:06<00:57,  8.51it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 514/1000 [01:06<00:58,  8.24it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 515/1000 [01:06<00:59,  8.11it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 517/1000 [01:07<00:55,  8.76it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 518/1000 [01:07<00:56,  8.50it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 519/1000 [01:07<00:55,  8.69it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 520/1000 [01:07<00:56,  8.43it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 521/1000 [01:07<00:58,  8.21it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 522/1000 [01:07<00:59,  8.07it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 523/1000 [01:07<00:59,  8.00it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 524/1000 [01:08<00:59,  7.93it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 526/1000 [01:08<00:55,  8.59it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 528/1000 [01:08<00:50,  9.35it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 529/1000 [01:08<00:52,  8.94it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 531/1000 [01:08<00:51,  9.20it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 532/1000 [01:08<00:50,  9.32it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 533/1000 [01:08<00:50,  9.33it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 534/1000 [01:09<00:52,  8.85it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 536/1000 [01:09<00:50,  9.22it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 538/1000 [01:09<00:49,  9.30it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 539/1000 [01:09<00:49,  9.30it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 540/1000 [01:09<00:51,  8.89it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 541/1000 [01:09<00:51,  8.98it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 542/1000 [01:09<00:53,  8.60it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 543/1000 [01:10<00:54,  8.33it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 544/1000 [01:10<00:55,  8.16it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 545/1000 [01:10<00:56,  8.01it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 546/1000 [01:10<00:54,  8.33it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 547/1000 [01:10<00:55,  8.15it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 548/1000 [01:10<00:56,  8.01it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 549/1000 [01:10<00:53,  8.36it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 550/1000 [01:10<00:52,  8.63it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 551/1000 [01:11<00:53,  8.35it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 552/1000 [01:11<00:58,  7.70it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 553/1000 [01:11<00:54,  8.14it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 555/1000 [01:11<00:48,  9.14it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 556/1000 [01:11<00:50,  8.74it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 557/1000 [01:11<00:52,  8.47it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 558/1000 [01:11<00:54,  8.17it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 559/1000 [01:12<00:52,  8.48it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 560/1000 [01:12<00:53,  8.22it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 561/1000 [01:12<00:56,  7.75it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 562/1000 [01:12<00:53,  8.15it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 563/1000 [01:12<00:51,  8.47it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 564/1000 [01:12<00:49,  8.73it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 565/1000 [01:12<00:51,  8.40it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 566/1000 [01:12<00:53,  8.10it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 567/1000 [01:13<00:54,  7.94it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 568/1000 [01:13<00:55,  7.82it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 569/1000 [01:13<00:55,  7.77it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 570/1000 [01:13<00:56,  7.67it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 571/1000 [01:13<00:53,  8.06it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 572/1000 [01:13<00:54,  7.89it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 573/1000 [01:13<00:55,  7.75it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 574/1000 [01:13<00:51,  8.24it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 575/1000 [01:14<00:49,  8.50it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 576/1000 [01:14<00:48,  8.69it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 577/1000 [01:14<00:50,  8.32it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 578/1000 [01:14<00:52,  8.07it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 579/1000 [01:14<00:53,  7.91it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 580/1000 [01:14<00:50,  8.24it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 581/1000 [01:14<00:48,  8.62it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 582/1000 [01:14<00:50,  8.30it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 583/1000 [01:14<00:51,  8.09it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 584/1000 [01:15<00:52,  7.95it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 585/1000 [01:15<00:53,  7.82it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 587/1000 [01:15<00:46,  8.84it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 588/1000 [01:15<00:45,  9.03it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 589/1000 [01:15<00:45,  9.05it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 590/1000 [01:15<00:47,  8.58it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 591/1000 [01:15<00:46,  8.74it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 593/1000 [01:16<00:45,  9.00it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 594/1000 [01:16<00:47,  8.56it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 595/1000 [01:16<00:48,  8.28it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 596/1000 [01:16<00:47,  8.47it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 597/1000 [01:16<00:49,  8.20it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 598/1000 [01:16<00:50,  8.01it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 599/1000 [01:16<00:50,  7.92it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 600/1000 [01:17<00:51,  7.79it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 601/1000 [01:17<00:48,  8.15it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 602/1000 [01:17<00:52,  7.62it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 603/1000 [01:17<00:52,  7.62it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 604/1000 [01:17<00:51,  7.62it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 605/1000 [01:17<00:51,  7.63it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 606/1000 [01:17<00:52,  7.55it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 607/1000 [01:17<00:51,  7.57it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 608/1000 [01:18<00:49,  8.00it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 609/1000 [01:18<00:49,  7.90it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 610/1000 [01:18<00:46,  8.37it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 611/1000 [01:18<00:45,  8.57it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 612/1000 [01:18<00:44,  8.73it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 613/1000 [01:18<00:46,  8.33it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 614/1000 [01:18<00:44,  8.72it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 615/1000 [01:18<00:46,  8.32it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 617/1000 [01:19<00:43,  8.79it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 618/1000 [01:19<00:42,  8.99it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 619/1000 [01:19<00:42,  9.02it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 620/1000 [01:19<00:41,  9.07it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 621/1000 [01:19<00:44,  8.55it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 622/1000 [01:19<00:45,  8.23it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 623/1000 [01:19<00:46,  8.07it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 624/1000 [01:19<00:44,  8.37it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 625/1000 [01:20<00:46,  8.14it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 626/1000 [01:20<00:46,  7.96it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 627/1000 [01:20<00:47,  7.88it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 628/1000 [01:20<00:47,  7.79it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 629/1000 [01:20<00:44,  8.28it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 630/1000 [01:20<00:42,  8.63it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 631/1000 [01:20<00:46,  7.92it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 632/1000 [01:20<00:47,  7.82it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 634/1000 [01:21<00:39,  9.31it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 635/1000 [01:21<00:41,  8.79it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 636/1000 [01:21<00:43,  8.43it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 637/1000 [01:21<00:46,  7.86it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 639/1000 [01:21<00:42,  8.52it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 640/1000 [01:21<00:43,  8.25it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 641/1000 [01:21<00:44,  8.09it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 642/1000 [01:22<00:45,  7.94it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 643/1000 [01:22<00:45,  7.86it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 644/1000 [01:22<00:45,  7.82it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 646/1000 [01:22<00:40,  8.81it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 647/1000 [01:22<00:41,  8.47it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 649/1000 [01:22<00:35,  9.80it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 650/1000 [01:22<00:38,  9.14it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 651/1000 [01:23<00:40,  8.67it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 653/1000 [01:23<00:38,  9.05it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 654/1000 [01:23<00:41,  8.38it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 655/1000 [01:23<00:40,  8.56it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 656/1000 [01:23<00:41,  8.30it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 657/1000 [01:23<00:42,  8.06it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 658/1000 [01:23<00:43,  7.93it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 659/1000 [01:24<00:43,  7.85it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 660/1000 [01:24<00:43,  7.76it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 661/1000 [01:24<00:41,  8.12it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 662/1000 [01:24<00:42,  7.97it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 663/1000 [01:24<00:40,  8.29it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 664/1000 [01:24<00:41,  8.09it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 665/1000 [01:24<00:42,  7.94it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 666/1000 [01:24<00:42,  7.77it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 667/1000 [01:25<00:43,  7.70it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 668/1000 [01:25<00:41,  8.08it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 669/1000 [01:25<00:41,  7.97it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 671/1000 [01:25<00:38,  8.55it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 672/1000 [01:25<00:39,  8.30it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 673/1000 [01:25<00:38,  8.50it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 674/1000 [01:25<00:39,  8.22it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 675/1000 [01:26<00:38,  8.46it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 676/1000 [01:26<00:42,  7.71it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 677/1000 [01:26<00:42,  7.66it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 678/1000 [01:26<00:42,  7.61it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 679/1000 [01:26<00:42,  7.59it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 680/1000 [01:26<00:42,  7.57it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 681/1000 [01:26<00:41,  7.62it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 682/1000 [01:27<00:43,  7.34it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 684/1000 [01:27<00:38,  8.21it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 685/1000 [01:27<00:40,  7.75it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 686/1000 [01:27<00:48,  6.45it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 687/1000 [01:27<00:46,  6.71it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 688/1000 [01:27<00:44,  6.96it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 689/1000 [01:27<00:41,  7.48it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 690/1000 [01:28<00:41,  7.52it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 691/1000 [01:28<00:41,  7.50it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 692/1000 [01:28<00:40,  7.53it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 693/1000 [01:28<00:38,  8.07it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 694/1000 [01:28<00:36,  8.37it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 695/1000 [01:28<00:37,  8.12it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 696/1000 [01:28<00:38,  7.95it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 697/1000 [01:28<00:36,  8.29it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 698/1000 [01:29<00:37,  8.09it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 699/1000 [01:29<00:37,  7.97it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 700/1000 [01:29<00:38,  7.81it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 701/1000 [01:29<00:38,  7.73it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 702/1000 [01:29<00:38,  7.70it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 703/1000 [01:29<00:38,  7.65it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 704/1000 [01:29<00:38,  7.63it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 705/1000 [01:30<00:38,  7.61it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 706/1000 [01:30<00:38,  7.60it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 707/1000 [01:30<00:36,  8.03it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 708/1000 [01:30<00:34,  8.50it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 709/1000 [01:30<00:33,  8.66it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 710/1000 [01:30<00:32,  8.94it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 711/1000 [01:30<00:33,  8.53it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 712/1000 [01:30<00:33,  8.71it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 713/1000 [01:30<00:32,  8.84it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 714/1000 [01:31<00:33,  8.44it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 715/1000 [01:31<00:34,  8.19it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 717/1000 [01:31<00:32,  8.82it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 718/1000 [01:31<00:33,  8.46it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 719/1000 [01:31<00:32,  8.68it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 721/1000 [01:31<00:30,  9.11it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 722/1000 [01:31<00:30,  9.13it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 723/1000 [01:32<00:31,  8.73it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 724/1000 [01:32<00:31,  8.86it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 725/1000 [01:32<00:32,  8.50it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 726/1000 [01:32<00:33,  8.22it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 728/1000 [01:32<00:29,  9.23it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 729/1000 [01:32<00:29,  9.20it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 730/1000 [01:32<00:31,  8.70it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 731/1000 [01:32<00:30,  8.83it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 732/1000 [01:33<00:31,  8.41it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 733/1000 [01:33<00:32,  8.13it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 734/1000 [01:33<00:33,  7.95it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 735/1000 [01:33<00:33,  7.84it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 736/1000 [01:33<00:32,  8.17it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 737/1000 [01:33<00:32,  8.00it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 738/1000 [01:33<00:31,  8.32it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 739/1000 [01:33<00:32,  8.10it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 740/1000 [01:34<00:32,  7.90it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 741/1000 [01:34<00:33,  7.82it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 742/1000 [01:34<00:33,  7.71it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 743/1000 [01:34<00:33,  7.69it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 744/1000 [01:34<00:34,  7.36it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 745/1000 [01:34<00:34,  7.43it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 746/1000 [01:34<00:34,  7.46it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 747/1000 [01:35<00:33,  7.49it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 748/1000 [01:35<00:31,  7.97it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 749/1000 [01:35<00:31,  7.87it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 750/1000 [01:35<00:32,  7.81it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 751/1000 [01:35<00:32,  7.71it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 752/1000 [01:35<00:32,  7.67it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 753/1000 [01:35<00:32,  7.63it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 755/1000 [01:36<00:29,  8.40it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 757/1000 [01:36<00:27,  8.70it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 758/1000 [01:36<00:28,  8.45it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 759/1000 [01:36<00:28,  8.59it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 760/1000 [01:36<00:27,  8.72it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 761/1000 [01:36<00:28,  8.38it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 762/1000 [01:36<00:29,  8.16it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 763/1000 [01:37<00:36,  6.52it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 764/1000 [01:37<00:34,  6.80it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 765/1000 [01:37<00:33,  7.02it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 766/1000 [01:37<00:32,  7.21it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 767/1000 [01:37<00:30,  7.69it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 768/1000 [01:37<00:30,  7.63it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 770/1000 [01:37<00:25,  8.88it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 771/1000 [01:38<00:26,  8.54it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 772/1000 [01:38<00:27,  8.21it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 773/1000 [01:38<00:28,  8.03it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 774/1000 [01:38<00:28,  7.88it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 775/1000 [01:38<00:28,  7.78it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 776/1000 [01:38<00:28,  7.73it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 777/1000 [01:38<00:28,  7.69it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 778/1000 [01:38<00:28,  7.68it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 779/1000 [01:39<00:27,  8.06it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 780/1000 [01:39<00:27,  7.91it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 781/1000 [01:39<00:28,  7.79it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 782/1000 [01:39<00:26,  8.14it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 783/1000 [01:39<00:25,  8.42it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 784/1000 [01:39<00:26,  8.18it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 785/1000 [01:39<00:26,  7.99it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 786/1000 [01:39<00:27,  7.85it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 787/1000 [01:40<00:27,  7.79it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 789/1000 [01:40<00:24,  8.53it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 790/1000 [01:40<00:25,  8.28it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 791/1000 [01:40<00:24,  8.52it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 792/1000 [01:40<00:23,  8.67it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 793/1000 [01:40<00:24,  8.35it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 794/1000 [01:40<00:25,  8.16it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 795/1000 [01:41<00:25,  8.00it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 796/1000 [01:41<00:24,  8.30it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 798/1000 [01:41<00:23,  8.73it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 799/1000 [01:41<00:22,  8.84it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 800/1000 [01:41<00:23,  8.45it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 801/1000 [01:41<00:23,  8.51it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 802/1000 [01:41<00:24,  8.22it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 803/1000 [01:41<00:22,  8.63it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 804/1000 [01:42<00:23,  8.31it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 805/1000 [01:42<00:24,  8.08it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 806/1000 [01:42<00:23,  8.36it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 807/1000 [01:42<00:22,  8.58it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 808/1000 [01:42<00:23,  8.26it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 809/1000 [01:42<00:22,  8.48it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 810/1000 [01:42<00:21,  8.66it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 811/1000 [01:42<00:22,  8.35it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 813/1000 [01:43<00:19,  9.37it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 814/1000 [01:43<00:21,  8.85it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 815/1000 [01:43<00:21,  8.47it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 816/1000 [01:43<00:22,  8.26it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 817/1000 [01:43<00:22,  8.07it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 818/1000 [01:43<00:22,  7.94it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 819/1000 [01:43<00:22,  7.88it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 821/1000 [01:44<00:20,  8.86it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 823/1000 [01:44<00:21,  8.10it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 824/1000 [01:44<00:21,  8.01it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 825/1000 [01:44<00:22,  7.62it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 826/1000 [01:44<00:22,  7.57it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 827/1000 [01:44<00:22,  7.60it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 828/1000 [01:45<00:21,  7.99it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 829/1000 [01:45<00:21,  7.87it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 830/1000 [01:45<00:21,  7.77it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 831/1000 [01:45<00:21,  7.71it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 832/1000 [01:45<00:22,  7.61it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 834/1000 [01:45<00:19,  8.46it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 835/1000 [01:45<00:19,  8.64it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 836/1000 [01:46<00:20,  8.01it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 837/1000 [01:46<00:20,  7.83it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 838/1000 [01:46<00:20,  7.76it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 839/1000 [01:46<00:20,  7.73it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 840/1000 [01:46<00:20,  7.71it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 841/1000 [01:46<00:20,  7.65it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 843/1000 [01:46<00:18,  8.50it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 844/1000 [01:47<00:18,  8.28it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 845/1000 [01:47<00:19,  8.11it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 846/1000 [01:47<00:18,  8.39it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 847/1000 [01:47<00:17,  8.59it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 848/1000 [01:47<00:18,  8.29it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 849/1000 [01:47<00:17,  8.68it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 850/1000 [01:47<00:18,  8.28it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 851/1000 [01:47<00:17,  8.52it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 852/1000 [01:47<00:18,  8.19it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 853/1000 [01:48<00:17,  8.30it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 854/1000 [01:48<00:18,  8.10it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 855/1000 [01:48<00:18,  7.98it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 856/1000 [01:48<00:18,  7.89it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 857/1000 [01:48<00:17,  8.21it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 858/1000 [01:48<00:16,  8.66it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 859/1000 [01:48<00:16,  8.34it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 861/1000 [01:49<00:15,  8.84it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 863/1000 [01:49<00:15,  9.04it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 864/1000 [01:49<00:14,  9.11it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 865/1000 [01:49<00:16,  8.37it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 866/1000 [01:49<00:16,  8.14it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 867/1000 [01:49<00:17,  7.68it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 868/1000 [01:49<00:17,  7.66it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 869/1000 [01:50<00:16,  8.01it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 870/1000 [01:50<00:16,  7.86it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 871/1000 [01:50<00:16,  7.79it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 872/1000 [01:50<00:15,  8.27it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 873/1000 [01:50<00:15,  8.06it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 874/1000 [01:50<00:15,  7.94it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 875/1000 [01:50<00:15,  7.84it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 876/1000 [01:50<00:16,  7.75it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 877/1000 [01:51<00:15,  7.72it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 878/1000 [01:51<00:14,  8.14it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 879/1000 [01:51<00:14,  8.42it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 880/1000 [01:51<00:14,  8.14it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 881/1000 [01:51<00:14,  8.41it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 882/1000 [01:51<00:14,  8.12it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 883/1000 [01:51<00:13,  8.44it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 884/1000 [01:51<00:13,  8.64it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 885/1000 [01:51<00:13,  8.25it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 886/1000 [01:52<00:13,  8.51it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 887/1000 [01:52<00:13,  8.24it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 888/1000 [01:52<00:13,  8.07it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 889/1000 [01:52<00:13,  8.35it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 890/1000 [01:52<00:13,  8.15it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 891/1000 [01:52<00:13,  7.94it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 892/1000 [01:52<00:13,  8.28it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 893/1000 [01:52<00:12,  8.53it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 894/1000 [01:53<00:12,  8.23it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 895/1000 [01:53<00:13,  8.01it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 896/1000 [01:53<00:13,  7.87it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 897/1000 [01:53<00:13,  7.78it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 898/1000 [01:53<00:13,  7.75it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 899/1000 [01:53<00:13,  7.69it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 900/1000 [01:53<00:13,  7.67it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 901/1000 [01:54<00:12,  7.67it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 902/1000 [01:54<00:12,  7.65it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 903/1000 [01:54<00:14,  6.80it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 904/1000 [01:54<00:13,  7.37it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 906/1000 [01:54<00:11,  7.90it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 907/1000 [01:54<00:11,  8.17it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 908/1000 [01:54<00:10,  8.41it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 909/1000 [01:55<00:11,  8.16it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 910/1000 [01:55<00:10,  8.40it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 911/1000 [01:55<00:10,  8.73it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 912/1000 [01:55<00:09,  8.85it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 913/1000 [01:55<00:09,  8.95it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 914/1000 [01:55<00:10,  8.50it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 915/1000 [01:55<00:10,  8.26it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 916/1000 [01:55<00:09,  8.52it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 917/1000 [01:55<00:10,  8.27it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 918/1000 [01:56<00:10,  8.12it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 919/1000 [01:56<00:10,  7.94it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 920/1000 [01:56<00:10,  7.92it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 921/1000 [01:56<00:10,  7.81it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 923/1000 [01:56<00:08,  9.17it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 924/1000 [01:56<00:08,  8.78it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 926/1000 [01:56<00:08,  9.03it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 927/1000 [01:57<00:08,  8.62it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 929/1000 [01:57<00:07,  8.96it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 930/1000 [01:57<00:07,  9.02it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 931/1000 [01:57<00:08,  8.33it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 932/1000 [01:57<00:07,  8.55it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 933/1000 [01:57<00:08,  8.25it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 934/1000 [01:57<00:08,  7.99it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 935/1000 [01:58<00:08,  7.87it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 936/1000 [01:58<00:08,  7.83it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 937/1000 [01:58<00:07,  8.18it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 938/1000 [01:58<00:07,  8.44it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 939/1000 [01:58<00:07,  8.19it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 940/1000 [01:58<00:07,  8.01it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 941/1000 [01:58<00:07,  8.30it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 943/1000 [01:58<00:06,  9.34it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 944/1000 [01:59<00:06,  8.84it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 945/1000 [01:59<00:06,  8.51it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 946/1000 [01:59<00:06,  8.52it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 947/1000 [01:59<00:06,  8.72it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 948/1000 [01:59<00:05,  8.85it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 949/1000 [01:59<00:06,  8.45it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 950/1000 [01:59<00:06,  8.18it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 951/1000 [01:59<00:05,  8.46it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 952/1000 [02:00<00:05,  8.19it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 953/1000 [02:00<00:05,  8.50it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 955/1000 [02:00<00:04,  9.29it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 956/1000 [02:00<00:04,  9.27it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 957/1000 [02:00<00:04,  8.76it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 958/1000 [02:00<00:04,  8.43it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 959/1000 [02:00<00:04,  8.62it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 960/1000 [02:01<00:04,  8.27it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 961/1000 [02:01<00:04,  8.10it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 962/1000 [02:01<00:04,  7.93it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 964/1000 [02:01<00:03,  9.21it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 965/1000 [02:01<00:03,  8.76it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 966/1000 [02:01<00:04,  8.39it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 967/1000 [02:01<00:04,  8.13it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 968/1000 [02:01<00:04,  7.96it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 969/1000 [02:02<00:03,  7.84it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 970/1000 [02:02<00:03,  8.17it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 971/1000 [02:02<00:03,  7.96it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 972/1000 [02:02<00:03,  7.89it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 973/1000 [02:02<00:04,  6.74it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 975/1000 [02:02<00:03,  7.78it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 976/1000 [02:02<00:02,  8.10it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 977/1000 [02:03<00:02,  8.36it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 978/1000 [02:03<00:02,  8.60it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 979/1000 [02:03<00:02,  8.75it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 980/1000 [02:03<00:02,  8.86it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 981/1000 [02:03<00:02,  8.42it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 982/1000 [02:03<00:02,  8.61it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 983/1000 [02:03<00:02,  8.30it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 984/1000 [02:03<00:01,  8.05it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 985/1000 [02:04<00:01,  7.89it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 986/1000 [02:04<00:01,  7.76it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 987/1000 [02:04<00:01,  7.73it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 988/1000 [02:04<00:01,  7.69it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 989/1000 [02:04<00:01,  7.68it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 990/1000 [02:04<00:01,  8.10it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 991/1000 [02:04<00:01,  7.97it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 992/1000 [02:04<00:01,  7.84it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 993/1000 [02:05<00:00,  7.74it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 994/1000 [02:05<00:00,  7.71it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 995/1000 [02:05<00:00,  7.66it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 996/1000 [02:05<00:00,  8.05it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 997/1000 [02:05<00:00,  7.94it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 998/1000 [02:05<00:00,  8.27it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 999/1000 [02:05<00:00,  8.01it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [02:05<00:00,  7.94it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [02:05<00:00,  7.94it/s]
DONE (3.18s)
DONE (7.36s)
loss_threshold ROC AUC: 0.5060359999999999, PR AUC: 0.5052751236388919, tpr_at_low_fpr: {0.001: 0.004, 0.01: 0.019}
min_k_threshold ROC AUC: 0.505032, PR AUC: 0.509114937535194, tpr_at_low_fpr: {0.001: 0.006, 0.01: 0.017}
zlib_threshold ROC AUC: 0.506259, PR AUC: 0.5063809248680765, tpr_at_low_fpr: {0.001: 0.005, 0.01: 0.017}
ref-stablelm-base-alpha-3b-v2_threshold ROC AUC: 0.5152019999999999, PR AUC: 0.5193081890607443, tpr_at_low_fpr: {0.001: 0.003, 0.01: 0.011}
loss_threshold roc_auc: 0.506
min_k_threshold roc_auc: 0.505
zlib_threshold roc_auc: 0.506
ref-stablelm-base-alpha-3b-v2_threshold roc_auc: 0.515
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
results_new/neo125_github_experiment/meta-llama_Llama-2-70b-hf/pubmed_central_ngram_13_<0.8_truncated
Saving results to absolute path: /home/chawins/mimir/tmp_results/neo125_github_experiment/meta-llama_Llama-2-70b-hf/pubmed_central_ngram_13_<0.8_truncated
LOG: cache_dir is /home/chawins/.cache
Using cache dir /home/chawins/.cache
Loading BASE model meta-llama/Llama-2-70b-hf...
Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.embed_tokens.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:   7%|â–‹         | 1/15 [00:00<00:04,  3.27it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  13%|â–ˆâ–Ž        | 2/15 [00:00<00:03,  3.41it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 3/15 [00:00<00:03,  3.45it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:01<00:03,  3.46it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:01<00:02,  3.48it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:01<00:02,  3.49it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:02<00:02,  3.48it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:02<00:02,  3.49it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:02<00:01,  3.49it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:02<00:01,  3.48it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:03<00:01,  3.48it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:03<00:00,  3.47it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:03<00:00,  3.47it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:04<00:00,  3.46it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for lm_head.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:04<00:00,  3.50it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:04<00:00,  3.48it/s]
  0%|          | 0/57 [00:00<?, ?w/s]model.embed_tokens.weight:   0%|          | 0/57 [00:00<?, ?w/s, dev=0]model.embed_tokens.weight:   2%|â–         | 1/57 [00:00<00:09,  5.83w/s, dev=0]model.layers.0.input_layernorm.weight:   2%|â–         | 1/57 [00:00<00:09,  5.82w/s, dev=0]model.layers.0.mlp.down_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:04, 11.60w/s, dev=0]  model.layers.0.mlp.down_proj.weight:   5%|â–Œ         | 3/57 [00:00<00:05,  9.24w/s, dev=0]model.layers.0.mlp.gate_proj.weight:   5%|â–Œ         | 3/57 [00:00<00:05,  9.24w/s, dev=0]model.layers.0.mlp.up_proj.weight:   7%|â–‹         | 4/57 [00:00<00:06,  8.40w/s, dev=0]  model.layers.0.mlp.up_proj.weight:   9%|â–‰         | 5/57 [00:00<00:06,  8.01w/s, dev=0]model.layers.0.post_attention_layernorm.weight:   9%|â–‰         | 5/57 [00:00<00:06,  8.01w/s, dev=0]model.layers.0.self_attn.k_proj.weight:  11%|â–ˆ         | 6/57 [00:00<00:05,  9.60w/s, dev=0]        model.layers.0.self_attn.o_proj.weight:  12%|â–ˆâ–        | 7/57 [00:00<00:04, 11.09w/s, dev=0]model.layers.0.self_attn.q_proj.weight:  14%|â–ˆâ–        | 8/57 [00:00<00:04, 11.83w/s, dev=0]model.layers.0.self_attn.q_proj.weight:  16%|â–ˆâ–Œ        | 9/57 [00:00<00:03, 12.35w/s, dev=0]model.layers.0.self_attn.rotary_emb.inv_freq:  16%|â–ˆâ–Œ        | 9/57 [00:00<00:03, 12.34w/s, dev=0]model.layers.0.self_attn.v_proj.weight:  18%|â–ˆâ–Š        | 10/57 [00:00<00:03, 13.71w/s, dev=0]     model.layers.1.input_layernorm.weight:  19%|â–ˆâ–‰        | 11/57 [00:00<00:03, 14.87w/s, dev=0] model.layers.1.mlp.down_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:00<00:02, 16.21w/s, dev=0]  model.layers.1.mlp.down_proj.weight:  23%|â–ˆâ–ˆâ–Ž       | 13/57 [00:00<00:03, 14.45w/s, dev=0]model.layers.1.mlp.gate_proj.weight:  23%|â–ˆâ–ˆâ–Ž       | 13/57 [00:00<00:03, 14.45w/s, dev=0]model.layers.1.mlp.up_proj.weight:  25%|â–ˆâ–ˆâ–       | 14/57 [00:01<00:03, 12.73w/s, dev=0]  model.layers.1.post_attention_layernorm.weight:  26%|â–ˆâ–ˆâ–‹       | 15/57 [00:01<00:03, 11.95w/s, dev=0]model.layers.1.self_attn.k_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:01<00:03, 12.74w/s, dev=0]        model.layers.1.self_attn.k_proj.weight:  30%|â–ˆâ–ˆâ–‰       | 17/57 [00:01<00:02, 13.45w/s, dev=0]model.layers.1.self_attn.o_proj.weight:  30%|â–ˆâ–ˆâ–‰       | 17/57 [00:01<00:02, 13.45w/s, dev=0]model.layers.1.self_attn.q_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 18/57 [00:01<00:02, 13.76w/s, dev=0]model.layers.1.self_attn.rotary_emb.inv_freq:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 19/57 [00:01<00:02, 13.94w/s, dev=0]model.layers.1.self_attn.v_proj.weight:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 20/57 [00:01<00:02, 14.67w/s, dev=0]      model.layers.1.self_attn.v_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 21/57 [00:01<00:02, 15.24w/s, dev=0]model.layers.2.input_layernorm.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 21/57 [00:01<00:02, 15.24w/s, dev=0] model.layers.2.mlp.down_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:01<00:02, 15.96w/s, dev=0]  model.layers.2.mlp.gate_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 23/57 [00:01<00:02, 14.96w/s, dev=0]model.layers.2.mlp.up_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/57 [00:01<00:02, 14.14w/s, dev=0]  model.layers.2.mlp.up_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/57 [00:02<00:02, 11.68w/s, dev=0]model.layers.2.post_attention_layernorm.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/57 [00:02<00:02, 11.68w/s, dev=0]model.layers.2.self_attn.k_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 26/57 [00:02<00:02, 12.15w/s, dev=0]        model.layers.2.self_attn.o_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 27/57 [00:02<00:02, 12.33w/s, dev=0]model.layers.2.self_attn.q_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:02<00:02, 12.51w/s, dev=0]model.layers.2.self_attn.q_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 29/57 [00:02<00:02, 12.70w/s, dev=0]model.layers.2.self_attn.rotary_emb.inv_freq:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 29/57 [00:02<00:02, 12.70w/s, dev=0]model.layers.2.self_attn.v_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 30/57 [00:02<00:02, 13.13w/s, dev=0]      model.layers.3.input_layernorm.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 31/57 [00:02<00:01, 13.52w/s, dev=0] model.layers.3.mlp.down_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 32/57 [00:02<00:01, 13.95w/s, dev=0]  model.layers.3.mlp.down_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 33/57 [00:02<00:01, 13.44w/s, dev=0]model.layers.3.mlp.gate_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 33/57 [00:02<00:01, 13.44w/s, dev=0]model.layers.3.mlp.up_proj.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:02<00:01, 13.02w/s, dev=0]  model.layers.3.post_attention_layernorm.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/57 [00:02<00:01, 12.65w/s, dev=0]model.layers.3.self_attn.k_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 36/57 [00:02<00:01, 13.01w/s, dev=0]        model.layers.3.self_attn.k_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 37/57 [00:02<00:01, 13.33w/s, dev=0]model.layers.3.self_attn.o_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 37/57 [00:02<00:01, 13.33w/s, dev=0]model.layers.3.self_attn.q_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 38/57 [00:02<00:01, 13.46w/s, dev=0]model.layers.3.self_attn.rotary_emb.inv_freq:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 39/57 [00:02<00:01, 13.60w/s, dev=0]model.layers.3.self_attn.v_proj.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:02<00:01, 13.94w/s, dev=0]      model.layers.3.self_attn.v_proj.weight:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/57 [00:02<00:01, 14.26w/s, dev=0]model.layers.4.input_layernorm.weight:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/57 [00:02<00:01, 14.26w/s, dev=0] model.layers.4.mlp.down_proj.weight:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 42/57 [00:02<00:01, 14.61w/s, dev=0]  model.layers.4.mlp.gate_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 43/57 [00:03<00:00, 14.15w/s, dev=0]model.layers.4.mlp.up_proj.weight:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 44/57 [00:03<00:00, 13.74w/s, dev=0]  model.layers.4.mlp.up_proj.weight:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 45/57 [00:03<00:00, 13.37w/s, dev=0]model.layers.4.post_attention_layernorm.weight:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 45/57 [00:03<00:00, 13.37w/s, dev=0]model.layers.4.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:03<00:00, 13.67w/s, dev=0]        model.layers.4.self_attn.o_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 47/57 [00:03<00:00, 13.94w/s, dev=0]model.layers.4.self_attn.q_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/57 [00:03<00:00, 14.04w/s, dev=0]model.layers.4.self_attn.rotary_emb.inv_freq:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 49/57 [00:03<00:00, 14.15w/s, dev=0]model.layers.4.self_attn.v_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 50/57 [00:03<00:00, 14.43w/s, dev=0]      model.layers.4.self_attn.v_proj.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 51/57 [00:03<00:00, 14.70w/s, dev=0]model.layers.5.mlp.gate_proj.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 51/57 [00:03<00:00, 14.70w/s, dev=0]   model.layers.5.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:03<00:00, 14.35w/s, dev=0]model.layers.5.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 53/57 [00:03<00:00, 14.59w/s, dev=0]model.layers.5.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/57 [00:03<00:00, 14.68w/s, dev=0]model.layers.5.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 55/57 [00:03<00:00, 14.78w/s, dev=0]model.layers.5.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 56/57 [00:03<00:00, 15.04w/s, dev=0]      model.layers.5.self_attn.v_proj.weight: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:03<00:00, 15.29w/s, dev=0]                                                                                               0%|          | 0/59 [00:00<?, ?w/s]model.layers.5.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=0]model.layers.5.mlp.down_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 1516.93w/s, dev=0]model.layers.5.mlp.down_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 12.99w/s, dev=0]  model.layers.5.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 12.97w/s, dev=0]  model.layers.5.post_attention_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:05,  9.65w/s, dev=0]model.layers.5.post_attention_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:04, 12.84w/s, dev=0]model.layers.6.input_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:04, 12.84w/s, dev=0]         model.layers.6.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 16.03w/s, dev=0]  model.layers.6.mlp.down_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:04, 12.84w/s, dev=0]model.layers.6.mlp.gate_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:04, 12.83w/s, dev=0]model.layers.6.mlp.up_proj.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:04, 11.15w/s, dev=0]  model.layers.6.mlp.up_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:05, 10.18w/s, dev=0]model.layers.6.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:05, 10.18w/s, dev=0]model.layers.6.self_attn.k_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:04, 11.44w/s, dev=0]        model.layers.6.self_attn.o_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:03, 12.61w/s, dev=0]model.layers.6.self_attn.q_proj.weight:  19%|â–ˆâ–Š        | 11/59 [00:00<00:03, 13.11w/s, dev=0]model.layers.6.self_attn.rotary_emb.inv_freq:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:03, 13.56w/s, dev=0]model.layers.6.self_attn.v_proj.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:03, 14.68w/s, dev=0]      model.layers.6.self_attn.v_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:02, 15.70w/s, dev=0]model.layers.7.input_layernorm.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:02, 15.70w/s, dev=0] model.layers.7.mlp.down_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:02, 16.82w/s, dev=0]  model.layers.7.mlp.gate_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:02, 15.04w/s, dev=0]model.layers.7.mlp.up_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:01<00:03, 13.77w/s, dev=0]  model.layers.7.post_attention_layernorm.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:03, 12.80w/s, dev=0]model.layers.7.self_attn.k_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:02, 13.50w/s, dev=0]        model.layers.7.self_attn.k_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:02, 14.14w/s, dev=0]model.layers.7.self_attn.o_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:02, 14.14w/s, dev=0]model.layers.7.self_attn.q_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:01<00:02, 14.39w/s, dev=0]model.layers.7.self_attn.rotary_emb.inv_freq:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:02, 14.66w/s, dev=0]model.layers.7.self_attn.v_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:02, 15.32w/s, dev=0]      model.layers.8.input_layernorm.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:02, 15.92w/s, dev=0] model.layers.8.mlp.down_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:02, 16.58w/s, dev=0]  model.layers.8.mlp.down_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:02, 15.72w/s, dev=0]model.layers.8.mlp.gate_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:02, 15.72w/s, dev=0]model.layers.8.mlp.up_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:01<00:02, 14.77w/s, dev=0]  model.layers.8.post_attention_layernorm.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:02, 14.03w/s, dev=0]model.layers.8.self_attn.k_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:02, 14.53w/s, dev=0]        model.layers.8.self_attn.o_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:02<00:01, 14.98w/s, dev=0]model.layers.8.self_attn.q_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:02<00:01, 15.11w/s, dev=0]model.layers.8.self_attn.q_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:02<00:01, 15.24w/s, dev=0]model.layers.8.self_attn.rotary_emb.inv_freq:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:02<00:01, 15.24w/s, dev=0]model.layers.8.self_attn.v_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:02<00:01, 15.72w/s, dev=0]      model.layers.9.input_layernorm.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:02<00:01, 16.14w/s, dev=0] model.layers.9.mlp.down_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:02<00:01, 16.61w/s, dev=0]  model.layers.9.mlp.gate_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:02<00:01, 15.87w/s, dev=0]model.layers.9.mlp.up_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:02<00:01, 15.26w/s, dev=0]  model.layers.9.mlp.up_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:02<00:01, 14.65w/s, dev=0]model.layers.9.post_attention_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:02<00:01, 14.65w/s, dev=0]model.layers.9.self_attn.k_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:02<00:01, 15.03w/s, dev=0]        model.layers.9.self_attn.o_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:02<00:01, 15.37w/s, dev=0]model.layers.9.self_attn.q_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:02<00:01, 15.46w/s, dev=0]model.layers.9.self_attn.rotary_emb.inv_freq:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:02<00:01, 15.55w/s, dev=0]model.layers.9.self_attn.v_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:02<00:01, 15.92w/s, dev=0]      model.layers.9.self_attn.v_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:02<00:00, 16.24w/s, dev=0]model.layers.10.input_layernorm.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:02<00:00, 16.24w/s, dev=0]model.layers.10.mlp.down_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:02<00:00, 16.61w/s, dev=0]  model.layers.10.mlp.gate_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:02<00:00, 16.05w/s, dev=0]model.layers.10.mlp.up_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:03<00:00, 15.49w/s, dev=0]  model.layers.10.post_attention_layernorm.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:03<00:00, 15.05w/s, dev=0]model.layers.10.self_attn.k_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:03<00:00, 15.36w/s, dev=0]        model.layers.10.self_attn.k_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:03<00:00, 15.63w/s, dev=0]model.layers.10.self_attn.o_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:03<00:00, 15.63w/s, dev=0]model.layers.10.self_attn.q_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:03<00:00, 15.72w/s, dev=0]model.layers.10.self_attn.rotary_emb.inv_freq:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:03<00:00, 15.82w/s, dev=0]model.layers.10.self_attn.v_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:03<00:00, 16.12w/s, dev=0]      model.layers.11.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:03<00:00, 16.39w/s, dev=0]model.layers.11.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:03<00:00, 16.67w/s, dev=0]model.layers.11.self_attn.o_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:03<00:00, 16.73w/s, dev=0]model.layers.11.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:03<00:00, 16.73w/s, dev=0]model.layers.11.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:03<00:00, 16.78w/s, dev=0]model.layers.11.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:03<00:00, 17.07w/s, dev=0]                                                                                                      0%|          | 0/55 [00:00<?, ?w/s]model.layers.11.input_layernorm.weight:   0%|          | 0/55 [00:00<?, ?w/s, dev=0]model.layers.11.mlp.down_proj.weight:   2%|â–         | 1/55 [00:00<00:00, 1399.03w/s, dev=0]model.layers.11.mlp.down_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:04, 12.45w/s, dev=0]  model.layers.11.mlp.gate_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:04, 12.43w/s, dev=0]model.layers.11.mlp.up_proj.weight:   5%|â–Œ         | 3/55 [00:00<00:05,  9.29w/s, dev=0]  model.layers.11.mlp.up_proj.weight:   7%|â–‹         | 4/55 [00:00<00:06,  8.23w/s, dev=0]model.layers.11.post_attention_layernorm.weight:   7%|â–‹         | 4/55 [00:00<00:06,  8.22w/s, dev=0]model.layers.12.input_layernorm.weight:   9%|â–‰         | 5/55 [00:00<00:04, 10.27w/s, dev=0]         model.layers.12.mlp.down_proj.weight:  11%|â–ˆ         | 6/55 [00:00<00:03, 12.32w/s, dev=0]  model.layers.12.mlp.down_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:04, 10.69w/s, dev=0]model.layers.12.mlp.gate_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:04, 10.69w/s, dev=0]model.layers.12.mlp.up_proj.weight:  15%|â–ˆâ–        | 8/55 [00:00<00:04,  9.79w/s, dev=0]  model.layers.12.post_attention_layernorm.weight:  16%|â–ˆâ–‹        | 9/55 [00:00<00:05,  9.19w/s, dev=0]model.layers.12.post_attention_layernorm.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:04, 10.20w/s, dev=0]model.layers.12.self_attn.k_proj.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:04, 10.20w/s, dev=0]        model.layers.12.self_attn.o_proj.weight:  20%|â–ˆâ–ˆ        | 11/55 [00:00<00:03, 11.15w/s, dev=0]model.layers.12.self_attn.q_proj.weight:  22%|â–ˆâ–ˆâ–       | 12/55 [00:01<00:03, 11.61w/s, dev=0]model.layers.12.self_attn.q_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:01<00:03, 12.02w/s, dev=0]model.layers.12.self_attn.rotary_emb.inv_freq:  24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:01<00:03, 12.02w/s, dev=0]model.layers.12.self_attn.v_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 14/55 [00:01<00:03, 12.94w/s, dev=0]      model.layers.13.input_layernorm.weight:  27%|â–ˆâ–ˆâ–‹       | 15/55 [00:01<00:02, 13.76w/s, dev=0] model.layers.13.mlp.down_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 16/55 [00:01<00:02, 14.67w/s, dev=0]  model.layers.13.mlp.down_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:01<00:02, 13.58w/s, dev=0]model.layers.13.mlp.gate_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:01<00:02, 13.58w/s, dev=0]model.layers.13.mlp.up_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/55 [00:01<00:02, 12.77w/s, dev=0]  model.layers.13.post_attention_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–      | 19/55 [00:01<00:02, 12.13w/s, dev=0]model.layers.13.self_attn.k_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:01<00:02, 12.76w/s, dev=0]        model.layers.13.self_attn.k_proj.weight:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/55 [00:01<00:02, 13.34w/s, dev=0]model.layers.13.self_attn.o_proj.weight:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/55 [00:01<00:02, 13.33w/s, dev=0]model.layers.13.self_attn.q_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/55 [00:01<00:02, 13.55w/s, dev=0]model.layers.13.self_attn.rotary_emb.inv_freq:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 23/55 [00:01<00:02, 13.77w/s, dev=0]model.layers.13.self_attn.v_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 24/55 [00:01<00:02, 14.36w/s, dev=0]      model.layers.13.self_attn.v_proj.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:01<00:02, 14.91w/s, dev=0]model.layers.14.input_layernorm.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:01<00:02, 14.90w/s, dev=0] model.layers.14.mlp.down_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 26/55 [00:01<00:01, 15.50w/s, dev=0]  model.layers.14.mlp.gate_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 27/55 [00:01<00:01, 14.68w/s, dev=0]model.layers.14.mlp.up_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 28/55 [00:01<00:01, 14.00w/s, dev=0]  model.layers.14.mlp.up_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/55 [00:02<00:01, 13.39w/s, dev=0]model.layers.14.post_attention_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/55 [00:02<00:01, 13.38w/s, dev=0]model.layers.14.self_attn.k_proj.weight:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 30/55 [00:02<00:01, 13.84w/s, dev=0]        model.layers.14.self_attn.o_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/55 [00:02<00:01, 14.26w/s, dev=0]model.layers.14.self_attn.q_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 32/55 [00:02<00:01, 14.42w/s, dev=0]model.layers.14.self_attn.rotary_emb.inv_freq:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/55 [00:02<00:01, 14.57w/s, dev=0]model.layers.14.self_attn.v_proj.weight:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/55 [00:02<00:01, 15.01w/s, dev=0]      model.layers.14.self_attn.v_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:02<00:01, 15.41w/s, dev=0]model.layers.15.input_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:02<00:01, 15.41w/s, dev=0] model.layers.15.mlp.down_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 36/55 [00:02<00:01, 15.85w/s, dev=0]  model.layers.15.mlp.gate_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 37/55 [00:02<00:01, 15.21w/s, dev=0]model.layers.15.mlp.up_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 38/55 [00:02<00:01, 14.61w/s, dev=0]  model.layers.15.post_attention_layernorm.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 39/55 [00:02<00:01, 14.09w/s, dev=0]model.layers.15.self_attn.k_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 40/55 [00:02<00:01, 14.45w/s, dev=0]        model.layers.15.self_attn.k_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/55 [00:02<00:00, 14.78w/s, dev=0]model.layers.15.self_attn.o_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/55 [00:02<00:00, 14.77w/s, dev=0]model.layers.15.self_attn.q_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 42/55 [00:02<00:00, 14.88w/s, dev=0]model.layers.15.self_attn.rotary_emb.inv_freq:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 43/55 [00:02<00:00, 14.97w/s, dev=0]model.layers.15.self_attn.v_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 44/55 [00:02<00:00, 15.32w/s, dev=0]      model.layers.16.input_layernorm.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:02<00:00, 15.62w/s, dev=0] model.layers.16.mlp.down_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 46/55 [00:02<00:00, 15.97w/s, dev=0]  model.layers.16.mlp.down_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 47/55 [00:03<00:00, 15.41w/s, dev=0]model.layers.16.mlp.gate_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 47/55 [00:03<00:00, 15.41w/s, dev=0]model.layers.16.mlp.up_proj.weight:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 48/55 [00:03<00:00, 14.93w/s, dev=0]  model.layers.16.post_attention_layernorm.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 49/55 [00:03<00:00, 14.55w/s, dev=0]model.layers.16.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 50/55 [00:03<00:00, 14.85w/s, dev=0]        model.layers.16.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 51/55 [00:03<00:00, 15.11w/s, dev=0]model.layers.16.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 52/55 [00:03<00:00, 15.21w/s, dev=0]model.layers.16.self_attn.q_proj.weight:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 53/55 [00:03<00:00, 15.28w/s, dev=0]model.layers.16.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 53/55 [00:03<00:00, 15.28w/s, dev=0]model.layers.16.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 54/55 [00:03<00:00, 15.57w/s, dev=0]                                                                                                      0%|          | 0/57 [00:00<?, ?w/s]model.layers.17.input_layernorm.weight:   0%|          | 0/57 [00:00<?, ?w/s, dev=0]model.layers.17.mlp.down_proj.weight:   2%|â–         | 1/57 [00:00<00:00, 1437.88w/s, dev=0]model.layers.17.mlp.down_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:04, 11.55w/s, dev=0]  model.layers.17.mlp.gate_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:04, 11.53w/s, dev=0]model.layers.17.mlp.up_proj.weight:   5%|â–Œ         | 3/57 [00:00<00:06,  7.87w/s, dev=0]  model.layers.17.mlp.up_proj.weight:   7%|â–‹         | 4/57 [00:00<00:07,  7.16w/s, dev=0]model.layers.17.post_attention_layernorm.weight:   7%|â–‹         | 4/57 [00:00<00:07,  7.16w/s, dev=0]model.layers.17.self_attn.k_proj.weight:   9%|â–‰         | 5/57 [00:00<00:05,  8.94w/s, dev=0]        model.layers.17.self_attn.o_proj.weight:  11%|â–ˆ         | 6/57 [00:00<00:04, 10.60w/s, dev=0]model.layers.17.self_attn.q_proj.weight:  12%|â–ˆâ–        | 7/57 [00:00<00:04, 11.36w/s, dev=0]model.layers.17.self_attn.q_proj.weight:  14%|â–ˆâ–        | 8/57 [00:00<00:04, 10.00w/s, dev=0]model.layers.17.self_attn.rotary_emb.inv_freq:  14%|â–ˆâ–        | 8/57 [00:00<00:04,  9.99w/s, dev=0]model.layers.17.self_attn.v_proj.weight:  16%|â–ˆâ–Œ        | 9/57 [00:00<00:04, 11.24w/s, dev=0]      model.layers.18.input_layernorm.weight:  18%|â–ˆâ–Š        | 10/57 [00:00<00:03, 12.29w/s, dev=0]model.layers.18.mlp.down_proj.weight:  19%|â–ˆâ–‰        | 11/57 [00:00<00:03, 13.52w/s, dev=0]  model.layers.18.mlp.down_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:00<00:03, 12.15w/s, dev=0]model.layers.18.mlp.gate_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:00<00:03, 12.15w/s, dev=0]model.layers.18.mlp.up_proj.weight:  23%|â–ˆâ–ˆâ–Ž       | 13/57 [00:01<00:03, 11.23w/s, dev=0]  model.layers.18.post_attention_layernorm.weight:  25%|â–ˆâ–ˆâ–       | 14/57 [00:01<00:04,  9.63w/s, dev=0]model.layers.18.self_attn.k_proj.weight:  26%|â–ˆâ–ˆâ–‹       | 15/57 [00:01<00:04, 10.31w/s, dev=0]        model.layers.18.self_attn.k_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:01<00:03, 10.94w/s, dev=0]model.layers.18.self_attn.o_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:01<00:03, 10.94w/s, dev=0]model.layers.18.self_attn.q_proj.weight:  30%|â–ˆâ–ˆâ–‰       | 17/57 [00:01<00:03, 11.20w/s, dev=0]model.layers.18.self_attn.rotary_emb.inv_freq:  32%|â–ˆâ–ˆâ–ˆâ–      | 18/57 [00:01<00:03, 11.48w/s, dev=0]model.layers.18.self_attn.v_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 19/57 [00:01<00:03, 12.11w/s, dev=0]      model.layers.18.self_attn.v_proj.weight:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 20/57 [00:01<00:02, 12.69w/s, dev=0]model.layers.19.input_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 20/57 [00:01<00:02, 12.68w/s, dev=0] model.layers.19.mlp.down_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 21/57 [00:01<00:02, 13.32w/s, dev=0]  model.layers.19.mlp.gate_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:01<00:02, 12.61w/s, dev=0]model.layers.19.mlp.up_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 23/57 [00:02<00:03, 11.28w/s, dev=0]  model.layers.19.mlp.up_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/57 [00:02<00:03, 10.94w/s, dev=0]model.layers.19.post_attention_layernorm.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/57 [00:02<00:03, 10.94w/s, dev=0]model.layers.19.self_attn.k_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/57 [00:02<00:02, 11.39w/s, dev=0]        model.layers.19.self_attn.o_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 26/57 [00:02<00:02, 11.81w/s, dev=0]model.layers.19.self_attn.q_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 27/57 [00:02<00:02, 12.01w/s, dev=0]model.layers.19.self_attn.q_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:02<00:02, 11.03w/s, dev=0]model.layers.19.self_attn.rotary_emb.inv_freq:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:02<00:02, 11.03w/s, dev=0]model.layers.19.self_attn.v_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 29/57 [00:02<00:02, 11.42w/s, dev=0]      model.layers.20.input_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 30/57 [00:02<00:02, 10.83w/s, dev=1] model.layers.20.mlp.down_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 31/57 [00:02<00:02, 11.16w/s, dev=1]  model.layers.20.mlp.down_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 32/57 [00:02<00:02, 10.89w/s, dev=1]model.layers.20.mlp.gate_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 32/57 [00:02<00:02, 10.89w/s, dev=1]model.layers.20.mlp.up_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 33/57 [00:03<00:02, 10.06w/s, dev=1]  model.layers.20.post_attention_layernorm.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:03<00:02,  9.86w/s, dev=1]model.layers.20.self_attn.k_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/57 [00:03<00:02, 10.15w/s, dev=1]        model.layers.20.self_attn.k_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 36/57 [00:03<00:02, 10.41w/s, dev=1]model.layers.20.self_attn.o_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 36/57 [00:03<00:02, 10.41w/s, dev=1]model.layers.20.self_attn.q_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 37/57 [00:03<00:01, 10.56w/s, dev=1]model.layers.20.self_attn.rotary_emb.inv_freq:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 38/57 [00:03<00:01, 10.03w/s, dev=1]model.layers.20.self_attn.v_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 39/57 [00:03<00:01, 10.30w/s, dev=1]      model.layers.20.self_attn.v_proj.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:03<00:01, 10.53w/s, dev=1]model.layers.21.input_layernorm.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:03<00:01, 10.53w/s, dev=1] model.layers.21.mlp.down_proj.weight:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/57 [00:03<00:01, 10.79w/s, dev=1]  model.layers.21.mlp.gate_proj.weight:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 42/57 [00:03<00:01, 10.61w/s, dev=1]model.layers.21.mlp.up_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 43/57 [00:04<00:01, 10.42w/s, dev=1]  model.layers.21.mlp.up_proj.weight:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 44/57 [00:04<00:01, 10.25w/s, dev=1]model.layers.21.post_attention_layernorm.weight:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 44/57 [00:04<00:01, 10.25w/s, dev=1]model.layers.21.self_attn.k_proj.weight:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 45/57 [00:04<00:01, 10.48w/s, dev=1]        model.layers.21.self_attn.o_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:04<00:01, 10.70w/s, dev=1]model.layers.21.self_attn.q_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 47/57 [00:04<00:00, 10.81w/s, dev=1]model.layers.21.self_attn.q_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/57 [00:04<00:00, 10.92w/s, dev=1]model.layers.21.self_attn.rotary_emb.inv_freq:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/57 [00:04<00:00, 10.92w/s, dev=1]model.layers.21.self_attn.v_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 49/57 [00:04<00:00, 11.14w/s, dev=1]      model.layers.22.mlp.down_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 50/57 [00:04<00:00, 11.36w/s, dev=1]   model.layers.22.mlp.gate_proj.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 51/57 [00:04<00:00, 11.16w/s, dev=1]model.layers.22.mlp.gate_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:04<00:00, 10.98w/s, dev=1]model.layers.22.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:04<00:00, 10.98w/s, dev=1]model.layers.22.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 53/57 [00:04<00:00, 11.17w/s, dev=1]model.layers.22.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/57 [00:04<00:00, 11.27w/s, dev=1]model.layers.22.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 55/57 [00:04<00:00, 11.37w/s, dev=1]model.layers.22.self_attn.rotary_emb.inv_freq:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 56/57 [00:04<00:00, 11.57w/s, dev=1]model.layers.22.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 56/57 [00:04<00:00, 11.57w/s, dev=1]                                                                                                      0%|          | 0/59 [00:00<?, ?w/s]model.layers.22.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=1]model.layers.22.mlp.up_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 1242.39w/s, dev=1]model.layers.22.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 12.95w/s, dev=1]  model.layers.22.post_attention_layernorm.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 12.93w/s, dev=1]model.layers.23.input_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:02, 19.34w/s, dev=1]         model.layers.23.mlp.down_proj.weight:   7%|â–‹         | 4/59 [00:00<00:02, 25.74w/s, dev=1]  model.layers.23.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 15.82w/s, dev=1]model.layers.23.mlp.gate_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 15.81w/s, dev=1]model.layers.23.mlp.up_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:04, 12.81w/s, dev=1]  model.layers.23.post_attention_layernorm.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:04, 11.43w/s, dev=1]model.layers.23.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:03, 13.05w/s, dev=1]model.layers.23.self_attn.k_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:03, 13.05w/s, dev=1]        model.layers.23.self_attn.o_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:03, 14.52w/s, dev=1]model.layers.23.self_attn.q_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:03, 15.09w/s, dev=1]model.layers.23.self_attn.rotary_emb.inv_freq:  19%|â–ˆâ–Š        | 11/59 [00:00<00:03, 15.54w/s, dev=1]model.layers.23.self_attn.v_proj.weight:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:02, 16.94w/s, dev=1]      model.layers.23.self_attn.v_proj.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:02, 18.01w/s, dev=1]model.layers.24.input_layernorm.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:02, 18.01w/s, dev=1] model.layers.24.mlp.down_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:02, 19.38w/s, dev=1]  model.layers.24.mlp.gate_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:02, 16.80w/s, dev=1]model.layers.24.mlp.up_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:02, 15.14w/s, dev=1]  model.layers.24.post_attention_layernorm.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:01<00:03, 13.95w/s, dev=1]model.layers.24.post_attention_layernorm.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:02, 14.76w/s, dev=1]model.layers.24.self_attn.k_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:02, 14.76w/s, dev=1]        model.layers.24.self_attn.o_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:02, 15.47w/s, dev=1]model.layers.24.self_attn.q_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:02, 15.67w/s, dev=1]model.layers.24.self_attn.rotary_emb.inv_freq:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:01<00:02, 15.85w/s, dev=1]model.layers.24.self_attn.v_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:02, 16.60w/s, dev=1]      model.layers.24.self_attn.v_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:02, 17.27w/s, dev=1]model.layers.25.input_layernorm.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:02, 17.27w/s, dev=1] model.layers.25.mlp.down_proj.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:01, 18.02w/s, dev=1]  model.layers.25.mlp.gate_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:02, 16.71w/s, dev=1]model.layers.25.mlp.up_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:02, 15.68w/s, dev=1]  model.layers.25.post_attention_layernorm.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:01<00:02, 14.87w/s, dev=1]model.layers.25.post_attention_layernorm.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:02, 15.42w/s, dev=1]model.layers.25.self_attn.k_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:02, 15.42w/s, dev=1]        model.layers.25.self_attn.o_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:01, 15.92w/s, dev=1]model.layers.25.self_attn.q_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:01<00:01, 16.10w/s, dev=1]model.layers.25.self_attn.rotary_emb.inv_freq:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:01<00:01, 16.25w/s, dev=1]model.layers.25.self_attn.v_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:01<00:01, 16.77w/s, dev=1]      model.layers.26.input_layernorm.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:01<00:01, 17.23w/s, dev=1] model.layers.26.mlp.down_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:01<00:01, 17.75w/s, dev=1]  model.layers.26.mlp.down_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:02<00:01, 16.88w/s, dev=1]model.layers.26.mlp.gate_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:02<00:01, 16.87w/s, dev=1]model.layers.26.mlp.up_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:02<00:01, 16.13w/s, dev=1]  model.layers.26.post_attention_layernorm.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:02<00:01, 15.55w/s, dev=1]model.layers.26.self_attn.k_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:02<00:01, 15.96w/s, dev=1]        model.layers.26.self_attn.o_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:02<00:01, 16.34w/s, dev=1]model.layers.26.self_attn.q_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:02<00:01, 16.44w/s, dev=1]model.layers.26.self_attn.rotary_emb.inv_freq:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:02<00:01, 16.53w/s, dev=1]model.layers.26.self_attn.rotary_emb.inv_freq:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:02<00:01, 16.93w/s, dev=1]model.layers.26.self_attn.v_proj.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:02<00:01, 16.92w/s, dev=1]      model.layers.27.input_layernorm.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:02<00:00, 17.28w/s, dev=1] model.layers.27.mlp.down_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:02<00:00, 17.68w/s, dev=1]  model.layers.27.mlp.gate_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:02<00:00, 17.02w/s, dev=1]model.layers.27.mlp.up_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:02<00:00, 16.35w/s, dev=1]  model.layers.27.post_attention_layernorm.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:02<00:00, 15.80w/s, dev=1]model.layers.27.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:02<00:00, 16.14w/s, dev=1]        model.layers.27.self_attn.k_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:02<00:00, 16.44w/s, dev=1]model.layers.27.self_attn.o_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:02<00:00, 16.44w/s, dev=1]model.layers.27.self_attn.q_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:03<00:00, 16.53w/s, dev=1]model.layers.27.self_attn.rotary_emb.inv_freq:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:03<00:00, 16.62w/s, dev=1]model.layers.27.self_attn.v_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:03<00:00, 16.94w/s, dev=1]      model.layers.28.mlp.gate_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:03<00:00, 17.22w/s, dev=1]   model.layers.28.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:03<00:00, 16.66w/s, dev=1]model.layers.28.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:03<00:00, 16.93w/s, dev=1]model.layers.28.self_attn.o_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:03<00:00, 17.00w/s, dev=1]model.layers.28.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:03<00:00, 16.99w/s, dev=1]model.layers.28.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:03<00:00, 17.05w/s, dev=1]model.layers.28.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:03<00:00, 17.35w/s, dev=1]                                                                                                      0%|          | 0/59 [00:00<?, ?w/s]model.layers.28.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=1]model.layers.28.mlp.down_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 1239.45w/s, dev=1]model.layers.28.mlp.down_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 12.45w/s, dev=1]  model.layers.28.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 12.43w/s, dev=1]  model.layers.28.post_attention_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:05,  9.63w/s, dev=1]model.layers.28.post_attention_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:04, 12.82w/s, dev=1]model.layers.29.input_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:04, 12.81w/s, dev=1]         model.layers.29.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 16.00w/s, dev=1]  model.layers.29.mlp.down_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:04, 12.55w/s, dev=1]model.layers.29.mlp.gate_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:04, 12.54w/s, dev=1]model.layers.29.mlp.up_proj.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:04, 10.87w/s, dev=1]  model.layers.29.mlp.up_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:05, 10.01w/s, dev=1]model.layers.29.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:05, 10.01w/s, dev=1]model.layers.29.self_attn.k_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:04, 11.25w/s, dev=1]        model.layers.29.self_attn.o_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:03, 12.41w/s, dev=1]model.layers.29.self_attn.q_proj.weight:  19%|â–ˆâ–Š        | 11/59 [00:00<00:03, 12.87w/s, dev=1]model.layers.29.self_attn.q_proj.weight:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:03, 13.33w/s, dev=1]model.layers.29.self_attn.rotary_emb.inv_freq:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:03, 13.33w/s, dev=1]model.layers.29.self_attn.v_proj.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:03, 14.43w/s, dev=1]      model.layers.30.input_layernorm.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:02, 15.43w/s, dev=1] model.layers.30.mlp.down_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:02, 16.52w/s, dev=1]  model.layers.30.mlp.down_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:02, 14.90w/s, dev=1]model.layers.30.mlp.gate_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:02, 14.90w/s, dev=1]model.layers.30.mlp.up_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:01<00:03, 13.83w/s, dev=1]  model.layers.30.post_attention_layernorm.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:03, 13.04w/s, dev=1]model.layers.30.self_attn.k_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:02, 13.76w/s, dev=1]        model.layers.30.self_attn.k_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:02, 14.42w/s, dev=1]model.layers.30.self_attn.o_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:02, 14.42w/s, dev=1]model.layers.30.self_attn.q_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:01<00:02, 14.69w/s, dev=1]model.layers.30.self_attn.rotary_emb.inv_freq:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:02, 14.96w/s, dev=1]model.layers.30.self_attn.v_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:02, 15.64w/s, dev=1]      model.layers.31.input_layernorm.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:02, 16.24w/s, dev=1] model.layers.31.mlp.down_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:02, 16.91w/s, dev=1]  model.layers.31.mlp.down_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:02, 15.90w/s, dev=1]model.layers.31.mlp.gate_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:02, 15.89w/s, dev=1]model.layers.31.mlp.up_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:01<00:02, 15.11w/s, dev=1]  model.layers.31.post_attention_layernorm.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:02, 14.39w/s, dev=1]model.layers.31.self_attn.k_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:02, 14.90w/s, dev=1]        model.layers.31.self_attn.o_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:01<00:01, 15.37w/s, dev=1]model.layers.31.self_attn.q_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:01<00:01, 15.56w/s, dev=1]model.layers.31.self_attn.q_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:02<00:01, 15.74w/s, dev=1]model.layers.31.self_attn.rotary_emb.inv_freq:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:02<00:01, 15.74w/s, dev=1]model.layers.31.self_attn.v_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:02<00:01, 16.23w/s, dev=1]      model.layers.32.input_layernorm.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:02<00:01, 16.67w/s, dev=1] model.layers.32.mlp.down_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:02<00:01, 17.16w/s, dev=1]  model.layers.32.mlp.gate_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:02<00:01, 16.36w/s, dev=1]model.layers.32.mlp.up_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:02<00:01, 15.80w/s, dev=1]  model.layers.32.mlp.up_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:02<00:01, 15.25w/s, dev=1]model.layers.32.post_attention_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:02<00:01, 15.25w/s, dev=1]model.layers.32.self_attn.k_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:02<00:01, 15.65w/s, dev=1]        model.layers.32.self_attn.o_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:02<00:01, 16.01w/s, dev=1]model.layers.32.self_attn.q_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:02<00:01, 16.11w/s, dev=1]model.layers.32.self_attn.rotary_emb.inv_freq:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:02<00:01, 16.23w/s, dev=1]model.layers.32.self_attn.v_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:02<00:00, 16.61w/s, dev=1]      model.layers.32.self_attn.v_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:02<00:00, 16.96w/s, dev=1]model.layers.33.input_layernorm.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:02<00:00, 16.96w/s, dev=1] model.layers.33.mlp.down_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:02<00:00, 17.35w/s, dev=1]  model.layers.33.mlp.gate_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:02<00:00, 16.77w/s, dev=1]model.layers.33.mlp.up_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:02<00:00, 16.27w/s, dev=1]  model.layers.33.post_attention_layernorm.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:03<00:00, 15.79w/s, dev=1]model.layers.33.self_attn.k_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:03<00:00, 16.12w/s, dev=1]        model.layers.33.self_attn.k_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:03<00:00, 16.42w/s, dev=1]model.layers.33.self_attn.o_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:03<00:00, 16.41w/s, dev=1]model.layers.33.self_attn.q_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:03<00:00, 16.52w/s, dev=1]model.layers.33.self_attn.rotary_emb.inv_freq:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:03<00:00, 16.62w/s, dev=1]model.layers.33.self_attn.v_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:03<00:00, 16.94w/s, dev=1]      model.layers.34.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:03<00:00, 17.23w/s, dev=1]model.layers.34.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:03<00:00, 17.51w/s, dev=1]model.layers.34.self_attn.o_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:03<00:00, 17.57w/s, dev=1]model.layers.34.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:03<00:00, 17.57w/s, dev=1]model.layers.34.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:03<00:00, 17.62w/s, dev=1]model.layers.34.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:03<00:00, 17.92w/s, dev=1]                                                                                                      0%|          | 0/55 [00:00<?, ?w/s]model.layers.34.input_layernorm.weight:   0%|          | 0/55 [00:00<?, ?w/s, dev=1]model.layers.34.mlp.down_proj.weight:   2%|â–         | 1/55 [00:00<00:00, 1263.34w/s, dev=1]model.layers.34.mlp.down_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:03, 13.59w/s, dev=1]  model.layers.34.mlp.gate_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:03, 13.56w/s, dev=1]model.layers.34.mlp.up_proj.weight:   5%|â–Œ         | 3/55 [00:00<00:05,  9.86w/s, dev=1]  model.layers.34.mlp.up_proj.weight:   7%|â–‹         | 4/55 [00:00<00:05,  8.58w/s, dev=1]model.layers.34.post_attention_layernorm.weight:   7%|â–‹         | 4/55 [00:00<00:05,  8.57w/s, dev=1]model.layers.35.input_layernorm.weight:   9%|â–‰         | 5/55 [00:00<00:04, 10.71w/s, dev=1]         model.layers.35.mlp.down_proj.weight:  11%|â–ˆ         | 6/55 [00:00<00:03, 12.84w/s, dev=1]  model.layers.35.mlp.down_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:04, 11.09w/s, dev=1]model.layers.35.mlp.gate_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:04, 11.08w/s, dev=1]model.layers.35.mlp.up_proj.weight:  15%|â–ˆâ–        | 8/55 [00:00<00:04, 10.11w/s, dev=1]  model.layers.35.post_attention_layernorm.weight:  16%|â–ˆâ–‹        | 9/55 [00:00<00:04,  9.81w/s, dev=1]model.layers.35.post_attention_layernorm.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:04, 10.89w/s, dev=1]model.layers.35.self_attn.k_proj.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:04, 10.89w/s, dev=1]        model.layers.35.self_attn.o_proj.weight:  20%|â–ˆâ–ˆ        | 11/55 [00:00<00:03, 11.89w/s, dev=1]model.layers.35.self_attn.q_proj.weight:  22%|â–ˆâ–ˆâ–       | 12/55 [00:00<00:03, 12.46w/s, dev=1]model.layers.35.self_attn.rotary_emb.inv_freq:  24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:00<00:03, 13.02w/s, dev=1]model.layers.35.self_attn.v_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 14/55 [00:00<00:02, 14.02w/s, dev=1]      model.layers.36.input_layernorm.weight:  27%|â–ˆâ–ˆâ–‹       | 15/55 [00:01<00:02, 14.94w/s, dev=1] model.layers.36.mlp.down_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 16/55 [00:01<00:02, 15.93w/s, dev=1]  model.layers.36.mlp.down_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:01<00:02, 15.20w/s, dev=1]model.layers.36.mlp.gate_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:01<00:02, 15.20w/s, dev=1]model.layers.36.mlp.up_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/55 [00:01<00:02, 14.67w/s, dev=1]  model.layers.36.post_attention_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–      | 19/55 [00:01<00:02, 14.18w/s, dev=1]model.layers.36.self_attn.k_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:01<00:02, 14.92w/s, dev=1]        model.layers.36.self_attn.o_proj.weight:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/55 [00:01<00:02, 15.60w/s, dev=1]model.layers.36.self_attn.q_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/55 [00:01<00:02, 15.97w/s, dev=1]model.layers.36.self_attn.rotary_emb.inv_freq:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 23/55 [00:01<00:01, 16.23w/s, dev=1]model.layers.36.self_attn.rotary_emb.inv_freq:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 24/55 [00:01<00:01, 16.93w/s, dev=1]model.layers.36.self_attn.v_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 24/55 [00:01<00:01, 16.93w/s, dev=1]      model.layers.37.input_layernorm.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:01<00:01, 17.53w/s, dev=1] model.layers.37.mlp.down_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 26/55 [00:01<00:01, 18.23w/s, dev=1]  model.layers.37.mlp.gate_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 27/55 [00:01<00:01, 17.40w/s, dev=1]model.layers.37.mlp.up_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 28/55 [00:01<00:01, 16.95w/s, dev=1]  model.layers.37.post_attention_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/55 [00:01<00:01, 16.36w/s, dev=1]model.layers.37.self_attn.k_proj.weight:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 30/55 [00:01<00:01, 16.92w/s, dev=1]        model.layers.37.self_attn.k_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/55 [00:01<00:01, 17.43w/s, dev=1]model.layers.37.self_attn.o_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/55 [00:01<00:01, 17.43w/s, dev=1]model.layers.37.self_attn.q_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 32/55 [00:01<00:01, 17.65w/s, dev=1]model.layers.37.self_attn.rotary_emb.inv_freq:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/55 [00:01<00:01, 17.86w/s, dev=1]model.layers.37.self_attn.v_proj.weight:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/55 [00:01<00:01, 18.40w/s, dev=1]      model.layers.38.input_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:01<00:01, 18.87w/s, dev=1] model.layers.38.mlp.down_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 36/55 [00:01<00:00, 19.41w/s, dev=1]  model.layers.38.mlp.gate_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 37/55 [00:01<00:00, 18.71w/s, dev=1]model.layers.38.mlp.gate_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 38/55 [00:02<00:00, 18.08w/s, dev=1]model.layers.38.mlp.up_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 38/55 [00:02<00:00, 18.08w/s, dev=1]  model.layers.38.post_attention_layernorm.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 39/55 [00:02<00:00, 17.56w/s, dev=1]model.layers.38.self_attn.k_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 40/55 [00:02<00:00, 18.00w/s, dev=1]        model.layers.38.self_attn.o_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/55 [00:02<00:00, 18.41w/s, dev=1]model.layers.38.self_attn.q_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 42/55 [00:02<00:00, 18.55w/s, dev=1]model.layers.38.self_attn.rotary_emb.inv_freq:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 43/55 [00:02<00:00, 18.66w/s, dev=1]model.layers.38.self_attn.v_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 44/55 [00:02<00:00, 19.09w/s, dev=1]      model.layers.38.self_attn.v_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:02<00:00, 19.47w/s, dev=1]model.layers.39.input_layernorm.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:02<00:00, 19.47w/s, dev=1] model.layers.39.mlp.down_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 46/55 [00:02<00:00, 19.90w/s, dev=1]  model.layers.39.mlp.gate_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 47/55 [00:02<00:00, 19.29w/s, dev=1]model.layers.39.mlp.up_proj.weight:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 48/55 [00:02<00:00, 18.74w/s, dev=1]  model.layers.39.post_attention_layernorm.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 49/55 [00:02<00:00, 18.39w/s, dev=1]model.layers.39.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 50/55 [00:02<00:00, 18.76w/s, dev=1]        model.layers.39.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 51/55 [00:02<00:00, 19.11w/s, dev=1]model.layers.39.self_attn.o_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 52/55 [00:02<00:00, 19.24w/s, dev=1]model.layers.39.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 52/55 [00:02<00:00, 19.24w/s, dev=1]model.layers.39.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 53/55 [00:02<00:00, 19.36w/s, dev=1]model.layers.39.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 54/55 [00:02<00:00, 19.72w/s, dev=1]                                                                                                      0%|          | 0/57 [00:00<?, ?w/s]model.layers.40.input_layernorm.weight:   0%|          | 0/57 [00:00<?, ?w/s, dev=1]model.layers.40.mlp.down_proj.weight:   2%|â–         | 1/57 [00:00<00:00, 1186.51w/s, dev=1]model.layers.40.mlp.down_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:03, 16.50w/s, dev=1]  model.layers.40.mlp.gate_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:03, 16.46w/s, dev=1]model.layers.40.mlp.up_proj.weight:   5%|â–Œ         | 3/57 [00:00<00:04, 12.15w/s, dev=1]  model.layers.40.mlp.up_proj.weight:   7%|â–‹         | 4/57 [00:00<00:04, 10.73w/s, dev=1]model.layers.40.post_attention_layernorm.weight:   7%|â–‹         | 4/57 [00:00<00:04, 10.72w/s, dev=1]model.layers.40.self_attn.k_proj.weight:   9%|â–‰         | 5/57 [00:00<00:03, 13.39w/s, dev=1]        model.layers.40.self_attn.o_proj.weight:  11%|â–ˆ         | 6/57 [00:00<00:03, 15.77w/s, dev=1]model.layers.40.self_attn.q_proj.weight:  12%|â–ˆâ–        | 7/57 [00:00<00:02, 16.72w/s, dev=1]model.layers.40.self_attn.rotary_emb.inv_freq:  14%|â–ˆâ–        | 8/57 [00:00<00:02, 17.57w/s, dev=1]model.layers.40.self_attn.v_proj.weight:  16%|â–ˆâ–Œ        | 9/57 [00:00<00:02, 19.75w/s, dev=1]      model.layers.41.input_layernorm.weight:  18%|â–ˆâ–Š        | 10/57 [00:00<00:02, 18.31w/s, dev=2]model.layers.41.input_layernorm.weight:  19%|â–ˆâ–‰        | 11/57 [00:00<00:02, 20.10w/s, dev=2]model.layers.41.mlp.down_proj.weight:  19%|â–ˆâ–‰        | 11/57 [00:00<00:02, 20.10w/s, dev=2]  model.layers.41.mlp.gate_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:00<00:02, 18.07w/s, dev=2]model.layers.41.mlp.up_proj.weight:  23%|â–ˆâ–ˆâ–Ž       | 13/57 [00:00<00:02, 16.61w/s, dev=2]  model.layers.41.post_attention_layernorm.weight:  25%|â–ˆâ–ˆâ–       | 14/57 [00:00<00:02, 15.48w/s, dev=2]model.layers.41.self_attn.k_proj.weight:  26%|â–ˆâ–ˆâ–‹       | 15/57 [00:00<00:02, 16.57w/s, dev=2]        model.layers.41.self_attn.o_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:00<00:02, 17.58w/s, dev=2]model.layers.41.self_attn.q_proj.weight:  30%|â–ˆâ–ˆâ–‰       | 17/57 [00:00<00:02, 18.06w/s, dev=2]model.layers.41.self_attn.q_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 18/57 [00:00<00:02, 18.54w/s, dev=2]model.layers.41.self_attn.rotary_emb.inv_freq:  32%|â–ˆâ–ˆâ–ˆâ–      | 18/57 [00:00<00:02, 18.54w/s, dev=2]model.layers.41.self_attn.v_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 19/57 [00:00<00:01, 19.56w/s, dev=2]      model.layers.42.input_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 20/57 [00:00<00:01, 20.50w/s, dev=2] model.layers.42.mlp.down_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 21/57 [00:00<00:01, 21.52w/s, dev=2]  model.layers.42.mlp.gate_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:01<00:01, 20.35w/s, dev=2]model.layers.42.mlp.up_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 23/57 [00:01<00:01, 19.24w/s, dev=2]  model.layers.42.post_attention_layernorm.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/57 [00:01<00:01, 18.29w/s, dev=2]model.layers.42.post_attention_layernorm.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/57 [00:01<00:01, 19.05w/s, dev=2]model.layers.42.self_attn.k_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/57 [00:01<00:01, 19.05w/s, dev=2]        model.layers.42.self_attn.o_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 26/57 [00:01<00:01, 19.70w/s, dev=2]model.layers.42.self_attn.q_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 27/57 [00:01<00:01, 19.94w/s, dev=2]model.layers.42.self_attn.rotary_emb.inv_freq:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:01<00:01, 20.21w/s, dev=2]model.layers.42.self_attn.v_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 29/57 [00:01<00:01, 20.93w/s, dev=2]      model.layers.43.input_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 30/57 [00:01<00:01, 21.55w/s, dev=2] model.layers.43.mlp.down_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 31/57 [00:01<00:01, 22.27w/s, dev=2]  model.layers.43.mlp.down_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 32/57 [00:01<00:01, 21.58w/s, dev=2]model.layers.43.mlp.gate_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 32/57 [00:01<00:01, 21.58w/s, dev=2]model.layers.43.mlp.up_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 33/57 [00:01<00:01, 20.58w/s, dev=2]  model.layers.43.post_attention_layernorm.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:01<00:01, 19.62w/s, dev=2]model.layers.43.self_attn.k_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/57 [00:01<00:01, 20.19w/s, dev=2]        model.layers.43.self_attn.o_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 36/57 [00:01<00:01, 20.69w/s, dev=2]model.layers.43.self_attn.q_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 37/57 [00:01<00:00, 20.80w/s, dev=2]model.layers.43.self_attn.rotary_emb.inv_freq:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 38/57 [00:01<00:00, 20.90w/s, dev=2]model.layers.43.self_attn.rotary_emb.inv_freq:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 39/57 [00:01<00:00, 21.44w/s, dev=2]model.layers.43.self_attn.v_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 39/57 [00:01<00:00, 21.44w/s, dev=2]      model.layers.44.input_layernorm.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:01<00:00, 21.90w/s, dev=2] model.layers.44.mlp.down_proj.weight:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/57 [00:01<00:00, 22.44w/s, dev=2]  model.layers.44.mlp.gate_proj.weight:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 42/57 [00:01<00:00, 21.47w/s, dev=2]model.layers.44.mlp.up_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 43/57 [00:02<00:00, 20.73w/s, dev=2]  model.layers.44.post_attention_layernorm.weight:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 44/57 [00:02<00:00, 20.19w/s, dev=2]model.layers.44.self_attn.k_proj.weight:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 45/57 [00:02<00:00, 20.65w/s, dev=2]        model.layers.44.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:02<00:00, 21.06w/s, dev=2]model.layers.44.self_attn.o_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:02<00:00, 21.06w/s, dev=2]model.layers.44.self_attn.q_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 47/57 [00:02<00:00, 21.18w/s, dev=2]model.layers.44.self_attn.rotary_emb.inv_freq:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/57 [00:02<00:00, 21.31w/s, dev=2]model.layers.44.self_attn.v_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 49/57 [00:02<00:00, 21.75w/s, dev=2]      model.layers.45.mlp.down_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 50/57 [00:02<00:00, 22.14w/s, dev=2]   model.layers.45.mlp.gate_proj.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 51/57 [00:02<00:00, 21.40w/s, dev=2]model.layers.45.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:02<00:00, 20.75w/s, dev=2]model.layers.45.self_attn.k_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 53/57 [00:02<00:00, 21.11w/s, dev=2]model.layers.45.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 53/57 [00:02<00:00, 21.10w/s, dev=2]model.layers.45.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/57 [00:02<00:00, 21.19w/s, dev=2]model.layers.45.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 55/57 [00:02<00:00, 21.29w/s, dev=2]model.layers.45.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 56/57 [00:02<00:00, 21.67w/s, dev=2]                                                                                                      0%|          | 0/59 [00:00<?, ?w/s]model.layers.45.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=2]model.layers.45.mlp.up_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 1016.31w/s, dev=2]model.layers.45.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:03, 17.74w/s, dev=2]  model.layers.45.post_attention_layernorm.weight:   3%|â–Ž         | 2/59 [00:00<00:03, 17.70w/s, dev=2]model.layers.46.input_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:02, 26.47w/s, dev=2]         model.layers.46.mlp.down_proj.weight:   7%|â–‹         | 4/59 [00:00<00:01, 35.21w/s, dev=2]  model.layers.46.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:02, 21.84w/s, dev=2]model.layers.46.mlp.gate_proj.weight:   8%|â–Š         | 5/59 [00:00<00:02, 21.82w/s, dev=2]model.layers.46.mlp.up_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:03, 17.52w/s, dev=2]  model.layers.46.post_attention_layernorm.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:03, 15.55w/s, dev=2]model.layers.46.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:02, 17.76w/s, dev=2]model.layers.46.self_attn.k_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:02, 17.75w/s, dev=2]        model.layers.46.self_attn.o_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:02, 19.75w/s, dev=2]model.layers.46.self_attn.q_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:02, 20.45w/s, dev=2]model.layers.46.self_attn.rotary_emb.inv_freq:  19%|â–ˆâ–Š        | 11/59 [00:00<00:02, 21.04w/s, dev=2]model.layers.46.self_attn.v_proj.weight:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:02, 22.93w/s, dev=2]      model.layers.47.input_layernorm.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:01, 24.51w/s, dev=2] model.layers.47.mlp.down_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:01, 26.37w/s, dev=2]  model.layers.47.mlp.down_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:01, 23.16w/s, dev=2]model.layers.47.mlp.gate_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:01, 23.15w/s, dev=2]model.layers.47.mlp.up_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:00<00:02, 20.98w/s, dev=2]  model.layers.47.post_attention_layernorm.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:00<00:02, 19.39w/s, dev=2]model.layers.47.self_attn.k_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:00<00:01, 20.51w/s, dev=2]        model.layers.47.self_attn.o_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:00<00:01, 21.53w/s, dev=2]model.layers.47.self_attn.q_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:00<00:01, 21.84w/s, dev=2]model.layers.47.self_attn.rotary_emb.inv_freq:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:00<00:01, 22.12w/s, dev=2]model.layers.47.self_attn.rotary_emb.inv_freq:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:00<00:01, 23.16w/s, dev=2]model.layers.47.self_attn.v_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:00<00:01, 23.16w/s, dev=2]      model.layers.48.input_layernorm.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:00<00:01, 24.05w/s, dev=2] model.layers.48.mlp.down_proj.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:00<00:01, 25.09w/s, dev=2]  model.layers.48.mlp.gate_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:01, 23.49w/s, dev=2]model.layers.48.mlp.up_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:01, 22.52w/s, dev=2]  model.layers.48.post_attention_layernorm.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:01<00:01, 21.27w/s, dev=2]model.layers.48.self_attn.k_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:01, 22.05w/s, dev=2]        model.layers.48.self_attn.k_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:01, 22.74w/s, dev=2]model.layers.48.self_attn.o_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:01, 22.73w/s, dev=2]model.layers.48.self_attn.q_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:01<00:01, 22.89w/s, dev=2]model.layers.48.self_attn.rotary_emb.inv_freq:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:01<00:01, 23.05w/s, dev=2]model.layers.48.self_attn.v_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:01<00:01, 23.78w/s, dev=2]      model.layers.49.input_layernorm.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:01<00:01, 24.40w/s, dev=2] model.layers.49.mlp.down_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:01<00:00, 25.13w/s, dev=2]  model.layers.49.mlp.gate_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:01<00:01, 23.83w/s, dev=2]model.layers.49.mlp.gate_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:01<00:01, 22.76w/s, dev=2]model.layers.49.mlp.up_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:01<00:01, 22.76w/s, dev=2]  model.layers.49.post_attention_layernorm.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:01<00:01, 21.75w/s, dev=2]model.layers.49.self_attn.k_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:01<00:00, 22.33w/s, dev=2]        model.layers.49.self_attn.o_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:01<00:00, 22.86w/s, dev=2]model.layers.49.self_attn.q_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:01<00:00, 22.99w/s, dev=2]model.layers.49.self_attn.rotary_emb.inv_freq:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:01<00:00, 23.12w/s, dev=2]model.layers.49.self_attn.v_proj.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:01<00:00, 23.68w/s, dev=2]      model.layers.49.self_attn.v_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:01<00:00, 24.16w/s, dev=2]model.layers.50.input_layernorm.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:01<00:00, 24.16w/s, dev=2] model.layers.50.mlp.down_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:01<00:00, 24.71w/s, dev=2]  model.layers.50.mlp.gate_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:01<00:00, 23.71w/s, dev=2]model.layers.50.mlp.up_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:02<00:00, 22.77w/s, dev=2]  model.layers.50.post_attention_layernorm.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:02<00:00, 22.20w/s, dev=2]model.layers.50.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:02<00:00, 22.66w/s, dev=2]        model.layers.50.self_attn.o_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:02<00:00, 21.66w/s, dev=2]model.layers.50.self_attn.o_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:02<00:00, 21.76w/s, dev=2]model.layers.50.self_attn.q_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:02<00:00, 21.76w/s, dev=2]model.layers.50.self_attn.rotary_emb.inv_freq:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:02<00:00, 21.86w/s, dev=2]model.layers.50.self_attn.v_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:02<00:00, 22.29w/s, dev=2]      model.layers.51.mlp.gate_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:02<00:00, 21.08w/s, dev=2]   model.layers.51.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:02<00:00, 20.45w/s, dev=2]model.layers.51.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:02<00:00, 20.03w/s, dev=2]model.layers.51.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:02<00:00, 20.12w/s, dev=2]model.layers.51.self_attn.q_proj.weight:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:02<00:00, 20.18w/s, dev=2]model.layers.51.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:02<00:00, 20.17w/s, dev=2]model.layers.51.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:02<00:00, 20.53w/s, dev=2]                                                                                                      0%|          | 0/59 [00:00<?, ?w/s]model.layers.51.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=2]model.layers.51.mlp.down_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 979.52w/s, dev=2]model.layers.51.mlp.down_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:03, 14.93w/s, dev=2] model.layers.51.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:03, 14.90w/s, dev=2]  model.layers.51.post_attention_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:06,  8.24w/s, dev=2]model.layers.51.post_attention_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:05, 10.97w/s, dev=2]model.layers.52.input_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:05, 10.97w/s, dev=2]         model.layers.52.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 13.70w/s, dev=2]  model.layers.52.mlp.down_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:06,  8.29w/s, dev=2]model.layers.52.mlp.gate_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:06,  8.29w/s, dev=2]model.layers.52.mlp.up_proj.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:06,  8.27w/s, dev=2]  model.layers.52.mlp.up_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:06,  8.34w/s, dev=2]model.layers.52.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:06,  8.34w/s, dev=2]model.layers.52.self_attn.k_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:05,  9.38w/s, dev=2]        model.layers.52.self_attn.o_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:04, 10.36w/s, dev=2]model.layers.52.self_attn.q_proj.weight:  19%|â–ˆâ–Š        | 11/59 [00:00<00:04, 11.04w/s, dev=2]model.layers.52.self_attn.q_proj.weight:  20%|â–ˆâ–ˆ        | 12/59 [00:01<00:04, 10.05w/s, dev=2]model.layers.52.self_attn.rotary_emb.inv_freq:  20%|â–ˆâ–ˆ        | 12/59 [00:01<00:04, 10.05w/s, dev=2]model.layers.52.self_attn.v_proj.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:01<00:04, 10.88w/s, dev=2]      model.layers.53.input_layernorm.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:01<00:03, 11.66w/s, dev=2] model.layers.53.mlp.down_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:01<00:03, 12.49w/s, dev=2]  model.layers.53.mlp.down_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:03, 12.17w/s, dev=2]model.layers.53.mlp.gate_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:03, 12.16w/s, dev=2]model.layers.53.mlp.up_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:01<00:03, 12.02w/s, dev=2]  model.layers.53.post_attention_layernorm.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:03, 10.60w/s, dev=2]model.layers.53.self_attn.k_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:03, 11.19w/s, dev=2]        model.layers.53.self_attn.k_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:03, 11.74w/s, dev=2]model.layers.53.self_attn.o_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:03, 11.73w/s, dev=2]model.layers.53.self_attn.q_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:01<00:03, 12.08w/s, dev=2]model.layers.53.self_attn.rotary_emb.inv_freq:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:02, 12.40w/s, dev=2]model.layers.53.self_attn.v_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:02, 12.96w/s, dev=2]      model.layers.54.input_layernorm.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:02, 13.47w/s, dev=2] model.layers.54.mlp.down_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:02, 14.03w/s, dev=2]  model.layers.54.mlp.down_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:02, 13.69w/s, dev=2]model.layers.54.mlp.gate_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:02, 13.69w/s, dev=2]model.layers.54.mlp.up_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:02<00:02, 12.70w/s, dev=2]  model.layers.54.post_attention_layernorm.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:02<00:02, 12.46w/s, dev=2]model.layers.54.self_attn.k_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:02<00:02, 12.91w/s, dev=2]        model.layers.54.self_attn.o_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:02<00:02, 13.32w/s, dev=2]model.layers.54.self_attn.q_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:02<00:02, 13.55w/s, dev=2]model.layers.54.self_attn.q_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:02<00:02, 12.80w/s, dev=2]model.layers.54.self_attn.rotary_emb.inv_freq:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:02<00:02, 12.80w/s, dev=2]model.layers.54.self_attn.v_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:02<00:01, 13.20w/s, dev=2]      model.layers.55.input_layernorm.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:02<00:01, 13.54w/s, dev=2] model.layers.55.mlp.down_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:02<00:01, 13.93w/s, dev=2]  model.layers.55.mlp.gate_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:02<00:01, 13.62w/s, dev=2]model.layers.55.mlp.up_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:02<00:01, 13.42w/s, dev=2]  model.layers.55.mlp.up_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:03<00:01, 12.48w/s, dev=2]model.layers.55.post_attention_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:03<00:01, 12.48w/s, dev=2]model.layers.55.self_attn.k_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:03<00:01, 12.81w/s, dev=2]        model.layers.55.self_attn.o_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:03<00:01, 13.08w/s, dev=2]model.layers.55.self_attn.q_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:03<00:01, 13.26w/s, dev=2]model.layers.55.self_attn.rotary_emb.inv_freq:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:03<00:01, 13.40w/s, dev=2]model.layers.55.self_attn.v_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:03<00:01, 13.72w/s, dev=2]      model.layers.55.self_attn.v_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:03<00:01, 13.96w/s, dev=2]model.layers.56.input_layernorm.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:03<00:01, 13.95w/s, dev=2] model.layers.56.mlp.down_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:03<00:00, 14.27w/s, dev=2]  model.layers.56.mlp.gate_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:03<00:00, 14.07w/s, dev=2]model.layers.56.mlp.up_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:03<00:00, 13.13w/s, dev=2]  model.layers.56.post_attention_layernorm.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:03<00:00, 13.00w/s, dev=2]model.layers.56.self_attn.k_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:03<00:00, 13.27w/s, dev=2]        model.layers.56.self_attn.k_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:03<00:00, 13.50w/s, dev=2]model.layers.56.self_attn.o_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:03<00:00, 13.50w/s, dev=2]model.layers.56.self_attn.q_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:03<00:00, 13.65w/s, dev=2]model.layers.56.self_attn.rotary_emb.inv_freq:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:03<00:00, 13.79w/s, dev=2]model.layers.56.self_attn.v_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:03<00:00, 14.06w/s, dev=2]      model.layers.57.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:03<00:00, 14.25w/s, dev=2]model.layers.57.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:03<00:00, 14.49w/s, dev=2]model.layers.57.self_attn.o_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:04<00:00, 13.74w/s, dev=2]model.layers.57.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:04<00:00, 13.74w/s, dev=2]model.layers.57.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:04<00:00, 13.35w/s, dev=2]model.layers.57.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:04<00:00, 13.58w/s, dev=2]                                                                                                      0%|          | 0/55 [00:00<?, ?w/s]model.layers.57.input_layernorm.weight:   0%|          | 0/55 [00:00<?, ?w/s, dev=2]model.layers.57.mlp.down_proj.weight:   2%|â–         | 1/55 [00:00<00:00, 1081.56w/s, dev=2]model.layers.57.mlp.down_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:03, 17.48w/s, dev=2]  model.layers.57.mlp.gate_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:03, 17.44w/s, dev=2]model.layers.57.mlp.up_proj.weight:   5%|â–Œ         | 3/55 [00:00<00:03, 13.38w/s, dev=2]  model.layers.57.mlp.up_proj.weight:   7%|â–‹         | 4/55 [00:00<00:05,  9.06w/s, dev=2]model.layers.57.post_attention_layernorm.weight:   7%|â–‹         | 4/55 [00:00<00:05,  9.06w/s, dev=2]model.layers.58.input_layernorm.weight:   9%|â–‰         | 5/55 [00:00<00:04, 11.31w/s, dev=2]         model.layers.58.mlp.down_proj.weight:  11%|â–ˆ         | 6/55 [00:00<00:03, 13.56w/s, dev=2]  model.layers.58.mlp.down_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:03, 12.55w/s, dev=2]model.layers.58.mlp.gate_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:03, 12.54w/s, dev=2]model.layers.58.mlp.up_proj.weight:  15%|â–ˆâ–        | 8/55 [00:00<00:03, 11.98w/s, dev=2]  model.layers.58.post_attention_layernorm.weight:  16%|â–ˆâ–‹        | 9/55 [00:00<00:04, 10.09w/s, dev=2]model.layers.58.post_attention_layernorm.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:04, 11.21w/s, dev=2]model.layers.58.self_attn.k_proj.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:04, 11.21w/s, dev=2]        model.layers.58.self_attn.o_proj.weight:  20%|â–ˆâ–ˆ        | 11/55 [00:00<00:03, 12.23w/s, dev=2]model.layers.58.self_attn.q_proj.weight:  22%|â–ˆâ–ˆâ–       | 12/55 [00:00<00:03, 12.86w/s, dev=2]model.layers.58.self_attn.rotary_emb.inv_freq:  24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:00<00:03, 13.43w/s, dev=2]model.layers.58.self_attn.v_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 14/55 [00:00<00:02, 14.45w/s, dev=2]      model.layers.59.input_layernorm.weight:  27%|â–ˆâ–ˆâ–‹       | 15/55 [00:00<00:02, 15.39w/s, dev=2] model.layers.59.mlp.down_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 16/55 [00:00<00:02, 16.41w/s, dev=2]  model.layers.59.mlp.down_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:01<00:02, 15.56w/s, dev=2]model.layers.59.mlp.gate_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:01<00:02, 15.55w/s, dev=2]model.layers.59.mlp.up_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/55 [00:01<00:02, 14.79w/s, dev=2]  model.layers.59.post_attention_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–      | 19/55 [00:01<00:02, 13.61w/s, dev=2]model.layers.59.self_attn.k_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:01<00:02, 14.32w/s, dev=2]        model.layers.59.self_attn.o_proj.weight:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/55 [00:01<00:02, 14.97w/s, dev=2]model.layers.59.self_attn.q_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/55 [00:01<00:02, 15.39w/s, dev=2]model.layers.59.self_attn.rotary_emb.inv_freq:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 23/55 [00:01<00:02, 15.79w/s, dev=2]model.layers.59.self_attn.rotary_emb.inv_freq:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 24/55 [00:01<00:01, 16.47w/s, dev=2]model.layers.59.self_attn.v_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 24/55 [00:01<00:01, 16.47w/s, dev=2]      model.layers.60.input_layernorm.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:01<00:01, 17.09w/s, dev=2] model.layers.60.mlp.down_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 26/55 [00:01<00:01, 17.77w/s, dev=2]  model.layers.60.mlp.gate_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 27/55 [00:01<00:01, 17.40w/s, dev=2]model.layers.60.mlp.up_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 28/55 [00:01<00:01, 17.06w/s, dev=2]  model.layers.60.post_attention_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/55 [00:01<00:01, 16.77w/s, dev=2]model.layers.60.self_attn.k_proj.weight:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 30/55 [00:01<00:01, 17.34w/s, dev=2]        model.layers.60.self_attn.k_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/55 [00:01<00:01, 15.95w/s, dev=2]model.layers.60.self_attn.o_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/55 [00:01<00:01, 15.95w/s, dev=2]model.layers.60.self_attn.q_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 32/55 [00:01<00:01, 16.24w/s, dev=2]model.layers.60.self_attn.rotary_emb.inv_freq:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/55 [00:01<00:01, 16.52w/s, dev=2]model.layers.60.self_attn.v_proj.weight:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/55 [00:01<00:01, 17.01w/s, dev=2]      model.layers.61.input_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:02<00:01, 17.46w/s, dev=2] model.layers.61.mlp.down_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 36/55 [00:02<00:01, 17.96w/s, dev=2]  model.layers.61.mlp.gate_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 37/55 [00:02<00:01, 17.58w/s, dev=2]model.layers.61.mlp.gate_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 38/55 [00:02<00:00, 17.14w/s, dev=2]model.layers.61.mlp.up_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 38/55 [00:02<00:00, 17.14w/s, dev=2]  model.layers.61.post_attention_layernorm.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 39/55 [00:02<00:00, 16.71w/s, dev=2]model.layers.61.self_attn.k_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 40/55 [00:02<00:00, 17.14w/s, dev=2]        model.layers.61.self_attn.o_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/55 [00:02<00:00, 15.93w/s, dev=2]model.layers.61.self_attn.q_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 42/55 [00:02<00:00, 16.10w/s, dev=2]model.layers.61.self_attn.rotary_emb.inv_freq:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 43/55 [00:02<00:00, 16.27w/s, dev=2]model.layers.61.self_attn.v_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 44/55 [00:02<00:00, 16.65w/s, dev=2]      model.layers.61.self_attn.v_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:02<00:00, 16.98w/s, dev=2]model.layers.62.input_layernorm.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:02<00:00, 15.29w/s, dev=3] model.layers.62.mlp.down_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 46/55 [00:03<00:00, 14.74w/s, dev=3]  model.layers.62.mlp.gate_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 47/55 [00:03<00:00, 14.54w/s, dev=3]model.layers.62.mlp.up_proj.weight:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 48/55 [00:03<00:00, 14.35w/s, dev=3]  model.layers.62.post_attention_layernorm.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 49/55 [00:03<00:00, 14.17w/s, dev=3]model.layers.62.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 50/55 [00:03<00:00, 14.45w/s, dev=3]        model.layers.62.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 51/55 [00:03<00:00, 13.75w/s, dev=3]model.layers.62.self_attn.o_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 52/55 [00:03<00:00, 13.87w/s, dev=3]model.layers.62.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 52/55 [00:03<00:00, 13.87w/s, dev=3]model.layers.62.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 53/55 [00:03<00:00, 14.01w/s, dev=3]model.layers.62.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 54/55 [00:03<00:00, 14.27w/s, dev=3]                                                                                                      0%|          | 0/57 [00:00<?, ?w/s]model.layers.63.input_layernorm.weight:   0%|          | 0/57 [00:00<?, ?w/s, dev=3]model.layers.63.mlp.down_proj.weight:   2%|â–         | 1/57 [00:00<00:00, 922.84w/s, dev=3]model.layers.63.mlp.gate_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:02, 20.64w/s, dev=3] model.layers.63.mlp.gate_proj.weight:   5%|â–Œ         | 3/57 [00:00<00:03, 14.37w/s, dev=3]model.layers.63.mlp.up_proj.weight:   5%|â–Œ         | 3/57 [00:00<00:03, 14.35w/s, dev=3]  model.layers.63.post_attention_layernorm.weight:   7%|â–‹         | 4/57 [00:00<00:04, 12.34w/s, dev=3]model.layers.63.self_attn.k_proj.weight:   9%|â–‰         | 5/57 [00:00<00:03, 15.40w/s, dev=3]        model.layers.63.self_attn.k_proj.weight:  11%|â–ˆ         | 6/57 [00:00<00:02, 18.08w/s, dev=3]model.layers.63.self_attn.o_proj.weight:  11%|â–ˆ         | 6/57 [00:00<00:02, 18.07w/s, dev=3]model.layers.63.self_attn.q_proj.weight:  12%|â–ˆâ–        | 7/57 [00:00<00:02, 19.04w/s, dev=3]model.layers.63.self_attn.rotary_emb.inv_freq:  14%|â–ˆâ–        | 8/57 [00:00<00:02, 19.87w/s, dev=3]model.layers.63.self_attn.v_proj.weight:  16%|â–ˆâ–Œ        | 9/57 [00:00<00:02, 22.33w/s, dev=3]      model.layers.64.input_layernorm.weight:  18%|â–ˆâ–Š        | 10/57 [00:00<00:01, 24.52w/s, dev=3]model.layers.64.mlp.down_proj.weight:  19%|â–ˆâ–‰        | 11/57 [00:00<00:01, 26.95w/s, dev=3]  model.layers.64.mlp.down_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:00<00:01, 22.81w/s, dev=3]model.layers.64.mlp.gate_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:00<00:01, 22.80w/s, dev=3]model.layers.64.mlp.up_proj.weight:  23%|â–ˆâ–ˆâ–Ž       | 13/57 [00:00<00:02, 20.31w/s, dev=3]  model.layers.64.post_attention_layernorm.weight:  25%|â–ˆâ–ˆâ–       | 14/57 [00:00<00:02, 18.53w/s, dev=3]model.layers.64.self_attn.k_proj.weight:  26%|â–ˆâ–ˆâ–‹       | 15/57 [00:00<00:02, 19.84w/s, dev=3]        model.layers.64.self_attn.o_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:00<00:01, 20.97w/s, dev=3]model.layers.64.self_attn.q_proj.weight:  30%|â–ˆâ–ˆâ–‰       | 17/57 [00:00<00:01, 21.27w/s, dev=3]model.layers.64.self_attn.q_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 18/57 [00:00<00:01, 21.53w/s, dev=3]model.layers.64.self_attn.rotary_emb.inv_freq:  32%|â–ˆâ–ˆâ–ˆâ–      | 18/57 [00:00<00:01, 21.52w/s, dev=3]model.layers.64.self_attn.v_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 19/57 [00:00<00:01, 22.71w/s, dev=3]      model.layers.65.input_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 20/57 [00:00<00:01, 23.74w/s, dev=3] model.layers.65.mlp.down_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 21/57 [00:00<00:01, 24.91w/s, dev=3]  model.layers.65.mlp.gate_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:00<00:01, 22.90w/s, dev=3]model.layers.65.mlp.up_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 23/57 [00:01<00:01, 21.71w/s, dev=3]  model.layers.65.mlp.up_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/57 [00:01<00:01, 20.39w/s, dev=3]model.layers.65.post_attention_layernorm.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/57 [00:01<00:01, 20.39w/s, dev=3]model.layers.65.self_attn.k_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/57 [00:01<00:01, 21.23w/s, dev=3]        model.layers.65.self_attn.o_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 26/57 [00:01<00:01, 21.97w/s, dev=3]model.layers.65.self_attn.q_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 27/57 [00:01<00:01, 22.12w/s, dev=3]model.layers.65.self_attn.rotary_emb.inv_freq:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:01<00:01, 22.42w/s, dev=3]model.layers.65.self_attn.v_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 29/57 [00:01<00:01, 23.21w/s, dev=3]      model.layers.66.input_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 30/57 [00:01<00:01, 23.91w/s, dev=3] model.layers.66.mlp.down_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 31/57 [00:01<00:01, 24.70w/s, dev=3]  model.layers.66.mlp.down_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 32/57 [00:01<00:01, 23.42w/s, dev=3]model.layers.66.mlp.gate_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 32/57 [00:01<00:01, 23.42w/s, dev=3]model.layers.66.mlp.up_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 33/57 [00:01<00:01, 22.23w/s, dev=3]  model.layers.66.post_attention_layernorm.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:01<00:01, 21.20w/s, dev=3]model.layers.66.self_attn.k_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/57 [00:01<00:01, 21.82w/s, dev=3]        model.layers.66.self_attn.o_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 36/57 [00:01<00:00, 22.37w/s, dev=3]model.layers.66.self_attn.q_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 37/57 [00:01<00:00, 22.53w/s, dev=3]model.layers.66.self_attn.rotary_emb.inv_freq:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 38/57 [00:01<00:00, 22.70w/s, dev=3]model.layers.66.self_attn.v_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 39/57 [00:01<00:00, 23.29w/s, dev=3]      model.layers.66.self_attn.v_proj.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:01<00:00, 23.80w/s, dev=3]model.layers.67.input_layernorm.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:01<00:00, 23.80w/s, dev=3] model.layers.67.mlp.down_proj.weight:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/57 [00:01<00:00, 24.39w/s, dev=3]  model.layers.67.mlp.gate_proj.weight:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 42/57 [00:01<00:00, 23.32w/s, dev=3]model.layers.67.mlp.up_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 43/57 [00:01<00:00, 22.29w/s, dev=3]  model.layers.67.post_attention_layernorm.weight:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 44/57 [00:02<00:00, 21.52w/s, dev=3]model.layers.67.self_attn.k_proj.weight:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 45/57 [00:02<00:00, 22.01w/s, dev=3]        model.layers.67.self_attn.o_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:02<00:00, 22.44w/s, dev=3]model.layers.67.self_attn.q_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 47/57 [00:02<00:00, 22.56w/s, dev=3]model.layers.67.self_attn.q_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/57 [00:02<00:00, 22.67w/s, dev=3]model.layers.67.self_attn.rotary_emb.inv_freq:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/57 [00:02<00:00, 22.67w/s, dev=3]model.layers.67.self_attn.v_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 49/57 [00:02<00:00, 23.14w/s, dev=3]      model.layers.68.mlp.down_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 50/57 [00:02<00:00, 23.55w/s, dev=3]   model.layers.68.mlp.gate_proj.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 51/57 [00:02<00:00, 22.99w/s, dev=3]model.layers.68.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:02<00:00, 22.23w/s, dev=3]model.layers.68.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 53/57 [00:02<00:00, 22.61w/s, dev=3]model.layers.68.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/57 [00:02<00:00, 22.69w/s, dev=3]model.layers.68.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 55/57 [00:02<00:00, 22.75w/s, dev=3]model.layers.68.self_attn.rotary_emb.inv_freq:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 56/57 [00:02<00:00, 23.16w/s, dev=3]model.layers.68.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 56/57 [00:02<00:00, 23.16w/s, dev=3]                                                                                                      0%|          | 0/59 [00:00<?, ?w/s]model.layers.68.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=3]model.layers.68.mlp.up_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 932.48w/s, dev=3]model.layers.68.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:03, 16.08w/s, dev=3] model.layers.68.post_attention_layernorm.weight:   3%|â–Ž         | 2/59 [00:00<00:03, 16.05w/s, dev=3]model.layers.69.input_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:02, 24.01w/s, dev=3]         model.layers.69.mlp.down_proj.weight:   7%|â–‹         | 4/59 [00:00<00:01, 31.95w/s, dev=3]  model.layers.69.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:02, 20.27w/s, dev=3]model.layers.69.mlp.gate_proj.weight:   8%|â–Š         | 5/59 [00:00<00:02, 20.25w/s, dev=3]model.layers.69.mlp.up_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:03, 16.63w/s, dev=3]  model.layers.69.post_attention_layernorm.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:03, 14.95w/s, dev=3]model.layers.69.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:02, 17.07w/s, dev=3]model.layers.69.self_attn.k_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:02, 17.07w/s, dev=3]        model.layers.69.self_attn.o_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:02, 18.95w/s, dev=3]model.layers.69.self_attn.q_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:02, 19.75w/s, dev=3]model.layers.69.self_attn.rotary_emb.inv_freq:  19%|â–ˆâ–Š        | 11/59 [00:00<00:02, 20.40w/s, dev=3]model.layers.69.self_attn.v_proj.weight:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:02, 22.23w/s, dev=3]      model.layers.70.input_layernorm.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:01, 23.80w/s, dev=3] model.layers.70.mlp.down_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:01, 25.61w/s, dev=3]  model.layers.70.mlp.down_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:01, 22.88w/s, dev=3]model.layers.70.mlp.gate_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:01, 22.87w/s, dev=3]model.layers.70.mlp.up_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:00<00:02, 21.08w/s, dev=3]  model.layers.70.post_attention_layernorm.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:00<00:02, 19.73w/s, dev=3]model.layers.70.self_attn.k_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:00<00:01, 20.88w/s, dev=3]        model.layers.70.self_attn.o_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:00<00:01, 21.87w/s, dev=3]model.layers.70.self_attn.q_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:00<00:01, 22.26w/s, dev=3]model.layers.70.self_attn.rotary_emb.inv_freq:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:00<00:01, 22.52w/s, dev=3]model.layers.70.self_attn.rotary_emb.inv_freq:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:00<00:01, 23.58w/s, dev=3]model.layers.70.self_attn.v_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:00<00:01, 23.58w/s, dev=3]      model.layers.71.input_layernorm.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:00<00:01, 24.51w/s, dev=3] model.layers.71.mlp.down_proj.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:00<00:01, 25.57w/s, dev=3]  model.layers.71.mlp.gate_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:01, 23.98w/s, dev=3]model.layers.71.mlp.up_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:01, 22.65w/s, dev=3]  model.layers.71.post_attention_layernorm.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:01<00:01, 21.59w/s, dev=3]model.layers.71.self_attn.k_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:01, 22.38w/s, dev=3]        model.layers.71.self_attn.k_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:01, 23.10w/s, dev=3]model.layers.71.self_attn.o_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:01, 23.10w/s, dev=3]model.layers.71.self_attn.q_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:01<00:01, 23.35w/s, dev=3]model.layers.71.self_attn.rotary_emb.inv_freq:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:01<00:01, 23.57w/s, dev=3]model.layers.71.self_attn.v_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:01<00:01, 24.32w/s, dev=3]      model.layers.72.input_layernorm.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:01<00:01, 24.99w/s, dev=3] model.layers.72.mlp.down_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:01<00:00, 25.75w/s, dev=3]  model.layers.72.mlp.gate_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:01<00:00, 24.60w/s, dev=3]model.layers.72.mlp.gate_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:01<00:00, 23.58w/s, dev=3]model.layers.72.mlp.up_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:01<00:00, 23.57w/s, dev=3]  model.layers.72.post_attention_layernorm.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:01<00:00, 22.66w/s, dev=3]model.layers.72.self_attn.k_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:01<00:00, 23.27w/s, dev=3]        model.layers.72.self_attn.o_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:01<00:00, 23.82w/s, dev=3]model.layers.72.self_attn.q_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:01<00:00, 23.99w/s, dev=3]model.layers.72.self_attn.rotary_emb.inv_freq:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:01<00:00, 24.15w/s, dev=3]model.layers.72.self_attn.v_proj.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:01<00:00, 24.73w/s, dev=3]      model.layers.72.self_attn.v_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:01<00:00, 25.26w/s, dev=3]model.layers.73.input_layernorm.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:01<00:00, 25.25w/s, dev=3] model.layers.73.mlp.down_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:01<00:00, 25.84w/s, dev=3]  model.layers.73.mlp.gate_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:01<00:00, 24.91w/s, dev=3]model.layers.73.mlp.up_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:01<00:00, 24.10w/s, dev=3]  model.layers.73.post_attention_layernorm.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:02<00:00, 23.35w/s, dev=3]model.layers.73.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:02<00:00, 23.84w/s, dev=3]        model.layers.73.self_attn.o_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:02<00:00, 24.26w/s, dev=3]model.layers.73.self_attn.o_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:02<00:00, 24.37w/s, dev=3]model.layers.73.self_attn.q_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:02<00:00, 24.37w/s, dev=3]model.layers.73.self_attn.rotary_emb.inv_freq:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:02<00:00, 24.47w/s, dev=3]model.layers.73.self_attn.v_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:02<00:00, 24.94w/s, dev=3]      model.layers.74.mlp.gate_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:02<00:00, 25.36w/s, dev=3]   model.layers.74.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:02<00:00, 24.64w/s, dev=3]model.layers.74.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:02<00:00, 25.04w/s, dev=3]model.layers.74.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:02<00:00, 25.15w/s, dev=3]model.layers.74.self_attn.q_proj.weight:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:02<00:00, 25.26w/s, dev=3]model.layers.74.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:02<00:00, 25.25w/s, dev=3]model.layers.74.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:02<00:00, 25.69w/s, dev=3]                                                                                                      0%|          | 0/55 [00:00<?, ?w/s]model.layers.74.input_layernorm.weight:   0%|          | 0/55 [00:00<?, ?w/s, dev=3]model.layers.74.mlp.down_proj.weight:   2%|â–         | 1/55 [00:00<00:00, 977.69w/s, dev=3]model.layers.74.mlp.down_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:02, 18.37w/s, dev=3] model.layers.74.mlp.up_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:02, 18.33w/s, dev=3]  model.layers.74.post_attention_layernorm.weight:   5%|â–Œ         | 3/55 [00:00<00:03, 14.84w/s, dev=3]model.layers.75.input_layernorm.weight:   7%|â–‹         | 4/55 [00:00<00:02, 19.74w/s, dev=3]         model.layers.75.mlp.down_proj.weight:   9%|â–‰         | 5/55 [00:00<00:02, 24.64w/s, dev=3]  model.layers.75.mlp.down_proj.weight:  11%|â–ˆ         | 6/55 [00:00<00:02, 20.67w/s, dev=3]model.layers.75.mlp.gate_proj.weight:  11%|â–ˆ         | 6/55 [00:00<00:02, 20.66w/s, dev=3]model.layers.75.mlp.up_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:02, 18.45w/s, dev=3]  model.layers.75.post_attention_layernorm.weight:  15%|â–ˆâ–        | 8/55 [00:00<00:02, 16.68w/s, dev=3]model.layers.75.self_attn.k_proj.weight:  16%|â–ˆâ–‹        | 9/55 [00:00<00:02, 18.75w/s, dev=3]        model.layers.75.self_attn.k_proj.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:02, 20.62w/s, dev=3]model.layers.75.self_attn.o_proj.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:02, 20.61w/s, dev=3]model.layers.75.self_attn.q_proj.weight:  20%|â–ˆâ–ˆ        | 11/55 [00:00<00:02, 21.26w/s, dev=3]model.layers.75.self_attn.rotary_emb.inv_freq:  22%|â–ˆâ–ˆâ–       | 12/55 [00:00<00:01, 21.78w/s, dev=3]model.layers.75.self_attn.v_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:00<00:01, 23.57w/s, dev=3]      model.layers.76.input_layernorm.weight:  25%|â–ˆâ–ˆâ–Œ       | 14/55 [00:00<00:01, 25.08w/s, dev=3] model.layers.76.mlp.down_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 15/55 [00:00<00:01, 26.86w/s, dev=3]  model.layers.76.mlp.down_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 16/55 [00:00<00:01, 24.18w/s, dev=3]model.layers.76.mlp.gate_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 16/55 [00:00<00:01, 24.17w/s, dev=3]model.layers.76.mlp.up_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:00<00:01, 22.48w/s, dev=3]  model.layers.76.post_attention_layernorm.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/55 [00:00<00:01, 20.89w/s, dev=3]model.layers.76.self_attn.k_proj.weight:  35%|â–ˆâ–ˆâ–ˆâ–      | 19/55 [00:00<00:01, 22.04w/s, dev=3]        model.layers.76.self_attn.o_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:00<00:01, 23.08w/s, dev=3]model.layers.76.self_attn.q_proj.weight:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/55 [00:00<00:01, 23.42w/s, dev=3]model.layers.76.self_attn.q_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/55 [00:00<00:01, 23.71w/s, dev=3]model.layers.76.self_attn.rotary_emb.inv_freq:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/55 [00:00<00:01, 23.71w/s, dev=3]model.layers.76.self_attn.v_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 23/55 [00:00<00:01, 24.78w/s, dev=3]      model.layers.77.input_layernorm.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 24/55 [00:00<00:01, 25.73w/s, dev=3] model.layers.77.mlp.down_proj.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:00<00:01, 26.79w/s, dev=3]  model.layers.77.mlp.gate_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 26/55 [00:01<00:01, 25.07w/s, dev=3]model.layers.77.mlp.up_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 27/55 [00:01<00:01, 23.51w/s, dev=3]  model.layers.77.mlp.up_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 28/55 [00:01<00:01, 22.26w/s, dev=3]model.layers.77.post_attention_layernorm.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 28/55 [00:01<00:01, 22.26w/s, dev=3]model.layers.77.self_attn.k_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/55 [00:01<00:01, 23.05w/s, dev=3]        model.layers.77.self_attn.o_proj.weight:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 30/55 [00:01<00:01, 23.77w/s, dev=3]model.layers.77.self_attn.q_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/55 [00:01<00:00, 24.07w/s, dev=3]model.layers.77.self_attn.rotary_emb.inv_freq:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 32/55 [00:01<00:00, 24.36w/s, dev=3]model.layers.77.self_attn.v_proj.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/55 [00:01<00:00, 25.11w/s, dev=3]      model.layers.78.input_layernorm.weight:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/55 [00:01<00:00, 25.80w/s, dev=3] model.layers.78.mlp.down_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:01<00:00, 26.55w/s, dev=3]  model.layers.78.mlp.down_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 36/55 [00:01<00:00, 25.29w/s, dev=3]model.layers.78.mlp.gate_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 36/55 [00:01<00:00, 25.28w/s, dev=3]model.layers.78.mlp.up_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 37/55 [00:01<00:00, 23.94w/s, dev=3]  model.layers.78.post_attention_layernorm.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 38/55 [00:01<00:00, 22.79w/s, dev=3]model.layers.78.self_attn.k_proj.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 39/55 [00:01<00:00, 23.38w/s, dev=3]        model.layers.78.self_attn.o_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 40/55 [00:01<00:00, 23.91w/s, dev=3]model.layers.78.self_attn.q_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/55 [00:01<00:00, 24.03w/s, dev=3]model.layers.78.self_attn.rotary_emb.inv_freq:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 42/55 [00:01<00:00, 24.14w/s, dev=3]model.layers.78.self_attn.v_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 43/55 [00:01<00:00, 24.70w/s, dev=3]      model.layers.78.self_attn.v_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 44/55 [00:01<00:00, 25.22w/s, dev=3]model.layers.79.input_layernorm.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 44/55 [00:01<00:00, 25.22w/s, dev=3] model.layers.79.mlp.down_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:01<00:00, 25.79w/s, dev=3]  model.layers.79.mlp.gate_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 46/55 [00:01<00:00, 24.70w/s, dev=3]model.layers.79.mlp.up_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 47/55 [00:01<00:00, 23.89w/s, dev=3]  model.layers.79.post_attention_layernorm.weight:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 48/55 [00:02<00:00, 23.19w/s, dev=3]model.layers.79.self_attn.k_proj.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 49/55 [00:02<00:00, 23.67w/s, dev=3]        model.layers.79.self_attn.o_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 50/55 [00:02<00:00, 24.07w/s, dev=3]model.layers.79.self_attn.q_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 51/55 [00:02<00:00, 24.18w/s, dev=3]model.layers.79.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 52/55 [00:02<00:00, 24.31w/s, dev=3]model.layers.79.self_attn.rotary_emb.inv_freq:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 52/55 [00:02<00:00, 24.31w/s, dev=3]model.layers.79.self_attn.v_proj.weight:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 53/55 [00:02<00:00, 24.77w/s, dev=3]      model.norm.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 54/55 [00:02<00:00, 25.19w/s, dev=3]                                                                                                0%|          | 0/1 [00:00<?, ?w/s]lm_head.weight:   0%|          | 0/1 [00:00<?, ?w/s, dev=3]lm_head.weight: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  8.93w/s, dev=3]                                                                   self.device: cuda:0
Loading BASE model stabilityai/stablelm-base-alpha-3b-v2...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:01<00:01,  1.50s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.15it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.04it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[2] MOVING BASE MODEL TO GPU.../home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for iamgroot42/mimir contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/iamgroot42/mimir
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
Loading dataset the_pile...
Loading from HuggingFace!
Downloading data:   0%|          | 0.00/1.47M [00:00<?, ?B/s]Downloading data:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1.26M/1.47M [00:00<00:00, 12.6MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.47M/1.47M [00:00<00:00, 14.3MB/s]
Downloading data:   0%|          | 0.00/709k [00:00<?, ?B/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 709k/709k [00:00<00:00, 8.87MB/s]
Downloading data:   0%|          | 0.00/18.8M [00:00<?, ?B/s]Downloading data:   7%|â–‹         | 1.35M/18.8M [00:00<00:01, 13.4MB/s]Downloading data:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 10.3M/18.8M [00:00<00:00, 58.3MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 18.7M/18.8M [00:00<00:00, 70.1MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18.8M/18.8M [00:00<00:00, 62.4MB/s]
Downloading data:   0%|          | 0.00/18.5M [00:00<?, ?B/s]Downloading data:   7%|â–‹         | 1.34M/18.5M [00:00<00:01, 13.4MB/s]Downloading data:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 9.14M/18.5M [00:00<00:00, 51.4MB/s]Downloading data:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 15.6M/18.5M [00:00<00:00, 57.2MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18.5M/18.5M [00:00<00:00, 54.4MB/s]
Downloading data:   0%|          | 0.00/1.47M [00:00<?, ?B/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.47M/1.47M [00:00<00:00, 19.1MB/s]
Downloading data:   0%|          | 0.00/1.46M [00:00<?, ?B/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.46M/1.46M [00:00<00:00, 14.8MB/s]
Downloading data:   0%|          | 0.00/38.3M [00:00<?, ?B/s]Downloading data:  21%|â–ˆâ–ˆâ–       | 8.16M/38.3M [00:00<00:00, 81.6MB/s]Downloading data:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 18.0M/38.3M [00:00<00:00, 91.7MB/s]Downloading data:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 28.0M/38.3M [00:00<00:00, 95.2MB/s]Downloading data:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 37.6M/38.3M [00:00<00:00, 95.7MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38.3M/38.3M [00:00<00:00, 94.0MB/s]
Downloading data:   0%|          | 0.00/38.1M [00:00<?, ?B/s]Downloading data:   4%|â–Ž         | 1.37M/38.1M [00:00<00:02, 13.7MB/s]Downloading data:  21%|â–ˆâ–ˆ        | 7.94M/38.1M [00:00<00:00, 44.3MB/s]Downloading data:  34%|â–ˆâ–ˆâ–ˆâ–      | 13.1M/38.1M [00:00<00:00, 47.4MB/s]Downloading data:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 19.2M/38.1M [00:00<00:00, 52.9MB/s]Downloading data:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 26.5M/38.1M [00:00<00:00, 60.3MB/s]Downloading data:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 35.6M/38.1M [00:00<00:00, 70.5MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38.1M/38.1M [00:00<00:00, 60.8MB/s]
Downloading data:   0%|          | 0.00/1.47M [00:00<?, ?B/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.47M/1.47M [00:00<00:00, 14.9MB/s]
Downloading data:   0%|          | 0.00/1.47M [00:00<?, ?B/s]Downloading data:  19%|â–ˆâ–‰        | 280k/1.47M [00:00<00:00, 2.79MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.47M/1.47M [00:00<00:00, 8.84MB/s]
Downloading data:   0%|          | 0.00/38.3M [00:00<?, ?B/s]Downloading data:   4%|â–Ž         | 1.38M/38.3M [00:00<00:02, 13.8MB/s]Downloading data:  21%|â–ˆâ–ˆ        | 8.03M/38.3M [00:00<00:00, 44.8MB/s]Downloading data:  34%|â–ˆâ–ˆâ–ˆâ–      | 13.2M/38.3M [00:00<00:00, 45.9MB/s]Downloading data:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19.7M/38.3M [00:00<00:00, 53.0MB/s]Downloading data:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 27.5M/38.3M [00:00<00:00, 62.1MB/s]Downloading data:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 36.8M/38.3M [00:00<00:00, 72.4MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38.3M/38.3M [00:00<00:00, 61.1MB/s]
Downloading data:   0%|          | 0.00/38.2M [00:00<?, ?B/s]Downloading data:   4%|â–Ž         | 1.43M/38.2M [00:00<00:02, 14.3MB/s]Downloading data:  22%|â–ˆâ–ˆâ–       | 8.56M/38.2M [00:00<00:00, 47.8MB/s]Downloading data:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 15.6M/38.2M [00:00<00:00, 58.2MB/s]Downloading data:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 24.6M/38.2M [00:00<00:00, 70.5MB/s]Downloading data:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 34.0M/38.2M [00:00<00:00, 79.0MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38.2M/38.2M [00:00<00:00, 70.5MB/s]
Generating ngram_7_0.2 split: 0 examples [00:00, ? examples/s]Generating ngram_7_0.2 split: 80 examples [00:00, 787.47 examples/s]Generating ngram_7_0.2 split: 159 examples [00:00, 783.32 examples/s]Generating ngram_7_0.2 split: 277 examples [00:00, 779.91 examples/s]Generating ngram_7_0.2 split: 357 examples [00:00, 782.12 examples/s]Generating ngram_7_0.2 split: 476 examples [00:00, 783.30 examples/s]Generating ngram_7_0.2 split: 491 examples [00:00, 627.46 examples/s]
Generating ngram_13_0.2 split: 0 examples [00:00, ? examples/s]Generating ngram_13_0.2 split: 74 examples [00:00, 728.25 examples/s]Generating ngram_13_0.2 split: 151 examples [00:00, 743.67 examples/s]Generating ngram_13_0.2 split: 230 examples [00:00, 760.17 examples/s]Generating ngram_13_0.2 split: 309 examples [00:00, 766.02 examples/s]Generating ngram_13_0.2 split: 418 examples [00:00, 745.52 examples/s]Generating ngram_13_0.2 split: 495 examples [00:00, 747.60 examples/s]Generating ngram_13_0.2 split: 574 examples [00:00, 757.69 examples/s]Generating ngram_13_0.2 split: 651 examples [00:00, 759.36 examples/s]Generating ngram_13_0.2 split: 729 examples [00:00, 762.64 examples/s]Generating ngram_13_0.2 split: 842 examples [00:01, 756.91 examples/s]Generating ngram_13_0.2 split: 921 examples [00:01, 762.56 examples/s]Generating ngram_13_0.2 split: 1000 examples [00:01, 413.11 examples/s]Generating ngram_13_0.2 split: 1000 examples [00:01, 614.06 examples/s]
Generating ngram_13_0.8 split: 0 examples [00:00, ? examples/s]Generating ngram_13_0.8 split: 75 examples [00:00, 738.00 examples/s]Generating ngram_13_0.8 split: 151 examples [00:00, 748.47 examples/s]Generating ngram_13_0.8 split: 231 examples [00:00, 766.54 examples/s]Generating ngram_13_0.8 split: 343 examples [00:00, 750.21 examples/s]Generating ngram_13_0.8 split: 420 examples [00:00, 753.30 examples/s]Generating ngram_13_0.8 split: 497 examples [00:00, 756.37 examples/s]Generating ngram_13_0.8 split: 575 examples [00:00, 761.69 examples/s]Generating ngram_13_0.8 split: 652 examples [00:00, 758.86 examples/s]Generating ngram_13_0.8 split: 728 examples [00:00, 755.45 examples/s]Generating ngram_13_0.8 split: 806 examples [00:01, 760.99 examples/s]Generating ngram_13_0.8 split: 884 examples [00:01, 763.12 examples/s]Generating ngram_13_0.8 split: 999 examples [00:01, 761.07 examples/s]Generating ngram_13_0.8 split: 1000 examples [00:01, 585.53 examples/s]
Loading dataset the_pile...
Loading from HuggingFace!
Generating samples:   0%|          | 0/21 [00:00<?, ?it/s]Generating samples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:00<00:00, 829.15it/s]
NEW N_SAMPLES IS  1000
Writing raw data to tmp_results/neo125_github_experiment/meta-llama_Llama-2-70b-hf/pubmed_central_ngram_13_<0.8_truncated/raw_data.json
Writing raw data to tmp_results/neo125_github_experiment/meta-llama_Llama-2-70b-hf/pubmed_central_ngram_13_<0.8_truncated/raw_data_lens.json
Computing criterion:   0%|          | 0/20 [00:00<?, ?it/s]Computing criterion:   5%|â–Œ         | 1/20 [00:29<09:24, 29.72s/it]Computing criterion:  10%|â–ˆ         | 2/20 [00:51<07:28, 24.94s/it]Computing criterion:  15%|â–ˆâ–Œ        | 3/20 [01:16<07:09, 25.25s/it]Computing criterion:  20%|â–ˆâ–ˆ        | 4/20 [01:38<06:19, 23.75s/it]Computing criterion:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [01:59<05:39, 22.66s/it]Computing criterion:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [02:20<05:09, 22.13s/it]Computing criterion:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [02:41<04:43, 21.84s/it]Computing criterion:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [03:03<04:22, 21.88s/it]Computing criterion:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [03:24<03:58, 21.65s/it]Computing criterion:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [03:45<03:34, 21.44s/it]Computing criterion:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [04:06<03:12, 21.35s/it]Computing criterion:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [04:27<02:50, 21.28s/it]Computing criterion:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [04:48<02:28, 21.22s/it]Computing criterion:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [05:10<02:07, 21.29s/it]Computing criterion:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [05:31<01:46, 21.39s/it]Computing criterion:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [05:56<01:29, 22.32s/it]Computing criterion:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [06:17<01:06, 22.10s/it]Computing criterion:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [06:39<00:43, 21.80s/it]Computing criterion:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [06:59<00:21, 21.46s/it]Computing criterion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [07:20<00:00, 21.36s/it]Computing criterion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [07:20<00:00, 22.05s/it]
Ref scores:   0%|          | 0/1000 [00:00<?, ?it/s]Ref scores:   0%|          | 1/1000 [00:14<3:57:08, 14.24s/it]Ref scores:   0%|          | 2/1000 [00:14<1:39:33,  5.99s/it]Ref scores:   0%|          | 3/1000 [00:14<55:22,  3.33s/it]  Ref scores:   0%|          | 4/1000 [00:14<34:52,  2.10s/it]Ref scores:   0%|          | 5/1000 [00:15<23:32,  1.42s/it]Ref scores:   1%|          | 6/1000 [00:15<16:30,  1.00it/s]Ref scores:   1%|          | 7/1000 [00:15<12:04,  1.37it/s]Ref scores:   1%|          | 8/1000 [00:15<09:09,  1.81it/s]Ref scores:   1%|          | 9/1000 [00:15<07:13,  2.28it/s]Ref scores:   1%|          | 10/1000 [00:15<06:01,  2.74it/s]Ref scores:   1%|          | 11/1000 [00:16<05:11,  3.17it/s]Ref scores:   1%|          | 12/1000 [00:16<04:36,  3.57it/s]Ref scores:   1%|â–         | 13/1000 [00:16<04:13,  3.90it/s]Ref scores:   1%|â–         | 14/1000 [00:16<03:55,  4.18it/s]Ref scores:   2%|â–         | 15/1000 [00:16<03:43,  4.41it/s]Ref scores:   2%|â–         | 16/1000 [00:17<03:34,  4.58it/s]Ref scores:   2%|â–         | 17/1000 [00:17<03:22,  4.85it/s]Ref scores:   2%|â–         | 18/1000 [00:17<03:11,  5.13it/s]Ref scores:   2%|â–         | 19/1000 [00:17<02:56,  5.55it/s]Ref scores:   2%|â–         | 20/1000 [00:17<03:04,  5.32it/s]Ref scores:   2%|â–         | 21/1000 [00:18<03:00,  5.42it/s]Ref scores:   2%|â–         | 22/1000 [00:18<02:57,  5.52it/s]Ref scores:   2%|â–         | 23/1000 [00:18<03:14,  5.01it/s]Ref scores:   2%|â–         | 24/1000 [00:18<03:07,  5.20it/s]Ref scores:   2%|â–Ž         | 25/1000 [00:18<03:13,  5.05it/s]Ref scores:   3%|â–Ž         | 26/1000 [00:19<03:06,  5.21it/s]Ref scores:   3%|â–Ž         | 27/1000 [00:19<03:24,  4.76it/s]Ref scores:   3%|â–Ž         | 28/1000 [00:19<03:14,  4.99it/s]Ref scores:   3%|â–Ž         | 29/1000 [00:19<03:05,  5.25it/s]Ref scores:   3%|â–Ž         | 30/1000 [00:19<03:01,  5.33it/s]Ref scores:   3%|â–Ž         | 31/1000 [00:19<02:58,  5.43it/s]Ref scores:   3%|â–Ž         | 32/1000 [00:20<03:37,  4.45it/s]Ref scores:   3%|â–Ž         | 33/1000 [00:20<03:23,  4.76it/s]Ref scores:   3%|â–Ž         | 34/1000 [00:20<03:11,  5.04it/s]Ref scores:   4%|â–Ž         | 35/1000 [00:20<03:11,  5.03it/s]Ref scores:   4%|â–Ž         | 36/1000 [00:21<03:04,  5.22it/s]Ref scores:   4%|â–Ž         | 37/1000 [00:21<02:59,  5.35it/s]Ref scores:   4%|â–         | 38/1000 [00:21<03:07,  5.13it/s]Ref scores:   4%|â–         | 39/1000 [00:21<03:02,  5.26it/s]Ref scores:   4%|â–         | 40/1000 [00:21<03:19,  4.81it/s]Ref scores:   4%|â–         | 41/1000 [00:22<03:10,  5.02it/s]Ref scores:   4%|â–         | 42/1000 [00:22<03:06,  5.12it/s]Ref scores:   4%|â–         | 43/1000 [00:22<03:02,  5.26it/s]Ref scores:   4%|â–         | 44/1000 [00:22<03:17,  4.84it/s]Ref scores:   4%|â–         | 45/1000 [00:22<03:19,  4.78it/s]Ref scores:   5%|â–         | 46/1000 [00:23<03:12,  4.96it/s]Ref scores:   5%|â–         | 47/1000 [00:23<03:15,  4.88it/s]Ref scores:   5%|â–         | 48/1000 [00:23<03:07,  5.08it/s]Ref scores:   5%|â–         | 49/1000 [00:23<03:00,  5.27it/s]Ref scores:   5%|â–Œ         | 50/1000 [00:23<02:56,  5.39it/s]Ref scores:   5%|â–Œ         | 51/1000 [00:23<02:53,  5.46it/s]Ref scores:   5%|â–Œ         | 52/1000 [00:24<02:52,  5.50it/s]Ref scores:   5%|â–Œ         | 53/1000 [00:24<02:51,  5.51it/s]Ref scores:   5%|â–Œ         | 54/1000 [00:24<02:50,  5.55it/s]Ref scores:   6%|â–Œ         | 55/1000 [00:24<02:48,  5.61it/s]Ref scores:   6%|â–Œ         | 56/1000 [00:24<02:39,  5.91it/s]Ref scores:   6%|â–Œ         | 57/1000 [00:24<02:41,  5.84it/s]Ref scores:   6%|â–Œ         | 58/1000 [00:25<02:43,  5.76it/s]Ref scores:   6%|â–Œ         | 59/1000 [00:25<02:45,  5.69it/s]Ref scores:   6%|â–Œ         | 60/1000 [00:25<02:48,  5.58it/s]Ref scores:   6%|â–Œ         | 61/1000 [00:25<02:49,  5.53it/s]Ref scores:   6%|â–Œ         | 62/1000 [00:25<02:48,  5.55it/s]Ref scores:   6%|â–‹         | 63/1000 [00:26<02:54,  5.36it/s]Ref scores:   6%|â–‹         | 64/1000 [00:26<02:52,  5.42it/s]Ref scores:   6%|â–‹         | 65/1000 [00:26<02:56,  5.29it/s]Ref scores:   7%|â–‹         | 66/1000 [00:26<03:01,  5.15it/s]Ref scores:   7%|â–‹         | 67/1000 [00:26<03:17,  4.73it/s]Ref scores:   7%|â–‹         | 68/1000 [00:27<03:07,  4.96it/s]Ref scores:   7%|â–‹         | 69/1000 [00:27<03:00,  5.17it/s]Ref scores:   7%|â–‹         | 70/1000 [00:27<02:53,  5.36it/s]Ref scores:   7%|â–‹         | 71/1000 [00:27<02:50,  5.46it/s]Ref scores:   7%|â–‹         | 72/1000 [00:27<02:49,  5.47it/s]Ref scores:   7%|â–‹         | 73/1000 [00:28<02:59,  5.17it/s]Ref scores:   7%|â–‹         | 74/1000 [00:28<03:14,  4.77it/s]Ref scores:   8%|â–Š         | 75/1000 [00:28<03:06,  4.96it/s]Ref scores:   8%|â–Š         | 76/1000 [00:28<03:06,  4.97it/s]Ref scores:   8%|â–Š         | 77/1000 [00:28<03:06,  4.94it/s]Ref scores:   8%|â–Š         | 78/1000 [00:29<02:59,  5.15it/s]Ref scores:   8%|â–Š         | 79/1000 [00:29<02:53,  5.30it/s]Ref scores:   8%|â–Š         | 80/1000 [00:29<02:51,  5.37it/s]Ref scores:   8%|â–Š         | 81/1000 [00:29<02:48,  5.45it/s]Ref scores:   8%|â–Š         | 82/1000 [00:29<02:47,  5.49it/s]Ref scores:   8%|â–Š         | 83/1000 [00:29<02:45,  5.53it/s]Ref scores:   8%|â–Š         | 84/1000 [00:30<02:43,  5.59it/s]Ref scores:   8%|â–Š         | 85/1000 [00:30<02:49,  5.41it/s]Ref scores:   9%|â–Š         | 86/1000 [00:30<02:47,  5.44it/s]Ref scores:   9%|â–Š         | 87/1000 [00:30<02:45,  5.50it/s]Ref scores:   9%|â–‰         | 88/1000 [00:30<02:43,  5.59it/s]Ref scores:   9%|â–‰         | 89/1000 [00:31<02:43,  5.57it/s]Ref scores:   9%|â–‰         | 90/1000 [00:31<02:41,  5.64it/s]Ref scores:   9%|â–‰         | 91/1000 [00:31<02:48,  5.40it/s]Ref scores:   9%|â–‰         | 92/1000 [00:31<02:46,  5.46it/s]Ref scores:   9%|â–‰         | 93/1000 [00:31<02:46,  5.46it/s]Ref scores:   9%|â–‰         | 94/1000 [00:31<02:44,  5.51it/s]Ref scores:  10%|â–‰         | 95/1000 [00:32<02:42,  5.57it/s]Ref scores:  10%|â–‰         | 96/1000 [00:32<02:43,  5.54it/s]Ref scores:  10%|â–‰         | 97/1000 [00:32<02:40,  5.61it/s]Ref scores:  10%|â–‰         | 98/1000 [00:32<02:39,  5.64it/s]Ref scores:  10%|â–‰         | 99/1000 [00:32<02:50,  5.29it/s]Ref scores:  10%|â–ˆ         | 100/1000 [00:33<02:47,  5.38it/s]Ref scores:  10%|â–ˆ         | 101/1000 [00:33<02:48,  5.35it/s]Ref scores:  10%|â–ˆ         | 102/1000 [00:33<02:58,  5.03it/s]Ref scores:  10%|â–ˆ         | 103/1000 [00:33<03:05,  4.84it/s]Ref scores:  10%|â–ˆ         | 104/1000 [00:33<03:04,  4.86it/s]Ref scores:  10%|â–ˆ         | 105/1000 [00:34<03:04,  4.86it/s]Ref scores:  11%|â–ˆ         | 106/1000 [00:34<02:57,  5.04it/s]Ref scores:  11%|â–ˆ         | 107/1000 [00:34<02:52,  5.19it/s]Ref scores:  11%|â–ˆ         | 108/1000 [00:34<02:50,  5.23it/s]Ref scores:  11%|â–ˆ         | 109/1000 [00:34<02:49,  5.26it/s]Ref scores:  11%|â–ˆ         | 110/1000 [00:34<02:46,  5.35it/s]Ref scores:  11%|â–ˆ         | 111/1000 [00:35<02:36,  5.68it/s]Ref scores:  11%|â–ˆ         | 112/1000 [00:35<02:38,  5.62it/s]Ref scores:  11%|â–ˆâ–        | 113/1000 [00:35<02:31,  5.84it/s]Ref scores:  11%|â–ˆâ–        | 114/1000 [00:35<02:32,  5.80it/s]Ref scores:  12%|â–ˆâ–        | 115/1000 [00:35<02:42,  5.45it/s]Ref scores:  12%|â–ˆâ–        | 116/1000 [00:36<02:41,  5.46it/s]Ref scores:  12%|â–ˆâ–        | 117/1000 [00:36<02:42,  5.44it/s]Ref scores:  12%|â–ˆâ–        | 118/1000 [00:36<02:39,  5.53it/s]Ref scores:  12%|â–ˆâ–        | 119/1000 [00:36<02:39,  5.53it/s]Ref scores:  12%|â–ˆâ–        | 120/1000 [00:36<02:39,  5.53it/s]Ref scores:  12%|â–ˆâ–        | 121/1000 [00:36<02:37,  5.57it/s]Ref scores:  12%|â–ˆâ–        | 122/1000 [00:37<02:40,  5.47it/s]Ref scores:  12%|â–ˆâ–        | 123/1000 [00:37<02:41,  5.45it/s]Ref scores:  12%|â–ˆâ–        | 124/1000 [00:37<02:46,  5.27it/s]Ref scores:  12%|â–ˆâ–Ž        | 125/1000 [00:37<02:42,  5.38it/s]Ref scores:  13%|â–ˆâ–Ž        | 126/1000 [00:37<02:39,  5.48it/s]Ref scores:  13%|â–ˆâ–Ž        | 127/1000 [00:38<02:31,  5.75it/s]Ref scores:  13%|â–ˆâ–Ž        | 128/1000 [00:38<02:33,  5.66it/s]Ref scores:  13%|â–ˆâ–Ž        | 129/1000 [00:38<02:35,  5.62it/s]Ref scores:  13%|â–ˆâ–Ž        | 130/1000 [00:38<02:44,  5.28it/s]Ref scores:  13%|â–ˆâ–Ž        | 131/1000 [00:38<02:42,  5.33it/s]Ref scores:  13%|â–ˆâ–Ž        | 132/1000 [00:39<02:48,  5.14it/s]Ref scores:  13%|â–ˆâ–Ž        | 133/1000 [00:39<02:45,  5.23it/s]Ref scores:  13%|â–ˆâ–Ž        | 134/1000 [00:39<02:41,  5.36it/s]Ref scores:  14%|â–ˆâ–Ž        | 135/1000 [00:39<02:59,  4.81it/s]Ref scores:  14%|â–ˆâ–Ž        | 136/1000 [00:39<02:53,  4.98it/s]Ref scores:  14%|â–ˆâ–Ž        | 137/1000 [00:39<02:49,  5.09it/s]Ref scores:  14%|â–ˆâ–        | 138/1000 [00:40<02:44,  5.24it/s]Ref scores:  14%|â–ˆâ–        | 139/1000 [00:40<02:42,  5.31it/s]Ref scores:  14%|â–ˆâ–        | 140/1000 [00:40<02:51,  5.01it/s]Ref scores:  14%|â–ˆâ–        | 141/1000 [00:40<02:44,  5.21it/s]Ref scores:  14%|â–ˆâ–        | 142/1000 [00:40<02:47,  5.13it/s]Ref scores:  14%|â–ˆâ–        | 143/1000 [00:41<02:44,  5.22it/s]Ref scores:  14%|â–ˆâ–        | 144/1000 [00:41<03:00,  4.73it/s]Ref scores:  14%|â–ˆâ–        | 145/1000 [00:41<02:52,  4.95it/s]Ref scores:  15%|â–ˆâ–        | 146/1000 [00:41<02:53,  4.92it/s]Ref scores:  15%|â–ˆâ–        | 147/1000 [00:41<02:55,  4.86it/s]Ref scores:  15%|â–ˆâ–        | 148/1000 [00:42<03:08,  4.52it/s]Ref scores:  15%|â–ˆâ–        | 149/1000 [00:42<02:59,  4.74it/s]Ref scores:  15%|â–ˆâ–Œ        | 150/1000 [00:42<02:59,  4.74it/s]Ref scores:  15%|â–ˆâ–Œ        | 151/1000 [00:42<02:52,  4.92it/s]Ref scores:  15%|â–ˆâ–Œ        | 152/1000 [00:43<02:48,  5.03it/s]Ref scores:  15%|â–ˆâ–Œ        | 153/1000 [00:43<02:43,  5.18it/s]Ref scores:  15%|â–ˆâ–Œ        | 154/1000 [00:43<02:39,  5.31it/s]Ref scores:  16%|â–ˆâ–Œ        | 155/1000 [00:43<02:37,  5.37it/s]Ref scores:  16%|â–ˆâ–Œ        | 156/1000 [00:43<02:43,  5.17it/s]Ref scores:  16%|â–ˆâ–Œ        | 157/1000 [00:43<02:39,  5.28it/s]Ref scores:  16%|â–ˆâ–Œ        | 158/1000 [00:44<02:46,  5.05it/s]Ref scores:  16%|â–ˆâ–Œ        | 159/1000 [00:44<02:50,  4.92it/s]Ref scores:  16%|â–ˆâ–Œ        | 160/1000 [00:44<03:04,  4.55it/s]Ref scores:  16%|â–ˆâ–Œ        | 161/1000 [00:44<02:55,  4.79it/s]Ref scores:  16%|â–ˆâ–Œ        | 162/1000 [00:45<02:55,  4.77it/s]Ref scores:  16%|â–ˆâ–‹        | 163/1000 [00:45<02:54,  4.81it/s]Ref scores:  16%|â–ˆâ–‹        | 164/1000 [00:45<02:56,  4.72it/s]Ref scores:  16%|â–ˆâ–‹        | 165/1000 [00:45<02:47,  4.98it/s]Ref scores:  17%|â–ˆâ–‹        | 166/1000 [00:45<02:41,  5.16it/s]Ref scores:  17%|â–ˆâ–‹        | 167/1000 [00:46<02:40,  5.19it/s]Ref scores:  17%|â–ˆâ–‹        | 168/1000 [00:46<02:39,  5.22it/s]Ref scores:  17%|â–ˆâ–‹        | 169/1000 [00:46<02:42,  5.12it/s]Ref scores:  17%|â–ˆâ–‹        | 170/1000 [00:46<02:45,  5.01it/s]Ref scores:  17%|â–ˆâ–‹        | 171/1000 [00:46<02:34,  5.38it/s]Ref scores:  17%|â–ˆâ–‹        | 172/1000 [00:46<02:31,  5.46it/s]Ref scores:  17%|â–ˆâ–‹        | 173/1000 [00:47<02:40,  5.15it/s]Ref scores:  17%|â–ˆâ–‹        | 174/1000 [00:47<02:37,  5.23it/s]Ref scores:  18%|â–ˆâ–Š        | 175/1000 [00:47<02:37,  5.25it/s]Ref scores:  18%|â–ˆâ–Š        | 176/1000 [00:47<02:42,  5.08it/s]Ref scores:  18%|â–ˆâ–Š        | 177/1000 [00:47<02:45,  4.98it/s]Ref scores:  18%|â–ˆâ–Š        | 178/1000 [00:48<02:40,  5.11it/s]Ref scores:  18%|â–ˆâ–Š        | 179/1000 [00:48<02:36,  5.25it/s]Ref scores:  18%|â–ˆâ–Š        | 180/1000 [00:48<02:35,  5.28it/s]Ref scores:  18%|â–ˆâ–Š        | 181/1000 [00:48<02:34,  5.29it/s]Ref scores:  18%|â–ˆâ–Š        | 182/1000 [00:48<02:31,  5.40it/s]Ref scores:  18%|â–ˆâ–Š        | 183/1000 [00:49<02:31,  5.41it/s]Ref scores:  18%|â–ˆâ–Š        | 184/1000 [00:49<02:47,  4.86it/s]Ref scores:  18%|â–ˆâ–Š        | 185/1000 [00:49<02:41,  5.04it/s]Ref scores:  19%|â–ˆâ–Š        | 186/1000 [00:49<02:30,  5.40it/s]Ref scores:  19%|â–ˆâ–Š        | 187/1000 [00:49<02:29,  5.45it/s]Ref scores:  19%|â–ˆâ–‰        | 188/1000 [00:49<02:28,  5.48it/s]Ref scores:  19%|â–ˆâ–‰        | 189/1000 [00:50<02:28,  5.47it/s]Ref scores:  19%|â–ˆâ–‰        | 190/1000 [00:50<02:36,  5.16it/s]Ref scores:  19%|â–ˆâ–‰        | 191/1000 [00:50<02:34,  5.25it/s]Ref scores:  19%|â–ˆâ–‰        | 192/1000 [00:50<02:32,  5.31it/s]Ref scores:  19%|â–ˆâ–‰        | 193/1000 [00:50<02:40,  5.02it/s]Ref scores:  19%|â–ˆâ–‰        | 194/1000 [00:51<02:54,  4.62it/s]Ref scores:  20%|â–ˆâ–‰        | 195/1000 [00:51<02:45,  4.85it/s]Ref scores:  20%|â–ˆâ–‰        | 196/1000 [00:51<02:40,  5.02it/s]Ref scores:  20%|â–ˆâ–‰        | 197/1000 [00:51<02:41,  4.96it/s]Ref scores:  20%|â–ˆâ–‰        | 198/1000 [00:52<02:37,  5.11it/s]Ref scores:  20%|â–ˆâ–‰        | 199/1000 [00:52<02:32,  5.25it/s]Ref scores:  20%|â–ˆâ–ˆ        | 200/1000 [00:52<02:29,  5.35it/s]Ref scores:  20%|â–ˆâ–ˆ        | 201/1000 [00:52<02:27,  5.40it/s]Ref scores:  20%|â–ˆâ–ˆ        | 202/1000 [00:52<02:27,  5.42it/s]Ref scores:  20%|â–ˆâ–ˆ        | 203/1000 [00:52<02:27,  5.42it/s]Ref scores:  20%|â–ˆâ–ˆ        | 204/1000 [00:53<02:34,  5.15it/s]Ref scores:  20%|â–ˆâ–ˆ        | 205/1000 [00:53<02:24,  5.52it/s]Ref scores:  21%|â–ˆâ–ˆ        | 206/1000 [00:53<02:16,  5.80it/s]Ref scores:  21%|â–ˆâ–ˆ        | 207/1000 [00:53<02:11,  6.04it/s]Ref scores:  21%|â–ˆâ–ˆ        | 208/1000 [00:53<02:13,  5.92it/s]Ref scores:  21%|â–ˆâ–ˆ        | 209/1000 [00:53<02:16,  5.78it/s]Ref scores:  21%|â–ˆâ–ˆ        | 210/1000 [00:54<02:17,  5.74it/s]Ref scores:  21%|â–ˆâ–ˆ        | 211/1000 [00:54<02:25,  5.43it/s]Ref scores:  21%|â–ˆâ–ˆ        | 212/1000 [00:54<02:24,  5.44it/s]Ref scores:  21%|â–ˆâ–ˆâ–       | 213/1000 [00:54<02:24,  5.45it/s]Ref scores:  21%|â–ˆâ–ˆâ–       | 214/1000 [00:54<02:33,  5.11it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 215/1000 [00:55<02:35,  5.05it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 216/1000 [00:55<02:32,  5.16it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 217/1000 [00:55<02:31,  5.17it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 218/1000 [00:55<02:22,  5.51it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 219/1000 [00:55<02:21,  5.54it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 220/1000 [00:56<02:27,  5.28it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 221/1000 [00:56<02:24,  5.40it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 222/1000 [00:56<02:23,  5.41it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 223/1000 [00:56<02:21,  5.50it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 224/1000 [00:56<02:27,  5.25it/s]Ref scores:  22%|â–ˆâ–ˆâ–Ž       | 225/1000 [00:56<02:26,  5.29it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 226/1000 [00:57<02:24,  5.35it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 227/1000 [00:57<02:06,  6.09it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 228/1000 [00:57<02:17,  5.61it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 229/1000 [00:57<02:17,  5.62it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 230/1000 [00:57<02:18,  5.55it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 231/1000 [00:58<02:23,  5.34it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 232/1000 [00:58<02:22,  5.38it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 233/1000 [00:58<02:30,  5.08it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 234/1000 [00:58<02:25,  5.25it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 235/1000 [00:58<02:24,  5.31it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 236/1000 [00:59<02:29,  5.12it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 237/1000 [00:59<02:27,  5.17it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 238/1000 [00:59<02:30,  5.07it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 239/1000 [00:59<02:28,  5.13it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 240/1000 [00:59<02:42,  4.67it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 241/1000 [01:00<02:36,  4.84it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 242/1000 [01:00<02:30,  5.02it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 243/1000 [01:00<02:19,  5.43it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 244/1000 [01:00<02:20,  5.39it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 245/1000 [01:00<02:19,  5.42it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 246/1000 [01:00<02:27,  5.10it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 247/1000 [01:01<02:23,  5.24it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 248/1000 [01:01<02:15,  5.56it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 249/1000 [01:01<02:16,  5.52it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 250/1000 [01:01<02:15,  5.54it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 251/1000 [01:01<02:15,  5.53it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 252/1000 [01:01<02:07,  5.85it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 253/1000 [01:02<02:11,  5.70it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 254/1000 [01:02<02:18,  5.38it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 255/1000 [01:02<02:18,  5.37it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 256/1000 [01:02<02:16,  5.45it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 257/1000 [01:02<02:16,  5.46it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 258/1000 [01:03<02:32,  4.86it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 259/1000 [01:03<02:20,  5.29it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 260/1000 [01:03<02:22,  5.18it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 261/1000 [01:03<02:25,  5.07it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 262/1000 [01:03<02:23,  5.13it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 263/1000 [01:04<02:27,  4.99it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 264/1000 [01:04<02:24,  5.10it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 265/1000 [01:04<02:19,  5.28it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 266/1000 [01:04<02:24,  5.09it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 267/1000 [01:04<02:26,  5.00it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 268/1000 [01:05<02:39,  4.59it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 269/1000 [01:05<02:30,  4.87it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 270/1000 [01:05<02:24,  5.05it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 271/1000 [01:05<02:19,  5.22it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 272/1000 [01:05<02:16,  5.34it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 273/1000 [01:06<02:23,  5.06it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 274/1000 [01:06<02:27,  4.91it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 275/1000 [01:06<02:23,  5.07it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 276/1000 [01:06<02:18,  5.22it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 277/1000 [01:06<02:22,  5.09it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 278/1000 [01:07<02:23,  5.04it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 279/1000 [01:07<02:24,  4.99it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 280/1000 [01:07<02:25,  4.93it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 281/1000 [01:07<02:19,  5.15it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 282/1000 [01:07<02:25,  4.92it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 283/1000 [01:08<02:20,  5.09it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 284/1000 [01:08<02:26,  4.89it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 285/1000 [01:08<02:21,  5.04it/s]Ref scores:  29%|â–ˆâ–ˆâ–Š       | 286/1000 [01:08<02:18,  5.15it/s]Ref scores:  29%|â–ˆâ–ˆâ–Š       | 287/1000 [01:08<02:21,  5.02it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 288/1000 [01:09<02:12,  5.39it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 289/1000 [01:09<02:11,  5.40it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 290/1000 [01:09<02:11,  5.42it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 291/1000 [01:09<02:11,  5.40it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 292/1000 [01:09<02:09,  5.47it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 293/1000 [01:09<02:02,  5.77it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 294/1000 [01:10<02:12,  5.32it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 295/1000 [01:10<02:09,  5.43it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 296/1000 [01:10<02:10,  5.40it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 297/1000 [01:10<02:10,  5.40it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 298/1000 [01:10<02:10,  5.39it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 299/1000 [01:11<02:03,  5.68it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 300/1000 [01:11<01:57,  5.95it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 301/1000 [01:11<02:00,  5.81it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 302/1000 [01:11<02:07,  5.48it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 303/1000 [01:11<02:11,  5.30it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 304/1000 [01:11<02:09,  5.37it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 305/1000 [01:12<02:09,  5.37it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 306/1000 [01:12<02:09,  5.37it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 307/1000 [01:12<02:09,  5.37it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 308/1000 [01:12<02:07,  5.42it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 309/1000 [01:12<02:05,  5.49it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 310/1000 [01:13<02:05,  5.49it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 311/1000 [01:13<02:06,  5.43it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 312/1000 [01:13<02:11,  5.23it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆâ–      | 313/1000 [01:13<02:09,  5.30it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆâ–      | 314/1000 [01:13<02:09,  5.29it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 315/1000 [01:14<02:06,  5.43it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 316/1000 [01:14<02:05,  5.45it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 317/1000 [01:14<02:04,  5.47it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 318/1000 [01:14<02:11,  5.19it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 319/1000 [01:14<02:10,  5.23it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 320/1000 [01:14<02:07,  5.34it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 321/1000 [01:15<02:06,  5.38it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 322/1000 [01:15<02:06,  5.36it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 323/1000 [01:15<02:06,  5.37it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 324/1000 [01:15<02:12,  5.11it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 325/1000 [01:15<02:10,  5.18it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 326/1000 [01:16<02:09,  5.20it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 327/1000 [01:16<02:14,  5.00it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 328/1000 [01:16<02:12,  5.08it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 329/1000 [01:16<02:10,  5.13it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 330/1000 [01:16<02:08,  5.23it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 331/1000 [01:17<02:05,  5.33it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 332/1000 [01:17<02:11,  5.06it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 333/1000 [01:17<02:08,  5.20it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 334/1000 [01:17<02:21,  4.70it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 335/1000 [01:17<02:23,  4.63it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 336/1000 [01:18<02:18,  4.80it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 337/1000 [01:18<02:14,  4.92it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 338/1000 [01:18<02:10,  5.06it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 339/1000 [01:18<02:24,  4.57it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 340/1000 [01:18<02:18,  4.75it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 341/1000 [01:19<02:14,  4.89it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 342/1000 [01:19<02:10,  5.05it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 343/1000 [01:19<02:12,  4.94it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 344/1000 [01:19<02:10,  5.02it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 345/1000 [01:19<02:09,  5.06it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 346/1000 [01:20<02:08,  5.11it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 347/1000 [01:20<02:06,  5.16it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 348/1000 [01:20<02:04,  5.25it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 349/1000 [01:20<02:09,  5.05it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 350/1000 [01:20<02:08,  5.07it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 351/1000 [01:21<02:14,  4.83it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 352/1000 [01:21<02:25,  4.47it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 353/1000 [01:21<02:18,  4.66it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 354/1000 [01:21<02:21,  4.58it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 355/1000 [01:22<02:20,  4.59it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 356/1000 [01:22<02:20,  4.57it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 357/1000 [01:22<02:15,  4.75it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 358/1000 [01:22<02:25,  4.43it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 359/1000 [01:23<02:31,  4.22it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 360/1000 [01:23<02:22,  4.48it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 361/1000 [01:23<02:20,  4.55it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 362/1000 [01:23<02:19,  4.57it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 363/1000 [01:23<02:14,  4.74it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 364/1000 [01:24<02:10,  4.89it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 365/1000 [01:24<02:07,  4.99it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 366/1000 [01:24<02:04,  5.08it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 367/1000 [01:24<02:03,  5.11it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 368/1000 [01:24<02:02,  5.17it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 369/1000 [01:24<02:08,  4.93it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 370/1000 [01:25<02:03,  5.12it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 371/1000 [01:25<02:00,  5.21it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 372/1000 [01:25<02:05,  5.01it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 373/1000 [01:25<02:02,  5.12it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 374/1000 [01:25<01:59,  5.22it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 375/1000 [01:26<02:02,  5.09it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 376/1000 [01:26<01:54,  5.44it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 377/1000 [01:26<01:53,  5.48it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 378/1000 [01:26<01:54,  5.43it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 379/1000 [01:26<01:54,  5.42it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 380/1000 [01:27<02:02,  5.07it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 381/1000 [01:27<01:59,  5.18it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 382/1000 [01:27<02:02,  5.04it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 383/1000 [01:27<02:07,  4.83it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 384/1000 [01:27<01:56,  5.27it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 385/1000 [01:28<01:55,  5.30it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 386/1000 [01:28<02:09,  4.76it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 387/1000 [01:28<02:05,  4.88it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 388/1000 [01:28<02:02,  5.00it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 389/1000 [01:28<01:59,  5.10it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 390/1000 [01:29<01:57,  5.18it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 391/1000 [01:29<02:03,  4.91it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 392/1000 [01:29<02:00,  5.04it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 393/1000 [01:29<01:58,  5.14it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 394/1000 [01:29<02:01,  4.97it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 395/1000 [01:30<01:58,  5.10it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 396/1000 [01:30<01:58,  5.09it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 397/1000 [01:30<01:57,  5.14it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 398/1000 [01:30<01:55,  5.23it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 399/1000 [01:30<01:59,  5.03it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 400/1000 [01:31<02:02,  4.89it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 401/1000 [01:31<01:54,  5.24it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 402/1000 [01:31<01:54,  5.23it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 403/1000 [01:31<01:54,  5.23it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 404/1000 [01:31<01:52,  5.28it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 405/1000 [01:31<01:52,  5.27it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 406/1000 [01:32<01:52,  5.28it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 407/1000 [01:32<01:52,  5.28it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 408/1000 [01:32<01:55,  5.12it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 409/1000 [01:32<02:01,  4.85it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 410/1000 [01:33<02:11,  4.48it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 411/1000 [01:33<02:09,  4.54it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 412/1000 [01:33<02:03,  4.76it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 413/1000 [01:33<02:00,  4.88it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 414/1000 [01:33<01:56,  5.01it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 415/1000 [01:34<01:52,  5.21it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 416/1000 [01:34<01:50,  5.30it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 417/1000 [01:34<01:56,  5.03it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 418/1000 [01:34<01:57,  4.95it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 419/1000 [01:34<01:53,  5.14it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 420/1000 [01:35<01:57,  4.94it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 421/1000 [01:35<01:54,  5.04it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 422/1000 [01:35<01:52,  5.15it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 423/1000 [01:35<01:48,  5.30it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 424/1000 [01:35<01:48,  5.29it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 425/1000 [01:35<01:48,  5.28it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 426/1000 [01:36<01:42,  5.60it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 427/1000 [01:36<01:45,  5.46it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 428/1000 [01:36<01:44,  5.49it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 429/1000 [01:36<01:49,  5.23it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 430/1000 [01:36<01:52,  5.06it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 431/1000 [01:37<01:54,  4.95it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 432/1000 [01:37<01:52,  5.05it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 433/1000 [01:37<01:54,  4.94it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 434/1000 [01:37<01:45,  5.35it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 435/1000 [01:37<01:39,  5.66it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 436/1000 [01:38<01:40,  5.61it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 437/1000 [01:38<01:42,  5.51it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 438/1000 [01:38<01:41,  5.56it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 439/1000 [01:38<01:44,  5.35it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 440/1000 [01:38<01:51,  5.04it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 441/1000 [01:39<02:00,  4.64it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 442/1000 [01:39<01:58,  4.70it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 443/1000 [01:39<01:54,  4.87it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 444/1000 [01:39<01:50,  5.02it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 445/1000 [01:39<01:54,  4.85it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 446/1000 [01:40<01:54,  4.82it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 447/1000 [01:40<01:51,  4.95it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 448/1000 [01:40<01:49,  5.04it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 449/1000 [01:40<01:46,  5.15it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 450/1000 [01:40<01:49,  5.03it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 451/1000 [01:40<01:41,  5.41it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 452/1000 [01:41<01:36,  5.68it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 453/1000 [01:41<01:37,  5.61it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 454/1000 [01:41<01:38,  5.52it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 455/1000 [01:41<01:47,  5.08it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 456/1000 [01:41<01:44,  5.23it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 457/1000 [01:42<01:43,  5.24it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 458/1000 [01:42<01:41,  5.31it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 459/1000 [01:42<01:40,  5.37it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 460/1000 [01:42<01:40,  5.38it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 461/1000 [01:42<01:52,  4.78it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 462/1000 [01:43<01:55,  4.65it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 463/1000 [01:43<01:50,  4.88it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 464/1000 [01:43<01:47,  4.98it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 465/1000 [01:43<01:45,  5.09it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 466/1000 [01:43<01:43,  5.15it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 467/1000 [01:44<01:48,  4.91it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 468/1000 [01:44<01:45,  5.04it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 469/1000 [01:44<01:48,  4.91it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 470/1000 [01:44<01:46,  4.96it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 471/1000 [01:44<01:43,  5.13it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 472/1000 [01:45<01:46,  4.98it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 473/1000 [01:45<01:48,  4.87it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 474/1000 [01:45<01:45,  4.99it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 475/1000 [01:45<01:42,  5.12it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 476/1000 [01:45<01:45,  4.98it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 477/1000 [01:46<01:47,  4.85it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 478/1000 [01:46<01:44,  4.98it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 479/1000 [01:46<01:43,  5.02it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 480/1000 [01:46<01:42,  5.09it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 481/1000 [01:46<01:40,  5.14it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 482/1000 [01:47<01:45,  4.90it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 483/1000 [01:47<01:42,  5.03it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 484/1000 [01:47<01:44,  4.96it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 485/1000 [01:47<01:41,  5.09it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 486/1000 [01:47<01:40,  5.11it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 487/1000 [01:48<01:33,  5.49it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 488/1000 [01:48<01:27,  5.85it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 489/1000 [01:48<01:29,  5.69it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 490/1000 [01:48<01:32,  5.51it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 491/1000 [01:48<01:33,  5.44it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 492/1000 [01:48<01:37,  5.21it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 493/1000 [01:49<01:39,  5.08it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 494/1000 [01:49<01:37,  5.16it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 495/1000 [01:49<01:37,  5.17it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 496/1000 [01:49<01:37,  5.16it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 497/1000 [01:49<01:40,  5.02it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 498/1000 [01:50<01:39,  5.06it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 499/1000 [01:50<01:37,  5.13it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 500/1000 [01:50<01:40,  4.99it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 501/1000 [01:50<01:40,  4.94it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 502/1000 [01:50<01:37,  5.10it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 503/1000 [01:51<01:31,  5.43it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 504/1000 [01:51<01:32,  5.37it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 505/1000 [01:51<01:37,  5.07it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 506/1000 [01:51<01:36,  5.12it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 507/1000 [01:51<01:30,  5.48it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 508/1000 [01:52<01:33,  5.24it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 509/1000 [01:52<01:33,  5.23it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 510/1000 [01:52<01:32,  5.29it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 511/1000 [01:52<01:28,  5.55it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 512/1000 [01:52<01:29,  5.44it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 513/1000 [01:53<01:29,  5.42it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 514/1000 [01:53<01:33,  5.17it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 515/1000 [01:53<01:33,  5.20it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 516/1000 [01:53<01:32,  5.24it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 517/1000 [01:53<01:42,  4.70it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 518/1000 [01:54<01:39,  4.83it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 519/1000 [01:54<01:37,  4.95it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 520/1000 [01:54<01:34,  5.08it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 521/1000 [01:54<01:36,  4.95it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 522/1000 [01:54<01:34,  5.07it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 523/1000 [01:55<01:35,  4.99it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 524/1000 [01:55<01:32,  5.16it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 525/1000 [01:55<01:31,  5.19it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 526/1000 [01:55<01:25,  5.52it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 527/1000 [01:55<01:26,  5.46it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 528/1000 [01:55<01:29,  5.26it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 529/1000 [01:56<01:32,  5.09it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 530/1000 [01:56<01:35,  4.93it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 531/1000 [01:56<01:32,  5.05it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 532/1000 [01:56<01:41,  4.62it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 533/1000 [01:57<01:37,  4.81it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 534/1000 [01:57<01:32,  5.02it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 535/1000 [01:57<01:33,  4.99it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 536/1000 [01:57<01:29,  5.19it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 537/1000 [01:57<01:31,  5.08it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 538/1000 [01:57<01:27,  5.26it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 539/1000 [01:58<01:26,  5.32it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 540/1000 [01:58<01:36,  4.78it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 541/1000 [01:58<01:32,  4.97it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 542/1000 [01:58<01:29,  5.13it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 543/1000 [01:58<01:27,  5.22it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 544/1000 [01:59<01:31,  4.98it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 545/1000 [01:59<01:29,  5.08it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 546/1000 [01:59<01:30,  4.99it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 547/1000 [01:59<01:33,  4.85it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 548/1000 [01:59<01:30,  5.02it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 549/1000 [02:00<01:28,  5.11it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 550/1000 [02:00<01:26,  5.21it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 551/1000 [02:00<01:24,  5.34it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 552/1000 [02:00<01:23,  5.37it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 553/1000 [02:00<01:23,  5.36it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 554/1000 [02:01<01:21,  5.48it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 555/1000 [02:01<01:22,  5.42it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 556/1000 [02:01<01:21,  5.46it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 557/1000 [02:01<01:20,  5.48it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 558/1000 [02:01<01:21,  5.43it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 559/1000 [02:02<01:31,  4.84it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 560/1000 [02:02<01:26,  5.07it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 561/1000 [02:02<01:27,  5.00it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 562/1000 [02:02<01:25,  5.12it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 563/1000 [02:02<01:23,  5.21it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 564/1000 [02:02<01:21,  5.32it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 565/1000 [02:03<01:24,  5.15it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 566/1000 [02:03<01:26,  5.04it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 567/1000 [02:03<01:28,  4.91it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 568/1000 [02:03<01:20,  5.35it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 569/1000 [02:03<01:19,  5.45it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 570/1000 [02:04<01:15,  5.73it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 571/1000 [02:04<01:15,  5.66it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 572/1000 [02:04<01:21,  5.25it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 573/1000 [02:04<01:19,  5.38it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 574/1000 [02:04<01:28,  4.80it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 575/1000 [02:05<01:21,  5.22it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 576/1000 [02:05<01:20,  5.27it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 577/1000 [02:05<01:19,  5.30it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 578/1000 [02:05<01:19,  5.29it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 579/1000 [02:05<01:18,  5.34it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 580/1000 [02:06<01:18,  5.33it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 581/1000 [02:06<01:17,  5.40it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 582/1000 [02:06<01:16,  5.48it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 583/1000 [02:06<01:25,  4.85it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 584/1000 [02:06<01:25,  4.85it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 585/1000 [02:07<01:25,  4.83it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 586/1000 [02:07<01:23,  4.97it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 587/1000 [02:07<01:25,  4.84it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 588/1000 [02:07<01:30,  4.53it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 589/1000 [02:07<01:28,  4.64it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 590/1000 [02:08<01:24,  4.85it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 591/1000 [02:08<01:21,  5.03it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 592/1000 [02:08<01:09,  5.84it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 593/1000 [02:08<01:09,  5.82it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 594/1000 [02:08<01:11,  5.70it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 595/1000 [02:08<01:11,  5.66it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 596/1000 [02:09<01:11,  5.65it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 597/1000 [02:09<01:18,  5.17it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 598/1000 [02:09<01:16,  5.26it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 599/1000 [02:09<01:15,  5.31it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 600/1000 [02:09<01:15,  5.30it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 601/1000 [02:10<01:14,  5.33it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 602/1000 [02:10<01:13,  5.39it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 603/1000 [02:10<01:13,  5.40it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 604/1000 [02:10<01:15,  5.23it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 605/1000 [02:10<01:16,  5.13it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 606/1000 [02:11<01:19,  4.93it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 607/1000 [02:11<01:12,  5.40it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 608/1000 [02:11<01:16,  5.11it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 609/1000 [02:11<01:14,  5.28it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 610/1000 [02:11<01:12,  5.34it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 611/1000 [02:11<01:12,  5.37it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 612/1000 [02:12<01:12,  5.35it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 613/1000 [02:12<01:11,  5.38it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 614/1000 [02:12<01:15,  5.12it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 615/1000 [02:12<01:16,  5.04it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 616/1000 [02:12<01:19,  4.85it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 617/1000 [02:13<01:15,  5.06it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 618/1000 [02:13<01:13,  5.18it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 619/1000 [02:13<01:12,  5.26it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 620/1000 [02:13<01:10,  5.38it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 621/1000 [02:13<01:09,  5.47it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 622/1000 [02:14<01:11,  5.30it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 623/1000 [02:14<01:10,  5.34it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 624/1000 [02:14<01:10,  5.33it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 625/1000 [02:14<01:09,  5.41it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 626/1000 [02:14<01:08,  5.43it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 627/1000 [02:15<01:12,  5.15it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 628/1000 [02:15<01:11,  5.21it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 629/1000 [02:15<01:10,  5.27it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 630/1000 [02:15<01:09,  5.34it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 631/1000 [02:15<01:08,  5.39it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 632/1000 [02:15<01:07,  5.42it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 633/1000 [02:16<01:07,  5.43it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 634/1000 [02:16<01:06,  5.49it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 635/1000 [02:16<01:06,  5.51it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 636/1000 [02:16<01:08,  5.30it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 637/1000 [02:16<01:08,  5.29it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 638/1000 [02:17<01:07,  5.35it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 639/1000 [02:17<01:07,  5.33it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 640/1000 [02:17<01:07,  5.36it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 641/1000 [02:17<01:09,  5.18it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 642/1000 [02:17<01:12,  4.93it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 643/1000 [02:18<01:09,  5.14it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 644/1000 [02:18<01:07,  5.26it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 645/1000 [02:18<01:03,  5.63it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 646/1000 [02:18<00:59,  5.92it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 647/1000 [02:18<01:04,  5.48it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 648/1000 [02:18<01:07,  5.23it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 649/1000 [02:19<01:08,  5.11it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 650/1000 [02:19<01:10,  4.94it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 651/1000 [02:19<01:08,  5.10it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 652/1000 [02:19<01:05,  5.28it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 653/1000 [02:19<01:05,  5.32it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 654/1000 [02:20<01:04,  5.33it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 655/1000 [02:20<01:07,  5.09it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 656/1000 [02:20<01:05,  5.21it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 657/1000 [02:20<01:13,  4.70it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 658/1000 [02:20<01:06,  5.14it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 659/1000 [02:21<01:12,  4.68it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 660/1000 [02:21<01:11,  4.79it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 661/1000 [02:21<01:06,  5.09it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 662/1000 [02:21<01:04,  5.21it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 663/1000 [02:21<01:02,  5.37it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 664/1000 [02:22<01:02,  5.41it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 665/1000 [02:22<00:58,  5.78it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 666/1000 [02:22<00:58,  5.70it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 667/1000 [02:22<01:00,  5.48it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 668/1000 [02:22<00:59,  5.55it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 669/1000 [02:22<00:59,  5.53it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 670/1000 [02:23<00:59,  5.54it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 671/1000 [02:23<01:02,  5.22it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 672/1000 [02:23<01:01,  5.33it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 673/1000 [02:23<01:03,  5.17it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 674/1000 [02:23<01:02,  5.24it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 675/1000 [02:24<01:03,  5.08it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 676/1000 [02:24<01:02,  5.15it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 677/1000 [02:24<00:58,  5.50it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 678/1000 [02:24<01:01,  5.26it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 679/1000 [02:24<00:59,  5.36it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 680/1000 [02:25<01:01,  5.17it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 681/1000 [02:25<01:03,  5.06it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 682/1000 [02:25<01:04,  4.94it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 683/1000 [02:25<01:05,  4.82it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 684/1000 [02:25<01:05,  4.79it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 685/1000 [02:26<01:02,  5.02it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 686/1000 [02:26<01:00,  5.16it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 687/1000 [02:26<00:59,  5.24it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 688/1000 [02:26<00:55,  5.58it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 689/1000 [02:26<00:58,  5.35it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 690/1000 [02:27<00:57,  5.42it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 691/1000 [02:27<00:56,  5.46it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 692/1000 [02:27<00:59,  5.21it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 693/1000 [02:27<01:00,  5.11it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 694/1000 [02:27<01:02,  4.91it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 695/1000 [02:28<01:00,  5.01it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 696/1000 [02:28<01:05,  4.62it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 697/1000 [02:28<01:02,  4.83it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 698/1000 [02:28<01:02,  4.83it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 699/1000 [02:28<00:59,  5.02it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 700/1000 [02:29<00:58,  5.17it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 701/1000 [02:29<01:03,  4.69it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 702/1000 [02:29<01:01,  4.86it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 703/1000 [02:29<01:02,  4.76it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 704/1000 [02:29<01:00,  4.92it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 705/1000 [02:30<00:58,  5.03it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 706/1000 [02:30<00:57,  5.15it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 707/1000 [02:30<01:02,  4.72it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 708/1000 [02:30<01:01,  4.72it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 709/1000 [02:30<00:59,  4.86it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 710/1000 [02:31<00:56,  5.09it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 711/1000 [02:31<00:55,  5.25it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 712/1000 [02:31<00:54,  5.31it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 713/1000 [02:31<00:53,  5.35it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 714/1000 [02:31<00:53,  5.37it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 715/1000 [02:32<00:54,  5.21it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 716/1000 [02:32<00:55,  5.12it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 717/1000 [02:32<00:54,  5.22it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 718/1000 [02:32<00:52,  5.34it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 719/1000 [02:32<00:52,  5.40it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 720/1000 [02:32<00:48,  5.73it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 721/1000 [02:33<00:48,  5.73it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 722/1000 [02:33<00:48,  5.67it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 723/1000 [02:33<00:48,  5.66it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 724/1000 [02:33<00:49,  5.56it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 725/1000 [02:33<01:02,  4.43it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 726/1000 [02:34<01:00,  4.54it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 727/1000 [02:34<00:57,  4.79it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 728/1000 [02:34<00:56,  4.83it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 729/1000 [02:34<00:54,  4.97it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 730/1000 [02:35<00:58,  4.60it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 731/1000 [02:35<00:55,  4.89it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 732/1000 [02:35<00:54,  4.87it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 733/1000 [02:35<00:52,  5.11it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 734/1000 [02:35<00:50,  5.25it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 735/1000 [02:35<00:52,  5.04it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 736/1000 [02:36<00:50,  5.22it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 737/1000 [02:36<00:49,  5.29it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 738/1000 [02:36<00:46,  5.63it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 739/1000 [02:36<00:49,  5.22it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 740/1000 [02:36<00:46,  5.58it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 741/1000 [02:37<00:47,  5.48it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 742/1000 [02:37<00:49,  5.25it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 743/1000 [02:37<00:48,  5.28it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 744/1000 [02:37<00:48,  5.29it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 745/1000 [02:37<00:46,  5.44it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 746/1000 [02:37<00:46,  5.43it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 747/1000 [02:38<00:48,  5.25it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 748/1000 [02:38<00:47,  5.30it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 749/1000 [02:38<00:48,  5.15it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 750/1000 [02:38<00:48,  5.19it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 751/1000 [02:38<00:46,  5.31it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 752/1000 [02:39<00:48,  5.12it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 753/1000 [02:39<00:46,  5.27it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 754/1000 [02:39<00:48,  5.06it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 755/1000 [02:39<00:46,  5.24it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 756/1000 [02:39<00:46,  5.26it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 757/1000 [02:40<00:47,  5.15it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 758/1000 [02:40<00:46,  5.24it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 759/1000 [02:40<00:43,  5.56it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 760/1000 [02:40<00:46,  5.17it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 761/1000 [02:40<00:46,  5.19it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 762/1000 [02:41<00:44,  5.30it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 763/1000 [02:41<00:44,  5.35it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 764/1000 [02:41<00:49,  4.79it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 765/1000 [02:41<00:48,  4.80it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 766/1000 [02:41<00:49,  4.74it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 767/1000 [02:42<00:46,  4.97it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 768/1000 [02:42<00:46,  4.95it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 769/1000 [02:42<00:47,  4.83it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 770/1000 [02:42<00:45,  5.07it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 771/1000 [02:42<00:44,  5.19it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 772/1000 [02:43<00:41,  5.55it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 773/1000 [02:43<00:43,  5.22it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 774/1000 [02:43<00:42,  5.26it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 775/1000 [02:43<00:44,  5.09it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 776/1000 [02:43<00:45,  4.91it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 777/1000 [02:44<00:43,  5.09it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 778/1000 [02:44<00:42,  5.25it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 779/1000 [02:44<00:40,  5.40it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 780/1000 [02:44<00:38,  5.71it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 781/1000 [02:44<00:41,  5.28it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 782/1000 [02:44<00:43,  5.01it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 783/1000 [02:45<00:39,  5.44it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 784/1000 [02:45<00:41,  5.26it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 785/1000 [02:45<00:44,  4.78it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 786/1000 [02:45<00:42,  5.00it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 787/1000 [02:45<00:41,  5.13it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 788/1000 [02:46<00:40,  5.28it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 789/1000 [02:46<00:37,  5.63it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 790/1000 [02:46<00:39,  5.33it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 791/1000 [02:46<00:39,  5.31it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 792/1000 [02:46<00:39,  5.32it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 793/1000 [02:47<00:38,  5.39it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 794/1000 [02:47<00:37,  5.43it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 795/1000 [02:47<00:40,  5.11it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 796/1000 [02:47<00:43,  4.66it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 797/1000 [02:47<00:41,  4.87it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 798/1000 [02:48<00:40,  4.98it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 799/1000 [02:48<00:39,  5.10it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 800/1000 [02:48<00:40,  4.93it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 801/1000 [02:48<00:40,  4.87it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 802/1000 [02:48<00:38,  5.08it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 803/1000 [02:49<00:37,  5.24it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 804/1000 [02:49<00:35,  5.57it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 805/1000 [02:49<00:34,  5.60it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 806/1000 [02:49<00:36,  5.30it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 807/1000 [02:49<00:38,  5.07it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 808/1000 [02:50<00:38,  5.01it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 809/1000 [02:50<00:37,  5.11it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 810/1000 [02:50<00:36,  5.23it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 811/1000 [02:50<00:37,  5.09it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 812/1000 [02:50<00:35,  5.22it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 813/1000 [02:50<00:35,  5.30it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 814/1000 [02:51<00:38,  4.80it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 815/1000 [02:51<00:36,  5.00it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 816/1000 [02:51<00:35,  5.19it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 817/1000 [02:51<00:34,  5.26it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 818/1000 [02:51<00:35,  5.11it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 819/1000 [02:52<00:35,  5.03it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 820/1000 [02:52<00:34,  5.15it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 821/1000 [02:52<00:33,  5.30it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 822/1000 [02:52<00:32,  5.41it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 823/1000 [02:52<00:36,  4.87it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 824/1000 [02:53<00:34,  5.09it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 825/1000 [02:53<00:35,  4.93it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 826/1000 [02:53<00:34,  5.02it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 827/1000 [02:53<00:33,  5.18it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 828/1000 [02:53<00:33,  5.20it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 829/1000 [02:54<00:32,  5.33it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 830/1000 [02:54<00:32,  5.19it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 831/1000 [02:54<00:32,  5.22it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 832/1000 [02:54<00:33,  4.98it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 833/1000 [02:54<00:32,  5.15it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 834/1000 [02:55<00:32,  5.04it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 835/1000 [02:55<00:31,  5.22it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 836/1000 [02:55<00:31,  5.23it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 837/1000 [02:55<00:29,  5.58it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 838/1000 [02:55<00:28,  5.62it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 839/1000 [02:55<00:29,  5.54it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 840/1000 [02:56<00:29,  5.49it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 841/1000 [02:56<00:30,  5.23it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 842/1000 [02:56<00:29,  5.34it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 843/1000 [02:56<00:30,  5.20it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 844/1000 [02:56<00:31,  4.96it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 845/1000 [02:57<00:30,  5.07it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 846/1000 [02:57<00:30,  5.02it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 847/1000 [02:57<00:29,  5.19it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 848/1000 [02:57<00:28,  5.29it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 849/1000 [02:57<00:28,  5.30it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 850/1000 [02:58<00:27,  5.40it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 851/1000 [02:58<00:27,  5.45it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 852/1000 [02:58<00:27,  5.48it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 853/1000 [02:58<00:28,  5.18it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 854/1000 [02:58<00:27,  5.33it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 855/1000 [02:59<00:26,  5.37it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 856/1000 [02:59<00:26,  5.35it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 857/1000 [02:59<00:26,  5.44it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 858/1000 [02:59<00:26,  5.46it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 859/1000 [02:59<00:26,  5.25it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 860/1000 [02:59<00:26,  5.28it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 861/1000 [03:00<00:27,  5.14it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 862/1000 [03:00<00:26,  5.30it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 863/1000 [03:00<00:25,  5.31it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 864/1000 [03:00<00:25,  5.40it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 865/1000 [03:00<00:24,  5.48it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 866/1000 [03:01<00:24,  5.54it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 867/1000 [03:01<00:24,  5.46it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 868/1000 [03:01<00:24,  5.47it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 869/1000 [03:01<00:22,  5.79it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 870/1000 [03:01<00:22,  5.69it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 871/1000 [03:01<00:21,  5.93it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 872/1000 [03:02<00:22,  5.77it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 873/1000 [03:02<00:23,  5.43it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 874/1000 [03:02<00:23,  5.46it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 875/1000 [03:02<00:23,  5.28it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 876/1000 [03:02<00:23,  5.39it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 877/1000 [03:03<00:22,  5.36it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 878/1000 [03:03<00:25,  4.83it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 879/1000 [03:03<00:24,  5.04it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 880/1000 [03:03<00:24,  4.87it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 881/1000 [03:03<00:23,  5.07it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 882/1000 [03:04<00:22,  5.20it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 883/1000 [03:04<00:24,  4.73it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 884/1000 [03:04<00:23,  4.97it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 885/1000 [03:04<00:22,  5.08it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 886/1000 [03:04<00:21,  5.19it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 887/1000 [03:05<00:21,  5.27it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 888/1000 [03:05<00:20,  5.34it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 889/1000 [03:05<00:21,  5.16it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 890/1000 [03:05<00:21,  5.07it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 891/1000 [03:05<00:21,  4.99it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 892/1000 [03:06<00:21,  4.93it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 893/1000 [03:06<00:20,  5.11it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 894/1000 [03:06<00:20,  5.17it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 895/1000 [03:06<00:20,  5.25it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 896/1000 [03:06<00:19,  5.30it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 897/1000 [03:06<00:18,  5.46it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 898/1000 [03:07<00:19,  5.28it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 899/1000 [03:07<00:19,  5.32it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 900/1000 [03:07<00:20,  4.77it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 901/1000 [03:07<00:21,  4.69it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 902/1000 [03:08<00:20,  4.74it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 903/1000 [03:08<00:20,  4.79it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 904/1000 [03:08<00:20,  4.74it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 905/1000 [03:08<00:19,  4.97it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 906/1000 [03:08<00:17,  5.37it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 907/1000 [03:08<00:16,  5.74it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 908/1000 [03:09<00:18,  5.05it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 909/1000 [03:09<00:17,  5.16it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 910/1000 [03:09<00:18,  4.94it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 911/1000 [03:09<00:17,  5.14it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 912/1000 [03:09<00:17,  4.95it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 913/1000 [03:10<00:17,  4.89it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 914/1000 [03:10<00:17,  5.05it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 915/1000 [03:10<00:16,  5.16it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 916/1000 [03:10<00:15,  5.32it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 917/1000 [03:10<00:16,  5.18it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 918/1000 [03:11<00:15,  5.33it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 919/1000 [03:11<00:15,  5.37it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 920/1000 [03:11<00:14,  5.43it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 921/1000 [03:11<00:14,  5.52it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 922/1000 [03:11<00:13,  5.60it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 923/1000 [03:12<00:13,  5.66it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 924/1000 [03:12<00:13,  5.70it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 925/1000 [03:12<00:14,  5.34it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 926/1000 [03:12<00:13,  5.44it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 927/1000 [03:12<00:13,  5.48it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 928/1000 [03:12<00:12,  5.79it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 929/1000 [03:13<00:12,  5.69it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 930/1000 [03:13<00:13,  5.33it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 931/1000 [03:13<00:12,  5.33it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 932/1000 [03:13<00:12,  5.45it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 933/1000 [03:13<00:12,  5.52it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 934/1000 [03:14<00:11,  5.52it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 935/1000 [03:14<00:13,  4.94it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 936/1000 [03:14<00:12,  5.05it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 937/1000 [03:14<00:12,  5.18it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 938/1000 [03:14<00:12,  5.10it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 939/1000 [03:15<00:11,  5.21it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 940/1000 [03:15<00:11,  5.29it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 941/1000 [03:15<00:11,  5.13it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 942/1000 [03:15<00:11,  5.23it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 943/1000 [03:15<00:10,  5.33it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 944/1000 [03:15<00:10,  5.38it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 945/1000 [03:16<00:10,  5.42it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 946/1000 [03:16<00:10,  5.38it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 947/1000 [03:16<00:09,  5.35it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 948/1000 [03:16<00:10,  5.08it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 949/1000 [03:16<00:09,  5.21it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 950/1000 [03:17<00:09,  5.32it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 951/1000 [03:17<00:09,  5.07it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 952/1000 [03:17<00:09,  5.20it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 953/1000 [03:17<00:08,  5.27it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 954/1000 [03:17<00:09,  5.05it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 955/1000 [03:18<00:08,  5.43it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 956/1000 [03:18<00:08,  5.47it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 957/1000 [03:18<00:08,  4.88it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 958/1000 [03:18<00:08,  5.07it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 959/1000 [03:18<00:08,  5.01it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 960/1000 [03:19<00:07,  5.15it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 961/1000 [03:19<00:07,  5.26it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 962/1000 [03:19<00:07,  5.35it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 963/1000 [03:19<00:06,  5.42it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 964/1000 [03:19<00:06,  5.50it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 965/1000 [03:19<00:06,  5.46it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 966/1000 [03:20<00:06,  5.45it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 967/1000 [03:20<00:06,  5.49it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 968/1000 [03:20<00:05,  5.49it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 969/1000 [03:20<00:05,  5.46it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 970/1000 [03:20<00:05,  5.49it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 971/1000 [03:21<00:05,  5.50it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 972/1000 [03:21<00:05,  5.54it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 973/1000 [03:21<00:04,  5.52it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 974/1000 [03:21<00:04,  5.55it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 975/1000 [03:21<00:04,  5.53it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 976/1000 [03:21<00:04,  5.52it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 977/1000 [03:22<00:04,  5.29it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 978/1000 [03:22<00:04,  5.18it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 979/1000 [03:22<00:03,  5.28it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 980/1000 [03:22<00:03,  5.41it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 981/1000 [03:22<00:03,  5.45it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 982/1000 [03:23<00:03,  5.43it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 983/1000 [03:23<00:03,  4.86it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 984/1000 [03:23<00:03,  5.05it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 985/1000 [03:23<00:02,  5.17it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 986/1000 [03:23<00:02,  5.29it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 987/1000 [03:24<00:02,  5.20it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 988/1000 [03:24<00:02,  5.32it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 989/1000 [03:24<00:02,  5.19it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 990/1000 [03:24<00:01,  5.29it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 991/1000 [03:24<00:01,  5.37it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 992/1000 [03:25<00:01,  5.21it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 993/1000 [03:25<00:01,  5.58it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 994/1000 [03:25<00:01,  5.32it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 995/1000 [03:25<00:00,  5.40it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 996/1000 [03:25<00:00,  5.38it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 997/1000 [03:25<00:00,  5.46it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 998/1000 [03:26<00:00,  5.22it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 999/1000 [03:26<00:00,  5.34it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [03:26<00:00,  5.40it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [03:26<00:00,  4.84it/s]
DONE (13.89s)
DONE (8.17s)
Computing criterion:   0%|          | 0/20 [00:00<?, ?it/s]Computing criterion:   5%|â–Œ         | 1/20 [00:21<06:44, 21.31s/it]Computing criterion:  10%|â–ˆ         | 2/20 [00:42<06:22, 21.25s/it]Computing criterion:  15%|â–ˆâ–Œ        | 3/20 [01:06<06:24, 22.65s/it]Computing criterion:  20%|â–ˆâ–ˆ        | 4/20 [01:28<05:55, 22.20s/it]Computing criterion:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [01:49<05:29, 21.98s/it]Computing criterion:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [02:11<05:07, 21.94s/it]Computing criterion:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [02:32<04:39, 21.52s/it]Computing criterion:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [02:53<04:14, 21.22s/it]Computing criterion:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [03:13<03:52, 21.12s/it]Computing criterion:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [03:34<03:30, 21.08s/it]Computing criterion:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [03:56<03:11, 21.26s/it]Computing criterion:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [04:17<02:50, 21.28s/it]Computing criterion:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [04:38<02:27, 21.03s/it]Computing criterion:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [04:59<02:05, 20.95s/it]Computing criterion:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [05:19<01:43, 20.64s/it]Computing criterion:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [05:38<01:21, 20.38s/it]Computing criterion:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [05:58<01:00, 20.25s/it]Computing criterion:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [06:18<00:40, 20.19s/it]Computing criterion:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [06:38<00:19, 19.93s/it]Computing criterion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [06:58<00:00, 20.18s/it]Computing criterion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [06:58<00:00, 20.95s/it]
Ref scores:   0%|          | 0/1000 [00:00<?, ?it/s]Ref scores:   0%|          | 1/1000 [00:02<34:41,  2.08s/it]Ref scores:   0%|          | 2/1000 [00:02<15:39,  1.06it/s]Ref scores:   0%|          | 3/1000 [00:02<09:33,  1.74it/s]Ref scores:   0%|          | 4/1000 [00:02<06:31,  2.54it/s]Ref scores:   0%|          | 5/1000 [00:02<05:00,  3.31it/s]Ref scores:   1%|          | 6/1000 [00:02<04:05,  4.05it/s]Ref scores:   1%|          | 7/1000 [00:02<03:53,  4.25it/s]Ref scores:   1%|          | 8/1000 [00:03<03:24,  4.86it/s]Ref scores:   1%|          | 9/1000 [00:03<03:05,  5.35it/s]Ref scores:   1%|          | 10/1000 [00:03<02:51,  5.76it/s]Ref scores:   1%|          | 11/1000 [00:03<02:43,  6.06it/s]Ref scores:   1%|          | 12/1000 [00:03<02:59,  5.50it/s]Ref scores:   1%|â–         | 13/1000 [00:03<02:47,  5.90it/s]Ref scores:   1%|â–         | 14/1000 [00:04<03:00,  5.47it/s]Ref scores:   2%|â–         | 15/1000 [00:04<03:02,  5.40it/s]Ref scores:   2%|â–         | 16/1000 [00:04<03:11,  5.14it/s]Ref scores:   2%|â–         | 17/1000 [00:04<03:27,  4.73it/s]Ref scores:   2%|â–         | 18/1000 [00:04<03:25,  4.78it/s]Ref scores:   2%|â–         | 19/1000 [00:05<03:05,  5.29it/s]Ref scores:   2%|â–         | 20/1000 [00:05<03:11,  5.13it/s]Ref scores:   2%|â–         | 21/1000 [00:05<02:54,  5.60it/s]Ref scores:   2%|â–         | 22/1000 [00:05<02:43,  6.00it/s]Ref scores:   2%|â–         | 23/1000 [00:05<02:27,  6.63it/s]Ref scores:   2%|â–         | 24/1000 [00:05<03:00,  5.40it/s]Ref scores:   2%|â–Ž         | 25/1000 [00:06<03:02,  5.34it/s]Ref scores:   3%|â–Ž         | 26/1000 [00:06<03:10,  5.12it/s]Ref scores:   3%|â–Ž         | 27/1000 [00:06<03:15,  4.98it/s]Ref scores:   3%|â–Ž         | 28/1000 [00:06<02:58,  5.43it/s]Ref scores:   3%|â–Ž         | 29/1000 [00:06<02:47,  5.81it/s]Ref scores:   3%|â–Ž         | 30/1000 [00:07<02:39,  6.09it/s]Ref scores:   3%|â–Ž         | 31/1000 [00:07<02:24,  6.69it/s]Ref scores:   3%|â–Ž         | 32/1000 [00:07<02:23,  6.76it/s]Ref scores:   3%|â–Ž         | 33/1000 [00:07<02:21,  6.86it/s]Ref scores:   3%|â–Ž         | 34/1000 [00:07<02:52,  5.59it/s]Ref scores:   4%|â–Ž         | 35/1000 [00:07<02:42,  5.95it/s]Ref scores:   4%|â–Ž         | 36/1000 [00:07<02:35,  6.21it/s]Ref scores:   4%|â–Ž         | 37/1000 [00:08<02:29,  6.44it/s]Ref scores:   4%|â–         | 38/1000 [00:08<02:26,  6.57it/s]Ref scores:   4%|â–         | 39/1000 [00:08<02:23,  6.69it/s]Ref scores:   4%|â–         | 40/1000 [00:08<02:45,  5.79it/s]Ref scores:   4%|â–         | 41/1000 [00:08<02:50,  5.61it/s]Ref scores:   4%|â–         | 42/1000 [00:09<03:13,  4.96it/s]Ref scores:   4%|â–         | 43/1000 [00:09<03:30,  4.55it/s]Ref scores:   4%|â–         | 44/1000 [00:09<03:31,  4.52it/s]Ref scores:   4%|â–         | 45/1000 [00:09<02:58,  5.35it/s]Ref scores:   5%|â–         | 46/1000 [00:09<02:45,  5.78it/s]Ref scores:   5%|â–         | 47/1000 [00:09<02:36,  6.07it/s]Ref scores:   5%|â–         | 48/1000 [00:10<02:22,  6.70it/s]Ref scores:   5%|â–         | 49/1000 [00:10<02:19,  6.82it/s]Ref scores:   5%|â–Œ         | 50/1000 [00:10<02:18,  6.88it/s]Ref scores:   5%|â–Œ         | 51/1000 [00:10<02:08,  7.38it/s]Ref scores:   5%|â–Œ         | 52/1000 [00:10<02:11,  7.20it/s]Ref scores:   5%|â–Œ         | 53/1000 [00:10<02:45,  5.72it/s]Ref scores:   5%|â–Œ         | 54/1000 [00:11<02:50,  5.55it/s]Ref scores:   6%|â–Œ         | 55/1000 [00:11<02:58,  5.29it/s]Ref scores:   6%|â–Œ         | 56/1000 [00:11<02:37,  6.00it/s]Ref scores:   6%|â–Œ         | 57/1000 [00:11<02:22,  6.62it/s]Ref scores:   6%|â–Œ         | 58/1000 [00:11<02:38,  5.94it/s]Ref scores:   6%|â–Œ         | 59/1000 [00:11<02:31,  6.21it/s]Ref scores:   6%|â–Œ         | 60/1000 [00:12<02:27,  6.39it/s]Ref scores:   6%|â–Œ         | 61/1000 [00:12<02:36,  6.02it/s]Ref scores:   6%|â–Œ         | 62/1000 [00:12<02:29,  6.28it/s]Ref scores:   6%|â–‹         | 63/1000 [00:12<02:25,  6.46it/s]Ref scores:   6%|â–‹         | 64/1000 [00:12<02:22,  6.57it/s]Ref scores:   6%|â–‹         | 65/1000 [00:12<02:19,  6.72it/s]Ref scores:   7%|â–‹         | 66/1000 [00:12<02:17,  6.78it/s]Ref scores:   7%|â–‹         | 67/1000 [00:13<02:30,  6.21it/s]Ref scores:   7%|â–‹         | 68/1000 [00:13<02:25,  6.42it/s]Ref scores:   7%|â–‹         | 69/1000 [00:13<02:13,  6.99it/s]Ref scores:   7%|â–‹         | 70/1000 [00:13<02:13,  6.96it/s]Ref scores:   7%|â–‹         | 71/1000 [00:13<02:30,  6.16it/s]Ref scores:   7%|â–‹         | 72/1000 [00:13<02:25,  6.36it/s]Ref scores:   7%|â–‹         | 73/1000 [00:14<02:21,  6.54it/s]Ref scores:   7%|â–‹         | 74/1000 [00:14<02:19,  6.65it/s]Ref scores:   8%|â–Š         | 75/1000 [00:14<02:38,  5.82it/s]Ref scores:   8%|â–Š         | 76/1000 [00:14<02:30,  6.12it/s]Ref scores:   8%|â–Š         | 77/1000 [00:14<02:25,  6.33it/s]Ref scores:   8%|â–Š         | 78/1000 [00:14<02:13,  6.92it/s]Ref scores:   8%|â–Š         | 79/1000 [00:14<02:12,  6.97it/s]Ref scores:   8%|â–Š         | 80/1000 [00:15<02:12,  6.96it/s]Ref scores:   8%|â–Š         | 81/1000 [00:15<02:25,  6.33it/s]Ref scores:   8%|â–Š         | 82/1000 [00:15<02:21,  6.48it/s]Ref scores:   8%|â–Š         | 83/1000 [00:15<02:15,  6.74it/s]Ref scores:   8%|â–Š         | 84/1000 [00:15<02:14,  6.80it/s]Ref scores:   8%|â–Š         | 85/1000 [00:15<02:35,  5.90it/s]Ref scores:   9%|â–Š         | 86/1000 [00:16<02:41,  5.68it/s]Ref scores:   9%|â–Š         | 87/1000 [00:16<02:52,  5.30it/s]Ref scores:   9%|â–‰         | 88/1000 [00:16<02:39,  5.73it/s]Ref scores:   9%|â–‰         | 89/1000 [00:16<02:31,  6.00it/s]Ref scores:   9%|â–‰         | 90/1000 [00:16<02:44,  5.54it/s]Ref scores:   9%|â–‰         | 91/1000 [00:16<02:34,  5.90it/s]Ref scores:   9%|â–‰         | 92/1000 [00:17<02:27,  6.15it/s]Ref scores:   9%|â–‰         | 93/1000 [00:17<02:52,  5.25it/s]Ref scores:   9%|â–‰         | 94/1000 [00:17<02:56,  5.13it/s]Ref scores:  10%|â–‰         | 95/1000 [00:17<02:42,  5.56it/s]Ref scores:  10%|â–‰         | 96/1000 [00:17<02:51,  5.27it/s]Ref scores:  10%|â–‰         | 97/1000 [00:18<02:39,  5.67it/s]Ref scores:  10%|â–‰         | 98/1000 [00:18<02:30,  5.99it/s]Ref scores:  10%|â–‰         | 99/1000 [00:18<02:55,  5.13it/s]Ref scores:  10%|â–ˆ         | 100/1000 [00:18<03:12,  4.67it/s]Ref scores:  10%|â–ˆ         | 101/1000 [00:18<02:53,  5.17it/s]Ref scores:  10%|â–ˆ         | 102/1000 [00:19<02:40,  5.59it/s]Ref scores:  10%|â–ˆ         | 103/1000 [00:19<02:53,  5.18it/s]Ref scores:  10%|â–ˆ         | 104/1000 [00:19<02:37,  5.70it/s]Ref scores:  10%|â–ˆ         | 105/1000 [00:19<02:45,  5.42it/s]Ref scores:  11%|â–ˆ         | 106/1000 [00:19<02:46,  5.36it/s]Ref scores:  11%|â–ˆ         | 107/1000 [00:19<02:34,  5.77it/s]Ref scores:  11%|â–ˆ         | 108/1000 [00:20<02:44,  5.43it/s]Ref scores:  11%|â–ˆ         | 109/1000 [00:20<02:25,  6.12it/s]Ref scores:  11%|â–ˆ         | 110/1000 [00:20<02:19,  6.36it/s]Ref scores:  11%|â–ˆ         | 111/1000 [00:20<02:16,  6.53it/s]Ref scores:  11%|â–ˆ         | 112/1000 [00:20<02:13,  6.66it/s]Ref scores:  11%|â–ˆâ–        | 113/1000 [00:20<02:27,  6.00it/s]Ref scores:  11%|â–ˆâ–        | 114/1000 [00:21<02:21,  6.24it/s]Ref scores:  12%|â–ˆâ–        | 115/1000 [00:21<02:35,  5.70it/s]Ref scores:  12%|â–ˆâ–        | 116/1000 [00:21<02:18,  6.37it/s]Ref scores:  12%|â–ˆâ–        | 117/1000 [00:21<02:31,  5.83it/s]Ref scores:  12%|â–ˆâ–        | 118/1000 [00:21<02:24,  6.12it/s]Ref scores:  12%|â–ˆâ–        | 119/1000 [00:21<02:16,  6.45it/s]Ref scores:  12%|â–ˆâ–        | 120/1000 [00:22<02:29,  5.87it/s]Ref scores:  12%|â–ˆâ–        | 121/1000 [00:22<02:23,  6.14it/s]Ref scores:  12%|â–ˆâ–        | 122/1000 [00:22<02:18,  6.35it/s]Ref scores:  12%|â–ˆâ–        | 123/1000 [00:22<02:35,  5.63it/s]Ref scores:  12%|â–ˆâ–        | 124/1000 [00:22<02:23,  6.08it/s]Ref scores:  12%|â–ˆâ–Ž        | 125/1000 [00:22<02:18,  6.30it/s]Ref scores:  13%|â–ˆâ–Ž        | 126/1000 [00:22<02:13,  6.53it/s]Ref scores:  13%|â–ˆâ–Ž        | 127/1000 [00:23<02:40,  5.43it/s]Ref scores:  13%|â–ˆâ–Ž        | 128/1000 [00:23<02:29,  5.81it/s]Ref scores:  13%|â–ˆâ–Ž        | 129/1000 [00:23<02:42,  5.36it/s]Ref scores:  13%|â–ˆâ–Ž        | 130/1000 [00:23<03:00,  4.82it/s]Ref scores:  13%|â–ˆâ–Ž        | 131/1000 [00:24<02:43,  5.31it/s]Ref scores:  13%|â–ˆâ–Ž        | 132/1000 [00:24<02:31,  5.75it/s]Ref scores:  13%|â–ˆâ–Ž        | 133/1000 [00:24<02:35,  5.59it/s]Ref scores:  13%|â–ˆâ–Ž        | 134/1000 [00:24<02:25,  5.96it/s]Ref scores:  14%|â–ˆâ–Ž        | 135/1000 [00:24<02:48,  5.15it/s]Ref scores:  14%|â–ˆâ–Ž        | 136/1000 [00:24<02:50,  5.06it/s]Ref scores:  14%|â–ˆâ–Ž        | 137/1000 [00:25<02:53,  4.97it/s]Ref scores:  14%|â–ˆâ–        | 138/1000 [00:25<02:38,  5.43it/s]Ref scores:  14%|â–ˆâ–        | 139/1000 [00:25<02:44,  5.22it/s]Ref scores:  14%|â–ˆâ–        | 140/1000 [00:25<02:32,  5.64it/s]Ref scores:  14%|â–ˆâ–        | 141/1000 [00:25<02:21,  6.08it/s]Ref scores:  14%|â–ˆâ–        | 142/1000 [00:25<02:16,  6.30it/s]Ref scores:  14%|â–ˆâ–        | 143/1000 [00:26<02:24,  5.94it/s]Ref scores:  14%|â–ˆâ–        | 144/1000 [00:26<02:29,  5.73it/s]Ref scores:  14%|â–ˆâ–        | 145/1000 [00:26<02:50,  5.01it/s]Ref scores:  15%|â–ˆâ–        | 146/1000 [00:26<02:35,  5.48it/s]Ref scores:  15%|â–ˆâ–        | 147/1000 [00:26<02:55,  4.85it/s]Ref scores:  15%|â–ˆâ–        | 148/1000 [00:27<02:55,  4.86it/s]Ref scores:  15%|â–ˆâ–        | 149/1000 [00:27<02:54,  4.86it/s]Ref scores:  15%|â–ˆâ–Œ        | 150/1000 [00:27<03:08,  4.51it/s]Ref scores:  15%|â–ˆâ–Œ        | 151/1000 [00:27<02:48,  5.03it/s]Ref scores:  15%|â–ˆâ–Œ        | 152/1000 [00:27<02:34,  5.48it/s]Ref scores:  15%|â–ˆâ–Œ        | 153/1000 [00:28<02:24,  5.85it/s]Ref scores:  15%|â–ˆâ–Œ        | 154/1000 [00:28<02:17,  6.14it/s]Ref scores:  16%|â–ˆâ–Œ        | 155/1000 [00:28<02:12,  6.36it/s]Ref scores:  16%|â–ˆâ–Œ        | 156/1000 [00:28<02:08,  6.57it/s]Ref scores:  16%|â–ˆâ–Œ        | 157/1000 [00:28<02:05,  6.71it/s]Ref scores:  16%|â–ˆâ–Œ        | 158/1000 [00:28<02:04,  6.75it/s]Ref scores:  16%|â–ˆâ–Œ        | 159/1000 [00:28<02:14,  6.26it/s]Ref scores:  16%|â–ˆâ–Œ        | 160/1000 [00:29<02:09,  6.47it/s]Ref scores:  16%|â–ˆâ–Œ        | 161/1000 [00:29<02:22,  5.90it/s]Ref scores:  16%|â–ˆâ–Œ        | 162/1000 [00:29<02:27,  5.69it/s]Ref scores:  16%|â–ˆâ–‹        | 163/1000 [00:29<02:18,  6.04it/s]Ref scores:  16%|â–ˆâ–‹        | 164/1000 [00:29<02:12,  6.29it/s]Ref scores:  16%|â–ˆâ–‹        | 165/1000 [00:29<02:09,  6.46it/s]Ref scores:  17%|â–ˆâ–‹        | 166/1000 [00:30<02:05,  6.62it/s]Ref scores:  17%|â–ˆâ–‹        | 167/1000 [00:30<02:15,  6.15it/s]Ref scores:  17%|â–ˆâ–‹        | 168/1000 [00:30<02:03,  6.75it/s]Ref scores:  17%|â–ˆâ–‹        | 169/1000 [00:30<02:22,  5.84it/s]Ref scores:  17%|â–ˆâ–‹        | 170/1000 [00:30<02:14,  6.18it/s]Ref scores:  17%|â–ˆâ–‹        | 171/1000 [00:30<02:10,  6.37it/s]Ref scores:  17%|â–ˆâ–‹        | 172/1000 [00:31<02:05,  6.59it/s]Ref scores:  17%|â–ˆâ–‹        | 173/1000 [00:31<02:03,  6.68it/s]Ref scores:  17%|â–ˆâ–‹        | 174/1000 [00:31<02:02,  6.74it/s]Ref scores:  18%|â–ˆâ–Š        | 175/1000 [00:31<01:54,  7.22it/s]Ref scores:  18%|â–ˆâ–Š        | 176/1000 [00:31<02:13,  6.17it/s]Ref scores:  18%|â–ˆâ–Š        | 177/1000 [00:31<02:24,  5.71it/s]Ref scores:  18%|â–ˆâ–Š        | 178/1000 [00:32<02:31,  5.43it/s]Ref scores:  18%|â–ˆâ–Š        | 179/1000 [00:32<02:36,  5.25it/s]Ref scores:  18%|â–ˆâ–Š        | 180/1000 [00:32<02:39,  5.13it/s]Ref scores:  18%|â–ˆâ–Š        | 181/1000 [00:32<02:42,  5.04it/s]Ref scores:  18%|â–ˆâ–Š        | 182/1000 [00:32<02:27,  5.54it/s]Ref scores:  18%|â–ˆâ–Š        | 183/1000 [00:32<02:17,  5.93it/s]Ref scores:  18%|â–ˆâ–Š        | 184/1000 [00:33<02:04,  6.57it/s]Ref scores:  18%|â–ˆâ–Š        | 185/1000 [00:33<02:01,  6.71it/s]Ref scores:  19%|â–ˆâ–Š        | 186/1000 [00:33<01:59,  6.80it/s]Ref scores:  19%|â–ˆâ–Š        | 187/1000 [00:33<02:13,  6.08it/s]Ref scores:  19%|â–ˆâ–‰        | 188/1000 [00:33<02:07,  6.36it/s]Ref scores:  19%|â–ˆâ–‰        | 189/1000 [00:33<02:15,  5.98it/s]Ref scores:  19%|â–ˆâ–‰        | 190/1000 [00:34<02:09,  6.26it/s]Ref scores:  19%|â–ˆâ–‰        | 191/1000 [00:34<02:05,  6.44it/s]Ref scores:  19%|â–ˆâ–‰        | 192/1000 [00:34<02:13,  6.05it/s]Ref scores:  19%|â–ˆâ–‰        | 193/1000 [00:34<02:22,  5.67it/s]Ref scores:  19%|â–ˆâ–‰        | 194/1000 [00:34<02:28,  5.41it/s]Ref scores:  20%|â–ˆâ–‰        | 195/1000 [00:34<02:35,  5.19it/s]Ref scores:  20%|â–ˆâ–‰        | 196/1000 [00:35<02:23,  5.62it/s]Ref scores:  20%|â–ˆâ–‰        | 197/1000 [00:35<02:14,  5.99it/s]Ref scores:  20%|â–ˆâ–‰        | 198/1000 [00:35<02:07,  6.29it/s]Ref scores:  20%|â–ˆâ–‰        | 199/1000 [00:35<02:03,  6.49it/s]Ref scores:  20%|â–ˆâ–ˆ        | 200/1000 [00:35<02:17,  5.83it/s]Ref scores:  20%|â–ˆâ–ˆ        | 201/1000 [00:35<02:10,  6.13it/s]Ref scores:  20%|â–ˆâ–ˆ        | 202/1000 [00:36<02:21,  5.64it/s]Ref scores:  20%|â–ˆâ–ˆ        | 203/1000 [00:36<02:28,  5.37it/s]Ref scores:  20%|â–ˆâ–ˆ        | 204/1000 [00:36<02:18,  5.76it/s]Ref scores:  20%|â–ˆâ–ˆ        | 205/1000 [00:36<02:11,  6.07it/s]Ref scores:  21%|â–ˆâ–ˆ        | 206/1000 [00:36<02:24,  5.49it/s]Ref scores:  21%|â–ˆâ–ˆ        | 207/1000 [00:36<02:15,  5.85it/s]Ref scores:  21%|â–ˆâ–ˆ        | 208/1000 [00:37<02:09,  6.14it/s]Ref scores:  21%|â–ˆâ–ˆ        | 209/1000 [00:37<02:02,  6.47it/s]Ref scores:  21%|â–ˆâ–ˆ        | 210/1000 [00:37<02:15,  5.84it/s]Ref scores:  21%|â–ˆâ–ˆ        | 211/1000 [00:37<02:27,  5.36it/s]Ref scores:  21%|â–ˆâ–ˆ        | 212/1000 [00:37<02:27,  5.34it/s]Ref scores:  21%|â–ˆâ–ˆâ–       | 213/1000 [00:38<02:17,  5.74it/s]Ref scores:  21%|â–ˆâ–ˆâ–       | 214/1000 [00:38<02:26,  5.38it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 215/1000 [00:38<02:16,  5.77it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 216/1000 [00:38<02:27,  5.33it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 217/1000 [00:38<02:26,  5.33it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 218/1000 [00:38<02:15,  5.77it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 219/1000 [00:39<02:19,  5.61it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 220/1000 [00:39<02:05,  6.24it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 221/1000 [00:39<02:01,  6.43it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 222/1000 [00:39<01:57,  6.62it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 223/1000 [00:39<02:14,  5.79it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 224/1000 [00:39<02:17,  5.64it/s]Ref scores:  22%|â–ˆâ–ˆâ–Ž       | 225/1000 [00:40<02:09,  5.96it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 226/1000 [00:40<02:04,  6.22it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 227/1000 [00:40<02:11,  5.90it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 228/1000 [00:40<02:20,  5.50it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 229/1000 [00:40<02:11,  5.85it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 230/1000 [00:40<02:16,  5.66it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 231/1000 [00:41<02:22,  5.41it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 232/1000 [00:41<02:12,  5.78it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 233/1000 [00:41<02:06,  6.07it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 234/1000 [00:41<02:01,  6.30it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 235/1000 [00:41<02:13,  5.74it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 236/1000 [00:41<02:05,  6.10it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 237/1000 [00:42<01:53,  6.71it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 238/1000 [00:42<01:52,  6.79it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 239/1000 [00:42<02:05,  6.05it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 240/1000 [00:42<02:00,  6.29it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 241/1000 [00:42<02:08,  5.92it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 242/1000 [00:42<02:12,  5.71it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 243/1000 [00:43<02:31,  4.98it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 244/1000 [00:43<02:18,  5.47it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 245/1000 [00:43<02:09,  5.85it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 246/1000 [00:43<02:20,  5.35it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 247/1000 [00:43<02:28,  5.06it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 248/1000 [00:44<02:14,  5.60it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 249/1000 [00:44<02:06,  5.95it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 250/1000 [00:44<02:00,  6.22it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 251/1000 [00:44<01:57,  6.39it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 252/1000 [00:44<02:08,  5.82it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 253/1000 [00:44<02:20,  5.32it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 254/1000 [00:45<02:27,  5.05it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 255/1000 [00:45<02:15,  5.51it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 256/1000 [00:45<02:31,  4.92it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 257/1000 [00:45<02:31,  4.90it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 258/1000 [00:45<02:17,  5.41it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 259/1000 [00:46<02:17,  5.37it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 260/1000 [00:46<02:08,  5.76it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 261/1000 [00:46<02:12,  5.59it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 262/1000 [00:46<02:04,  5.93it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 263/1000 [00:46<02:08,  5.73it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 264/1000 [00:46<02:12,  5.57it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 265/1000 [00:47<02:21,  5.20it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 266/1000 [00:47<02:10,  5.62it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 267/1000 [00:47<02:19,  5.26it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 268/1000 [00:47<02:08,  5.68it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 269/1000 [00:47<02:01,  6.00it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 270/1000 [00:48<02:10,  5.60it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 271/1000 [00:48<02:15,  5.38it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 272/1000 [00:48<02:05,  5.79it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 273/1000 [00:48<01:58,  6.12it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 274/1000 [00:48<01:54,  6.35it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 275/1000 [00:48<01:51,  6.49it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 276/1000 [00:48<01:49,  6.62it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 277/1000 [00:49<02:02,  5.90it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 278/1000 [00:49<02:13,  5.40it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 279/1000 [00:49<02:22,  5.07it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 280/1000 [00:49<02:10,  5.50it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 281/1000 [00:49<02:02,  5.86it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 282/1000 [00:50<02:11,  5.47it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 283/1000 [00:50<02:16,  5.25it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 284/1000 [00:50<02:20,  5.09it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 285/1000 [00:50<02:09,  5.54it/s]Ref scores:  29%|â–ˆâ–ˆâ–Š       | 286/1000 [00:50<02:14,  5.29it/s]Ref scores:  29%|â–ˆâ–ˆâ–Š       | 287/1000 [00:51<02:18,  5.16it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 288/1000 [00:51<02:24,  4.94it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 289/1000 [00:51<02:11,  5.39it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 290/1000 [00:51<02:19,  5.07it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 291/1000 [00:51<02:23,  4.94it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 292/1000 [00:52<02:04,  5.67it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 293/1000 [00:52<02:10,  5.40it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 294/1000 [00:52<02:01,  5.81it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 295/1000 [00:52<01:55,  6.08it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 296/1000 [00:52<01:58,  5.95it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 297/1000 [00:52<01:53,  6.21it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 298/1000 [00:52<01:50,  6.38it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 299/1000 [00:53<01:46,  6.59it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 300/1000 [00:53<01:57,  5.94it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 301/1000 [00:53<01:51,  6.26it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 302/1000 [00:53<02:04,  5.61it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 303/1000 [00:53<01:57,  5.94it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 304/1000 [00:54<02:08,  5.43it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 305/1000 [00:54<01:59,  5.83it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 306/1000 [00:54<01:52,  6.15it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 307/1000 [00:54<02:03,  5.63it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 308/1000 [00:54<01:56,  5.96it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 309/1000 [00:54<01:50,  6.24it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 310/1000 [00:55<01:54,  6.05it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 311/1000 [00:55<01:49,  6.29it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 312/1000 [00:55<01:59,  5.75it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆâ–      | 313/1000 [00:55<01:52,  6.09it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆâ–      | 314/1000 [00:55<01:48,  6.30it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 316/1000 [00:55<01:34,  7.20it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 317/1000 [00:56<01:46,  6.40it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 318/1000 [00:56<01:56,  5.86it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 319/1000 [00:56<01:59,  5.68it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 320/1000 [00:56<01:53,  6.00it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 321/1000 [00:56<01:48,  6.24it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 322/1000 [00:56<01:54,  5.91it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 323/1000 [00:57<02:11,  5.15it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 324/1000 [00:57<02:01,  5.58it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 325/1000 [00:57<01:53,  5.93it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 326/1000 [00:57<01:48,  6.19it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 327/1000 [00:57<01:39,  6.77it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 328/1000 [00:57<01:38,  6.85it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 329/1000 [00:58<01:50,  6.05it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 330/1000 [00:58<01:46,  6.31it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 331/1000 [00:58<01:42,  6.54it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 332/1000 [00:58<01:40,  6.64it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 333/1000 [00:58<01:38,  6.77it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 334/1000 [00:58<01:36,  6.88it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 335/1000 [00:59<01:57,  5.65it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 336/1000 [00:59<01:44,  6.35it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 337/1000 [00:59<01:41,  6.53it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 338/1000 [00:59<01:33,  7.09it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 339/1000 [00:59<01:47,  6.16it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 340/1000 [00:59<01:43,  6.40it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 341/1000 [00:59<01:40,  6.55it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 342/1000 [01:00<01:38,  6.70it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 343/1000 [01:00<01:37,  6.74it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 344/1000 [01:00<01:30,  7.24it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 345/1000 [01:00<01:30,  7.22it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 346/1000 [01:00<01:43,  6.31it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 347/1000 [01:00<01:40,  6.52it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 348/1000 [01:00<01:38,  6.64it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 349/1000 [01:01<01:48,  5.99it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 350/1000 [01:01<01:43,  6.26it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 351/1000 [01:01<01:53,  5.73it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 352/1000 [01:01<02:09,  4.99it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 353/1000 [01:01<01:58,  5.44it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 354/1000 [01:02<01:50,  5.83it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 355/1000 [01:02<01:45,  6.14it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 356/1000 [01:02<01:53,  5.70it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 357/1000 [01:02<01:46,  6.05it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 358/1000 [01:02<01:42,  6.27it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 359/1000 [01:02<01:39,  6.46it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 360/1000 [01:03<01:36,  6.61it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 361/1000 [01:03<01:35,  6.72it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 362/1000 [01:03<01:33,  6.84it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 363/1000 [01:03<01:32,  6.86it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 364/1000 [01:03<01:31,  6.93it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 365/1000 [01:03<01:31,  6.94it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 366/1000 [01:03<01:40,  6.34it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 367/1000 [01:04<01:37,  6.49it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 368/1000 [01:04<01:35,  6.59it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 369/1000 [01:04<01:48,  5.82it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 370/1000 [01:04<01:42,  6.13it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 371/1000 [01:04<01:38,  6.37it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 372/1000 [01:04<01:35,  6.57it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 373/1000 [01:05<01:34,  6.66it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 374/1000 [01:05<01:33,  6.71it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 375/1000 [01:05<01:32,  6.79it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 376/1000 [01:05<01:30,  6.87it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 377/1000 [01:05<01:30,  6.86it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 378/1000 [01:05<01:30,  6.89it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 379/1000 [01:05<01:29,  6.90it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 380/1000 [01:06<01:29,  6.95it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 381/1000 [01:06<01:37,  6.35it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 382/1000 [01:06<01:43,  5.99it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 383/1000 [01:06<01:50,  5.57it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 384/1000 [01:06<01:43,  5.93it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 385/1000 [01:06<01:45,  5.84it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 386/1000 [01:07<01:40,  6.12it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 387/1000 [01:07<01:36,  6.34it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 388/1000 [01:07<01:47,  5.69it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 389/1000 [01:07<01:53,  5.37it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 390/1000 [01:07<01:57,  5.18it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 391/1000 [01:08<02:00,  5.05it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 392/1000 [01:08<01:50,  5.50it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 393/1000 [01:08<01:55,  5.24it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 394/1000 [01:08<02:02,  4.97it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 395/1000 [01:08<01:59,  5.07it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 396/1000 [01:08<01:49,  5.52it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 397/1000 [01:09<01:56,  5.19it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 398/1000 [01:09<01:46,  5.64it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 399/1000 [01:09<01:35,  6.32it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 400/1000 [01:09<01:44,  5.73it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 401/1000 [01:09<01:33,  6.40it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 402/1000 [01:09<01:30,  6.60it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 403/1000 [01:10<01:43,  5.79it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 404/1000 [01:10<01:37,  6.10it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 405/1000 [01:10<01:42,  5.82it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 406/1000 [01:10<01:45,  5.63it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 407/1000 [01:10<01:39,  5.96it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 408/1000 [01:10<01:43,  5.72it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 409/1000 [01:11<01:37,  6.06it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 410/1000 [01:11<01:33,  6.29it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 411/1000 [01:11<01:25,  6.86it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 412/1000 [01:11<01:37,  6.03it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 413/1000 [01:11<01:45,  5.57it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 414/1000 [01:12<01:50,  5.30it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 415/1000 [01:12<01:42,  5.72it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 416/1000 [01:12<01:44,  5.58it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 417/1000 [01:12<01:38,  5.93it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 418/1000 [01:12<01:33,  6.24it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 419/1000 [01:12<01:38,  5.91it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 420/1000 [01:13<01:44,  5.55it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 421/1000 [01:13<01:37,  5.91it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 422/1000 [01:13<01:41,  5.71it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 423/1000 [01:13<01:35,  6.03it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 424/1000 [01:13<01:45,  5.48it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 425/1000 [01:13<01:37,  5.88it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 426/1000 [01:14<01:52,  5.12it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 427/1000 [01:14<01:42,  5.57it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 428/1000 [01:14<01:36,  5.92it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 429/1000 [01:14<01:32,  6.19it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 430/1000 [01:14<01:27,  6.51it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 431/1000 [01:14<01:25,  6.67it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 432/1000 [01:14<01:23,  6.77it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 433/1000 [01:15<01:23,  6.78it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 434/1000 [01:15<01:22,  6.85it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 435/1000 [01:15<01:33,  6.03it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 436/1000 [01:15<01:29,  6.27it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 437/1000 [01:15<01:37,  5.75it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 438/1000 [01:16<01:40,  5.58it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 439/1000 [01:16<01:45,  5.34it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 440/1000 [01:16<01:36,  5.78it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 441/1000 [01:16<01:43,  5.40it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 442/1000 [01:16<01:47,  5.19it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 443/1000 [01:17<01:49,  5.09it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 444/1000 [01:17<01:51,  5.01it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 445/1000 [01:17<01:54,  4.86it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 446/1000 [01:17<01:56,  4.77it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 447/1000 [01:17<01:44,  5.28it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 448/1000 [01:17<01:37,  5.67it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 449/1000 [01:18<01:26,  6.36it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 450/1000 [01:18<01:19,  6.92it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 451/1000 [01:18<01:19,  6.93it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 452/1000 [01:18<01:37,  5.61it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 453/1000 [01:18<01:31,  5.99it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 454/1000 [01:18<01:27,  6.24it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 455/1000 [01:18<01:25,  6.39it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 456/1000 [01:19<01:23,  6.54it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 457/1000 [01:19<01:40,  5.39it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 458/1000 [01:19<01:34,  5.76it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 459/1000 [01:19<01:29,  6.07it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 460/1000 [01:19<01:36,  5.60it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 461/1000 [01:20<01:30,  5.94it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 462/1000 [01:20<01:21,  6.59it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 463/1000 [01:20<01:20,  6.67it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 464/1000 [01:20<01:24,  6.32it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 465/1000 [01:20<01:22,  6.48it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 466/1000 [01:20<01:28,  6.04it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 467/1000 [01:20<01:24,  6.30it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 468/1000 [01:21<01:40,  5.31it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 469/1000 [01:21<01:42,  5.17it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 470/1000 [01:21<01:34,  5.61it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 471/1000 [01:21<01:29,  5.92it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 472/1000 [01:21<01:42,  5.13it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 473/1000 [01:22<01:40,  5.27it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 474/1000 [01:22<01:33,  5.65it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 475/1000 [01:22<01:27,  5.98it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 476/1000 [01:22<01:23,  6.26it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 477/1000 [01:22<01:28,  5.92it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 478/1000 [01:22<01:33,  5.58it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 479/1000 [01:23<01:27,  5.93it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 480/1000 [01:23<01:33,  5.57it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 481/1000 [01:23<01:36,  5.36it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 482/1000 [01:23<01:29,  5.76it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 483/1000 [01:23<01:24,  6.10it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 484/1000 [01:23<01:21,  6.32it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 485/1000 [01:24<01:19,  6.47it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 486/1000 [01:24<01:28,  5.82it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 487/1000 [01:24<01:34,  5.44it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 488/1000 [01:24<01:28,  5.81it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 489/1000 [01:24<01:23,  6.13it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 490/1000 [01:24<01:15,  6.74it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 491/1000 [01:25<01:22,  6.19it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 492/1000 [01:25<01:19,  6.40it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 493/1000 [01:25<01:24,  6.02it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 494/1000 [01:25<01:27,  5.78it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 495/1000 [01:25<01:22,  6.12it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 496/1000 [01:25<01:28,  5.67it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 497/1000 [01:26<01:24,  5.99it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 498/1000 [01:26<01:29,  5.59it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 499/1000 [01:26<01:31,  5.47it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 500/1000 [01:26<01:25,  5.85it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 501/1000 [01:26<01:21,  6.14it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 502/1000 [01:27<01:25,  5.84it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 503/1000 [01:27<01:21,  6.13it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 504/1000 [01:27<01:18,  6.33it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 505/1000 [01:27<01:16,  6.50it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 506/1000 [01:27<01:23,  5.89it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 507/1000 [01:27<01:19,  6.20it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 508/1000 [01:27<01:17,  6.37it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 509/1000 [01:28<01:26,  5.70it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 510/1000 [01:28<01:30,  5.42it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 511/1000 [01:28<01:24,  5.80it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 512/1000 [01:28<01:20,  6.10it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 513/1000 [01:28<01:16,  6.34it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 514/1000 [01:29<01:24,  5.75it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 515/1000 [01:29<01:19,  6.11it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 516/1000 [01:29<01:27,  5.51it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 517/1000 [01:29<01:22,  5.87it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 518/1000 [01:29<01:18,  6.17it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 519/1000 [01:29<01:22,  5.86it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 520/1000 [01:30<01:34,  5.08it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 521/1000 [01:30<01:35,  5.02it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 522/1000 [01:30<01:27,  5.46it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 523/1000 [01:30<01:30,  5.26it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 524/1000 [01:30<01:23,  5.67it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 525/1000 [01:30<01:18,  6.03it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 526/1000 [01:31<01:24,  5.62it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 527/1000 [01:31<01:19,  5.97it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 528/1000 [01:31<01:15,  6.27it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 529/1000 [01:31<01:28,  5.32it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 530/1000 [01:31<01:22,  5.73it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 531/1000 [01:31<01:17,  6.04it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 532/1000 [01:32<01:13,  6.33it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 533/1000 [01:32<01:20,  5.82it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 534/1000 [01:32<01:25,  5.45it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 535/1000 [01:32<01:28,  5.23it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 536/1000 [01:32<01:22,  5.64it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 537/1000 [01:33<01:26,  5.35it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 538/1000 [01:33<01:29,  5.18it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 539/1000 [01:33<01:37,  4.73it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 540/1000 [01:33<01:43,  4.45it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 541/1000 [01:33<01:32,  4.98it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 542/1000 [01:34<01:24,  5.44it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 543/1000 [01:34<01:24,  5.38it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 544/1000 [01:34<01:19,  5.76it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 545/1000 [01:34<01:15,  6.05it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 546/1000 [01:34<01:28,  5.15it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 547/1000 [01:35<01:31,  4.96it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 548/1000 [01:35<01:23,  5.43it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 549/1000 [01:35<01:17,  5.81it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 550/1000 [01:35<01:22,  5.44it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 551/1000 [01:35<01:17,  5.80it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 552/1000 [01:35<01:19,  5.60it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 553/1000 [01:36<01:14,  5.97it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 554/1000 [01:36<01:11,  6.21it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 555/1000 [01:36<01:17,  5.75it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 556/1000 [01:36<01:13,  6.04it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 557/1000 [01:36<01:15,  5.90it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 558/1000 [01:36<01:07,  6.53it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 559/1000 [01:36<01:06,  6.65it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 560/1000 [01:37<01:11,  6.16it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 561/1000 [01:37<01:08,  6.41it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 562/1000 [01:37<01:06,  6.56it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 563/1000 [01:37<01:05,  6.64it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 564/1000 [01:37<01:04,  6.76it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 565/1000 [01:38<01:18,  5.57it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 566/1000 [01:38<01:23,  5.19it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 567/1000 [01:38<01:13,  5.92it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 568/1000 [01:38<01:24,  5.10it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 569/1000 [01:38<01:25,  5.04it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 570/1000 [01:38<01:18,  5.49it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 571/1000 [01:39<01:13,  5.86it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 572/1000 [01:39<01:19,  5.39it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 573/1000 [01:39<01:21,  5.22it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 574/1000 [01:39<01:24,  5.03it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 575/1000 [01:39<01:25,  4.94it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 576/1000 [01:40<01:18,  5.41it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 577/1000 [01:40<01:12,  5.81it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 578/1000 [01:40<01:17,  5.48it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 579/1000 [01:40<01:17,  5.40it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 580/1000 [01:40<01:12,  5.80it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 581/1000 [01:40<01:17,  5.42it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 582/1000 [01:41<01:10,  5.94it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 583/1000 [01:41<01:11,  5.85it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 584/1000 [01:41<01:26,  4.80it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 585/1000 [01:41<01:26,  4.80it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 586/1000 [01:41<01:14,  5.57it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 587/1000 [01:42<01:18,  5.27it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 588/1000 [01:42<01:12,  5.69it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 589/1000 [01:42<01:08,  6.00it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 590/1000 [01:42<01:05,  6.26it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 591/1000 [01:42<01:03,  6.40it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 592/1000 [01:42<01:02,  6.52it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 593/1000 [01:43<01:09,  5.88it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 594/1000 [01:43<01:05,  6.20it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 595/1000 [01:43<00:59,  6.79it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 596/1000 [01:43<00:58,  6.97it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 597/1000 [01:43<00:57,  7.00it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 598/1000 [01:43<00:57,  6.99it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 599/1000 [01:43<00:57,  6.97it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 600/1000 [01:44<00:57,  6.98it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 601/1000 [01:44<00:56,  7.01it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 602/1000 [01:44<00:57,  6.98it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 603/1000 [01:44<00:57,  6.92it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 604/1000 [01:44<00:57,  6.89it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 605/1000 [01:44<01:01,  6.46it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 606/1000 [01:44<01:04,  6.07it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 607/1000 [01:45<01:11,  5.51it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 608/1000 [01:45<01:06,  5.86it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 609/1000 [01:45<01:03,  6.12it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 611/1000 [01:45<01:01,  6.32it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 612/1000 [01:45<00:59,  6.49it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 613/1000 [01:46<01:05,  5.94it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 614/1000 [01:46<01:06,  5.77it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 615/1000 [01:46<01:03,  6.08it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 616/1000 [01:46<01:07,  5.66it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 617/1000 [01:46<01:04,  5.97it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 618/1000 [01:47<01:08,  5.61it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 619/1000 [01:47<01:03,  6.04it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 620/1000 [01:47<01:05,  5.78it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 621/1000 [01:47<01:09,  5.44it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 622/1000 [01:47<01:13,  5.14it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 623/1000 [01:47<01:16,  4.94it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 624/1000 [01:48<01:06,  5.68it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 625/1000 [01:48<01:02,  6.00it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 626/1000 [01:48<00:59,  6.29it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 627/1000 [01:48<00:57,  6.51it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 628/1000 [01:48<00:56,  6.62it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 629/1000 [01:48<01:00,  6.16it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 630/1000 [01:49<01:06,  5.55it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 631/1000 [01:49<01:10,  5.27it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 632/1000 [01:49<01:04,  5.69it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 633/1000 [01:49<01:01,  6.00it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 634/1000 [01:49<00:58,  6.28it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 635/1000 [01:49<00:56,  6.44it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 636/1000 [01:50<00:55,  6.57it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 637/1000 [01:50<01:01,  5.92it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 638/1000 [01:50<00:58,  6.21it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 639/1000 [01:50<01:03,  5.72it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 640/1000 [01:50<01:06,  5.44it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 641/1000 [01:50<01:08,  5.20it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 642/1000 [01:51<01:03,  5.62it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 643/1000 [01:51<00:59,  5.98it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 644/1000 [01:51<01:05,  5.47it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 645/1000 [01:51<01:00,  5.85it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 646/1000 [01:51<00:57,  6.12it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 647/1000 [01:51<00:55,  6.34it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 648/1000 [01:52<00:54,  6.49it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 649/1000 [01:52<01:01,  5.75it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 650/1000 [01:52<00:57,  6.07it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 651/1000 [01:52<00:55,  6.30it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 652/1000 [01:52<00:53,  6.48it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 653/1000 [01:52<00:52,  6.64it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 654/1000 [01:53<00:51,  6.74it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 655/1000 [01:53<00:58,  5.88it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 656/1000 [01:53<01:04,  5.37it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 657/1000 [01:53<00:59,  5.79it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 658/1000 [01:53<00:55,  6.12it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 659/1000 [01:54<01:05,  5.24it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 660/1000 [01:54<00:56,  5.97it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 661/1000 [01:54<00:54,  6.21it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 662/1000 [01:54<00:52,  6.42it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 663/1000 [01:54<01:02,  5.39it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 664/1000 [01:54<00:58,  5.78it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 665/1000 [01:55<01:01,  5.45it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 666/1000 [01:55<00:57,  5.83it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 667/1000 [01:55<01:05,  5.09it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 668/1000 [01:55<01:11,  4.66it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 669/1000 [01:55<01:03,  5.20it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 670/1000 [01:55<00:58,  5.65it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 671/1000 [01:56<01:02,  5.26it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 672/1000 [01:56<01:04,  5.10it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 673/1000 [01:56<00:59,  5.52it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 674/1000 [01:56<01:02,  5.21it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 675/1000 [01:56<00:57,  5.62it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 676/1000 [01:57<00:54,  5.96it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 677/1000 [01:57<00:57,  5.60it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 678/1000 [01:57<00:53,  6.00it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 679/1000 [01:57<00:51,  6.28it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 680/1000 [01:57<00:49,  6.49it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 681/1000 [01:57<00:54,  5.89it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 682/1000 [01:58<01:01,  5.15it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 683/1000 [01:58<01:00,  5.23it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 684/1000 [01:58<00:55,  5.69it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 685/1000 [01:58<00:52,  6.03it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 686/1000 [01:58<00:49,  6.29it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 687/1000 [01:58<00:51,  6.07it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 688/1000 [01:59<00:49,  6.31it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 689/1000 [01:59<00:53,  5.77it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 690/1000 [01:59<00:58,  5.33it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 691/1000 [01:59<00:52,  5.88it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 692/1000 [01:59<00:57,  5.40it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 693/1000 [01:59<00:53,  5.79it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 694/1000 [02:00<00:55,  5.47it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 695/1000 [02:00<00:52,  5.83it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 696/1000 [02:00<00:56,  5.34it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 697/1000 [02:00<00:52,  5.74it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 698/1000 [02:00<00:49,  6.04it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 699/1000 [02:00<00:47,  6.29it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 700/1000 [02:01<00:53,  5.61it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 701/1000 [02:01<00:55,  5.36it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 702/1000 [02:01<00:51,  5.74it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 703/1000 [02:01<00:48,  6.06it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 704/1000 [02:01<00:53,  5.50it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 705/1000 [02:02<00:50,  5.86it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 706/1000 [02:02<00:53,  5.49it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 707/1000 [02:02<00:50,  5.85it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 708/1000 [02:02<00:47,  6.12it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 709/1000 [02:02<00:45,  6.40it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 710/1000 [02:02<00:44,  6.52it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 711/1000 [02:02<00:43,  6.64it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 712/1000 [02:03<00:43,  6.68it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 713/1000 [02:03<00:48,  5.86it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 714/1000 [02:03<00:46,  6.13it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 715/1000 [02:03<00:44,  6.37it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 716/1000 [02:03<00:43,  6.55it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 717/1000 [02:03<00:45,  6.26it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 718/1000 [02:04<00:41,  6.86it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 719/1000 [02:04<00:44,  6.28it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 720/1000 [02:04<00:43,  6.46it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 721/1000 [02:04<00:39,  7.00it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 722/1000 [02:04<00:37,  7.45it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 723/1000 [02:04<00:38,  7.26it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 724/1000 [02:04<00:38,  7.20it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 725/1000 [02:05<00:44,  6.25it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 726/1000 [02:05<00:48,  5.65it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 727/1000 [02:05<00:45,  6.00it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 728/1000 [02:05<00:48,  5.56it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 729/1000 [02:05<00:51,  5.23it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 730/1000 [02:06<00:47,  5.67it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 731/1000 [02:06<00:51,  5.24it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 732/1000 [02:06<00:47,  5.67it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 733/1000 [02:06<00:42,  6.33it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 734/1000 [02:06<00:45,  5.80it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 735/1000 [02:06<00:43,  6.10it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 736/1000 [02:07<00:45,  5.82it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 737/1000 [02:07<00:49,  5.36it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 738/1000 [02:07<00:48,  5.36it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 739/1000 [02:07<00:50,  5.21it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 740/1000 [02:07<00:46,  5.63it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 741/1000 [02:07<00:41,  6.30it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 742/1000 [02:08<00:43,  5.93it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 743/1000 [02:08<00:41,  6.19it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 744/1000 [02:08<00:39,  6.42it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 745/1000 [02:08<00:47,  5.37it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 746/1000 [02:08<00:49,  5.12it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 747/1000 [02:09<00:45,  5.57it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 748/1000 [02:09<00:42,  5.90it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 749/1000 [02:09<00:46,  5.40it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 750/1000 [02:09<00:40,  6.10it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 751/1000 [02:09<00:37,  6.71it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 752/1000 [02:09<00:41,  6.02it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 753/1000 [02:10<00:42,  5.79it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 754/1000 [02:10<00:43,  5.64it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 755/1000 [02:10<00:46,  5.31it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 756/1000 [02:10<00:42,  5.71it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 757/1000 [02:10<00:46,  5.25it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 758/1000 [02:10<00:42,  5.67it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 759/1000 [02:11<00:43,  5.55it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 760/1000 [02:11<00:40,  5.91it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 761/1000 [02:11<00:41,  5.69it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 762/1000 [02:11<00:39,  6.00it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 763/1000 [02:11<00:37,  6.25it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 764/1000 [02:11<00:39,  5.93it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 765/1000 [02:12<00:40,  5.85it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 766/1000 [02:12<00:41,  5.69it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 767/1000 [02:12<00:36,  6.35it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 768/1000 [02:12<00:35,  6.50it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 769/1000 [02:12<00:34,  6.63it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 770/1000 [02:12<00:32,  7.15it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 771/1000 [02:12<00:32,  7.12it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 772/1000 [02:13<00:36,  6.23it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 773/1000 [02:13<00:35,  6.44it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 774/1000 [02:13<00:39,  5.71it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 775/1000 [02:13<00:41,  5.46it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 776/1000 [02:13<00:38,  5.86it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 777/1000 [02:14<00:40,  5.52it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 778/1000 [02:14<00:42,  5.27it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 779/1000 [02:14<00:38,  5.67it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 780/1000 [02:14<00:36,  6.04it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 781/1000 [02:14<00:34,  6.28it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 782/1000 [02:14<00:33,  6.49it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 783/1000 [02:15<00:32,  6.60it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 784/1000 [02:15<00:35,  6.16it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 785/1000 [02:15<00:31,  6.76it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 786/1000 [02:15<00:36,  5.88it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 787/1000 [02:15<00:38,  5.54it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 788/1000 [02:15<00:35,  5.92it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 789/1000 [02:16<00:37,  5.69it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 790/1000 [02:16<00:38,  5.43it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 791/1000 [02:16<00:38,  5.39it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 792/1000 [02:16<00:39,  5.24it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 793/1000 [02:16<00:36,  5.74it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 794/1000 [02:17<00:36,  5.59it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 795/1000 [02:17<00:32,  6.27it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 796/1000 [02:17<00:31,  6.43it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 797/1000 [02:17<00:30,  6.57it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 798/1000 [02:17<00:30,  6.67it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 799/1000 [02:17<00:29,  6.75it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 800/1000 [02:17<00:29,  6.78it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 801/1000 [02:18<00:29,  6.86it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 802/1000 [02:18<00:28,  6.94it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 803/1000 [02:18<00:28,  6.91it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 804/1000 [02:18<00:28,  6.92it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 805/1000 [02:18<00:28,  6.82it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 806/1000 [02:18<00:30,  6.28it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 807/1000 [02:18<00:29,  6.46it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 808/1000 [02:19<00:31,  6.05it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 809/1000 [02:19<00:34,  5.53it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 810/1000 [02:19<00:32,  5.90it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 811/1000 [02:19<00:33,  5.72it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 812/1000 [02:19<00:31,  6.06it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 813/1000 [02:19<00:29,  6.30it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 814/1000 [02:20<00:28,  6.46it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 815/1000 [02:20<00:31,  5.87it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 816/1000 [02:20<00:30,  6.13it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 817/1000 [02:20<00:28,  6.40it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 818/1000 [02:20<00:27,  6.53it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 819/1000 [02:20<00:30,  5.89it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 820/1000 [02:21<00:32,  5.51it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 821/1000 [02:21<00:30,  5.91it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 822/1000 [02:21<00:28,  6.20it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 823/1000 [02:21<00:27,  6.43it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 824/1000 [02:21<00:30,  5.68it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 825/1000 [02:22<00:32,  5.40it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 826/1000 [02:22<00:28,  6.12it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 827/1000 [02:22<00:25,  6.74it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 828/1000 [02:22<00:23,  7.40it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 829/1000 [02:22<00:23,  7.22it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 830/1000 [02:22<00:26,  6.49it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 831/1000 [02:22<00:25,  6.63it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 832/1000 [02:23<00:28,  5.91it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 833/1000 [02:23<00:30,  5.51it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 834/1000 [02:23<00:31,  5.30it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 835/1000 [02:23<00:28,  5.72it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 836/1000 [02:23<00:30,  5.31it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 837/1000 [02:24<00:32,  5.04it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 838/1000 [02:24<00:32,  4.99it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 839/1000 [02:24<00:29,  5.47it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 840/1000 [02:24<00:27,  5.85it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 841/1000 [02:24<00:28,  5.52it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 842/1000 [02:24<00:30,  5.26it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 843/1000 [02:25<00:27,  5.67it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 844/1000 [02:25<00:26,  5.98it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 845/1000 [02:25<00:27,  5.56it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 846/1000 [02:25<00:24,  6.24it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 847/1000 [02:25<00:27,  5.55it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 848/1000 [02:25<00:25,  5.92it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 849/1000 [02:26<00:24,  6.17it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 850/1000 [02:26<00:23,  6.36it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 851/1000 [02:26<00:22,  6.53it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 852/1000 [02:26<00:22,  6.64it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 853/1000 [02:26<00:24,  5.98it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 854/1000 [02:26<00:25,  5.75it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 855/1000 [02:27<00:23,  6.04it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 856/1000 [02:27<00:22,  6.31it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 857/1000 [02:27<00:22,  6.47it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 858/1000 [02:27<00:24,  5.84it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 859/1000 [02:27<00:22,  6.17it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 860/1000 [02:27<00:21,  6.43it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 861/1000 [02:28<00:24,  5.70it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 862/1000 [02:28<00:22,  6.03it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 863/1000 [02:28<00:21,  6.25it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 864/1000 [02:28<00:25,  5.30it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 865/1000 [02:28<00:26,  5.17it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 866/1000 [02:28<00:26,  5.06it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 867/1000 [02:29<00:24,  5.50it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 868/1000 [02:29<00:21,  6.19it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 869/1000 [02:29<00:20,  6.41it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 870/1000 [02:29<00:21,  6.02it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 871/1000 [02:29<00:20,  6.26it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 872/1000 [02:29<00:19,  6.42it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 873/1000 [02:30<00:19,  6.57it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 874/1000 [02:30<00:18,  6.67it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 875/1000 [02:30<00:21,  5.89it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 876/1000 [02:30<00:20,  6.18it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 877/1000 [02:30<00:21,  5.70it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 878/1000 [02:30<00:20,  6.02it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 879/1000 [02:31<00:23,  5.12it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 880/1000 [02:31<00:21,  5.57it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 881/1000 [02:31<00:19,  6.24it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 882/1000 [02:31<00:17,  6.84it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 883/1000 [02:31<00:17,  6.85it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 884/1000 [02:31<00:16,  6.84it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 885/1000 [02:31<00:16,  6.87it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 886/1000 [02:32<00:18,  6.02it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 887/1000 [02:32<00:19,  5.78it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 888/1000 [02:32<00:19,  5.62it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 889/1000 [02:32<00:22,  4.96it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 890/1000 [02:32<00:19,  5.71it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 891/1000 [02:33<00:17,  6.40it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 892/1000 [02:33<00:15,  6.98it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 893/1000 [02:33<00:17,  6.13it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 894/1000 [02:33<00:16,  6.33it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 895/1000 [02:33<00:18,  5.76it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 896/1000 [02:33<00:19,  5.36it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 897/1000 [02:34<00:19,  5.35it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 898/1000 [02:34<00:17,  5.76it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 899/1000 [02:34<00:16,  6.05it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 900/1000 [02:34<00:18,  5.48it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 901/1000 [02:34<00:18,  5.31it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 902/1000 [02:34<00:17,  5.74it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 903/1000 [02:35<00:18,  5.28it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 904/1000 [02:35<00:18,  5.15it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 905/1000 [02:35<00:16,  5.60it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 906/1000 [02:35<00:15,  5.95it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 907/1000 [02:35<00:17,  5.46it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 908/1000 [02:36<00:17,  5.41it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 909/1000 [02:36<00:15,  5.78it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 910/1000 [02:36<00:14,  6.12it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 911/1000 [02:36<00:13,  6.38it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 912/1000 [02:36<00:15,  5.77it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 913/1000 [02:36<00:14,  6.11it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 914/1000 [02:37<00:13,  6.36it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 915/1000 [02:37<00:13,  6.54it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 916/1000 [02:37<00:12,  6.70it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 917/1000 [02:37<00:11,  7.22it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 918/1000 [02:37<00:11,  7.18it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 919/1000 [02:37<00:11,  7.17it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 920/1000 [02:37<00:11,  6.68it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 921/1000 [02:37<00:11,  6.80it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 922/1000 [02:38<00:11,  6.91it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 923/1000 [02:38<00:12,  6.15it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 924/1000 [02:38<00:11,  6.44it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 925/1000 [02:38<00:12,  5.90it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 926/1000 [02:38<00:13,  5.56it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 927/1000 [02:39<00:13,  5.33it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 928/1000 [02:39<00:13,  5.32it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 929/1000 [02:39<00:12,  5.81it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 930/1000 [02:39<00:10,  6.46it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 931/1000 [02:39<00:10,  6.71it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 932/1000 [02:39<00:11,  6.03it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 933/1000 [02:40<00:10,  6.31it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 934/1000 [02:40<00:10,  6.54it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 935/1000 [02:40<00:09,  6.71it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 936/1000 [02:40<00:10,  6.21it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 937/1000 [02:40<00:09,  6.41it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 938/1000 [02:40<00:08,  6.95it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 939/1000 [02:40<00:09,  6.12it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 940/1000 [02:41<00:09,  6.32it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 941/1000 [02:41<00:10,  5.75it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 942/1000 [02:41<00:09,  6.05it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 943/1000 [02:41<00:09,  5.80it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 944/1000 [02:41<00:09,  6.08it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 945/1000 [02:41<00:08,  6.30it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 946/1000 [02:42<00:08,  6.48it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 947/1000 [02:42<00:10,  5.11it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 948/1000 [02:42<00:11,  4.67it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 949/1000 [02:42<00:09,  5.44it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 950/1000 [02:42<00:08,  5.80it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 951/1000 [02:43<00:08,  6.10it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 952/1000 [02:43<00:07,  6.34it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 953/1000 [02:43<00:07,  6.50it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 954/1000 [02:43<00:06,  6.66it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 955/1000 [02:43<00:07,  5.93it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 956/1000 [02:43<00:07,  6.20it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 957/1000 [02:43<00:06,  6.40it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 958/1000 [02:44<00:07,  5.82it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 959/1000 [02:44<00:08,  4.79it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 960/1000 [02:44<00:07,  5.03it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 961/1000 [02:44<00:07,  4.98it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 962/1000 [02:45<00:07,  4.93it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 963/1000 [02:45<00:07,  5.01it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 964/1000 [02:45<00:06,  5.50it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 965/1000 [02:45<00:05,  5.86it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 966/1000 [02:45<00:05,  6.12it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 967/1000 [02:45<00:05,  5.85it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 968/1000 [02:46<00:05,  6.15it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 969/1000 [02:46<00:05,  5.56it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 970/1000 [02:46<00:05,  5.23it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 971/1000 [02:46<00:05,  5.66it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 972/1000 [02:46<00:04,  6.01it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 973/1000 [02:46<00:04,  6.27it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 974/1000 [02:47<00:04,  5.59it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 975/1000 [02:47<00:04,  5.96it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 976/1000 [02:47<00:03,  6.23it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 977/1000 [02:47<00:03,  6.43it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 978/1000 [02:47<00:03,  6.99it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 979/1000 [02:47<00:03,  5.59it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 980/1000 [02:48<00:03,  5.92it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 981/1000 [02:48<00:03,  5.49it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 982/1000 [02:48<00:03,  5.89it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 983/1000 [02:48<00:03,  5.51it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 984/1000 [02:48<00:03,  5.29it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 985/1000 [02:48<00:02,  5.70it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 986/1000 [02:49<00:02,  5.58it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 987/1000 [02:49<00:02,  5.91it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 988/1000 [02:49<00:01,  6.20it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 989/1000 [02:49<00:02,  4.73it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 990/1000 [02:49<00:01,  5.24it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 991/1000 [02:50<00:01,  5.00it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 992/1000 [02:50<00:01,  5.08it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 993/1000 [02:50<00:01,  5.54it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 994/1000 [02:50<00:01,  5.30it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 995/1000 [02:50<00:00,  5.13it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 996/1000 [02:51<00:00,  5.00it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 997/1000 [02:51<00:00,  5.49it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 998/1000 [02:51<00:00,  5.96it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 999/1000 [02:51<00:00,  6.22it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [02:51<00:00,  5.63it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [02:51<00:00,  5.82it/s]
DONE (1.94s)
DONE (7.74s)
loss_threshold ROC AUC: 0.487757, PR AUC: 0.4957081325850717, tpr_at_low_fpr: {0.001: 0.0, 0.01: 0.012}
min_k_threshold ROC AUC: 0.48833400000000005, PR AUC: 0.49634276461624005, tpr_at_low_fpr: {0.001: 0.0, 0.01: 0.008}
zlib_threshold ROC AUC: 0.48933, PR AUC: 0.49654322321680466, tpr_at_low_fpr: {0.001: 0.0, 0.01: 0.009}
ref-stablelm-base-alpha-3b-v2_threshold ROC AUC: 0.489557, PR AUC: 0.4922176522399221, tpr_at_low_fpr: {0.001: 0.001, 0.01: 0.008}
loss_threshold roc_auc: 0.488
min_k_threshold roc_auc: 0.488
zlib_threshold roc_auc: 0.489
ref-stablelm-base-alpha-3b-v2_threshold roc_auc: 0.490
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
results_new/neo125_github_experiment/meta-llama_Llama-2-70b-hf/arxiv_ngram_13_<0.8_truncated
Saving results to absolute path: /home/chawins/mimir/tmp_results/neo125_github_experiment/meta-llama_Llama-2-70b-hf/arxiv_ngram_13_<0.8_truncated
LOG: cache_dir is /home/chawins/.cache
Using cache dir /home/chawins/.cache
Loading BASE model meta-llama/Llama-2-70b-hf...
Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.embed_tokens.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:   7%|â–‹         | 1/15 [00:00<00:02,  6.07it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  13%|â–ˆâ–Ž        | 2/15 [00:00<00:02,  6.08it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 3/15 [00:00<00:01,  6.18it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:00<00:01,  6.25it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:00<00:01,  6.24it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:00<00:01,  6.28it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:01<00:01,  6.32it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:01<00:01,  6.33it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:01<00:00,  6.29it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:01<00:00,  6.29it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:01<00:00,  6.25it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:01<00:00,  6.24it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:02<00:00,  6.24it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:02<00:00,  6.29it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for lm_head.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:02<00:00,  6.42it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:02<00:00,  6.29it/s]
  0%|          | 0/57 [00:00<?, ?w/s]model.embed_tokens.weight:   0%|          | 0/57 [00:00<?, ?w/s, dev=0]model.embed_tokens.weight:   2%|â–         | 1/57 [00:00<00:10,  5.24w/s, dev=0]model.layers.0.input_layernorm.weight:   2%|â–         | 1/57 [00:00<00:10,  5.23w/s, dev=0]model.layers.0.mlp.down_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:05, 10.44w/s, dev=0]  model.layers.0.mlp.down_proj.weight:   5%|â–Œ         | 3/57 [00:00<00:06,  8.42w/s, dev=0]model.layers.0.mlp.gate_proj.weight:   5%|â–Œ         | 3/57 [00:00<00:06,  8.42w/s, dev=0]model.layers.0.mlp.up_proj.weight:   7%|â–‹         | 4/57 [00:00<00:06,  7.62w/s, dev=0]  model.layers.0.mlp.up_proj.weight:   9%|â–‰         | 5/57 [00:00<00:07,  7.31w/s, dev=0]model.layers.0.post_attention_layernorm.weight:   9%|â–‰         | 5/57 [00:00<00:07,  7.31w/s, dev=0]model.layers.0.self_attn.k_proj.weight:  11%|â–ˆ         | 6/57 [00:00<00:05,  8.77w/s, dev=0]        model.layers.0.self_attn.o_proj.weight:  12%|â–ˆâ–        | 7/57 [00:00<00:04, 10.10w/s, dev=0]model.layers.0.self_attn.q_proj.weight:  14%|â–ˆâ–        | 8/57 [00:00<00:04, 10.72w/s, dev=0]model.layers.0.self_attn.q_proj.weight:  16%|â–ˆâ–Œ        | 9/57 [00:00<00:04, 11.32w/s, dev=0]model.layers.0.self_attn.rotary_emb.inv_freq:  16%|â–ˆâ–Œ        | 9/57 [00:00<00:04, 11.31w/s, dev=0]model.layers.0.self_attn.v_proj.weight:  18%|â–ˆâ–Š        | 10/57 [00:00<00:03, 12.56w/s, dev=0]     model.layers.1.input_layernorm.weight:  19%|â–ˆâ–‰        | 11/57 [00:00<00:03, 13.71w/s, dev=0] model.layers.1.mlp.down_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:00<00:03, 14.95w/s, dev=0]  model.layers.1.mlp.down_proj.weight:  23%|â–ˆâ–ˆâ–Ž       | 13/57 [00:00<00:03, 13.30w/s, dev=0]model.layers.1.mlp.gate_proj.weight:  23%|â–ˆâ–ˆâ–Ž       | 13/57 [00:00<00:03, 13.30w/s, dev=0]model.layers.1.mlp.up_proj.weight:  25%|â–ˆâ–ˆâ–       | 14/57 [00:01<00:03, 12.12w/s, dev=0]  model.layers.1.post_attention_layernorm.weight:  26%|â–ˆâ–ˆâ–‹       | 15/57 [00:01<00:03, 11.27w/s, dev=0]model.layers.1.self_attn.k_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:01<00:03, 12.02w/s, dev=0]        model.layers.1.self_attn.k_proj.weight:  30%|â–ˆâ–ˆâ–‰       | 17/57 [00:01<00:03, 12.71w/s, dev=0]model.layers.1.self_attn.o_proj.weight:  30%|â–ˆâ–ˆâ–‰       | 17/57 [00:01<00:03, 12.71w/s, dev=0]model.layers.1.self_attn.q_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 18/57 [00:01<00:02, 13.02w/s, dev=0]model.layers.1.self_attn.rotary_emb.inv_freq:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 19/57 [00:01<00:02, 13.31w/s, dev=0]model.layers.1.self_attn.v_proj.weight:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 20/57 [00:01<00:02, 14.01w/s, dev=0]      model.layers.2.input_layernorm.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 21/57 [00:01<00:02, 14.65w/s, dev=0] model.layers.2.mlp.down_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:01<00:02, 15.34w/s, dev=0]  model.layers.2.mlp.down_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 23/57 [00:01<00:02, 14.48w/s, dev=0]model.layers.2.mlp.gate_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 23/57 [00:01<00:02, 14.47w/s, dev=0]model.layers.2.mlp.up_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/57 [00:01<00:02, 13.71w/s, dev=0]  model.layers.2.post_attention_layernorm.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/57 [00:01<00:02, 13.00w/s, dev=0]model.layers.2.self_attn.k_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 26/57 [00:01<00:02, 13.52w/s, dev=0]        model.layers.2.self_attn.o_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 27/57 [00:01<00:02, 13.98w/s, dev=0]model.layers.2.self_attn.q_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:01<00:02, 14.17w/s, dev=0]model.layers.2.self_attn.q_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 29/57 [00:02<00:01, 14.33w/s, dev=0]model.layers.2.self_attn.rotary_emb.inv_freq:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 29/57 [00:02<00:01, 14.33w/s, dev=0]model.layers.2.self_attn.v_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 30/57 [00:02<00:01, 14.82w/s, dev=0]      model.layers.3.input_layernorm.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 31/57 [00:02<00:01, 15.25w/s, dev=0] model.layers.3.mlp.down_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 32/57 [00:02<00:01, 15.74w/s, dev=0]  model.layers.3.mlp.gate_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 33/57 [00:02<00:01, 15.09w/s, dev=0]model.layers.3.mlp.up_proj.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:02<00:01, 14.50w/s, dev=0]  model.layers.3.mlp.up_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/57 [00:02<00:01, 13.99w/s, dev=0]model.layers.3.post_attention_layernorm.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/57 [00:02<00:01, 13.99w/s, dev=0]model.layers.3.self_attn.k_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 36/57 [00:02<00:01, 14.39w/s, dev=0]        model.layers.3.self_attn.o_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 37/57 [00:02<00:01, 14.74w/s, dev=0]model.layers.3.self_attn.q_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 38/57 [00:02<00:01, 14.86w/s, dev=0]model.layers.3.self_attn.rotary_emb.inv_freq:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 39/57 [00:02<00:01, 14.98w/s, dev=0]model.layers.3.self_attn.v_proj.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:02<00:01, 15.37w/s, dev=0]      model.layers.3.self_attn.v_proj.weight:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/57 [00:02<00:01, 15.71w/s, dev=0]model.layers.4.input_layernorm.weight:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/57 [00:02<00:01, 15.71w/s, dev=0] model.layers.4.mlp.down_proj.weight:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 42/57 [00:02<00:00, 16.09w/s, dev=0]  model.layers.4.mlp.gate_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 43/57 [00:02<00:00, 15.47w/s, dev=0]model.layers.4.mlp.up_proj.weight:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 44/57 [00:02<00:00, 14.90w/s, dev=0]  model.layers.4.post_attention_layernorm.weight:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 45/57 [00:03<00:00, 14.45w/s, dev=0]model.layers.4.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:03<00:00, 14.76w/s, dev=0]        model.layers.4.self_attn.k_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 47/57 [00:03<00:00, 15.05w/s, dev=0]model.layers.4.self_attn.o_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 47/57 [00:03<00:00, 15.05w/s, dev=0]model.layers.4.self_attn.q_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/57 [00:03<00:00, 15.15w/s, dev=0]model.layers.4.self_attn.rotary_emb.inv_freq:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 49/57 [00:03<00:00, 15.24w/s, dev=0]model.layers.4.self_attn.v_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 50/57 [00:03<00:00, 15.55w/s, dev=0]      model.layers.5.mlp.gate_proj.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 51/57 [00:03<00:00, 15.82w/s, dev=0]   model.layers.5.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:03<00:00, 15.31w/s, dev=0]model.layers.5.self_attn.k_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 53/57 [00:03<00:00, 15.57w/s, dev=0]model.layers.5.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 53/57 [00:03<00:00, 15.57w/s, dev=0]model.layers.5.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/57 [00:03<00:00, 15.63w/s, dev=0]model.layers.5.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 55/57 [00:03<00:00, 15.68w/s, dev=0]model.layers.5.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 56/57 [00:03<00:00, 15.97w/s, dev=0]                                                                                                     0%|          | 0/59 [00:00<?, ?w/s]model.layers.5.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=0]model.layers.5.mlp.down_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 1508.74w/s, dev=0]model.layers.5.mlp.down_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 11.47w/s, dev=0]  model.layers.5.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 11.45w/s, dev=0]  model.layers.5.post_attention_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:06,  8.57w/s, dev=0]model.layers.5.post_attention_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:04, 11.41w/s, dev=0]model.layers.6.input_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:04, 11.41w/s, dev=0]         model.layers.6.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 14.25w/s, dev=0]  model.layers.6.mlp.down_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:04, 11.65w/s, dev=0]model.layers.6.mlp.gate_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:04, 11.64w/s, dev=0]model.layers.6.mlp.up_proj.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:05, 10.39w/s, dev=0]  model.layers.6.mlp.up_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:06,  8.16w/s, dev=0]model.layers.6.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:06,  8.15w/s, dev=0]model.layers.6.self_attn.k_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:05,  9.17w/s, dev=0]        model.layers.6.self_attn.o_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:04, 10.10w/s, dev=0]model.layers.6.self_attn.q_proj.weight:  19%|â–ˆâ–Š        | 11/59 [00:01<00:04, 10.56w/s, dev=0]model.layers.6.self_attn.q_proj.weight:  20%|â–ˆâ–ˆ        | 12/59 [00:01<00:04, 10.98w/s, dev=0]model.layers.6.self_attn.rotary_emb.inv_freq:  20%|â–ˆâ–ˆ        | 12/59 [00:01<00:04, 10.98w/s, dev=0]model.layers.6.self_attn.v_proj.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:01<00:03, 11.89w/s, dev=0]      model.layers.7.input_layernorm.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:01<00:03, 12.70w/s, dev=0] model.layers.7.mlp.down_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:01<00:03, 13.60w/s, dev=0]  model.layers.7.mlp.down_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:03, 12.74w/s, dev=0]model.layers.7.mlp.gate_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:03, 12.74w/s, dev=0]model.layers.7.mlp.up_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:01<00:03, 11.38w/s, dev=0]  model.layers.7.post_attention_layernorm.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:03, 11.04w/s, dev=0]model.layers.7.self_attn.k_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:03, 11.65w/s, dev=0]        model.layers.7.self_attn.k_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:03, 11.05w/s, dev=0]model.layers.7.self_attn.o_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:03, 11.05w/s, dev=0]model.layers.7.self_attn.q_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:01<00:03, 11.28w/s, dev=0]model.layers.7.self_attn.rotary_emb.inv_freq:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:03, 11.50w/s, dev=0]model.layers.7.self_attn.v_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:02, 12.02w/s, dev=0]      model.layers.7.self_attn.v_proj.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:02, 12.45w/s, dev=0]model.layers.8.input_layernorm.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:02, 12.45w/s, dev=0] model.layers.8.mlp.down_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:02, 12.96w/s, dev=0]  model.layers.8.mlp.gate_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:02<00:02, 12.42w/s, dev=0]model.layers.8.mlp.up_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:02<00:02, 12.08w/s, dev=0]  model.layers.8.mlp.up_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:02<00:02, 11.16w/s, dev=0]model.layers.8.post_attention_layernorm.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:02<00:02, 11.15w/s, dev=0]model.layers.8.self_attn.k_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:02<00:02, 11.55w/s, dev=0]        model.layers.8.self_attn.o_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:02<00:02, 11.91w/s, dev=0]model.layers.8.self_attn.q_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:02<00:02, 12.12w/s, dev=0]model.layers.8.self_attn.rotary_emb.inv_freq:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:02<00:02, 12.30w/s, dev=0]model.layers.8.self_attn.v_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:02<00:02, 12.68w/s, dev=0]      model.layers.8.self_attn.v_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:02<00:01, 13.02w/s, dev=0]model.layers.9.input_layernorm.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:02<00:01, 13.02w/s, dev=0] model.layers.9.mlp.down_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:02<00:01, 13.40w/s, dev=0]  model.layers.9.mlp.gate_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:02<00:01, 13.05w/s, dev=0]model.layers.9.mlp.up_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:03<00:01, 11.74w/s, dev=0]  model.layers.9.post_attention_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:03<00:01, 11.57w/s, dev=0]model.layers.9.self_attn.k_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:03<00:01, 11.87w/s, dev=0]        model.layers.9.self_attn.k_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:03<00:01, 12.11w/s, dev=0]model.layers.9.self_attn.o_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:03<00:01, 12.11w/s, dev=0]model.layers.9.self_attn.q_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:03<00:01, 12.24w/s, dev=0]model.layers.9.self_attn.rotary_emb.inv_freq:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:03<00:01, 11.47w/s, dev=0]model.layers.9.self_attn.v_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:03<00:01, 11.74w/s, dev=0]      model.layers.10.input_layernorm.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:03<00:01, 11.99w/s, dev=0]model.layers.10.mlp.down_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:03<00:01, 12.26w/s, dev=0]  model.layers.10.mlp.down_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:03<00:01, 12.01w/s, dev=0]model.layers.10.mlp.gate_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:03<00:01, 12.01w/s, dev=0]model.layers.10.mlp.up_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:03<00:01, 11.79w/s, dev=0]  model.layers.10.post_attention_layernorm.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:04<00:00, 11.02w/s, dev=0]model.layers.10.self_attn.k_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:04<00:00, 11.25w/s, dev=0]        model.layers.10.self_attn.o_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:04<00:00, 11.46w/s, dev=0]model.layers.10.self_attn.q_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:04<00:00, 11.55w/s, dev=0]model.layers.10.self_attn.q_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:04<00:00, 11.62w/s, dev=0]model.layers.10.self_attn.rotary_emb.inv_freq:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:04<00:00, 11.62w/s, dev=0]model.layers.10.self_attn.v_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:04<00:00, 11.85w/s, dev=0]      model.layers.11.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:04<00:00, 12.05w/s, dev=0]model.layers.11.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:04<00:00, 12.20w/s, dev=0]model.layers.11.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:04<00:00, 12.31w/s, dev=0]model.layers.11.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:04<00:00, 12.39w/s, dev=0]model.layers.11.self_attn.rotary_emb.inv_freq:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:04<00:00, 12.61w/s, dev=0]model.layers.11.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:04<00:00, 12.61w/s, dev=0]                                                                                                      0%|          | 0/55 [00:00<?, ?w/s]model.layers.11.input_layernorm.weight:   0%|          | 0/55 [00:00<?, ?w/s, dev=0]model.layers.11.mlp.down_proj.weight:   2%|â–         | 1/55 [00:00<00:00, 1633.30w/s, dev=0]model.layers.11.mlp.down_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:04, 13.24w/s, dev=0]  model.layers.11.mlp.gate_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:04, 13.22w/s, dev=0]model.layers.11.mlp.up_proj.weight:   5%|â–Œ         | 3/55 [00:00<00:05, 10.06w/s, dev=0]  model.layers.11.mlp.up_proj.weight:   7%|â–‹         | 4/55 [00:00<00:05,  9.08w/s, dev=0]model.layers.11.post_attention_layernorm.weight:   7%|â–‹         | 4/55 [00:00<00:05,  9.07w/s, dev=0]model.layers.12.input_layernorm.weight:   9%|â–‰         | 5/55 [00:00<00:04, 11.33w/s, dev=0]         model.layers.12.mlp.down_proj.weight:  11%|â–ˆ         | 6/55 [00:00<00:03, 13.59w/s, dev=0]  model.layers.12.mlp.down_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:03, 12.05w/s, dev=0]model.layers.12.mlp.gate_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:03, 12.05w/s, dev=0]model.layers.12.mlp.up_proj.weight:  15%|â–ˆâ–        | 8/55 [00:00<00:04, 10.81w/s, dev=0]  model.layers.12.post_attention_layernorm.weight:  16%|â–ˆâ–‹        | 9/55 [00:00<00:04,  9.97w/s, dev=0]model.layers.12.post_attention_layernorm.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:04, 11.07w/s, dev=0]model.layers.12.self_attn.k_proj.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:04, 11.07w/s, dev=0]        model.layers.12.self_attn.o_proj.weight:  20%|â–ˆâ–ˆ        | 11/55 [00:00<00:03, 12.09w/s, dev=0]model.layers.12.self_attn.q_proj.weight:  22%|â–ˆâ–ˆâ–       | 12/55 [00:00<00:03, 12.57w/s, dev=0]model.layers.12.self_attn.rotary_emb.inv_freq:  24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:00<00:03, 13.01w/s, dev=0]model.layers.12.self_attn.v_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 14/55 [00:00<00:02, 14.00w/s, dev=0]      model.layers.12.self_attn.v_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 15/55 [00:01<00:02, 14.91w/s, dev=0]model.layers.13.input_layernorm.weight:  27%|â–ˆâ–ˆâ–‹       | 15/55 [00:01<00:02, 14.90w/s, dev=0] model.layers.13.mlp.down_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 16/55 [00:01<00:02, 15.89w/s, dev=0]  model.layers.13.mlp.gate_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:01<00:02, 14.65w/s, dev=0]model.layers.13.mlp.up_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/55 [00:01<00:02, 13.65w/s, dev=0]  model.layers.13.post_attention_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–      | 19/55 [00:01<00:02, 12.93w/s, dev=0]model.layers.13.post_attention_layernorm.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:01<00:02, 13.61w/s, dev=0]model.layers.13.self_attn.k_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:01<00:02, 13.60w/s, dev=0]        model.layers.13.self_attn.o_proj.weight:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/55 [00:01<00:02, 14.22w/s, dev=0]model.layers.13.self_attn.q_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/55 [00:01<00:02, 14.46w/s, dev=0]model.layers.13.self_attn.rotary_emb.inv_freq:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 23/55 [00:01<00:02, 14.64w/s, dev=0]model.layers.13.self_attn.v_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 24/55 [00:01<00:02, 15.27w/s, dev=0]      model.layers.13.self_attn.v_proj.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:01<00:01, 15.84w/s, dev=0]model.layers.14.input_layernorm.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:01<00:01, 15.83w/s, dev=0] model.layers.14.mlp.down_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 26/55 [00:01<00:01, 16.46w/s, dev=0]  model.layers.14.mlp.gate_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 27/55 [00:01<00:01, 15.47w/s, dev=0]model.layers.14.mlp.up_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 28/55 [00:01<00:01, 14.65w/s, dev=0]  model.layers.14.post_attention_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/55 [00:02<00:01, 14.07w/s, dev=0]model.layers.14.post_attention_layernorm.weight:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 30/55 [00:02<00:01, 14.55w/s, dev=0]model.layers.14.self_attn.k_proj.weight:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 30/55 [00:02<00:01, 14.55w/s, dev=0]        model.layers.14.self_attn.o_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/55 [00:02<00:01, 14.98w/s, dev=0]model.layers.14.self_attn.q_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 32/55 [00:02<00:01, 15.15w/s, dev=0]model.layers.14.self_attn.rotary_emb.inv_freq:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/55 [00:02<00:01, 15.30w/s, dev=0]model.layers.14.self_attn.v_proj.weight:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/55 [00:02<00:01, 15.76w/s, dev=0]      model.layers.14.self_attn.v_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:02<00:01, 16.17w/s, dev=0]model.layers.15.input_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:02<00:01, 16.17w/s, dev=0] model.layers.15.mlp.down_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 36/55 [00:02<00:01, 16.63w/s, dev=0]  model.layers.15.mlp.gate_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 37/55 [00:02<00:01, 16.03w/s, dev=0]model.layers.15.mlp.up_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 38/55 [00:02<00:01, 15.55w/s, dev=0]  model.layers.15.post_attention_layernorm.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 39/55 [00:02<00:01, 15.05w/s, dev=0]model.layers.15.post_attention_layernorm.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 40/55 [00:02<00:00, 15.43w/s, dev=0]model.layers.15.self_attn.k_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 40/55 [00:02<00:00, 15.43w/s, dev=0]        model.layers.15.self_attn.o_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/55 [00:02<00:00, 15.77w/s, dev=0]model.layers.15.self_attn.q_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 42/55 [00:02<00:00, 15.88w/s, dev=0]model.layers.15.self_attn.rotary_emb.inv_freq:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 43/55 [00:02<00:00, 15.98w/s, dev=0]model.layers.15.self_attn.v_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 44/55 [00:02<00:00, 16.35w/s, dev=0]      model.layers.15.self_attn.v_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:02<00:00, 16.67w/s, dev=0]model.layers.16.input_layernorm.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:02<00:00, 16.67w/s, dev=0] model.layers.16.mlp.down_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 46/55 [00:02<00:00, 17.04w/s, dev=0]  model.layers.16.mlp.gate_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 47/55 [00:02<00:00, 16.50w/s, dev=0]model.layers.16.mlp.up_proj.weight:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 48/55 [00:02<00:00, 16.01w/s, dev=0]  model.layers.16.post_attention_layernorm.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 49/55 [00:03<00:00, 15.55w/s, dev=0]model.layers.16.post_attention_layernorm.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 50/55 [00:03<00:00, 15.86w/s, dev=0]model.layers.16.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 50/55 [00:03<00:00, 15.86w/s, dev=0]        model.layers.16.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 51/55 [00:03<00:00, 16.14w/s, dev=0]model.layers.16.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 52/55 [00:03<00:00, 16.22w/s, dev=0]model.layers.16.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 53/55 [00:03<00:00, 16.30w/s, dev=0]model.layers.16.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 54/55 [00:03<00:00, 16.60w/s, dev=0]      model.layers.16.self_attn.v_proj.weight: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55/55 [00:03<00:00, 16.88w/s, dev=0]                                                                                                0%|          | 0/57 [00:00<?, ?w/s]model.layers.17.input_layernorm.weight:   0%|          | 0/57 [00:00<?, ?w/s, dev=0]model.layers.17.mlp.down_proj.weight:   2%|â–         | 1/57 [00:00<00:00, 1392.07w/s, dev=0]model.layers.17.mlp.down_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:04, 13.24w/s, dev=0]  model.layers.17.mlp.gate_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:04, 13.22w/s, dev=0]model.layers.17.mlp.up_proj.weight:   5%|â–Œ         | 3/57 [00:00<00:05,  9.96w/s, dev=0]  model.layers.17.mlp.up_proj.weight:   7%|â–‹         | 4/57 [00:00<00:05,  8.84w/s, dev=0]model.layers.17.post_attention_layernorm.weight:   7%|â–‹         | 4/57 [00:00<00:05,  8.83w/s, dev=0]model.layers.17.self_attn.k_proj.weight:   9%|â–‰         | 5/57 [00:00<00:04, 11.03w/s, dev=0]        model.layers.17.self_attn.o_proj.weight:  11%|â–ˆ         | 6/57 [00:00<00:03, 13.05w/s, dev=0]model.layers.17.self_attn.q_proj.weight:  12%|â–ˆâ–        | 7/57 [00:00<00:03, 13.75w/s, dev=0]model.layers.17.self_attn.q_proj.weight:  14%|â–ˆâ–        | 8/57 [00:00<00:03, 14.32w/s, dev=0]model.layers.17.self_attn.rotary_emb.inv_freq:  14%|â–ˆâ–        | 8/57 [00:00<00:03, 14.31w/s, dev=0]model.layers.17.self_attn.v_proj.weight:  16%|â–ˆâ–Œ        | 9/57 [00:00<00:02, 16.09w/s, dev=0]      model.layers.18.input_layernorm.weight:  18%|â–ˆâ–Š        | 10/57 [00:00<00:02, 17.66w/s, dev=0]model.layers.18.mlp.down_proj.weight:  19%|â–ˆâ–‰        | 11/57 [00:00<00:02, 19.42w/s, dev=0]  model.layers.18.mlp.down_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:00<00:02, 16.84w/s, dev=0]model.layers.18.mlp.gate_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:00<00:02, 16.83w/s, dev=0]model.layers.18.mlp.up_proj.weight:  23%|â–ˆâ–ˆâ–Ž       | 13/57 [00:00<00:02, 15.01w/s, dev=0]  model.layers.18.post_attention_layernorm.weight:  25%|â–ˆâ–ˆâ–       | 14/57 [00:01<00:03, 13.68w/s, dev=0]model.layers.18.self_attn.k_proj.weight:  26%|â–ˆâ–ˆâ–‹       | 15/57 [00:01<00:02, 14.65w/s, dev=0]        model.layers.18.self_attn.k_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:01<00:02, 15.53w/s, dev=0]model.layers.18.self_attn.o_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:01<00:02, 15.53w/s, dev=0]model.layers.18.self_attn.q_proj.weight:  30%|â–ˆâ–ˆâ–‰       | 17/57 [00:01<00:02, 15.89w/s, dev=0]model.layers.18.self_attn.rotary_emb.inv_freq:  32%|â–ˆâ–ˆâ–ˆâ–      | 18/57 [00:01<00:02, 16.10w/s, dev=0]model.layers.18.self_attn.v_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 19/57 [00:01<00:02, 16.99w/s, dev=0]      model.layers.19.input_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 20/57 [00:01<00:02, 17.77w/s, dev=0] model.layers.19.mlp.down_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 21/57 [00:01<00:01, 18.65w/s, dev=0]  model.layers.19.mlp.down_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:01<00:02, 16.99w/s, dev=0]model.layers.19.mlp.gate_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:01<00:02, 16.98w/s, dev=0]model.layers.19.mlp.up_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 23/57 [00:01<00:02, 15.76w/s, dev=0]  model.layers.19.post_attention_layernorm.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/57 [00:01<00:02, 14.84w/s, dev=0]model.layers.19.self_attn.k_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/57 [00:01<00:02, 15.45w/s, dev=0]        model.layers.19.self_attn.o_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 26/57 [00:01<00:01, 16.01w/s, dev=0]model.layers.19.self_attn.q_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 27/57 [00:01<00:01, 16.18w/s, dev=0]model.layers.19.self_attn.q_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:01<00:01, 16.35w/s, dev=0]model.layers.19.self_attn.rotary_emb.inv_freq:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:01<00:01, 16.35w/s, dev=0]model.layers.19.self_attn.v_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 29/57 [00:01<00:01, 16.93w/s, dev=0]      model.layers.20.input_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 30/57 [00:01<00:01, 15.61w/s, dev=1] model.layers.20.mlp.down_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 31/57 [00:01<00:01, 16.12w/s, dev=1]  model.layers.20.mlp.gate_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 32/57 [00:02<00:01, 15.29w/s, dev=1]model.layers.20.mlp.up_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 33/57 [00:02<00:01, 14.64w/s, dev=1]  model.layers.20.mlp.up_proj.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:02<00:01, 14.03w/s, dev=1]model.layers.20.post_attention_layernorm.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:02<00:01, 14.03w/s, dev=1]model.layers.20.self_attn.k_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/57 [00:02<00:01, 14.44w/s, dev=1]        model.layers.20.self_attn.o_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 36/57 [00:02<00:01, 14.81w/s, dev=1]model.layers.20.self_attn.q_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 37/57 [00:02<00:01, 14.93w/s, dev=1]model.layers.20.self_attn.rotary_emb.inv_freq:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 38/57 [00:02<00:01, 15.04w/s, dev=1]model.layers.20.self_attn.v_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 39/57 [00:02<00:01, 15.43w/s, dev=1]      model.layers.20.self_attn.v_proj.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:02<00:01, 15.79w/s, dev=1]model.layers.21.input_layernorm.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:02<00:01, 15.79w/s, dev=1] model.layers.21.mlp.down_proj.weight:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/57 [00:02<00:00, 16.18w/s, dev=1]  model.layers.21.mlp.gate_proj.weight:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 42/57 [00:02<00:00, 15.53w/s, dev=1]model.layers.21.mlp.up_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 43/57 [00:02<00:00, 14.93w/s, dev=1]  model.layers.21.post_attention_layernorm.weight:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 44/57 [00:03<00:00, 14.48w/s, dev=1]model.layers.21.self_attn.k_proj.weight:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 45/57 [00:03<00:00, 14.80w/s, dev=1]        model.layers.21.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:03<00:00, 15.10w/s, dev=1]model.layers.21.self_attn.o_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:03<00:00, 15.10w/s, dev=1]model.layers.21.self_attn.q_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 47/57 [00:03<00:00, 15.18w/s, dev=1]model.layers.21.self_attn.rotary_emb.inv_freq:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/57 [00:03<00:00, 15.26w/s, dev=1]model.layers.21.self_attn.v_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 49/57 [00:03<00:00, 15.58w/s, dev=1]      model.layers.22.mlp.down_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 50/57 [00:03<00:00, 15.86w/s, dev=1]   model.layers.22.mlp.gate_proj.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 51/57 [00:03<00:00, 15.34w/s, dev=1]model.layers.22.mlp.gate_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:03<00:00, 14.88w/s, dev=1]model.layers.22.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:03<00:00, 14.88w/s, dev=1]model.layers.22.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 53/57 [00:03<00:00, 15.14w/s, dev=1]model.layers.22.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/57 [00:03<00:00, 15.21w/s, dev=1]model.layers.22.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 55/57 [00:03<00:00, 15.28w/s, dev=1]model.layers.22.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 56/57 [00:03<00:00, 15.56w/s, dev=1]                                                                                                      0%|          | 0/59 [00:00<?, ?w/s]model.layers.22.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=1]model.layers.22.mlp.up_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 1256.53w/s, dev=1]model.layers.22.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 12.37w/s, dev=1]  model.layers.22.post_attention_layernorm.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 12.35w/s, dev=1]model.layers.23.input_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:03, 18.48w/s, dev=1]         model.layers.23.mlp.down_proj.weight:   7%|â–‹         | 4/59 [00:00<00:02, 24.60w/s, dev=1]  model.layers.23.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 15.35w/s, dev=1]model.layers.23.mlp.gate_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 15.34w/s, dev=1]model.layers.23.mlp.up_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:04, 12.45w/s, dev=1]  model.layers.23.post_attention_layernorm.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:04, 10.93w/s, dev=1]model.layers.23.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:04, 12.48w/s, dev=1]model.layers.23.self_attn.k_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:04, 12.48w/s, dev=1]        model.layers.23.self_attn.o_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:03, 13.88w/s, dev=1]model.layers.23.self_attn.q_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:03, 14.38w/s, dev=1]model.layers.23.self_attn.q_proj.weight:  19%|â–ˆâ–Š        | 11/59 [00:00<00:03, 14.78w/s, dev=1]model.layers.23.self_attn.rotary_emb.inv_freq:  19%|â–ˆâ–Š        | 11/59 [00:00<00:03, 14.78w/s, dev=1]model.layers.23.self_attn.v_proj.weight:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:02, 16.12w/s, dev=1]      model.layers.24.input_layernorm.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:02, 17.29w/s, dev=1] model.layers.24.mlp.down_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:02, 18.61w/s, dev=1]  model.layers.24.mlp.down_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:02, 16.56w/s, dev=1]model.layers.24.mlp.gate_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:02, 16.56w/s, dev=1]model.layers.24.mlp.up_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:02, 14.92w/s, dev=1]  model.layers.24.post_attention_layernorm.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:01<00:03, 13.85w/s, dev=1]model.layers.24.self_attn.k_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:02, 14.66w/s, dev=1]        model.layers.24.self_attn.k_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:02, 15.36w/s, dev=1]model.layers.24.self_attn.o_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:02, 15.36w/s, dev=1]model.layers.24.self_attn.q_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:02, 15.56w/s, dev=1]model.layers.24.self_attn.rotary_emb.inv_freq:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:01<00:02, 15.74w/s, dev=1]model.layers.24.self_attn.v_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:02, 16.49w/s, dev=1]      model.layers.24.self_attn.v_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:02, 17.15w/s, dev=1]model.layers.25.input_layernorm.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:02, 17.15w/s, dev=1] model.layers.25.mlp.down_proj.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:01, 17.89w/s, dev=1]  model.layers.25.mlp.gate_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:02, 16.65w/s, dev=1]model.layers.25.mlp.up_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:02, 15.69w/s, dev=1]  model.layers.25.mlp.up_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:01<00:02, 14.84w/s, dev=1]model.layers.25.post_attention_layernorm.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:01<00:02, 14.84w/s, dev=1]model.layers.25.self_attn.k_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:02, 15.39w/s, dev=1]        model.layers.25.self_attn.o_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:01, 15.88w/s, dev=1]model.layers.25.self_attn.q_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:01<00:01, 16.01w/s, dev=1]model.layers.25.self_attn.q_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:01<00:01, 16.13w/s, dev=1]model.layers.25.self_attn.rotary_emb.inv_freq:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:01<00:01, 16.13w/s, dev=1]model.layers.25.self_attn.v_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:01<00:01, 16.65w/s, dev=1]      model.layers.26.input_layernorm.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:01<00:01, 17.11w/s, dev=1] model.layers.26.mlp.down_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:01<00:01, 17.63w/s, dev=1]  model.layers.26.mlp.down_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:02<00:01, 16.72w/s, dev=1]model.layers.26.mlp.gate_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:02<00:01, 16.71w/s, dev=1]model.layers.26.mlp.up_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:02<00:01, 15.95w/s, dev=1]  model.layers.26.post_attention_layernorm.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:02<00:01, 15.28w/s, dev=1]model.layers.26.self_attn.k_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:02<00:01, 15.69w/s, dev=1]        model.layers.26.self_attn.k_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:02<00:01, 16.06w/s, dev=1]model.layers.26.self_attn.o_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:02<00:01, 16.05w/s, dev=1]model.layers.26.self_attn.q_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:02<00:01, 16.15w/s, dev=1]model.layers.26.self_attn.rotary_emb.inv_freq:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:02<00:01, 16.24w/s, dev=1]model.layers.26.self_attn.v_proj.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:02<00:01, 16.63w/s, dev=1]      model.layers.26.self_attn.v_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:02<00:00, 16.98w/s, dev=1]model.layers.27.input_layernorm.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:02<00:00, 16.98w/s, dev=1] model.layers.27.mlp.down_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:02<00:00, 17.37w/s, dev=1]  model.layers.27.mlp.gate_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:02<00:00, 16.70w/s, dev=1]model.layers.27.mlp.up_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:02<00:00, 16.16w/s, dev=1]  model.layers.27.mlp.up_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:03<00:00, 15.64w/s, dev=1]model.layers.27.post_attention_layernorm.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:03<00:00, 15.64w/s, dev=1]model.layers.27.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:03<00:00, 15.97w/s, dev=1]        model.layers.27.self_attn.o_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:03<00:00, 16.27w/s, dev=1]model.layers.27.self_attn.q_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:03<00:00, 16.36w/s, dev=1]model.layers.27.self_attn.rotary_emb.inv_freq:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:03<00:00, 16.44w/s, dev=1]model.layers.27.self_attn.v_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:03<00:00, 16.76w/s, dev=1]      model.layers.27.self_attn.v_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:03<00:00, 17.03w/s, dev=1]model.layers.28.mlp.gate_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:03<00:00, 17.03w/s, dev=1]   model.layers.28.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:03<00:00, 16.47w/s, dev=1]model.layers.28.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:03<00:00, 16.74w/s, dev=1]model.layers.28.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:03<00:00, 16.80w/s, dev=1]model.layers.28.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:03<00:00, 16.85w/s, dev=1]model.layers.28.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:03<00:00, 17.15w/s, dev=1]      model.layers.28.self_attn.v_proj.weight: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 59/59 [00:03<00:00, 17.39w/s, dev=1]                                                                                                0%|          | 0/59 [00:00<?, ?w/s]model.layers.28.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=1]model.layers.28.mlp.down_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 1319.79w/s, dev=1]model.layers.28.mlp.down_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 12.04w/s, dev=1]  model.layers.28.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 12.02w/s, dev=1]  model.layers.28.post_attention_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:06,  9.07w/s, dev=1]model.layers.28.post_attention_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:04, 12.07w/s, dev=1]model.layers.29.input_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:04, 12.06w/s, dev=1]         model.layers.29.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 15.07w/s, dev=1]  model.layers.29.mlp.down_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:04, 12.48w/s, dev=1]model.layers.29.mlp.gate_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:04, 12.47w/s, dev=1]model.layers.29.mlp.up_proj.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:04, 11.00w/s, dev=1]  model.layers.29.mlp.up_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:05, 10.06w/s, dev=1]model.layers.29.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:05, 10.06w/s, dev=1]model.layers.29.self_attn.k_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:04, 11.31w/s, dev=1]        model.layers.29.self_attn.o_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:03, 12.43w/s, dev=1]model.layers.29.self_attn.q_proj.weight:  19%|â–ˆâ–Š        | 11/59 [00:00<00:03, 12.93w/s, dev=1]model.layers.29.self_attn.q_proj.weight:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:03, 13.39w/s, dev=1]model.layers.29.self_attn.rotary_emb.inv_freq:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:03, 13.38w/s, dev=1]model.layers.29.self_attn.v_proj.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:03, 14.49w/s, dev=1]      model.layers.30.input_layernorm.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:02, 15.48w/s, dev=1] model.layers.30.mlp.down_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:02, 16.58w/s, dev=1]  model.layers.30.mlp.down_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:02, 15.14w/s, dev=1]model.layers.30.mlp.gate_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:02, 15.14w/s, dev=1]model.layers.30.mlp.up_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:01<00:02, 14.03w/s, dev=1]  model.layers.30.post_attention_layernorm.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:03, 13.18w/s, dev=1]model.layers.30.self_attn.k_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:02, 13.90w/s, dev=1]        model.layers.30.self_attn.k_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:02, 14.57w/s, dev=1]model.layers.30.self_attn.o_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:02, 14.56w/s, dev=1]model.layers.30.self_attn.q_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:01<00:02, 14.84w/s, dev=1]model.layers.30.self_attn.rotary_emb.inv_freq:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:02, 15.06w/s, dev=1]model.layers.30.self_attn.v_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:02, 15.74w/s, dev=1]      model.layers.31.input_layernorm.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:02, 16.33w/s, dev=1] model.layers.31.mlp.down_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:01, 17.01w/s, dev=1]  model.layers.31.mlp.down_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:02, 15.92w/s, dev=1]model.layers.31.mlp.gate_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:02, 15.92w/s, dev=1]model.layers.31.mlp.up_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:01<00:02, 15.11w/s, dev=1]  model.layers.31.post_attention_layernorm.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:02, 14.34w/s, dev=1]model.layers.31.self_attn.k_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:02, 14.85w/s, dev=1]        model.layers.31.self_attn.o_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:01<00:01, 15.29w/s, dev=1]model.layers.31.self_attn.q_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:02<00:01, 15.41w/s, dev=1]model.layers.31.self_attn.q_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:02<00:01, 15.46w/s, dev=1]model.layers.31.self_attn.rotary_emb.inv_freq:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:02<00:01, 15.46w/s, dev=1]model.layers.31.self_attn.v_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:02<00:01, 15.94w/s, dev=1]      model.layers.32.input_layernorm.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:02<00:01, 16.37w/s, dev=1] model.layers.32.mlp.down_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:02<00:01, 16.85w/s, dev=1]  model.layers.32.mlp.gate_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:02<00:01, 16.04w/s, dev=1]model.layers.32.mlp.up_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:02<00:01, 15.41w/s, dev=1]  model.layers.32.mlp.up_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:02<00:01, 14.80w/s, dev=1]model.layers.32.post_attention_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:02<00:01, 14.80w/s, dev=1]model.layers.32.self_attn.k_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:02<00:01, 15.19w/s, dev=1]        model.layers.32.self_attn.o_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:02<00:01, 15.54w/s, dev=1]model.layers.32.self_attn.q_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:02<00:01, 15.63w/s, dev=1]model.layers.32.self_attn.rotary_emb.inv_freq:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:02<00:01, 15.73w/s, dev=1]model.layers.32.self_attn.v_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:02<00:00, 16.10w/s, dev=1]      model.layers.32.self_attn.v_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:02<00:00, 16.44w/s, dev=1]model.layers.33.input_layernorm.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:02<00:00, 16.43w/s, dev=1] model.layers.33.mlp.down_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:02<00:00, 16.81w/s, dev=1]  model.layers.33.mlp.gate_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:02<00:00, 16.20w/s, dev=1]model.layers.33.mlp.up_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:03<00:00, 15.65w/s, dev=1]  model.layers.33.post_attention_layernorm.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:03<00:00, 15.15w/s, dev=1]model.layers.33.self_attn.k_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:03<00:00, 15.47w/s, dev=1]        model.layers.33.self_attn.k_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:03<00:00, 15.75w/s, dev=1]model.layers.33.self_attn.o_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:03<00:00, 15.75w/s, dev=1]model.layers.33.self_attn.q_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:03<00:00, 15.83w/s, dev=1]model.layers.33.self_attn.rotary_emb.inv_freq:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:03<00:00, 15.91w/s, dev=1]model.layers.33.self_attn.v_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:03<00:00, 16.22w/s, dev=1]      model.layers.34.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:03<00:00, 16.49w/s, dev=1]model.layers.34.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:03<00:00, 16.76w/s, dev=1]model.layers.34.self_attn.o_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:03<00:00, 16.83w/s, dev=1]model.layers.34.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:03<00:00, 16.83w/s, dev=1]model.layers.34.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:03<00:00, 16.88w/s, dev=1]model.layers.34.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:03<00:00, 17.18w/s, dev=1]                                                                                                      0%|          | 0/55 [00:00<?, ?w/s]model.layers.34.input_layernorm.weight:   0%|          | 0/55 [00:00<?, ?w/s, dev=1]model.layers.34.mlp.down_proj.weight:   2%|â–         | 1/55 [00:00<00:00, 1253.53w/s, dev=1]model.layers.34.mlp.down_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:04, 12.68w/s, dev=1]  model.layers.34.mlp.gate_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:04, 12.66w/s, dev=1]model.layers.34.mlp.up_proj.weight:   5%|â–Œ         | 3/55 [00:00<00:05,  9.36w/s, dev=1]  model.layers.34.mlp.up_proj.weight:   7%|â–‹         | 4/55 [00:00<00:06,  8.32w/s, dev=1]model.layers.34.post_attention_layernorm.weight:   7%|â–‹         | 4/55 [00:00<00:06,  8.32w/s, dev=1]model.layers.35.input_layernorm.weight:   9%|â–‰         | 5/55 [00:00<00:04, 10.39w/s, dev=1]         model.layers.35.mlp.down_proj.weight:  11%|â–ˆ         | 6/55 [00:00<00:03, 12.46w/s, dev=1]  model.layers.35.mlp.down_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:04, 10.76w/s, dev=1]model.layers.35.mlp.gate_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:04, 10.76w/s, dev=1]model.layers.35.mlp.up_proj.weight:  15%|â–ˆâ–        | 8/55 [00:00<00:04,  9.99w/s, dev=1]  model.layers.35.post_attention_layernorm.weight:  16%|â–ˆâ–‹        | 9/55 [00:00<00:04,  9.34w/s, dev=1]model.layers.35.post_attention_layernorm.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:04, 10.37w/s, dev=1]model.layers.35.self_attn.k_proj.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:04, 10.37w/s, dev=1]        model.layers.35.self_attn.o_proj.weight:  20%|â–ˆâ–ˆ        | 11/55 [00:00<00:03, 11.30w/s, dev=1]model.layers.35.self_attn.q_proj.weight:  22%|â–ˆâ–ˆâ–       | 12/55 [00:01<00:03, 11.71w/s, dev=1]model.layers.35.self_attn.q_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:01<00:03, 12.10w/s, dev=1]model.layers.35.self_attn.rotary_emb.inv_freq:  24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:01<00:03, 12.09w/s, dev=1]model.layers.35.self_attn.v_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 14/55 [00:01<00:03, 13.02w/s, dev=1]      model.layers.36.input_layernorm.weight:  27%|â–ˆâ–ˆâ–‹       | 15/55 [00:01<00:02, 13.86w/s, dev=1] model.layers.36.mlp.down_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 16/55 [00:01<00:02, 14.78w/s, dev=1]  model.layers.36.mlp.down_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:01<00:02, 13.65w/s, dev=1]model.layers.36.mlp.gate_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:01<00:02, 13.65w/s, dev=1]model.layers.36.mlp.up_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/55 [00:01<00:02, 12.88w/s, dev=1]  model.layers.36.post_attention_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–      | 19/55 [00:01<00:02, 12.24w/s, dev=1]model.layers.36.self_attn.k_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:01<00:02, 12.88w/s, dev=1]        model.layers.36.self_attn.k_proj.weight:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/55 [00:01<00:02, 13.47w/s, dev=1]model.layers.36.self_attn.o_proj.weight:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/55 [00:01<00:02, 13.47w/s, dev=1]model.layers.36.self_attn.q_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/55 [00:01<00:02, 13.72w/s, dev=1]model.layers.36.self_attn.rotary_emb.inv_freq:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 23/55 [00:01<00:02, 13.94w/s, dev=1]model.layers.36.self_attn.v_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 24/55 [00:01<00:02, 14.55w/s, dev=1]      model.layers.37.input_layernorm.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:01<00:01, 15.09w/s, dev=1] model.layers.37.mlp.down_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 26/55 [00:01<00:01, 15.70w/s, dev=1]  model.layers.37.mlp.down_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 27/55 [00:01<00:01, 14.88w/s, dev=1]model.layers.37.mlp.gate_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 27/55 [00:01<00:01, 14.88w/s, dev=1]model.layers.37.mlp.up_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 28/55 [00:01<00:01, 14.22w/s, dev=1]  model.layers.37.post_attention_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/55 [00:02<00:01, 13.61w/s, dev=1]model.layers.37.self_attn.k_proj.weight:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 30/55 [00:02<00:01, 14.08w/s, dev=1]        model.layers.37.self_attn.o_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/55 [00:02<00:01, 14.50w/s, dev=1]model.layers.37.self_attn.q_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 32/55 [00:02<00:01, 14.67w/s, dev=1]model.layers.37.self_attn.q_proj.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/55 [00:02<00:01, 14.81w/s, dev=1]model.layers.37.self_attn.rotary_emb.inv_freq:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/55 [00:02<00:01, 14.81w/s, dev=1]model.layers.37.self_attn.v_proj.weight:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/55 [00:02<00:01, 15.26w/s, dev=1]      model.layers.38.input_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:02<00:01, 15.66w/s, dev=1] model.layers.38.mlp.down_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 36/55 [00:02<00:01, 16.11w/s, dev=1]  model.layers.38.mlp.gate_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 37/55 [00:02<00:01, 15.48w/s, dev=1]model.layers.38.mlp.up_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 38/55 [00:02<00:01, 14.87w/s, dev=1]  model.layers.38.mlp.up_proj.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 39/55 [00:02<00:01, 14.38w/s, dev=1]model.layers.38.post_attention_layernorm.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 39/55 [00:02<00:01, 14.38w/s, dev=1]model.layers.38.self_attn.k_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 40/55 [00:02<00:01, 14.75w/s, dev=1]        model.layers.38.self_attn.o_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/55 [00:02<00:00, 15.07w/s, dev=1]model.layers.38.self_attn.q_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 42/55 [00:02<00:00, 15.18w/s, dev=1]model.layers.38.self_attn.rotary_emb.inv_freq:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 43/55 [00:02<00:00, 15.30w/s, dev=1]model.layers.38.self_attn.v_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 44/55 [00:02<00:00, 15.65w/s, dev=1]      model.layers.38.self_attn.v_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:02<00:00, 15.97w/s, dev=1]model.layers.39.input_layernorm.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:02<00:00, 15.97w/s, dev=1] model.layers.39.mlp.down_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 46/55 [00:02<00:00, 16.32w/s, dev=1]  model.layers.39.mlp.gate_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 47/55 [00:02<00:00, 15.77w/s, dev=1]model.layers.39.mlp.up_proj.weight:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 48/55 [00:03<00:00, 15.23w/s, dev=1]  model.layers.39.post_attention_layernorm.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 49/55 [00:03<00:00, 14.84w/s, dev=1]model.layers.39.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 50/55 [00:03<00:00, 15.14w/s, dev=1]        model.layers.39.self_attn.k_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 51/55 [00:03<00:00, 15.40w/s, dev=1]model.layers.39.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 51/55 [00:03<00:00, 15.40w/s, dev=1]model.layers.39.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 52/55 [00:03<00:00, 15.49w/s, dev=1]model.layers.39.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 53/55 [00:03<00:00, 15.56w/s, dev=1]model.layers.39.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 54/55 [00:03<00:00, 15.85w/s, dev=1]                                                                                                      0%|          | 0/57 [00:00<?, ?w/s]model.layers.40.input_layernorm.weight:   0%|          | 0/57 [00:00<?, ?w/s, dev=1]model.layers.40.mlp.down_proj.weight:   2%|â–         | 1/57 [00:00<00:00, 1259.93w/s, dev=1]model.layers.40.mlp.down_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:04, 12.16w/s, dev=1]  model.layers.40.mlp.gate_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:04, 12.14w/s, dev=1]model.layers.40.mlp.up_proj.weight:   5%|â–Œ         | 3/57 [00:00<00:05,  9.56w/s, dev=1]  model.layers.40.mlp.up_proj.weight:   7%|â–‹         | 4/57 [00:00<00:06,  8.61w/s, dev=1]model.layers.40.post_attention_layernorm.weight:   7%|â–‹         | 4/57 [00:00<00:06,  8.61w/s, dev=1]model.layers.40.self_attn.k_proj.weight:   9%|â–‰         | 5/57 [00:00<00:04, 10.75w/s, dev=1]        model.layers.40.self_attn.k_proj.weight:  11%|â–ˆ         | 6/57 [00:00<00:04, 10.61w/s, dev=1]model.layers.40.self_attn.o_proj.weight:  11%|â–ˆ         | 6/57 [00:00<00:04, 10.61w/s, dev=1]model.layers.40.self_attn.q_proj.weight:  12%|â–ˆâ–        | 7/57 [00:00<00:04, 11.46w/s, dev=1]model.layers.40.self_attn.q_proj.weight:  14%|â–ˆâ–        | 8/57 [00:00<00:05,  9.67w/s, dev=1]model.layers.40.self_attn.rotary_emb.inv_freq:  14%|â–ˆâ–        | 8/57 [00:00<00:05,  9.66w/s, dev=1]model.layers.40.self_attn.v_proj.weight:  16%|â–ˆâ–Œ        | 9/57 [00:00<00:04, 10.87w/s, dev=1]      model.layers.40.self_attn.v_proj.weight:  18%|â–ˆâ–Š        | 10/57 [00:01<00:04,  9.90w/s, dev=1]model.layers.41.input_layernorm.weight:  18%|â–ˆâ–Š        | 10/57 [00:01<00:05,  9.29w/s, dev=2] model.layers.41.mlp.down_proj.weight:  19%|â–ˆâ–‰        | 11/57 [00:01<00:05,  8.87w/s, dev=2]  model.layers.41.mlp.down_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:01<00:05,  8.38w/s, dev=2]model.layers.41.mlp.gate_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:01<00:05,  8.38w/s, dev=2]model.layers.41.mlp.up_proj.weight:  23%|â–ˆâ–ˆâ–Ž       | 13/57 [00:01<00:05,  7.55w/s, dev=2]  model.layers.41.mlp.up_proj.weight:  25%|â–ˆâ–ˆâ–       | 14/57 [00:02<00:06,  6.85w/s, dev=2]model.layers.41.post_attention_layernorm.weight:  25%|â–ˆâ–ˆâ–       | 14/57 [00:02<00:06,  6.84w/s, dev=2]model.layers.41.self_attn.k_proj.weight:  26%|â–ˆâ–ˆâ–‹       | 15/57 [00:02<00:05,  7.33w/s, dev=2]        model.layers.41.self_attn.k_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:02<00:05,  7.13w/s, dev=2]model.layers.41.self_attn.o_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:02<00:05,  7.13w/s, dev=2]model.layers.41.self_attn.q_proj.weight:  30%|â–ˆâ–ˆâ–‰       | 17/57 [00:02<00:05,  7.42w/s, dev=2]model.layers.41.self_attn.q_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 18/57 [00:02<00:05,  6.98w/s, dev=2]model.layers.41.self_attn.rotary_emb.inv_freq:  32%|â–ˆâ–ˆâ–ˆâ–      | 18/57 [00:02<00:05,  6.98w/s, dev=2]model.layers.41.self_attn.v_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 19/57 [00:02<00:05,  7.36w/s, dev=2]      model.layers.42.input_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 20/57 [00:02<00:04,  7.73w/s, dev=2] model.layers.42.mlp.down_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 21/57 [00:02<00:04,  8.11w/s, dev=2]  model.layers.42.mlp.down_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:02<00:04,  8.00w/s, dev=2]model.layers.42.mlp.gate_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:02<00:04,  8.00w/s, dev=2]model.layers.42.mlp.up_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 23/57 [00:02<00:04,  7.89w/s, dev=2]  model.layers.42.post_attention_layernorm.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/57 [00:03<00:04,  7.80w/s, dev=2]model.layers.42.self_attn.k_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/57 [00:03<00:03,  8.12w/s, dev=2]        model.layers.42.self_attn.k_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 26/57 [00:03<00:03,  8.05w/s, dev=2]model.layers.42.self_attn.o_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 26/57 [00:03<00:03,  8.05w/s, dev=2]model.layers.42.self_attn.q_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 27/57 [00:03<00:03,  8.24w/s, dev=2]model.layers.42.self_attn.rotary_emb.inv_freq:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:03<00:03,  8.42w/s, dev=2]model.layers.42.self_attn.v_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 29/57 [00:03<00:03,  8.72w/s, dev=2]      model.layers.42.self_attn.v_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 30/57 [00:03<00:03,  8.99w/s, dev=2]model.layers.43.input_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 30/57 [00:03<00:03,  8.99w/s, dev=2] model.layers.43.mlp.down_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 31/57 [00:03<00:02,  9.29w/s, dev=2]  model.layers.43.mlp.gate_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 32/57 [00:03<00:02,  9.14w/s, dev=2]model.layers.43.mlp.up_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 33/57 [00:03<00:02,  8.63w/s, dev=2]  model.layers.43.mlp.up_proj.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:04<00:02,  8.49w/s, dev=2]model.layers.43.post_attention_layernorm.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:04<00:02,  8.49w/s, dev=2]model.layers.43.self_attn.k_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/57 [00:04<00:02,  8.74w/s, dev=2]        model.layers.43.self_attn.o_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 36/57 [00:04<00:02,  8.89w/s, dev=2]model.layers.43.self_attn.q_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 37/57 [00:04<00:02,  9.01w/s, dev=2]model.layers.43.self_attn.q_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 38/57 [00:04<00:02,  9.13w/s, dev=2]model.layers.43.self_attn.rotary_emb.inv_freq:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 38/57 [00:04<00:02,  9.13w/s, dev=2]model.layers.43.self_attn.v_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 39/57 [00:04<00:01,  9.37w/s, dev=2]      model.layers.44.input_layernorm.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:04<00:01,  9.60w/s, dev=2] model.layers.44.mlp.down_proj.weight:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/57 [00:04<00:01,  9.83w/s, dev=2]  model.layers.44.mlp.down_proj.weight:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 42/57 [00:04<00:01,  9.67w/s, dev=2]model.layers.44.mlp.gate_proj.weight:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 42/57 [00:04<00:01,  9.66w/s, dev=2]model.layers.44.mlp.up_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 43/57 [00:04<00:01,  9.26w/s, dev=2]  model.layers.44.post_attention_layernorm.weight:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 44/57 [00:04<00:01,  9.17w/s, dev=2]model.layers.44.self_attn.k_proj.weight:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 45/57 [00:04<00:01,  9.37w/s, dev=2]        model.layers.44.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:04<00:01,  9.31w/s, dev=2]model.layers.44.self_attn.o_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:04<00:01,  9.31w/s, dev=2]model.layers.44.self_attn.q_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 47/57 [00:04<00:01,  9.41w/s, dev=2]model.layers.44.self_attn.rotary_emb.inv_freq:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/57 [00:05<00:00,  9.51w/s, dev=2]model.layers.44.self_attn.v_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 49/57 [00:05<00:00,  9.71w/s, dev=2]      model.layers.44.self_attn.v_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 50/57 [00:05<00:00,  9.87w/s, dev=2]model.layers.45.mlp.down_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 50/57 [00:05<00:00,  9.87w/s, dev=2]   model.layers.45.mlp.gate_proj.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 51/57 [00:05<00:00,  9.75w/s, dev=2]model.layers.45.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:05<00:00,  9.61w/s, dev=2]model.layers.45.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 53/57 [00:05<00:00,  9.63w/s, dev=2]model.layers.45.self_attn.o_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/57 [00:05<00:00,  9.71w/s, dev=2]model.layers.45.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/57 [00:05<00:00,  9.71w/s, dev=2]model.layers.45.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 55/57 [00:05<00:00,  9.80w/s, dev=2]model.layers.45.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 56/57 [00:05<00:00,  9.97w/s, dev=2]                                                                                                      0%|          | 0/59 [00:00<?, ?w/s]model.layers.45.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=2]model.layers.45.mlp.up_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 1104.93w/s, dev=2]model.layers.45.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:08,  6.51w/s, dev=2]  model.layers.45.post_attention_layernorm.weight:   3%|â–Ž         | 2/59 [00:00<00:08,  6.50w/s, dev=2]model.layers.46.input_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:05,  9.74w/s, dev=2]         model.layers.46.mlp.down_proj.weight:   7%|â–‹         | 4/59 [00:00<00:04, 12.98w/s, dev=2]  model.layers.46.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:05, 10.71w/s, dev=2]model.layers.46.mlp.gate_proj.weight:   8%|â–Š         | 5/59 [00:00<00:05, 10.71w/s, dev=2]model.layers.46.mlp.up_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:07,  6.81w/s, dev=2]  model.layers.46.post_attention_layernorm.weight:  12%|â–ˆâ–        | 7/59 [00:01<00:07,  6.75w/s, dev=2]model.layers.46.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:01<00:06,  7.71w/s, dev=2]model.layers.46.self_attn.k_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:01<00:06,  7.71w/s, dev=2]        model.layers.46.self_attn.o_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:01<00:05,  8.61w/s, dev=2]model.layers.46.self_attn.q_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:01<00:05,  9.16w/s, dev=2]model.layers.46.self_attn.q_proj.weight:  19%|â–ˆâ–Š        | 11/59 [00:01<00:05,  8.43w/s, dev=2]model.layers.46.self_attn.rotary_emb.inv_freq:  19%|â–ˆâ–Š        | 11/59 [00:01<00:05,  8.42w/s, dev=2]model.layers.46.self_attn.v_proj.weight:  20%|â–ˆâ–ˆ        | 12/59 [00:01<00:05,  9.19w/s, dev=2]      model.layers.47.input_layernorm.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:01<00:04,  9.86w/s, dev=2] model.layers.47.mlp.down_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:01<00:04, 10.62w/s, dev=2]  model.layers.47.mlp.down_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:01<00:04, 10.13w/s, dev=2]model.layers.47.mlp.gate_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:01<00:04, 10.12w/s, dev=2]model.layers.47.mlp.up_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:04,  9.74w/s, dev=2]  model.layers.47.post_attention_layernorm.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:01<00:04,  8.87w/s, dev=2]model.layers.47.self_attn.k_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:04,  9.38w/s, dev=2]        model.layers.47.self_attn.k_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:04,  9.87w/s, dev=2]model.layers.47.self_attn.o_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:04,  9.86w/s, dev=2]model.layers.47.self_attn.q_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:03, 10.12w/s, dev=2]model.layers.47.self_attn.rotary_emb.inv_freq:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:02<00:03, 10.35w/s, dev=2]model.layers.47.self_attn.v_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:02<00:03, 10.84w/s, dev=2]      model.layers.47.self_attn.v_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:02<00:03, 11.29w/s, dev=2]model.layers.48.input_layernorm.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:02<00:03, 11.29w/s, dev=2] model.layers.48.mlp.down_proj.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:02<00:02, 11.78w/s, dev=2]  model.layers.48.mlp.gate_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:02<00:02, 11.40w/s, dev=2]model.layers.48.mlp.up_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:02<00:03, 10.75w/s, dev=2]  model.layers.48.mlp.up_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:02<00:03, 10.47w/s, dev=2]model.layers.48.post_attention_layernorm.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:02<00:03, 10.46w/s, dev=2]model.layers.48.self_attn.k_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:02<00:02, 10.85w/s, dev=2]        model.layers.48.self_attn.o_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:02<00:02, 11.20w/s, dev=2]model.layers.48.self_attn.q_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:02<00:02, 11.38w/s, dev=2]model.layers.48.self_attn.q_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:02<00:02, 10.66w/s, dev=2]model.layers.48.self_attn.rotary_emb.inv_freq:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:02<00:02, 10.66w/s, dev=2]model.layers.48.self_attn.v_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:02<00:02, 11.00w/s, dev=2]      model.layers.49.input_layernorm.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:02<00:02, 11.32w/s, dev=2] model.layers.49.mlp.down_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:02<00:02, 11.66w/s, dev=2]  model.layers.49.mlp.down_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:03<00:02, 11.35w/s, dev=2]model.layers.49.mlp.gate_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:03<00:02, 11.35w/s, dev=2]model.layers.49.mlp.up_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:03<00:02, 11.11w/s, dev=2]  model.layers.49.post_attention_layernorm.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:03<00:02, 10.19w/s, dev=2]model.layers.49.self_attn.k_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:03<00:02, 10.46w/s, dev=2]        model.layers.49.self_attn.k_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:03<00:01, 10.70w/s, dev=2]model.layers.49.self_attn.o_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:03<00:01, 10.70w/s, dev=2]model.layers.49.self_attn.q_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:03<00:01, 10.82w/s, dev=2]model.layers.49.self_attn.rotary_emb.inv_freq:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:03<00:01, 10.92w/s, dev=2]model.layers.49.self_attn.v_proj.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:03<00:01, 11.19w/s, dev=2]      model.layers.49.self_attn.v_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:03<00:01, 11.43w/s, dev=2]model.layers.50.input_layernorm.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:03<00:01, 11.43w/s, dev=2] model.layers.50.mlp.down_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:03<00:01, 11.70w/s, dev=2]  model.layers.50.mlp.gate_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:03<00:01, 11.49w/s, dev=2]model.layers.50.mlp.up_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:04<00:01, 11.05w/s, dev=2]  model.layers.50.mlp.up_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:04<00:01, 10.89w/s, dev=2]model.layers.50.post_attention_layernorm.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:04<00:01, 10.89w/s, dev=2]model.layers.50.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:04<00:00, 11.12w/s, dev=2]        model.layers.50.self_attn.o_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:04<00:00, 11.33w/s, dev=2]model.layers.50.self_attn.q_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:04<00:00, 11.40w/s, dev=2]model.layers.50.self_attn.q_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:04<00:00, 11.49w/s, dev=2]model.layers.50.self_attn.rotary_emb.inv_freq:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:04<00:00, 11.49w/s, dev=2]model.layers.50.self_attn.v_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:04<00:00, 11.71w/s, dev=2]      model.layers.51.mlp.gate_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:04<00:00, 11.92w/s, dev=2]   model.layers.51.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:04<00:00, 11.18w/s, dev=2]model.layers.51.self_attn.k_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:04<00:00, 11.36w/s, dev=2]model.layers.51.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:04<00:00, 11.36w/s, dev=2]model.layers.51.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:04<00:00, 11.46w/s, dev=2]model.layers.51.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:04<00:00, 11.55w/s, dev=2]model.layers.51.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:04<00:00, 11.75w/s, dev=2]      model.layers.51.self_attn.v_proj.weight: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 59/59 [00:04<00:00, 11.92w/s, dev=2]                                                                                                0%|          | 0/59 [00:00<?, ?w/s]model.layers.51.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=2]model.layers.51.mlp.down_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 1015.08w/s, dev=2]model.layers.51.mlp.down_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 12.17w/s, dev=2]  model.layers.51.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 12.15w/s, dev=2]  model.layers.51.post_attention_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:05,  9.40w/s, dev=2]model.layers.51.post_attention_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:04, 12.51w/s, dev=2]model.layers.52.input_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:04, 12.51w/s, dev=2]         model.layers.52.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 15.63w/s, dev=2]  model.layers.52.mlp.down_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:04, 12.85w/s, dev=2]model.layers.52.mlp.gate_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:04, 12.84w/s, dev=2]model.layers.52.mlp.up_proj.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:04, 11.45w/s, dev=2]  model.layers.52.mlp.up_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:04, 10.50w/s, dev=2]model.layers.52.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:04, 10.49w/s, dev=2]model.layers.52.self_attn.k_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:04, 11.80w/s, dev=2]        model.layers.52.self_attn.o_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:03, 12.97w/s, dev=2]model.layers.52.self_attn.q_proj.weight:  19%|â–ˆâ–Š        | 11/59 [00:00<00:03, 13.43w/s, dev=2]model.layers.52.self_attn.q_proj.weight:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:03, 13.85w/s, dev=2]model.layers.52.self_attn.rotary_emb.inv_freq:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:03, 13.85w/s, dev=2]model.layers.52.self_attn.v_proj.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:03, 15.00w/s, dev=2]      model.layers.53.input_layernorm.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:02, 16.03w/s, dev=2] model.layers.53.mlp.down_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:02, 17.17w/s, dev=2]  model.layers.53.mlp.down_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:02, 15.48w/s, dev=2]model.layers.53.mlp.gate_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:02, 15.48w/s, dev=2]model.layers.53.mlp.up_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:01<00:02, 14.30w/s, dev=2]  model.layers.53.post_attention_layernorm.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:03, 13.35w/s, dev=2]model.layers.53.self_attn.k_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:02, 14.08w/s, dev=2]        model.layers.53.self_attn.k_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:02, 14.73w/s, dev=2]model.layers.53.self_attn.o_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:02, 14.73w/s, dev=2]model.layers.53.self_attn.q_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:01<00:02, 14.95w/s, dev=2]model.layers.53.self_attn.rotary_emb.inv_freq:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:02, 15.17w/s, dev=2]model.layers.53.self_attn.v_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:02, 15.85w/s, dev=2]      model.layers.53.self_attn.v_proj.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:02, 16.44w/s, dev=2]model.layers.54.input_layernorm.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:02, 16.44w/s, dev=2] model.layers.54.mlp.down_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:01, 17.12w/s, dev=2]  model.layers.54.mlp.gate_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:02, 16.07w/s, dev=2]model.layers.54.mlp.up_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:01<00:02, 15.20w/s, dev=2]  model.layers.54.mlp.up_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:02, 14.49w/s, dev=2]model.layers.54.post_attention_layernorm.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:02, 14.49w/s, dev=2]model.layers.54.self_attn.k_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:01, 15.01w/s, dev=2]        model.layers.54.self_attn.o_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:01<00:01, 15.47w/s, dev=2]model.layers.54.self_attn.q_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:01<00:01, 15.65w/s, dev=2]model.layers.54.self_attn.rotary_emb.inv_freq:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:02<00:01, 15.81w/s, dev=2]model.layers.54.self_attn.v_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:02<00:01, 16.30w/s, dev=2]      model.layers.55.input_layernorm.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:02<00:01, 16.75w/s, dev=2] model.layers.55.mlp.down_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:02<00:01, 17.24w/s, dev=2]  model.layers.55.mlp.down_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:02<00:01, 16.54w/s, dev=2]model.layers.55.mlp.gate_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:02<00:01, 16.54w/s, dev=2]model.layers.55.mlp.up_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:02<00:01, 15.85w/s, dev=2]  model.layers.55.post_attention_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:02<00:01, 15.23w/s, dev=2]model.layers.55.self_attn.k_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:02<00:01, 15.63w/s, dev=2]        model.layers.55.self_attn.o_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:02<00:01, 15.99w/s, dev=2]model.layers.55.self_attn.q_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:02<00:01, 16.09w/s, dev=2]model.layers.55.self_attn.rotary_emb.inv_freq:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:02<00:01, 16.20w/s, dev=2]model.layers.55.self_attn.v_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:02<00:00, 16.58w/s, dev=2]      model.layers.55.self_attn.v_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:02<00:00, 16.93w/s, dev=2]model.layers.56.input_layernorm.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:02<00:00, 16.93w/s, dev=2] model.layers.56.mlp.down_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:02<00:00, 17.31w/s, dev=2]  model.layers.56.mlp.gate_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:02<00:00, 16.70w/s, dev=2]model.layers.56.mlp.up_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:02<00:00, 16.16w/s, dev=2]  model.layers.56.post_attention_layernorm.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:03<00:00, 15.70w/s, dev=2]model.layers.56.self_attn.k_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:03<00:00, 16.02w/s, dev=2]        model.layers.56.self_attn.o_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:03<00:00, 16.32w/s, dev=2]model.layers.56.self_attn.q_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:03<00:00, 16.39w/s, dev=2]model.layers.56.self_attn.q_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:03<00:00, 16.47w/s, dev=2]model.layers.56.self_attn.rotary_emb.inv_freq:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:03<00:00, 16.47w/s, dev=2]model.layers.56.self_attn.v_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:03<00:00, 16.79w/s, dev=2]      model.layers.57.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:03<00:00, 17.07w/s, dev=2]model.layers.57.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:03<00:00, 17.34w/s, dev=2]model.layers.57.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:03<00:00, 17.40w/s, dev=2]model.layers.57.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:03<00:00, 17.45w/s, dev=2]model.layers.57.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:03<00:00, 17.76w/s, dev=2]                                                                                                      0%|          | 0/55 [00:00<?, ?w/s]model.layers.57.input_layernorm.weight:   0%|          | 0/55 [00:00<?, ?w/s, dev=2]model.layers.57.mlp.down_proj.weight:   2%|â–         | 1/55 [00:00<00:00, 1080.45w/s, dev=2]model.layers.57.mlp.down_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:04, 12.65w/s, dev=2]  model.layers.57.mlp.gate_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:04, 12.63w/s, dev=2]model.layers.57.mlp.up_proj.weight:   5%|â–Œ         | 3/55 [00:00<00:05,  9.68w/s, dev=2]  model.layers.57.mlp.up_proj.weight:   7%|â–‹         | 4/55 [00:00<00:06,  8.50w/s, dev=2]model.layers.57.post_attention_layernorm.weight:   7%|â–‹         | 4/55 [00:00<00:06,  8.49w/s, dev=2]model.layers.58.input_layernorm.weight:   9%|â–‰         | 5/55 [00:00<00:04, 10.61w/s, dev=2]         model.layers.58.mlp.down_proj.weight:  11%|â–ˆ         | 6/55 [00:00<00:03, 12.72w/s, dev=2]  model.layers.58.mlp.down_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:04, 11.14w/s, dev=2]model.layers.58.mlp.gate_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:04, 11.14w/s, dev=2]model.layers.58.mlp.up_proj.weight:  15%|â–ˆâ–        | 8/55 [00:00<00:04, 10.23w/s, dev=2]  model.layers.58.post_attention_layernorm.weight:  16%|â–ˆâ–‹        | 9/55 [00:00<00:04,  9.57w/s, dev=2]model.layers.58.post_attention_layernorm.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:04, 10.63w/s, dev=2]model.layers.58.self_attn.k_proj.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:04, 10.63w/s, dev=2]        model.layers.58.self_attn.o_proj.weight:  20%|â–ˆâ–ˆ        | 11/55 [00:00<00:03, 11.58w/s, dev=2]model.layers.58.self_attn.q_proj.weight:  22%|â–ˆâ–ˆâ–       | 12/55 [00:00<00:03, 12.06w/s, dev=2]model.layers.58.self_attn.q_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:01<00:03, 12.48w/s, dev=2]model.layers.58.self_attn.rotary_emb.inv_freq:  24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:01<00:03, 12.48w/s, dev=2]model.layers.58.self_attn.v_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 14/55 [00:01<00:03, 13.43w/s, dev=2]      model.layers.59.input_layernorm.weight:  27%|â–ˆâ–ˆâ–‹       | 15/55 [00:01<00:02, 14.31w/s, dev=2] model.layers.59.mlp.down_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 16/55 [00:01<00:02, 15.25w/s, dev=2]  model.layers.59.mlp.down_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:01<00:02, 14.16w/s, dev=2]model.layers.59.mlp.gate_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:01<00:02, 14.16w/s, dev=2]model.layers.59.mlp.up_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/55 [00:01<00:02, 13.36w/s, dev=2]  model.layers.59.post_attention_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–      | 19/55 [00:01<00:02, 12.69w/s, dev=2]model.layers.59.self_attn.k_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:01<00:02, 13.35w/s, dev=2]        model.layers.59.self_attn.k_proj.weight:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/55 [00:01<00:02, 13.96w/s, dev=2]model.layers.59.self_attn.o_proj.weight:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/55 [00:01<00:02, 13.96w/s, dev=2]model.layers.59.self_attn.q_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/55 [00:01<00:02, 14.22w/s, dev=2]model.layers.59.self_attn.rotary_emb.inv_freq:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 23/55 [00:01<00:02, 14.46w/s, dev=2]model.layers.59.self_attn.v_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 24/55 [00:01<00:02, 15.09w/s, dev=2]      model.layers.60.input_layernorm.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:01<00:01, 15.66w/s, dev=2] model.layers.60.mlp.down_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 26/55 [00:01<00:01, 16.28w/s, dev=2]  model.layers.60.mlp.down_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 27/55 [00:01<00:01, 15.50w/s, dev=2]model.layers.60.mlp.gate_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 27/55 [00:01<00:01, 15.50w/s, dev=2]model.layers.60.mlp.up_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 28/55 [00:01<00:01, 14.80w/s, dev=2]  model.layers.60.post_attention_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/55 [00:02<00:01, 14.21w/s, dev=2]model.layers.60.self_attn.k_proj.weight:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 30/55 [00:02<00:01, 14.70w/s, dev=2]        model.layers.60.self_attn.o_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/55 [00:02<00:01, 15.14w/s, dev=2]model.layers.60.self_attn.q_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 32/55 [00:02<00:01, 15.31w/s, dev=2]model.layers.60.self_attn.q_proj.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/55 [00:02<00:01, 15.47w/s, dev=2]model.layers.60.self_attn.rotary_emb.inv_freq:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/55 [00:02<00:01, 15.47w/s, dev=2]model.layers.60.self_attn.v_proj.weight:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/55 [00:02<00:01, 15.94w/s, dev=2]      model.layers.61.input_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:02<00:01, 16.35w/s, dev=2] model.layers.61.mlp.down_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 36/55 [00:02<00:01, 16.82w/s, dev=2]  model.layers.61.mlp.gate_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 37/55 [00:02<00:01, 16.13w/s, dev=2]model.layers.61.mlp.up_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 38/55 [00:02<00:01, 15.48w/s, dev=2]  model.layers.61.mlp.up_proj.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 39/55 [00:02<00:01, 14.95w/s, dev=2]model.layers.61.post_attention_layernorm.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 39/55 [00:02<00:01, 14.95w/s, dev=2]model.layers.61.self_attn.k_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 40/55 [00:02<00:00, 15.33w/s, dev=2]        model.layers.61.self_attn.o_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/55 [00:02<00:00, 15.67w/s, dev=2]model.layers.61.self_attn.q_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 42/55 [00:02<00:00, 15.78w/s, dev=2]model.layers.61.self_attn.rotary_emb.inv_freq:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 43/55 [00:02<00:00, 15.88w/s, dev=2]model.layers.61.self_attn.v_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 44/55 [00:02<00:00, 16.24w/s, dev=2]      model.layers.61.self_attn.v_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:02<00:00, 16.56w/s, dev=2]model.layers.62.input_layernorm.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:03<00:00, 14.84w/s, dev=3] model.layers.62.mlp.down_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 46/55 [00:03<00:00, 15.16w/s, dev=3]  model.layers.62.mlp.gate_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 47/55 [00:03<00:00, 14.74w/s, dev=3]model.layers.62.mlp.up_proj.weight:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 48/55 [00:03<00:00, 14.33w/s, dev=3]  model.layers.62.post_attention_layernorm.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 49/55 [00:03<00:00, 13.96w/s, dev=3]model.layers.62.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 50/55 [00:03<00:00, 14.24w/s, dev=3]        model.layers.62.self_attn.k_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 51/55 [00:03<00:00, 14.50w/s, dev=3]model.layers.62.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 51/55 [00:03<00:00, 14.50w/s, dev=3]model.layers.62.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 52/55 [00:03<00:00, 14.60w/s, dev=3]model.layers.62.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 53/55 [00:03<00:00, 14.69w/s, dev=3]model.layers.62.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 54/55 [00:03<00:00, 14.96w/s, dev=3]                                                                                                      0%|          | 0/57 [00:00<?, ?w/s]model.layers.63.input_layernorm.weight:   0%|          | 0/57 [00:00<?, ?w/s, dev=3]model.layers.63.mlp.down_proj.weight:   2%|â–         | 1/57 [00:00<00:00, 1007.76w/s, dev=3]model.layers.63.mlp.down_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:04, 13.00w/s, dev=3]  model.layers.63.mlp.gate_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:04, 12.98w/s, dev=3]model.layers.63.mlp.up_proj.weight:   5%|â–Œ         | 3/57 [00:00<00:05,  9.50w/s, dev=3]  model.layers.63.mlp.up_proj.weight:   7%|â–‹         | 4/57 [00:00<00:06,  8.31w/s, dev=3]model.layers.63.post_attention_layernorm.weight:   7%|â–‹         | 4/57 [00:00<00:06,  8.30w/s, dev=3]model.layers.63.self_attn.k_proj.weight:   9%|â–‰         | 5/57 [00:00<00:05, 10.37w/s, dev=3]        model.layers.63.self_attn.o_proj.weight:  11%|â–ˆ         | 6/57 [00:00<00:04, 12.27w/s, dev=3]model.layers.63.self_attn.q_proj.weight:  12%|â–ˆâ–        | 7/57 [00:00<00:03, 13.01w/s, dev=3]model.layers.63.self_attn.q_proj.weight:  14%|â–ˆâ–        | 8/57 [00:00<00:03, 13.65w/s, dev=3]model.layers.63.self_attn.rotary_emb.inv_freq:  14%|â–ˆâ–        | 8/57 [00:00<00:03, 13.65w/s, dev=3]model.layers.63.self_attn.v_proj.weight:  16%|â–ˆâ–Œ        | 9/57 [00:00<00:03, 15.34w/s, dev=3]      model.layers.64.input_layernorm.weight:  18%|â–ˆâ–Š        | 10/57 [00:00<00:02, 16.86w/s, dev=3]model.layers.64.mlp.down_proj.weight:  19%|â–ˆâ–‰        | 11/57 [00:00<00:02, 18.54w/s, dev=3]  model.layers.64.mlp.down_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:00<00:02, 16.02w/s, dev=3]model.layers.64.mlp.gate_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:00<00:02, 16.02w/s, dev=3]model.layers.64.mlp.up_proj.weight:  23%|â–ˆâ–ˆâ–Ž       | 13/57 [00:00<00:03, 14.23w/s, dev=3]  model.layers.64.post_attention_layernorm.weight:  25%|â–ˆâ–ˆâ–       | 14/57 [00:01<00:03, 13.04w/s, dev=3]model.layers.64.self_attn.k_proj.weight:  26%|â–ˆâ–ˆâ–‹       | 15/57 [00:01<00:03, 13.96w/s, dev=3]        model.layers.64.self_attn.k_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:01<00:02, 14.80w/s, dev=3]model.layers.64.self_attn.o_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:01<00:02, 14.80w/s, dev=3]model.layers.64.self_attn.q_proj.weight:  30%|â–ˆâ–ˆâ–‰       | 17/57 [00:01<00:02, 15.09w/s, dev=3]model.layers.64.self_attn.rotary_emb.inv_freq:  32%|â–ˆâ–ˆâ–ˆâ–      | 18/57 [00:01<00:02, 15.35w/s, dev=3]model.layers.64.self_attn.v_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 19/57 [00:01<00:02, 16.20w/s, dev=3]      model.layers.65.input_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 20/57 [00:01<00:02, 16.96w/s, dev=3] model.layers.65.mlp.down_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 21/57 [00:01<00:02, 17.80w/s, dev=3]  model.layers.65.mlp.down_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:01<00:02, 16.44w/s, dev=3]model.layers.65.mlp.gate_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:01<00:02, 16.44w/s, dev=3]model.layers.65.mlp.up_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 23/57 [00:01<00:02, 15.37w/s, dev=3]  model.layers.65.post_attention_layernorm.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/57 [00:01<00:02, 14.56w/s, dev=3]model.layers.65.self_attn.k_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/57 [00:01<00:02, 15.16w/s, dev=3]        model.layers.65.self_attn.o_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 26/57 [00:01<00:01, 15.71w/s, dev=3]model.layers.65.self_attn.q_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 27/57 [00:01<00:01, 15.90w/s, dev=3]model.layers.65.self_attn.q_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:01<00:01, 16.07w/s, dev=3]model.layers.65.self_attn.rotary_emb.inv_freq:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:01<00:01, 16.07w/s, dev=3]model.layers.65.self_attn.v_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 29/57 [00:01<00:01, 16.64w/s, dev=3]      model.layers.66.input_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 30/57 [00:01<00:01, 17.14w/s, dev=3] model.layers.66.mlp.down_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 31/57 [00:01<00:01, 17.71w/s, dev=3]  model.layers.66.mlp.gate_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 32/57 [00:01<00:01, 16.77w/s, dev=3]model.layers.66.mlp.up_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 33/57 [00:02<00:01, 15.95w/s, dev=3]  model.layers.66.mlp.up_proj.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:02<00:01, 15.24w/s, dev=3]model.layers.66.post_attention_layernorm.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:02<00:01, 15.24w/s, dev=3]model.layers.66.self_attn.k_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/57 [00:02<00:01, 15.69w/s, dev=3]        model.layers.66.self_attn.o_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 36/57 [00:02<00:01, 16.08w/s, dev=3]model.layers.66.self_attn.q_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 37/57 [00:02<00:01, 16.20w/s, dev=3]model.layers.66.self_attn.rotary_emb.inv_freq:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 38/57 [00:02<00:01, 16.31w/s, dev=3]model.layers.66.self_attn.v_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 39/57 [00:02<00:01, 16.74w/s, dev=3]      model.layers.66.self_attn.v_proj.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:02<00:00, 17.10w/s, dev=3]model.layers.67.input_layernorm.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:02<00:00, 17.09w/s, dev=3] model.layers.67.mlp.down_proj.weight:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/57 [00:02<00:00, 17.52w/s, dev=3]  model.layers.67.mlp.gate_proj.weight:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 42/57 [00:02<00:00, 16.82w/s, dev=3]model.layers.67.mlp.up_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 43/57 [00:02<00:00, 16.26w/s, dev=3]  model.layers.67.post_attention_layernorm.weight:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 44/57 [00:02<00:00, 15.71w/s, dev=3]model.layers.67.self_attn.k_proj.weight:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 45/57 [00:02<00:00, 16.07w/s, dev=3]        model.layers.67.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:02<00:00, 16.37w/s, dev=3]model.layers.67.self_attn.o_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:02<00:00, 16.37w/s, dev=3]model.layers.67.self_attn.q_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 47/57 [00:02<00:00, 16.44w/s, dev=3]model.layers.67.self_attn.rotary_emb.inv_freq:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/57 [00:02<00:00, 16.51w/s, dev=3]model.layers.67.self_attn.v_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 49/57 [00:02<00:00, 16.85w/s, dev=3]      model.layers.68.mlp.down_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 50/57 [00:02<00:00, 17.16w/s, dev=3]   model.layers.68.mlp.gate_proj.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 51/57 [00:03<00:00, 16.63w/s, dev=3]model.layers.68.mlp.gate_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:03<00:00, 16.18w/s, dev=3]model.layers.68.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:03<00:00, 16.18w/s, dev=3]model.layers.68.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 53/57 [00:03<00:00, 16.45w/s, dev=3]model.layers.68.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/57 [00:03<00:00, 16.53w/s, dev=3]model.layers.68.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 55/57 [00:03<00:00, 16.61w/s, dev=3]model.layers.68.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 56/57 [00:03<00:00, 16.91w/s, dev=3]                                                                                                      0%|          | 0/59 [00:00<?, ?w/s]model.layers.68.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=3]model.layers.68.mlp.up_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 906.48w/s, dev=3]model.layers.68.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 13.82w/s, dev=3] model.layers.68.post_attention_layernorm.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 13.79w/s, dev=3]model.layers.69.input_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:02, 20.64w/s, dev=3]         model.layers.69.mlp.down_proj.weight:   7%|â–‹         | 4/59 [00:00<00:02, 27.47w/s, dev=3]  model.layers.69.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 17.24w/s, dev=3]model.layers.69.mlp.gate_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 17.22w/s, dev=3]model.layers.69.mlp.up_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:03, 13.42w/s, dev=3]  model.layers.69.post_attention_layernorm.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:04, 11.68w/s, dev=3]model.layers.69.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:03, 13.34w/s, dev=3]model.layers.69.self_attn.k_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:03, 13.33w/s, dev=3]        model.layers.69.self_attn.o_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:03, 14.84w/s, dev=3]model.layers.69.self_attn.q_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:03, 15.36w/s, dev=3]model.layers.69.self_attn.rotary_emb.inv_freq:  19%|â–ˆâ–Š        | 11/59 [00:00<00:03, 15.75w/s, dev=3]model.layers.69.self_attn.v_proj.weight:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:02, 17.17w/s, dev=3]      model.layers.69.self_attn.v_proj.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:02, 18.40w/s, dev=3]model.layers.70.input_layernorm.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:02, 18.39w/s, dev=3] model.layers.70.mlp.down_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:02, 19.80w/s, dev=3]  model.layers.70.mlp.gate_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:02, 17.41w/s, dev=3]model.layers.70.mlp.up_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:02, 15.88w/s, dev=3]  model.layers.70.post_attention_layernorm.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:01<00:02, 14.77w/s, dev=3]model.layers.70.post_attention_layernorm.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:02, 15.63w/s, dev=3]model.layers.70.self_attn.k_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:02, 15.63w/s, dev=3]        model.layers.70.self_attn.o_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:02, 16.41w/s, dev=3]model.layers.70.self_attn.q_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:02, 16.68w/s, dev=3]model.layers.70.self_attn.rotary_emb.inv_freq:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:01<00:02, 16.88w/s, dev=3]model.layers.70.self_attn.v_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:02, 17.68w/s, dev=3]      model.layers.71.input_layernorm.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:01, 18.39w/s, dev=3] model.layers.71.mlp.down_proj.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:01, 19.18w/s, dev=3]  model.layers.71.mlp.down_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:01, 17.93w/s, dev=3]model.layers.71.mlp.gate_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:01, 17.93w/s, dev=3]model.layers.71.mlp.up_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:01, 16.86w/s, dev=3]  model.layers.71.post_attention_layernorm.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:01<00:01, 16.01w/s, dev=3]model.layers.71.self_attn.k_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:01, 16.60w/s, dev=3]        model.layers.71.self_attn.o_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:01, 17.13w/s, dev=3]model.layers.71.self_attn.q_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:01<00:01, 17.29w/s, dev=3]model.layers.71.self_attn.rotary_emb.inv_freq:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:01<00:01, 17.44w/s, dev=3]model.layers.71.self_attn.rotary_emb.inv_freq:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:01<00:01, 18.00w/s, dev=3]model.layers.71.self_attn.v_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:01<00:01, 18.00w/s, dev=3]      model.layers.72.input_layernorm.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:01<00:01, 18.49w/s, dev=3] model.layers.72.mlp.down_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:01<00:01, 19.04w/s, dev=3]  model.layers.72.mlp.gate_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:01<00:01, 18.13w/s, dev=3]model.layers.72.mlp.up_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:02<00:01, 17.34w/s, dev=3]  model.layers.72.post_attention_layernorm.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:02<00:01, 16.68w/s, dev=3]model.layers.72.self_attn.k_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:02<00:01, 17.13w/s, dev=3]        model.layers.72.self_attn.k_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:02<00:01, 17.53w/s, dev=3]model.layers.72.self_attn.o_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:02<00:01, 17.53w/s, dev=3]model.layers.72.self_attn.q_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:02<00:01, 17.65w/s, dev=3]model.layers.72.self_attn.rotary_emb.inv_freq:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:02<00:01, 17.76w/s, dev=3]model.layers.72.self_attn.v_proj.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:02<00:00, 18.19w/s, dev=3]      model.layers.73.input_layernorm.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:02<00:00, 18.56w/s, dev=3] model.layers.73.mlp.down_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:02<00:00, 18.99w/s, dev=3]  model.layers.73.mlp.gate_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:02<00:00, 18.27w/s, dev=3]model.layers.73.mlp.gate_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:02<00:00, 17.64w/s, dev=3]model.layers.73.mlp.up_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:02<00:00, 17.64w/s, dev=3]  model.layers.73.post_attention_layernorm.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:02<00:00, 17.08w/s, dev=3]model.layers.73.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:02<00:00, 17.44w/s, dev=3]        model.layers.73.self_attn.o_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:02<00:00, 17.75w/s, dev=3]model.layers.73.self_attn.q_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:02<00:00, 17.84w/s, dev=3]model.layers.73.self_attn.rotary_emb.inv_freq:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:02<00:00, 17.91w/s, dev=3]model.layers.73.self_attn.v_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:02<00:00, 18.26w/s, dev=3]      model.layers.73.self_attn.v_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:02<00:00, 18.56w/s, dev=3]model.layers.74.mlp.gate_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:02<00:00, 18.56w/s, dev=3]   model.layers.74.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:03<00:00, 17.99w/s, dev=3]model.layers.74.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:03<00:00, 18.27w/s, dev=3]model.layers.74.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:03<00:00, 18.33w/s, dev=3]model.layers.74.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:03<00:00, 18.40w/s, dev=3]model.layers.74.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:03<00:00, 18.72w/s, dev=3]                                                                                                      0%|          | 0/55 [00:00<?, ?w/s]model.layers.74.input_layernorm.weight:   0%|          | 0/55 [00:00<?, ?w/s, dev=3]model.layers.74.mlp.down_proj.weight:   2%|â–         | 1/55 [00:00<00:00, 900.26w/s, dev=3]model.layers.74.mlp.down_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:04, 12.53w/s, dev=3] model.layers.74.mlp.up_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:04, 12.51w/s, dev=3]  model.layers.74.post_attention_layernorm.weight:   5%|â–Œ         | 3/55 [00:00<00:05,  9.59w/s, dev=3]model.layers.74.post_attention_layernorm.weight:   7%|â–‹         | 4/55 [00:00<00:03, 12.77w/s, dev=3]model.layers.75.input_layernorm.weight:   7%|â–‹         | 4/55 [00:00<00:03, 12.77w/s, dev=3]         model.layers.75.mlp.down_proj.weight:   9%|â–‰         | 5/55 [00:00<00:03, 15.95w/s, dev=3]  model.layers.75.mlp.down_proj.weight:  11%|â–ˆ         | 6/55 [00:00<00:03, 13.01w/s, dev=3]model.layers.75.mlp.gate_proj.weight:  11%|â–ˆ         | 6/55 [00:00<00:03, 13.00w/s, dev=3]model.layers.75.mlp.up_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:04, 11.43w/s, dev=3]  model.layers.75.mlp.up_proj.weight:  15%|â–ˆâ–        | 8/55 [00:00<00:04, 10.53w/s, dev=3]model.layers.75.post_attention_layernorm.weight:  15%|â–ˆâ–        | 8/55 [00:00<00:04, 10.53w/s, dev=3]model.layers.75.self_attn.k_proj.weight:  16%|â–ˆâ–‹        | 9/55 [00:00<00:03, 11.84w/s, dev=3]        model.layers.75.self_attn.o_proj.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:03, 13.05w/s, dev=3]model.layers.75.self_attn.q_proj.weight:  20%|â–ˆâ–ˆ        | 11/55 [00:00<00:03, 13.59w/s, dev=3]model.layers.75.self_attn.rotary_emb.inv_freq:  22%|â–ˆâ–ˆâ–       | 12/55 [00:00<00:03, 14.08w/s, dev=3]model.layers.75.self_attn.v_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:00<00:02, 15.24w/s, dev=3]      model.layers.76.input_layernorm.weight:  25%|â–ˆâ–ˆâ–Œ       | 14/55 [00:00<00:02, 16.30w/s, dev=3] model.layers.76.mlp.down_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 15/55 [00:00<00:02, 17.46w/s, dev=3]  model.layers.76.mlp.down_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 16/55 [00:01<00:02, 15.86w/s, dev=3]model.layers.76.mlp.gate_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 16/55 [00:01<00:02, 15.86w/s, dev=3]model.layers.76.mlp.up_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:01<00:02, 14.66w/s, dev=3]  model.layers.76.post_attention_layernorm.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/55 [00:01<00:02, 13.80w/s, dev=3]model.layers.76.self_attn.k_proj.weight:  35%|â–ˆâ–ˆâ–ˆâ–      | 19/55 [00:01<00:02, 14.56w/s, dev=3]        model.layers.76.self_attn.o_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:01<00:02, 15.26w/s, dev=3]model.layers.76.self_attn.q_proj.weight:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/55 [00:01<00:02, 15.53w/s, dev=3]model.layers.76.self_attn.rotary_emb.inv_freq:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/55 [00:01<00:02, 15.77w/s, dev=3]model.layers.76.self_attn.v_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 23/55 [00:01<00:01, 16.48w/s, dev=3]      model.layers.76.self_attn.v_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 24/55 [00:01<00:01, 17.11w/s, dev=3]model.layers.77.input_layernorm.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 24/55 [00:01<00:01, 17.11w/s, dev=3] model.layers.77.mlp.down_proj.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:01<00:01, 17.82w/s, dev=3]  model.layers.77.mlp.gate_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 26/55 [00:01<00:01, 16.78w/s, dev=3]model.layers.77.mlp.up_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 27/55 [00:01<00:01, 15.84w/s, dev=3]  model.layers.77.post_attention_layernorm.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 28/55 [00:01<00:01, 15.07w/s, dev=3]model.layers.77.self_attn.k_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/55 [00:01<00:01, 15.60w/s, dev=3]        model.layers.77.self_attn.o_proj.weight:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 30/55 [00:01<00:01, 16.08w/s, dev=3]model.layers.77.self_attn.q_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/55 [00:01<00:01, 16.24w/s, dev=3]model.layers.77.self_attn.q_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 32/55 [00:01<00:01, 16.38w/s, dev=3]model.layers.77.self_attn.rotary_emb.inv_freq:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 32/55 [00:01<00:01, 16.38w/s, dev=3]model.layers.77.self_attn.v_proj.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/55 [00:01<00:01, 16.89w/s, dev=3]      model.layers.78.input_layernorm.weight:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/55 [00:01<00:01, 17.33w/s, dev=3] model.layers.78.mlp.down_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:01<00:01, 17.84w/s, dev=3]  model.layers.78.mlp.gate_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 36/55 [00:02<00:01, 17.01w/s, dev=3]model.layers.78.mlp.up_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 37/55 [00:02<00:01, 16.36w/s, dev=3]  model.layers.78.post_attention_layernorm.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 38/55 [00:02<00:01, 15.77w/s, dev=3]model.layers.78.self_attn.k_proj.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 39/55 [00:02<00:00, 16.18w/s, dev=3]        model.layers.78.self_attn.k_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 40/55 [00:02<00:00, 16.55w/s, dev=3]model.layers.78.self_attn.o_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 40/55 [00:02<00:00, 16.55w/s, dev=3]model.layers.78.self_attn.q_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/55 [00:02<00:00, 16.64w/s, dev=3]model.layers.78.self_attn.rotary_emb.inv_freq:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 42/55 [00:02<00:00, 16.73w/s, dev=3]model.layers.78.self_attn.v_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 43/55 [00:02<00:00, 17.13w/s, dev=3]      model.layers.79.input_layernorm.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 44/55 [00:02<00:00, 17.47w/s, dev=3] model.layers.79.mlp.down_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:02<00:00, 17.87w/s, dev=3]  model.layers.79.mlp.gate_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 46/55 [00:02<00:00, 17.17w/s, dev=3]model.layers.79.mlp.up_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 47/55 [00:02<00:00, 16.65w/s, dev=3]  model.layers.79.mlp.up_proj.weight:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 48/55 [00:02<00:00, 16.17w/s, dev=3]model.layers.79.post_attention_layernorm.weight:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 48/55 [00:02<00:00, 16.17w/s, dev=3]model.layers.79.self_attn.k_proj.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 49/55 [00:02<00:00, 16.51w/s, dev=3]        model.layers.79.self_attn.o_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 50/55 [00:02<00:00, 16.80w/s, dev=3]model.layers.79.self_attn.q_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 51/55 [00:03<00:00, 16.89w/s, dev=3]model.layers.79.self_attn.rotary_emb.inv_freq:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 52/55 [00:03<00:00, 16.98w/s, dev=3]model.layers.79.self_attn.v_proj.weight:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 53/55 [00:03<00:00, 17.30w/s, dev=3]      model.norm.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 54/55 [00:03<00:00, 17.59w/s, dev=3]                                                                                                0%|          | 0/1 [00:00<?, ?w/s]lm_head.weight:   0%|          | 0/1 [00:00<?, ?w/s, dev=3]lm_head.weight: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  6.28w/s, dev=3]                                                                   self.device: cuda:0
Loading BASE model stabilityai/stablelm-base-alpha-3b-v2...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:01<00:01,  1.49s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.24it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.10it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[2] MOVING BASE MODEL TO GPU.../home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for iamgroot42/mimir contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/iamgroot42/mimir
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
Loading dataset the_pile...
Loading from HuggingFace!
Downloading data:   0%|          | 0.00/1.42M [00:00<?, ?B/s]Downloading data:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1.01M/1.42M [00:00<00:00, 10.1MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.42M/1.42M [00:00<00:00, 12.8MB/s]
Downloading data:   0%|          | 0.00/703k [00:00<?, ?B/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 703k/703k [00:00<00:00, 15.6MB/s]
Downloading data:   0%|          | 0.00/18.9M [00:00<?, ?B/s]Downloading data:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7.73M/18.9M [00:00<00:00, 77.3MB/s]Downloading data:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 17.3M/18.9M [00:00<00:00, 87.9MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18.9M/18.9M [00:00<00:00, 87.0MB/s]
Downloading data:   0%|          | 0.00/18.5M [00:00<?, ?B/s]Downloading data:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 7.96M/18.5M [00:00<00:00, 79.6MB/s]Downloading data:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 18.1M/18.5M [00:00<00:00, 92.5MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18.5M/18.5M [00:00<00:00, 90.6MB/s]
Downloading data:   0%|          | 0.00/1.42M [00:00<?, ?B/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.42M/1.42M [00:00<00:00, 14.5MB/s]
Downloading data:   0%|          | 0.00/1.42M [00:00<?, ?B/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.42M/1.42M [00:00<00:00, 14.4MB/s]
Downloading data:   0%|          | 0.00/37.7M [00:00<?, ?B/s]Downloading data:  21%|â–ˆâ–ˆâ–       | 8.09M/37.7M [00:00<00:00, 80.9MB/s]Downloading data:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 17.9M/37.7M [00:00<00:00, 90.9MB/s]Downloading data:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 27.7M/37.7M [00:00<00:00, 94.3MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37.7M/37.7M [00:00<00:00, 94.8MB/s]
Downloading data:   0%|          | 0.00/37.6M [00:00<?, ?B/s]Downloading data:   4%|â–Ž         | 1.37M/37.6M [00:00<00:02, 13.7MB/s]Downloading data:  14%|â–ˆâ–        | 5.18M/37.6M [00:00<00:01, 27.9MB/s]Downloading data:  26%|â–ˆâ–ˆâ–Œ       | 9.62M/37.6M [00:00<00:00, 35.4MB/s]Downloading data:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 15.3M/37.6M [00:00<00:00, 43.5MB/s]Downloading data:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 21.6M/37.6M [00:00<00:00, 50.7MB/s]Downloading data:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 30.0M/37.6M [00:00<00:00, 61.7MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37.6M/37.6M [00:00<00:00, 54.9MB/s]
Downloading data:   0%|          | 0.00/1.42M [00:00<?, ?B/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.42M/1.42M [00:00<00:00, 20.8MB/s]
Downloading data:   0%|          | 0.00/1.43M [00:00<?, ?B/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.43M/1.43M [00:00<00:00, 14.3MB/s]
Downloading data:   0%|          | 0.00/37.7M [00:00<?, ?B/s]Downloading data:   4%|â–         | 1.43M/37.7M [00:00<00:02, 14.3MB/s]Downloading data:  17%|â–ˆâ–‹        | 6.46M/37.7M [00:00<00:00, 33.3MB/s]Downloading data:  29%|â–ˆâ–ˆâ–‰       | 10.9M/37.7M [00:00<00:00, 38.0MB/s]Downloading data:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 16.0M/37.7M [00:00<00:00, 43.1MB/s]Downloading data:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 22.3M/37.7M [00:00<00:00, 50.2MB/s]Downloading data:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 29.9M/37.7M [00:00<00:00, 58.9MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37.7M/37.7M [00:00<00:00, 53.2MB/s]
Downloading data:   0%|          | 0.00/37.8M [00:00<?, ?B/s]Downloading data:  20%|â–ˆâ–ˆ        | 7.68M/37.8M [00:00<00:00, 76.8MB/s]Downloading data:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 17.0M/37.8M [00:00<00:00, 86.4MB/s]Downloading data:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 26.4M/37.8M [00:00<00:00, 89.9MB/s]Downloading data:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 36.2M/37.8M [00:00<00:00, 93.1MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37.8M/37.8M [00:00<00:00, 90.8MB/s]
Generating ngram_7_0.2 split: 0 examples [00:00, ? examples/s]Generating ngram_7_0.2 split: 68 examples [00:00, 666.07 examples/s]Generating ngram_7_0.2 split: 136 examples [00:00, 668.23 examples/s]Generating ngram_7_0.2 split: 205 examples [00:00, 676.64 examples/s]Generating ngram_7_0.2 split: 274 examples [00:00, 678.31 examples/s]Generating ngram_7_0.2 split: 375 examples [00:00, 668.65 examples/s]Generating ngram_7_0.2 split: 444 examples [00:00, 673.10 examples/s]Generating ngram_7_0.2 split: 500 examples [00:00, 552.88 examples/s]
Generating ngram_13_0.2 split: 0 examples [00:00, ? examples/s]Generating ngram_13_0.2 split: 67 examples [00:00, 659.83 examples/s]Generating ngram_13_0.2 split: 135 examples [00:00, 662.53 examples/s]Generating ngram_13_0.2 split: 235 examples [00:00, 658.20 examples/s]Generating ngram_13_0.2 split: 302 examples [00:00, 656.21 examples/s]Generating ngram_13_0.2 split: 368 examples [00:00, 652.25 examples/s]Generating ngram_13_0.2 split: 438 examples [00:00, 661.81 examples/s]Generating ngram_13_0.2 split: 536 examples [00:00, 653.90 examples/s]Generating ngram_13_0.2 split: 603 examples [00:00, 656.56 examples/s]Generating ngram_13_0.2 split: 672 examples [00:01, 662.22 examples/s]Generating ngram_13_0.2 split: 740 examples [00:01, 664.40 examples/s]Generating ngram_13_0.2 split: 841 examples [00:01, 664.12 examples/s]Generating ngram_13_0.2 split: 912 examples [00:01, 673.36 examples/s]Generating ngram_13_0.2 split: 1000 examples [00:01, 369.62 examples/s]Generating ngram_13_0.2 split: 1000 examples [00:01, 545.64 examples/s]
Generating ngram_13_0.8 split: 0 examples [00:00, ? examples/s]Generating ngram_13_0.8 split: 66 examples [00:00, 649.86 examples/s]Generating ngram_13_0.8 split: 134 examples [00:00, 660.84 examples/s]Generating ngram_13_0.8 split: 202 examples [00:00, 664.03 examples/s]Generating ngram_13_0.8 split: 302 examples [00:00, 659.75 examples/s]Generating ngram_13_0.8 split: 369 examples [00:00, 660.11 examples/s]Generating ngram_13_0.8 split: 438 examples [00:00, 662.76 examples/s]Generating ngram_13_0.8 split: 538 examples [00:00, 662.76 examples/s]Generating ngram_13_0.8 split: 607 examples [00:00, 668.51 examples/s]Generating ngram_13_0.8 split: 679 examples [00:01, 679.92 examples/s]Generating ngram_13_0.8 split: 780 examples [00:01, 676.00 examples/s]Generating ngram_13_0.8 split: 882 examples [00:01, 673.51 examples/s]Generating ngram_13_0.8 split: 981 examples [00:01, 663.49 examples/s]Generating ngram_13_0.8 split: 1000 examples [00:01, 541.37 examples/s]
Loading dataset the_pile...
Loading from HuggingFace!
Generating samples:   0%|          | 0/21 [00:00<?, ?it/s]Generating samples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:00<00:00, 770.40it/s]
NEW N_SAMPLES IS  1000
Writing raw data to tmp_results/neo125_github_experiment/meta-llama_Llama-2-70b-hf/arxiv_ngram_13_<0.8_truncated/raw_data.json
Writing raw data to tmp_results/neo125_github_experiment/meta-llama_Llama-2-70b-hf/arxiv_ngram_13_<0.8_truncated/raw_data_lens.json
Computing criterion:   0%|          | 0/20 [00:00<?, ?it/s]Computing criterion:   5%|â–Œ         | 1/20 [00:24<07:46, 24.58s/it]Computing criterion:  10%|â–ˆ         | 2/20 [00:43<06:26, 21.46s/it]Computing criterion:  15%|â–ˆâ–Œ        | 3/20 [01:03<05:49, 20.54s/it]Computing criterion:  20%|â–ˆâ–ˆ        | 4/20 [01:23<05:23, 20.24s/it]Computing criterion:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [01:43<05:03, 20.21s/it]Computing criterion:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [02:03<04:41, 20.11s/it]Computing criterion:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [02:23<04:22, 20.20s/it]Computing criterion:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [02:43<04:02, 20.17s/it]Computing criterion:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [03:03<03:41, 20.12s/it]Computing criterion:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [03:24<03:21, 20.20s/it]Computing criterion:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [03:42<02:57, 19.78s/it]Computing criterion:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [04:01<02:35, 19.38s/it]Computing criterion:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [04:19<02:13, 19.07s/it]Computing criterion:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [04:38<01:53, 18.90s/it]Computing criterion:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [04:56<01:33, 18.77s/it]Computing criterion:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [05:14<01:14, 18.59s/it]Computing criterion:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [05:32<00:55, 18.42s/it]Computing criterion:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [05:50<00:36, 18.30s/it]Computing criterion:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [06:09<00:18, 18.28s/it]Computing criterion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [06:27<00:00, 18.31s/it]Computing criterion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [06:27<00:00, 19.38s/it]
Ref scores:   0%|          | 0/1000 [00:00<?, ?it/s]Ref scores:   0%|          | 1/1000 [00:11<3:14:07, 11.66s/it]Ref scores:   0%|          | 2/1000 [00:11<1:21:54,  4.92s/it]Ref scores:   0%|          | 3/1000 [00:12<45:41,  2.75s/it]  Ref scores:   0%|          | 4/1000 [00:12<29:01,  1.75s/it]Ref scores:   0%|          | 5/1000 [00:12<19:28,  1.17s/it]Ref scores:   1%|          | 6/1000 [00:12<14:00,  1.18it/s]Ref scores:   1%|          | 7/1000 [00:12<10:13,  1.62it/s]Ref scores:   1%|          | 8/1000 [00:12<07:43,  2.14it/s]Ref scores:   1%|          | 9/1000 [00:13<06:38,  2.48it/s]Ref scores:   1%|          | 10/1000 [00:13<05:19,  3.10it/s]Ref scores:   1%|          | 11/1000 [00:13<04:25,  3.73it/s]Ref scores:   1%|          | 12/1000 [00:13<03:49,  4.31it/s]Ref scores:   1%|â–         | 13/1000 [00:13<03:24,  4.82it/s]Ref scores:   1%|â–         | 14/1000 [00:13<03:24,  4.81it/s]Ref scores:   2%|â–         | 15/1000 [00:14<03:07,  5.27it/s]Ref scores:   2%|â–         | 16/1000 [00:14<02:54,  5.66it/s]Ref scores:   2%|â–         | 17/1000 [00:14<03:05,  5.29it/s]Ref scores:   2%|â–         | 18/1000 [00:14<03:11,  5.14it/s]Ref scores:   2%|â–         | 19/1000 [00:14<03:26,  4.75it/s]Ref scores:   2%|â–         | 20/1000 [00:15<03:06,  5.25it/s]Ref scores:   2%|â–         | 21/1000 [00:15<03:14,  5.04it/s]Ref scores:   2%|â–         | 22/1000 [00:15<02:59,  5.46it/s]Ref scores:   2%|â–         | 23/1000 [00:15<02:47,  5.82it/s]Ref scores:   2%|â–         | 24/1000 [00:15<02:41,  6.05it/s]Ref scores:   2%|â–Ž         | 25/1000 [00:15<02:54,  5.59it/s]Ref scores:   3%|â–Ž         | 26/1000 [00:16<02:37,  6.18it/s]Ref scores:   3%|â–Ž         | 27/1000 [00:16<02:33,  6.32it/s]Ref scores:   3%|â–Ž         | 28/1000 [00:16<02:31,  6.42it/s]Ref scores:   3%|â–Ž         | 29/1000 [00:16<02:50,  5.69it/s]Ref scores:   3%|â–Ž         | 30/1000 [00:16<02:42,  5.96it/s]Ref scores:   3%|â–Ž         | 31/1000 [00:16<02:57,  5.45it/s]Ref scores:   3%|â–Ž         | 32/1000 [00:17<02:52,  5.60it/s]Ref scores:   3%|â–Ž         | 33/1000 [00:17<02:44,  5.88it/s]Ref scores:   3%|â–Ž         | 34/1000 [00:17<02:58,  5.42it/s]Ref scores:   4%|â–Ž         | 35/1000 [00:17<02:48,  5.71it/s]Ref scores:   4%|â–Ž         | 36/1000 [00:17<02:41,  5.98it/s]Ref scores:   4%|â–Ž         | 37/1000 [00:18<03:08,  5.10it/s]Ref scores:   4%|â–         | 38/1000 [00:18<02:54,  5.50it/s]Ref scores:   4%|â–         | 39/1000 [00:18<03:03,  5.23it/s]Ref scores:   4%|â–         | 40/1000 [00:18<02:51,  5.60it/s]Ref scores:   4%|â–         | 41/1000 [00:18<02:47,  5.72it/s]Ref scores:   4%|â–         | 42/1000 [00:18<02:58,  5.36it/s]Ref scores:   4%|â–         | 43/1000 [00:19<02:47,  5.72it/s]Ref scores:   4%|â–         | 44/1000 [00:19<02:39,  5.98it/s]Ref scores:   4%|â–         | 45/1000 [00:19<03:06,  5.12it/s]Ref scores:   5%|â–         | 46/1000 [00:19<02:53,  5.51it/s]Ref scores:   5%|â–         | 47/1000 [00:19<03:01,  5.25it/s]Ref scores:   5%|â–         | 48/1000 [00:20<03:07,  5.08it/s]Ref scores:   5%|â–         | 49/1000 [00:20<03:33,  4.46it/s]Ref scores:   5%|â–Œ         | 50/1000 [00:20<03:11,  4.96it/s]Ref scores:   5%|â–Œ         | 51/1000 [00:20<02:56,  5.38it/s]Ref scores:   5%|â–Œ         | 52/1000 [00:20<03:18,  4.78it/s]Ref scores:   5%|â–Œ         | 53/1000 [00:21<03:00,  5.25it/s]Ref scores:   5%|â–Œ         | 54/1000 [00:21<02:48,  5.62it/s]Ref scores:   6%|â–Œ         | 55/1000 [00:21<02:31,  6.23it/s]Ref scores:   6%|â–Œ         | 56/1000 [00:21<02:58,  5.29it/s]Ref scores:   6%|â–Œ         | 57/1000 [00:21<03:08,  5.01it/s]Ref scores:   6%|â–Œ         | 58/1000 [00:21<02:58,  5.27it/s]Ref scores:   6%|â–Œ         | 59/1000 [00:22<02:46,  5.66it/s]Ref scores:   6%|â–Œ         | 60/1000 [00:22<02:43,  5.75it/s]Ref scores:   6%|â–Œ         | 61/1000 [00:22<02:37,  5.98it/s]Ref scores:   6%|â–Œ         | 62/1000 [00:22<02:32,  6.15it/s]Ref scores:   6%|â–‹         | 63/1000 [00:22<02:29,  6.29it/s]Ref scores:   6%|â–‹         | 64/1000 [00:22<02:31,  6.19it/s]Ref scores:   6%|â–‹         | 65/1000 [00:23<02:33,  6.10it/s]Ref scores:   7%|â–‹         | 66/1000 [00:23<02:29,  6.27it/s]Ref scores:   7%|â–‹         | 67/1000 [00:23<02:26,  6.39it/s]Ref scores:   7%|â–‹         | 68/1000 [00:23<02:54,  5.35it/s]Ref scores:   7%|â–‹         | 69/1000 [00:23<02:43,  5.69it/s]Ref scores:   7%|â–‹         | 70/1000 [00:23<02:35,  5.97it/s]Ref scores:   7%|â–‹         | 71/1000 [00:24<02:30,  6.19it/s]Ref scores:   7%|â–‹         | 72/1000 [00:24<02:45,  5.61it/s]Ref scores:   7%|â–‹         | 73/1000 [00:24<02:39,  5.80it/s]Ref scores:   7%|â–‹         | 74/1000 [00:24<02:30,  6.14it/s]Ref scores:   8%|â–Š         | 75/1000 [00:24<02:46,  5.57it/s]Ref scores:   8%|â–Š         | 76/1000 [00:24<02:35,  5.94it/s]Ref scores:   8%|â–Š         | 77/1000 [00:25<02:28,  6.21it/s]Ref scores:   8%|â–Š         | 78/1000 [00:25<02:23,  6.42it/s]Ref scores:   8%|â–Š         | 79/1000 [00:25<02:19,  6.59it/s]Ref scores:   8%|â–Š         | 80/1000 [00:25<02:09,  7.12it/s]Ref scores:   8%|â–Š         | 81/1000 [00:25<02:02,  7.50it/s]Ref scores:   8%|â–Š         | 82/1000 [00:25<02:23,  6.38it/s]Ref scores:   8%|â–Š         | 83/1000 [00:26<02:39,  5.73it/s]Ref scores:   8%|â–Š         | 84/1000 [00:26<02:50,  5.37it/s]Ref scores:   8%|â–Š         | 85/1000 [00:26<02:38,  5.78it/s]Ref scores:   9%|â–Š         | 86/1000 [00:26<02:52,  5.31it/s]Ref scores:   9%|â–Š         | 87/1000 [00:26<03:01,  5.02it/s]Ref scores:   9%|â–‰         | 88/1000 [00:27<02:51,  5.31it/s]Ref scores:   9%|â–‰         | 89/1000 [00:27<02:40,  5.66it/s]Ref scores:   9%|â–‰         | 90/1000 [00:27<03:05,  4.90it/s]Ref scores:   9%|â–‰         | 91/1000 [00:27<02:48,  5.40it/s]Ref scores:   9%|â–‰         | 92/1000 [00:27<02:37,  5.76it/s]Ref scores:   9%|â–‰         | 93/1000 [00:27<02:29,  6.06it/s]Ref scores:   9%|â–‰         | 94/1000 [00:28<02:24,  6.28it/s]Ref scores:  10%|â–‰         | 95/1000 [00:28<02:21,  6.40it/s]Ref scores:  10%|â–‰         | 96/1000 [00:28<02:18,  6.51it/s]Ref scores:  10%|â–‰         | 97/1000 [00:28<02:16,  6.60it/s]Ref scores:  10%|â–‰         | 98/1000 [00:28<02:15,  6.67it/s]Ref scores:  10%|â–‰         | 99/1000 [00:28<02:36,  5.75it/s]Ref scores:  10%|â–ˆ         | 100/1000 [00:28<02:27,  6.09it/s]Ref scores:  10%|â–ˆ         | 101/1000 [00:29<02:41,  5.56it/s]Ref scores:  10%|â–ˆ         | 102/1000 [00:29<02:33,  5.87it/s]Ref scores:  10%|â–ˆ         | 103/1000 [00:29<02:26,  6.13it/s]Ref scores:  10%|â–ˆ         | 104/1000 [00:29<02:21,  6.33it/s]Ref scores:  10%|â–ˆ         | 105/1000 [00:29<02:17,  6.52it/s]Ref scores:  11%|â–ˆ         | 106/1000 [00:30<02:33,  5.81it/s]Ref scores:  11%|â–ˆ         | 107/1000 [00:30<02:25,  6.13it/s]Ref scores:  11%|â–ˆ         | 108/1000 [00:30<02:21,  6.31it/s]Ref scores:  11%|â–ˆ         | 109/1000 [00:30<02:48,  5.28it/s]Ref scores:  11%|â–ˆ         | 110/1000 [00:30<02:41,  5.51it/s]Ref scores:  11%|â–ˆ         | 111/1000 [00:30<02:32,  5.84it/s]Ref scores:  11%|â–ˆ         | 112/1000 [00:31<02:25,  6.08it/s]Ref scores:  11%|â–ˆâ–        | 113/1000 [00:31<02:19,  6.37it/s]Ref scores:  11%|â–ˆâ–        | 114/1000 [00:31<02:15,  6.54it/s]Ref scores:  12%|â–ˆâ–        | 115/1000 [00:31<02:10,  6.76it/s]Ref scores:  12%|â–ˆâ–        | 116/1000 [00:31<02:09,  6.85it/s]Ref scores:  12%|â–ˆâ–        | 117/1000 [00:31<02:38,  5.59it/s]Ref scores:  12%|â–ˆâ–        | 118/1000 [00:32<02:45,  5.34it/s]Ref scores:  12%|â–ˆâ–        | 119/1000 [00:32<02:33,  5.73it/s]Ref scores:  12%|â–ˆâ–        | 120/1000 [00:32<02:18,  6.38it/s]Ref scores:  12%|â–ˆâ–        | 121/1000 [00:32<02:13,  6.57it/s]Ref scores:  12%|â–ˆâ–        | 122/1000 [00:32<02:10,  6.72it/s]Ref scores:  12%|â–ˆâ–        | 123/1000 [00:32<02:07,  6.88it/s]Ref scores:  12%|â–ˆâ–        | 124/1000 [00:32<02:36,  5.61it/s]Ref scores:  12%|â–ˆâ–Ž        | 125/1000 [00:33<02:44,  5.33it/s]Ref scores:  13%|â–ˆâ–Ž        | 126/1000 [00:33<02:32,  5.74it/s]Ref scores:  13%|â–ˆâ–Ž        | 127/1000 [00:33<02:23,  6.08it/s]Ref scores:  13%|â–ˆâ–Ž        | 128/1000 [00:33<02:21,  6.16it/s]Ref scores:  13%|â–ˆâ–Ž        | 129/1000 [00:33<02:09,  6.72it/s]Ref scores:  13%|â–ˆâ–Ž        | 130/1000 [00:33<02:08,  6.77it/s]Ref scores:  13%|â–ˆâ–Ž        | 131/1000 [00:34<02:07,  6.83it/s]Ref scores:  13%|â–ˆâ–Ž        | 132/1000 [00:34<02:06,  6.88it/s]Ref scores:  13%|â–ˆâ–Ž        | 133/1000 [00:34<02:23,  6.04it/s]Ref scores:  13%|â–ˆâ–Ž        | 134/1000 [00:34<02:16,  6.34it/s]Ref scores:  14%|â–ˆâ–Ž        | 135/1000 [00:34<02:11,  6.58it/s]Ref scores:  14%|â–ˆâ–Ž        | 136/1000 [00:34<02:38,  5.46it/s]Ref scores:  14%|â–ˆâ–Ž        | 137/1000 [00:35<02:45,  5.22it/s]Ref scores:  14%|â–ˆâ–        | 138/1000 [00:35<02:32,  5.64it/s]Ref scores:  14%|â–ˆâ–        | 139/1000 [00:35<02:17,  6.28it/s]Ref scores:  14%|â–ˆâ–        | 140/1000 [00:35<02:12,  6.48it/s]Ref scores:  14%|â–ˆâ–        | 141/1000 [00:35<02:09,  6.62it/s]Ref scores:  14%|â–ˆâ–        | 142/1000 [00:35<02:07,  6.74it/s]Ref scores:  14%|â–ˆâ–        | 143/1000 [00:35<02:05,  6.83it/s]Ref scores:  14%|â–ˆâ–        | 144/1000 [00:36<02:33,  5.59it/s]Ref scores:  14%|â–ˆâ–        | 145/1000 [00:36<02:40,  5.33it/s]Ref scores:  15%|â–ˆâ–        | 146/1000 [00:36<02:47,  5.10it/s]Ref scores:  15%|â–ˆâ–        | 147/1000 [00:36<02:26,  5.80it/s]Ref scores:  15%|â–ˆâ–        | 148/1000 [00:36<02:19,  6.13it/s]Ref scores:  15%|â–ˆâ–        | 149/1000 [00:37<02:33,  5.55it/s]Ref scores:  15%|â–ˆâ–Œ        | 150/1000 [00:37<02:23,  5.93it/s]Ref scores:  15%|â–ˆâ–Œ        | 151/1000 [00:37<02:17,  6.19it/s]Ref scores:  15%|â–ˆâ–Œ        | 152/1000 [00:37<02:12,  6.39it/s]Ref scores:  15%|â–ˆâ–Œ        | 153/1000 [00:37<02:38,  5.34it/s]Ref scores:  15%|â–ˆâ–Œ        | 154/1000 [00:38<02:46,  5.07it/s]Ref scores:  16%|â–ˆâ–Œ        | 155/1000 [00:38<02:32,  5.52it/s]Ref scores:  16%|â–ˆâ–Œ        | 156/1000 [00:38<02:43,  5.16it/s]Ref scores:  16%|â–ˆâ–Œ        | 157/1000 [00:38<02:51,  4.92it/s]Ref scores:  16%|â–ˆâ–Œ        | 158/1000 [00:38<02:36,  5.38it/s]Ref scores:  16%|â–ˆâ–Œ        | 159/1000 [00:38<02:30,  5.60it/s]Ref scores:  16%|â–ˆâ–Œ        | 160/1000 [00:39<02:20,  5.97it/s]Ref scores:  16%|â–ˆâ–Œ        | 161/1000 [00:39<02:14,  6.22it/s]Ref scores:  16%|â–ˆâ–Œ        | 162/1000 [00:39<02:10,  6.42it/s]Ref scores:  16%|â–ˆâ–‹        | 163/1000 [00:39<02:07,  6.57it/s]Ref scores:  16%|â–ˆâ–‹        | 164/1000 [00:39<02:05,  6.65it/s]Ref scores:  16%|â–ˆâ–‹        | 165/1000 [00:39<02:07,  6.54it/s]Ref scores:  17%|â–ˆâ–‹        | 166/1000 [00:40<02:24,  5.76it/s]Ref scores:  17%|â–ˆâ–‹        | 167/1000 [00:40<02:16,  6.09it/s]Ref scores:  17%|â–ˆâ–‹        | 168/1000 [00:40<02:40,  5.20it/s]Ref scores:  17%|â–ˆâ–‹        | 169/1000 [00:40<02:47,  4.96it/s]Ref scores:  17%|â–ˆâ–‹        | 170/1000 [00:40<02:34,  5.36it/s]Ref scores:  17%|â–ˆâ–‹        | 171/1000 [00:41<02:43,  5.07it/s]Ref scores:  17%|â–ˆâ–‹        | 172/1000 [00:41<02:47,  4.94it/s]Ref scores:  17%|â–ˆâ–‹        | 173/1000 [00:41<02:49,  4.87it/s]Ref scores:  17%|â–ˆâ–‹        | 174/1000 [00:41<02:58,  4.63it/s]Ref scores:  18%|â–ˆâ–Š        | 175/1000 [00:41<02:40,  5.15it/s]Ref scores:  18%|â–ˆâ–Š        | 176/1000 [00:41<02:31,  5.44it/s]Ref scores:  18%|â–ˆâ–Š        | 177/1000 [00:42<02:25,  5.67it/s]Ref scores:  18%|â–ˆâ–Š        | 178/1000 [00:42<02:16,  6.04it/s]Ref scores:  18%|â–ˆâ–Š        | 179/1000 [00:42<02:10,  6.28it/s]Ref scores:  18%|â–ˆâ–Š        | 180/1000 [00:42<02:06,  6.48it/s]Ref scores:  18%|â–ˆâ–Š        | 181/1000 [00:42<02:32,  5.37it/s]Ref scores:  18%|â–ˆâ–Š        | 182/1000 [00:42<02:21,  5.78it/s]Ref scores:  18%|â–ˆâ–Š        | 183/1000 [00:43<02:14,  6.09it/s]Ref scores:  18%|â–ˆâ–Š        | 184/1000 [00:43<02:07,  6.39it/s]Ref scores:  18%|â–ˆâ–Š        | 185/1000 [00:43<02:03,  6.58it/s]Ref scores:  19%|â–ˆâ–Š        | 186/1000 [00:43<02:30,  5.41it/s]Ref scores:  19%|â–ˆâ–Š        | 187/1000 [00:43<02:19,  5.81it/s]Ref scores:  19%|â–ˆâ–‰        | 188/1000 [00:44<02:29,  5.42it/s]Ref scores:  19%|â–ˆâ–‰        | 189/1000 [00:44<02:19,  5.83it/s]Ref scores:  19%|â–ˆâ–‰        | 190/1000 [00:44<02:31,  5.36it/s]Ref scores:  19%|â–ˆâ–‰        | 191/1000 [00:44<02:19,  5.78it/s]Ref scores:  19%|â–ˆâ–‰        | 192/1000 [00:44<02:29,  5.42it/s]Ref scores:  19%|â–ˆâ–‰        | 193/1000 [00:44<02:36,  5.14it/s]Ref scores:  19%|â–ˆâ–‰        | 194/1000 [00:45<02:23,  5.60it/s]Ref scores:  20%|â–ˆâ–‰        | 195/1000 [00:45<02:35,  5.19it/s]Ref scores:  20%|â–ˆâ–‰        | 196/1000 [00:45<02:35,  5.16it/s]Ref scores:  20%|â–ˆâ–‰        | 197/1000 [00:45<02:21,  5.66it/s]Ref scores:  20%|â–ˆâ–‰        | 198/1000 [00:45<02:12,  6.05it/s]Ref scores:  20%|â–ˆâ–‰        | 199/1000 [00:45<02:07,  6.28it/s]Ref scores:  20%|â–ˆâ–ˆ        | 200/1000 [00:46<02:17,  5.82it/s]Ref scores:  20%|â–ˆâ–ˆ        | 201/1000 [00:46<02:09,  6.19it/s]Ref scores:  20%|â–ˆâ–ˆ        | 202/1000 [00:46<02:04,  6.41it/s]Ref scores:  20%|â–ˆâ–ˆ        | 203/1000 [00:46<02:00,  6.60it/s]Ref scores:  20%|â–ˆâ–ˆ        | 204/1000 [00:46<02:16,  5.82it/s]Ref scores:  20%|â–ˆâ–ˆ        | 205/1000 [00:47<02:28,  5.34it/s]Ref scores:  21%|â–ˆâ–ˆ        | 206/1000 [00:47<02:21,  5.60it/s]Ref scores:  21%|â–ˆâ–ˆ        | 207/1000 [00:47<02:29,  5.30it/s]Ref scores:  21%|â–ˆâ–ˆ        | 208/1000 [00:47<02:30,  5.25it/s]Ref scores:  21%|â–ˆâ–ˆ        | 209/1000 [00:47<02:38,  5.00it/s]Ref scores:  21%|â–ˆâ–ˆ        | 210/1000 [00:48<02:39,  4.94it/s]Ref scores:  21%|â–ˆâ–ˆ        | 211/1000 [00:48<02:53,  4.56it/s]Ref scores:  21%|â–ˆâ–ˆ        | 212/1000 [00:48<02:50,  4.63it/s]Ref scores:  21%|â–ˆâ–ˆâ–       | 213/1000 [00:48<02:51,  4.58it/s]Ref scores:  21%|â–ˆâ–ˆâ–       | 214/1000 [00:48<02:48,  4.66it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 215/1000 [00:49<02:31,  5.18it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 216/1000 [00:49<02:19,  5.61it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 217/1000 [00:49<02:25,  5.36it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 218/1000 [00:49<02:13,  5.84it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 219/1000 [00:49<02:01,  6.44it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 220/1000 [00:49<01:59,  6.54it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 221/1000 [00:49<01:57,  6.65it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 222/1000 [00:50<01:54,  6.79it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 223/1000 [00:50<01:54,  6.81it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 224/1000 [00:50<02:12,  5.87it/s]Ref scores:  22%|â–ˆâ–ˆâ–Ž       | 225/1000 [00:50<02:22,  5.45it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 226/1000 [00:50<02:39,  4.84it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 227/1000 [00:51<02:24,  5.34it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 228/1000 [00:51<02:14,  5.74it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 229/1000 [00:51<02:07,  6.05it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 230/1000 [00:51<02:18,  5.57it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 231/1000 [00:51<02:25,  5.27it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 232/1000 [00:51<02:30,  5.11it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 233/1000 [00:52<02:18,  5.55it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 234/1000 [00:52<02:25,  5.26it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 235/1000 [00:52<02:13,  5.71it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 236/1000 [00:52<02:05,  6.09it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 237/1000 [00:52<01:54,  6.68it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 238/1000 [00:52<01:51,  6.84it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 239/1000 [00:53<02:16,  5.59it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 240/1000 [00:53<02:34,  4.93it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 241/1000 [00:53<02:38,  4.79it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 242/1000 [00:53<02:23,  5.30it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 243/1000 [00:53<02:05,  6.05it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 244/1000 [00:54<02:14,  5.60it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 245/1000 [00:54<02:31,  4.99it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 246/1000 [00:54<02:21,  5.31it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 247/1000 [00:54<02:37,  4.79it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 248/1000 [00:54<02:37,  4.78it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 249/1000 [00:55<02:22,  5.25it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 250/1000 [00:55<02:12,  5.66it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 251/1000 [00:55<02:05,  5.97it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 252/1000 [00:55<01:59,  6.26it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 253/1000 [00:55<01:56,  6.43it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 254/1000 [00:55<02:19,  5.33it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 255/1000 [00:56<02:24,  5.15it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 256/1000 [00:56<02:28,  5.00it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 257/1000 [00:56<02:16,  5.44it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 258/1000 [00:56<02:22,  5.19it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 259/1000 [00:56<02:10,  5.67it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 260/1000 [00:57<02:28,  4.97it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 261/1000 [00:57<02:16,  5.41it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 262/1000 [00:57<02:22,  5.19it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 263/1000 [00:57<02:11,  5.62it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 264/1000 [00:57<02:03,  5.95it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 265/1000 [00:57<02:13,  5.51it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 266/1000 [00:58<02:29,  4.90it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 267/1000 [00:58<02:20,  5.23it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 268/1000 [00:58<02:03,  5.91it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 269/1000 [00:58<02:13,  5.47it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 270/1000 [00:58<02:29,  4.89it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 271/1000 [00:59<02:41,  4.52it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 272/1000 [00:59<02:24,  5.05it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 273/1000 [00:59<02:06,  5.75it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 274/1000 [00:59<02:14,  5.40it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 275/1000 [00:59<02:05,  5.78it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 276/1000 [01:00<02:15,  5.34it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 277/1000 [01:00<02:22,  5.09it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 278/1000 [01:00<02:10,  5.54it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 279/1000 [01:00<02:01,  5.92it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 280/1000 [01:00<02:00,  6.00it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 281/1000 [01:00<01:54,  6.27it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 282/1000 [01:01<02:06,  5.70it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 283/1000 [01:01<01:58,  6.04it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 284/1000 [01:01<01:48,  6.59it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 285/1000 [01:01<01:50,  6.47it/s]Ref scores:  29%|â–ˆâ–ˆâ–Š       | 286/1000 [01:01<01:51,  6.39it/s]Ref scores:  29%|â–ˆâ–ˆâ–Š       | 287/1000 [01:01<01:49,  6.53it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 288/1000 [01:01<01:47,  6.64it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 289/1000 [01:02<01:49,  6.48it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 290/1000 [01:02<02:12,  5.35it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 291/1000 [01:02<02:03,  5.75it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 292/1000 [01:02<01:57,  6.04it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 293/1000 [01:02<01:53,  6.25it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 294/1000 [01:03<02:03,  5.71it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 295/1000 [01:03<01:57,  6.01it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 296/1000 [01:03<02:06,  5.58it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 297/1000 [01:03<01:58,  5.93it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 298/1000 [01:03<02:11,  5.36it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 299/1000 [01:03<02:01,  5.76it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 300/1000 [01:04<02:10,  5.38it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 301/1000 [01:04<02:16,  5.11it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 302/1000 [01:04<02:22,  4.89it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 303/1000 [01:04<02:23,  4.85it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 304/1000 [01:04<02:09,  5.36it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 305/1000 [01:05<02:01,  5.73it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 306/1000 [01:05<02:08,  5.38it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 307/1000 [01:05<02:03,  5.62it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 308/1000 [01:05<02:12,  5.24it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 309/1000 [01:05<02:02,  5.64it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 310/1000 [01:05<01:54,  6.02it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 311/1000 [01:06<02:03,  5.57it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 312/1000 [01:06<02:09,  5.30it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆâ–      | 313/1000 [01:06<02:00,  5.71it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆâ–      | 314/1000 [01:06<02:18,  4.95it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 315/1000 [01:07<02:22,  4.80it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 316/1000 [01:07<02:09,  5.27it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 317/1000 [01:07<02:24,  4.73it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 318/1000 [01:07<02:09,  5.25it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 319/1000 [01:07<02:00,  5.63it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 320/1000 [01:07<01:54,  5.94it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 321/1000 [01:08<02:05,  5.43it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 322/1000 [01:08<02:13,  5.09it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 323/1000 [01:08<02:18,  4.89it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 324/1000 [01:08<02:06,  5.36it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 325/1000 [01:08<01:57,  5.77it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 326/1000 [01:08<01:50,  6.08it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 327/1000 [01:09<01:45,  6.37it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 328/1000 [01:09<01:57,  5.71it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 329/1000 [01:09<01:50,  6.05it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 330/1000 [01:09<02:16,  4.92it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 331/1000 [01:09<02:16,  4.90it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 332/1000 [01:10<01:58,  5.63it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 333/1000 [01:10<01:51,  6.00it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 334/1000 [01:10<01:59,  5.56it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 335/1000 [01:10<02:14,  4.94it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 336/1000 [01:10<02:02,  5.42it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 337/1000 [01:11<02:07,  5.21it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 338/1000 [01:11<02:12,  5.00it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 339/1000 [01:11<01:55,  5.71it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 340/1000 [01:11<01:49,  6.03it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 341/1000 [01:11<02:00,  5.48it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 342/1000 [01:11<01:55,  5.69it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 343/1000 [01:12<01:52,  5.83it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 344/1000 [01:12<02:10,  5.04it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 345/1000 [01:12<01:58,  5.53it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 346/1000 [01:12<02:13,  4.91it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 347/1000 [01:12<02:04,  5.24it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 348/1000 [01:13<02:17,  4.75it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 349/1000 [01:13<02:04,  5.25it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 350/1000 [01:13<01:54,  5.68it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 351/1000 [01:13<01:48,  6.01it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 352/1000 [01:13<01:59,  5.44it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 353/1000 [01:13<02:04,  5.19it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 354/1000 [01:14<02:07,  5.06it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 355/1000 [01:14<02:12,  4.89it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 356/1000 [01:14<02:14,  4.80it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 357/1000 [01:14<02:00,  5.33it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 358/1000 [01:14<01:54,  5.59it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 359/1000 [01:15<01:50,  5.78it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 360/1000 [01:15<01:48,  5.92it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 361/1000 [01:15<01:43,  6.20it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 362/1000 [01:15<01:40,  6.34it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 363/1000 [01:15<01:52,  5.69it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 364/1000 [01:15<01:46,  6.00it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 365/1000 [01:16<01:56,  5.46it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 366/1000 [01:16<01:48,  5.84it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 367/1000 [01:16<01:43,  6.14it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 368/1000 [01:16<01:53,  5.56it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 369/1000 [01:16<01:46,  5.93it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 370/1000 [01:16<01:54,  5.50it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 371/1000 [01:17<02:02,  5.14it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 372/1000 [01:17<01:55,  5.43it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 373/1000 [01:17<01:43,  6.09it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 374/1000 [01:17<01:51,  5.60it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 375/1000 [01:17<01:44,  5.96it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 376/1000 [01:18<01:43,  6.02it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 377/1000 [01:18<01:53,  5.47it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 378/1000 [01:18<02:00,  5.14it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 379/1000 [01:18<02:12,  4.69it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 380/1000 [01:18<02:11,  4.72it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 381/1000 [01:19<01:58,  5.23it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 382/1000 [01:19<01:49,  5.62it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 383/1000 [01:19<01:43,  5.97it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 384/1000 [01:19<01:50,  5.58it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 385/1000 [01:19<01:43,  5.93it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 386/1000 [01:19<01:38,  6.20it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 387/1000 [01:20<01:36,  6.38it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 388/1000 [01:20<01:33,  6.53it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 389/1000 [01:20<01:44,  5.86it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 390/1000 [01:20<01:38,  6.17it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 391/1000 [01:20<01:55,  5.27it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 392/1000 [01:20<02:00,  5.04it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 393/1000 [01:21<02:05,  4.85it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 394/1000 [01:21<02:07,  4.77it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 395/1000 [01:21<01:54,  5.26it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 396/1000 [01:21<01:46,  5.67it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 397/1000 [01:21<01:54,  5.26it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 398/1000 [01:22<01:58,  5.08it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 399/1000 [01:22<01:44,  5.77it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 400/1000 [01:22<01:50,  5.44it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 401/1000 [01:22<01:56,  5.15it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 402/1000 [01:22<01:58,  5.03it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 403/1000 [01:23<01:48,  5.50it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 404/1000 [01:23<02:02,  4.88it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 405/1000 [01:23<01:53,  5.22it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 406/1000 [01:23<01:45,  5.62it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 407/1000 [01:23<01:51,  5.30it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 408/1000 [01:24<02:03,  4.78it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 409/1000 [01:24<01:52,  5.26it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 410/1000 [01:24<01:43,  5.70it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 411/1000 [01:24<01:37,  6.03it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 412/1000 [01:24<01:54,  5.15it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 413/1000 [01:24<01:40,  5.83it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 414/1000 [01:25<01:49,  5.37it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 415/1000 [01:25<01:54,  5.12it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 416/1000 [01:25<01:58,  4.95it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 417/1000 [01:25<01:47,  5.43it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 418/1000 [01:25<01:43,  5.65it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 419/1000 [01:25<01:37,  5.99it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 420/1000 [01:26<01:32,  6.24it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 421/1000 [01:26<01:29,  6.48it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 422/1000 [01:26<01:47,  5.40it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 423/1000 [01:26<01:52,  5.12it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 424/1000 [01:26<01:39,  5.81it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 425/1000 [01:27<01:45,  5.43it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 426/1000 [01:27<01:49,  5.22it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 427/1000 [01:27<01:37,  5.89it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 428/1000 [01:27<01:32,  6.22it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 429/1000 [01:27<01:29,  6.40it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 430/1000 [01:27<01:40,  5.70it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 431/1000 [01:28<01:45,  5.37it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 432/1000 [01:28<01:38,  5.79it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 433/1000 [01:28<01:33,  6.09it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 434/1000 [01:28<01:49,  5.19it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 435/1000 [01:28<01:43,  5.47it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 436/1000 [01:29<01:49,  5.16it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 437/1000 [01:29<01:40,  5.62it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 438/1000 [01:29<01:37,  5.79it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 439/1000 [01:29<01:32,  6.09it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 440/1000 [01:29<01:31,  6.12it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 441/1000 [01:29<01:41,  5.52it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 442/1000 [01:30<01:34,  5.88it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 443/1000 [01:30<01:49,  5.07it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 444/1000 [01:30<01:53,  4.88it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 445/1000 [01:30<01:54,  4.83it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 446/1000 [01:30<01:55,  4.81it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 447/1000 [01:31<01:57,  4.69it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 448/1000 [01:31<01:54,  4.81it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 449/1000 [01:31<01:43,  5.32it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 450/1000 [01:31<01:54,  4.80it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 451/1000 [01:31<01:43,  5.32it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 452/1000 [01:32<01:59,  4.59it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 453/1000 [01:32<01:59,  4.58it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 454/1000 [01:32<02:06,  4.32it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 455/1000 [01:32<02:04,  4.36it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 456/1000 [01:33<01:53,  4.78it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 457/1000 [01:33<01:42,  5.28it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 458/1000 [01:33<01:30,  5.96it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 459/1000 [01:33<01:39,  5.42it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 460/1000 [01:33<01:33,  5.81it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 461/1000 [01:33<01:39,  5.42it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 462/1000 [01:34<01:44,  5.13it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 463/1000 [01:34<01:39,  5.42it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 464/1000 [01:34<01:31,  5.84it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 465/1000 [01:34<01:39,  5.39it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 466/1000 [01:34<01:45,  5.08it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 467/1000 [01:34<01:32,  5.76it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 468/1000 [01:35<01:46,  5.00it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 469/1000 [01:35<01:37,  5.47it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 470/1000 [01:35<01:43,  5.13it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 471/1000 [01:35<01:43,  5.12it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 472/1000 [01:35<01:34,  5.57it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 473/1000 [01:36<01:28,  5.95it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 474/1000 [01:36<01:36,  5.42it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 475/1000 [01:36<01:32,  5.67it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 476/1000 [01:36<01:26,  6.06it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 477/1000 [01:36<01:23,  6.30it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 478/1000 [01:36<01:20,  6.50it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 479/1000 [01:37<01:18,  6.64it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 480/1000 [01:37<01:17,  6.69it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 481/1000 [01:37<01:12,  7.14it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 482/1000 [01:37<01:08,  7.53it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 483/1000 [01:37<01:27,  5.90it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 484/1000 [01:37<01:33,  5.52it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 485/1000 [01:38<01:30,  5.71it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 486/1000 [01:38<01:21,  6.33it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 487/1000 [01:38<01:18,  6.51it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 488/1000 [01:38<01:17,  6.63it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 489/1000 [01:38<01:27,  5.82it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 490/1000 [01:38<01:35,  5.33it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 491/1000 [01:39<01:45,  4.81it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 492/1000 [01:39<01:46,  4.78it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 493/1000 [01:39<01:45,  4.79it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 494/1000 [01:39<01:38,  5.12it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 495/1000 [01:39<01:30,  5.57it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 496/1000 [01:39<01:25,  5.92it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 497/1000 [01:40<01:38,  5.12it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 498/1000 [01:40<01:47,  4.69it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 499/1000 [01:40<01:46,  4.72it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 500/1000 [01:40<01:35,  5.22it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 501/1000 [01:41<01:28,  5.64it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 502/1000 [01:41<01:25,  5.80it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 503/1000 [01:41<01:33,  5.32it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 505/1000 [01:41<01:15,  6.56it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 506/1000 [01:41<01:28,  5.60it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 507/1000 [01:42<01:23,  5.92it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 508/1000 [01:42<01:20,  6.14it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 509/1000 [01:42<01:26,  5.67it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 510/1000 [01:42<01:21,  5.98it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 511/1000 [01:42<01:18,  6.24it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 512/1000 [01:42<01:25,  5.74it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 513/1000 [01:43<01:31,  5.33it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 514/1000 [01:43<01:20,  6.01it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 515/1000 [01:43<01:26,  5.58it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 516/1000 [01:43<01:32,  5.23it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 517/1000 [01:43<01:35,  5.08it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 518/1000 [01:43<01:27,  5.52it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 519/1000 [01:44<01:21,  5.89it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 520/1000 [01:44<01:20,  6.00it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 521/1000 [01:44<01:27,  5.46it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 522/1000 [01:44<01:31,  5.24it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 523/1000 [01:44<01:24,  5.65it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 524/1000 [01:45<01:19,  5.98it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 525/1000 [01:45<01:12,  6.58it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 526/1000 [01:45<01:26,  5.50it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 527/1000 [01:45<01:36,  4.92it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 528/1000 [01:45<01:43,  4.56it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 529/1000 [01:46<01:29,  5.27it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 530/1000 [01:46<01:38,  4.80it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 531/1000 [01:46<01:30,  5.17it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 532/1000 [01:46<01:23,  5.63it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 533/1000 [01:46<01:17,  6.03it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 534/1000 [01:46<01:16,  6.08it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 535/1000 [01:47<01:16,  6.11it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 536/1000 [01:47<01:28,  5.22it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 537/1000 [01:47<01:21,  5.66it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 538/1000 [01:47<01:17,  5.98it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 539/1000 [01:47<01:23,  5.52it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 540/1000 [01:47<01:14,  6.18it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 541/1000 [01:48<01:21,  5.60it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 542/1000 [01:48<01:27,  5.22it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 543/1000 [01:48<01:35,  4.76it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 544/1000 [01:48<01:27,  5.24it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 545/1000 [01:48<01:19,  5.71it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 546/1000 [01:49<01:25,  5.32it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 547/1000 [01:49<01:34,  4.80it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 548/1000 [01:49<01:24,  5.33it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 549/1000 [01:49<01:27,  5.14it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 550/1000 [01:49<01:30,  4.97it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 551/1000 [01:50<01:22,  5.42it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 552/1000 [01:50<01:32,  4.84it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 553/1000 [01:50<01:33,  4.77it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 554/1000 [01:50<01:24,  5.28it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 555/1000 [01:50<01:18,  5.67it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 556/1000 [01:51<01:24,  5.23it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 557/1000 [01:51<01:33,  4.72it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 558/1000 [01:51<01:24,  5.22it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 559/1000 [01:51<01:33,  4.71it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 560/1000 [01:51<01:39,  4.41it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 561/1000 [01:52<01:39,  4.43it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 562/1000 [01:52<01:30,  4.86it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 563/1000 [01:52<01:21,  5.34it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 564/1000 [01:52<01:26,  5.06it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 565/1000 [01:52<01:28,  4.92it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 566/1000 [01:53<01:20,  5.39it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 567/1000 [01:53<01:23,  5.18it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 568/1000 [01:53<01:16,  5.65it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 569/1000 [01:53<01:20,  5.36it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 570/1000 [01:53<01:23,  5.16it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 571/1000 [01:54<01:25,  5.01it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 572/1000 [01:54<01:18,  5.45it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 573/1000 [01:54<01:21,  5.23it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 574/1000 [01:54<01:15,  5.66it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 575/1000 [01:54<01:07,  6.29it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 576/1000 [01:54<01:05,  6.46it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 577/1000 [01:54<01:04,  6.59it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 578/1000 [01:55<01:03,  6.70it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 579/1000 [01:55<01:02,  6.74it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 580/1000 [01:55<01:02,  6.76it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 581/1000 [01:55<00:57,  7.23it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 582/1000 [01:55<01:13,  5.72it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 583/1000 [01:55<01:17,  5.39it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 584/1000 [01:56<01:08,  6.05it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 585/1000 [01:56<01:13,  5.64it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 586/1000 [01:56<01:08,  6.00it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 587/1000 [01:56<01:05,  6.27it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 588/1000 [01:56<01:12,  5.70it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 589/1000 [01:57<01:17,  5.28it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 590/1000 [01:57<01:18,  5.23it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 591/1000 [01:57<01:12,  5.64it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 592/1000 [01:57<01:07,  6.00it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 593/1000 [01:57<01:04,  6.30it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 594/1000 [01:57<01:10,  5.74it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 595/1000 [01:57<01:03,  6.36it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 596/1000 [01:58<01:11,  5.65it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 597/1000 [01:58<01:04,  6.28it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 598/1000 [01:58<01:09,  5.74it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 599/1000 [01:58<01:06,  6.05it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 600/1000 [01:58<01:11,  5.60it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 601/1000 [01:59<01:20,  4.96it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 602/1000 [01:59<01:13,  5.40it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 603/1000 [01:59<01:08,  5.76it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 604/1000 [01:59<01:05,  6.06it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 605/1000 [01:59<01:02,  6.30it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 606/1000 [01:59<01:14,  5.31it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 607/1000 [02:00<01:08,  5.73it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 608/1000 [02:00<01:12,  5.41it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 609/1000 [02:00<01:16,  5.12it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 610/1000 [02:00<01:19,  4.90it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 611/1000 [02:00<01:14,  5.24it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 612/1000 [02:01<01:17,  4.99it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 613/1000 [02:01<01:10,  5.48it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 614/1000 [02:01<01:03,  6.13it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 615/1000 [02:01<01:09,  5.53it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 616/1000 [02:01<01:05,  5.88it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 617/1000 [02:01<01:02,  6.12it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 618/1000 [02:02<00:57,  6.68it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 619/1000 [02:02<01:05,  5.80it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 620/1000 [02:02<01:02,  6.10it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 621/1000 [02:02<00:57,  6.65it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 622/1000 [02:02<00:55,  6.76it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 623/1000 [02:02<00:54,  6.86it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 624/1000 [02:03<01:07,  5.58it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 625/1000 [02:03<01:03,  5.94it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 626/1000 [02:03<01:00,  6.22it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 627/1000 [02:03<01:06,  5.62it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 628/1000 [02:03<01:11,  5.21it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 629/1000 [02:03<01:06,  5.61it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 630/1000 [02:04<01:15,  4.91it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 631/1000 [02:04<01:10,  5.24it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 632/1000 [02:04<01:13,  5.03it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 633/1000 [02:04<01:06,  5.52it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 634/1000 [02:04<01:02,  5.86it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 635/1000 [02:05<01:06,  5.50it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 636/1000 [02:05<01:02,  5.87it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 637/1000 [02:05<01:07,  5.39it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 638/1000 [02:05<01:10,  5.13it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 639/1000 [02:05<01:04,  5.56it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 640/1000 [02:05<01:00,  5.91it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 641/1000 [02:06<00:57,  6.20it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 642/1000 [02:06<00:52,  6.76it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 643/1000 [02:06<00:52,  6.80it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 644/1000 [02:06<00:49,  7.22it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 645/1000 [02:06<00:49,  7.12it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 646/1000 [02:06<00:55,  6.36it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 647/1000 [02:06<00:54,  6.52it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 648/1000 [02:07<01:04,  5.44it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 649/1000 [02:07<01:00,  5.82it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 650/1000 [02:07<01:04,  5.43it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 651/1000 [02:07<00:59,  5.82it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 652/1000 [02:07<01:05,  5.31it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 653/1000 [02:08<01:00,  5.73it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 654/1000 [02:08<00:56,  6.09it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 655/1000 [02:08<00:54,  6.33it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 656/1000 [02:08<00:52,  6.52it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 657/1000 [02:08<01:03,  5.39it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 658/1000 [02:08<01:06,  5.18it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 659/1000 [02:09<01:00,  5.59it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 660/1000 [02:09<01:09,  4.92it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 661/1000 [02:09<01:10,  4.80it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 662/1000 [02:09<01:03,  5.28it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 663/1000 [02:09<01:06,  5.10it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 664/1000 [02:10<01:00,  5.52it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 665/1000 [02:10<00:56,  5.92it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 666/1000 [02:10<01:00,  5.51it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 667/1000 [02:10<01:03,  5.28it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 668/1000 [02:10<00:58,  5.70it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 669/1000 [02:10<00:55,  6.00it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 670/1000 [02:11<00:52,  6.28it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 671/1000 [02:11<01:02,  5.30it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 672/1000 [02:11<01:04,  5.12it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 673/1000 [02:11<01:05,  5.01it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 674/1000 [02:11<01:06,  4.92it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 675/1000 [02:12<01:00,  5.40it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 676/1000 [02:12<00:57,  5.63it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 677/1000 [02:12<00:53,  6.01it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 678/1000 [02:12<00:51,  6.23it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 679/1000 [02:12<00:49,  6.45it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 680/1000 [02:12<00:50,  6.37it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 681/1000 [02:13<00:50,  6.32it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 682/1000 [02:13<00:48,  6.52it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 683/1000 [02:13<00:47,  6.62it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 684/1000 [02:13<00:53,  5.92it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 685/1000 [02:13<01:04,  4.87it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 686/1000 [02:14<01:04,  4.85it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 687/1000 [02:14<00:58,  5.32it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 688/1000 [02:14<00:54,  5.71it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 689/1000 [02:14<00:51,  6.05it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 690/1000 [02:14<00:50,  6.11it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 691/1000 [02:14<00:49,  6.31it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 692/1000 [02:15<00:53,  5.74it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 693/1000 [02:15<00:57,  5.31it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 694/1000 [02:15<00:53,  5.72it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 695/1000 [02:15<00:58,  5.26it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 696/1000 [02:15<00:55,  5.50it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 697/1000 [02:15<00:51,  5.85it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 698/1000 [02:16<00:49,  6.14it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 699/1000 [02:16<00:47,  6.33it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 700/1000 [02:16<00:53,  5.63it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 701/1000 [02:16<00:54,  5.48it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 702/1000 [02:16<00:56,  5.25it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 703/1000 [02:16<00:52,  5.68it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 704/1000 [02:17<00:48,  6.04it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 705/1000 [02:17<00:56,  5.20it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 706/1000 [02:17<01:01,  4.75it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 707/1000 [02:17<00:53,  5.47it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 708/1000 [02:17<01:00,  4.86it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 709/1000 [02:18<01:01,  4.76it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 710/1000 [02:18<01:01,  4.69it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 711/1000 [02:18<00:55,  5.20it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 712/1000 [02:18<00:50,  5.67it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 713/1000 [02:18<00:53,  5.36it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 714/1000 [02:19<00:49,  5.75it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 715/1000 [02:19<00:48,  5.87it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 716/1000 [02:19<00:46,  6.17it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 717/1000 [02:19<00:44,  6.38it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 718/1000 [02:19<00:49,  5.66it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 719/1000 [02:19<00:52,  5.38it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 720/1000 [02:20<00:54,  5.09it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 721/1000 [02:20<00:55,  5.02it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 722/1000 [02:20<00:56,  4.96it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 723/1000 [02:20<00:51,  5.43it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 724/1000 [02:20<00:47,  5.80it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 725/1000 [02:21<00:50,  5.45it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 726/1000 [02:21<00:47,  5.81it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 727/1000 [02:21<00:44,  6.11it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 728/1000 [02:21<00:42,  6.36it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 729/1000 [02:21<00:41,  6.50it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 730/1000 [02:21<00:47,  5.71it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 731/1000 [02:22<00:44,  6.05it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 732/1000 [02:22<00:42,  6.31it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 733/1000 [02:22<00:47,  5.62it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 734/1000 [02:22<00:53,  4.98it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 735/1000 [02:22<00:55,  4.81it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 736/1000 [02:23<00:49,  5.34it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 737/1000 [02:23<00:46,  5.71it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 738/1000 [02:23<00:43,  6.00it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 739/1000 [02:23<00:47,  5.48it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 740/1000 [02:23<00:44,  5.89it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 741/1000 [02:23<00:43,  5.97it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 742/1000 [02:23<00:39,  6.54it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 743/1000 [02:24<00:43,  5.85it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 744/1000 [02:24<00:46,  5.47it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 745/1000 [02:24<00:43,  5.85it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 746/1000 [02:24<00:41,  6.13it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 747/1000 [02:24<00:39,  6.38it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 748/1000 [02:25<00:43,  5.76it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 749/1000 [02:25<00:41,  6.08it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 750/1000 [02:25<00:45,  5.54it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 751/1000 [02:25<00:40,  6.20it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 752/1000 [02:25<00:36,  6.73it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 753/1000 [02:25<00:36,  6.78it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 754/1000 [02:25<00:34,  7.20it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 755/1000 [02:26<00:34,  7.09it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 756/1000 [02:26<00:32,  7.55it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 757/1000 [02:26<00:34,  7.08it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 758/1000 [02:26<00:40,  6.03it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 759/1000 [02:26<00:49,  4.91it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 760/1000 [02:27<00:49,  4.85it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 761/1000 [02:27<00:44,  5.34it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 762/1000 [02:27<00:46,  5.16it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 763/1000 [02:27<00:42,  5.57it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 764/1000 [02:27<00:47,  4.93it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 765/1000 [02:27<00:48,  4.88it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 766/1000 [02:28<00:48,  4.79it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 767/1000 [02:28<00:49,  4.73it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 768/1000 [02:28<00:44,  5.23it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 769/1000 [02:28<00:46,  5.00it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 770/1000 [02:28<00:47,  4.86it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 771/1000 [02:29<00:42,  5.36it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 772/1000 [02:29<00:39,  5.78it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 773/1000 [02:29<00:45,  5.03it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 774/1000 [02:29<00:45,  4.92it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 775/1000 [02:29<00:41,  5.43it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 776/1000 [02:30<00:39,  5.65it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 777/1000 [02:30<00:41,  5.33it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 778/1000 [02:30<00:46,  4.78it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 779/1000 [02:30<00:46,  4.80it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 780/1000 [02:30<00:41,  5.29it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 781/1000 [02:30<00:36,  5.98it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 782/1000 [02:31<00:36,  6.04it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 783/1000 [02:31<00:39,  5.52it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 784/1000 [02:31<00:36,  5.89it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 785/1000 [02:31<00:33,  6.49it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 786/1000 [02:31<00:36,  5.86it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 787/1000 [02:31<00:34,  6.15it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 788/1000 [02:32<00:34,  6.18it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 789/1000 [02:32<00:33,  6.38it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 790/1000 [02:32<00:30,  6.87it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 791/1000 [02:32<00:37,  5.62it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 792/1000 [02:32<00:34,  5.96it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 793/1000 [02:32<00:33,  6.23it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 794/1000 [02:33<00:31,  6.44it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 795/1000 [02:33<00:31,  6.55it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 796/1000 [02:33<00:30,  6.68it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 797/1000 [02:33<00:30,  6.74it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 798/1000 [02:33<00:29,  6.79it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 799/1000 [02:33<00:34,  5.90it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 800/1000 [02:34<00:32,  6.20it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 801/1000 [02:34<00:35,  5.62it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 802/1000 [02:34<00:31,  6.27it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 803/1000 [02:34<00:34,  5.75it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 804/1000 [02:34<00:35,  5.55it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 805/1000 [02:34<00:32,  5.92it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 806/1000 [02:35<00:31,  6.20it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 807/1000 [02:35<00:30,  6.37it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 808/1000 [02:35<00:33,  5.80it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 809/1000 [02:35<00:29,  6.42it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 810/1000 [02:35<00:28,  6.56it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 811/1000 [02:35<00:34,  5.45it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 812/1000 [02:36<00:32,  5.85it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 813/1000 [02:36<00:31,  5.96it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 814/1000 [02:36<00:29,  6.27it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 815/1000 [02:36<00:32,  5.63it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 816/1000 [02:36<00:34,  5.33it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 817/1000 [02:36<00:31,  5.72it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 818/1000 [02:37<00:36,  5.04it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 819/1000 [02:37<00:36,  4.89it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 820/1000 [02:37<00:33,  5.36it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 821/1000 [02:37<00:37,  4.79it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 822/1000 [02:37<00:33,  5.30it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 823/1000 [02:38<00:33,  5.24it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 824/1000 [02:38<00:29,  5.90it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 825/1000 [02:38<00:28,  6.19it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 826/1000 [02:38<00:25,  6.74it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 827/1000 [02:38<00:33,  5.22it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 828/1000 [02:38<00:30,  5.66it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 829/1000 [02:39<00:28,  5.97it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 830/1000 [02:39<00:27,  6.20it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 831/1000 [02:39<00:26,  6.47it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 832/1000 [02:39<00:28,  5.83it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 833/1000 [02:39<00:27,  6.13it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 834/1000 [02:40<00:32,  5.18it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 835/1000 [02:40<00:29,  5.62it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 836/1000 [02:40<00:30,  5.32it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 837/1000 [02:40<00:34,  4.76it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 838/1000 [02:40<00:31,  5.12it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 839/1000 [02:40<00:28,  5.58it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 840/1000 [02:41<00:30,  5.28it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 841/1000 [02:41<00:28,  5.54it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 842/1000 [02:41<00:29,  5.30it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 843/1000 [02:41<00:27,  5.70it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 844/1000 [02:41<00:25,  6.07it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 845/1000 [02:42<00:27,  5.55it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 846/1000 [02:42<00:26,  5.89it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 847/1000 [02:42<00:24,  6.15it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 848/1000 [02:42<00:24,  6.17it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 849/1000 [02:42<00:22,  6.71it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 850/1000 [02:42<00:27,  5.54it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 851/1000 [02:42<00:25,  5.91it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 852/1000 [02:43<00:22,  6.50it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 853/1000 [02:43<00:25,  5.84it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 854/1000 [02:43<00:27,  5.38it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 855/1000 [02:43<00:25,  5.77it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 856/1000 [02:43<00:22,  6.37it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 857/1000 [02:43<00:21,  6.54it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 858/1000 [02:44<00:26,  5.39it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 859/1000 [02:44<00:24,  5.81it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 860/1000 [02:44<00:22,  6.12it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 861/1000 [02:44<00:21,  6.39it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 862/1000 [02:44<00:21,  6.53it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 863/1000 [02:44<00:20,  6.64it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 864/1000 [02:45<00:19,  7.09it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 865/1000 [02:45<00:18,  7.44it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 866/1000 [02:45<00:21,  6.20it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 867/1000 [02:45<00:20,  6.41it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 868/1000 [02:45<00:20,  6.57it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 869/1000 [02:45<00:19,  6.69it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 870/1000 [02:46<00:23,  5.52it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 871/1000 [02:46<00:29,  4.42it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 872/1000 [02:46<00:25,  4.98it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 873/1000 [02:46<00:23,  5.46it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 874/1000 [02:46<00:24,  5.17it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 875/1000 [02:47<00:22,  5.61it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 876/1000 [02:47<00:23,  5.32it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 877/1000 [02:47<00:24,  5.06it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 878/1000 [02:47<00:22,  5.37it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 879/1000 [02:47<00:21,  5.74it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 880/1000 [02:47<00:19,  6.08it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 881/1000 [02:48<00:18,  6.28it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 882/1000 [02:48<00:17,  6.82it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 883/1000 [02:48<00:17,  6.62it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 884/1000 [02:48<00:17,  6.49it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 885/1000 [02:48<00:17,  6.39it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 886/1000 [02:48<00:17,  6.56it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 887/1000 [02:48<00:16,  6.69it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 888/1000 [02:49<00:16,  6.75it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 889/1000 [02:49<00:16,  6.57it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 890/1000 [02:49<00:17,  6.44it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 891/1000 [02:49<00:17,  6.36it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 892/1000 [02:49<00:17,  6.30it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 893/1000 [02:49<00:16,  6.53it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 894/1000 [02:50<00:15,  6.64it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 895/1000 [02:50<00:17,  5.84it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 896/1000 [02:50<00:16,  6.47it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 897/1000 [02:50<00:16,  6.38it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 898/1000 [02:50<00:15,  6.54it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 899/1000 [02:50<00:15,  6.66it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 900/1000 [02:50<00:14,  6.76it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 901/1000 [02:51<00:14,  6.79it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 902/1000 [02:51<00:14,  6.80it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 903/1000 [02:51<00:17,  5.56it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 904/1000 [02:51<00:16,  5.92it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 905/1000 [02:51<00:17,  5.54it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 906/1000 [02:52<00:16,  5.73it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 907/1000 [02:52<00:15,  6.04it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 908/1000 [02:52<00:14,  6.32it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 909/1000 [02:52<00:14,  6.48it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 910/1000 [02:52<00:15,  5.86it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 911/1000 [02:52<00:16,  5.49it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 912/1000 [02:53<00:15,  5.70it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 913/1000 [02:53<00:17,  5.02it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 914/1000 [02:53<00:15,  5.45it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 915/1000 [02:53<00:14,  5.83it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 916/1000 [02:53<00:15,  5.37it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 917/1000 [02:53<00:14,  5.75it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 918/1000 [02:54<00:13,  5.87it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 919/1000 [02:54<00:13,  6.14it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 920/1000 [02:54<00:12,  6.35it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 921/1000 [02:54<00:12,  6.52it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 922/1000 [02:54<00:11,  6.66it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 923/1000 [02:54<00:11,  6.80it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 924/1000 [02:55<00:12,  5.93it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 925/1000 [02:55<00:12,  6.22it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 926/1000 [02:55<00:11,  6.43it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 927/1000 [02:55<00:11,  6.56it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 928/1000 [02:55<00:10,  6.67it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 929/1000 [02:55<00:12,  5.88it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 930/1000 [02:55<00:11,  6.17it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 931/1000 [02:56<00:10,  6.40it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 932/1000 [02:56<00:10,  6.54it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 933/1000 [02:56<00:11,  5.76it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 934/1000 [02:56<00:13,  4.98it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 935/1000 [02:56<00:11,  5.70it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 936/1000 [02:57<00:10,  6.00it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 937/1000 [02:57<00:11,  5.47it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 938/1000 [02:57<00:11,  5.25it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 939/1000 [02:57<00:10,  5.66it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 940/1000 [02:57<00:09,  6.00it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 941/1000 [02:57<00:09,  6.26it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 942/1000 [02:58<00:09,  6.43it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 943/1000 [02:58<00:10,  5.41it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 944/1000 [02:58<00:09,  5.79it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 945/1000 [02:58<00:09,  6.08it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 946/1000 [02:58<00:09,  5.50it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 947/1000 [02:59<00:10,  4.89it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 948/1000 [02:59<00:09,  5.38it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 949/1000 [02:59<00:10,  5.06it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 950/1000 [02:59<00:10,  4.97it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 951/1000 [02:59<00:10,  4.60it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 952/1000 [03:00<00:10,  4.59it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 953/1000 [03:00<00:09,  5.10it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 954/1000 [03:00<00:08,  5.54it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 955/1000 [03:00<00:07,  5.93it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 956/1000 [03:00<00:07,  6.19it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 957/1000 [03:00<00:07,  5.59it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 958/1000 [03:01<00:07,  5.95it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 959/1000 [03:01<00:06,  6.21it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 960/1000 [03:01<00:06,  6.43it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 961/1000 [03:01<00:07,  5.14it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 962/1000 [03:01<00:07,  5.02it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 963/1000 [03:02<00:07,  4.95it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 964/1000 [03:02<00:06,  5.43it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 965/1000 [03:02<00:05,  6.10it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 966/1000 [03:02<00:06,  5.64it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 967/1000 [03:02<00:05,  5.95it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 968/1000 [03:02<00:05,  6.02it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 969/1000 [03:02<00:04,  6.27it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 970/1000 [03:03<00:04,  6.47it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 971/1000 [03:03<00:04,  6.58it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 972/1000 [03:03<00:04,  5.79it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 973/1000 [03:03<00:04,  5.92it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 974/1000 [03:03<00:04,  6.18it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 975/1000 [03:03<00:03,  6.37it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 976/1000 [03:04<00:03,  6.53it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 977/1000 [03:04<00:03,  6.67it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 978/1000 [03:04<00:04,  5.50it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 979/1000 [03:04<00:03,  5.89it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 980/1000 [03:04<00:03,  5.10it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 981/1000 [03:04<00:03,  5.56it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 982/1000 [03:05<00:03,  5.89it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 983/1000 [03:05<00:03,  4.82it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 984/1000 [03:05<00:03,  4.79it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 985/1000 [03:05<00:02,  5.30it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 986/1000 [03:05<00:02,  5.70it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 987/1000 [03:06<00:02,  6.04it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 988/1000 [03:06<00:01,  6.01it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 989/1000 [03:06<00:01,  6.32it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 990/1000 [03:06<00:01,  6.54it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 991/1000 [03:06<00:01,  6.61it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 992/1000 [03:06<00:01,  6.71it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 993/1000 [03:06<00:01,  6.79it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 994/1000 [03:07<00:00,  6.79it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 995/1000 [03:07<00:00,  5.99it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 996/1000 [03:07<00:00,  6.06it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 997/1000 [03:07<00:00,  6.27it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 998/1000 [03:07<00:00,  6.45it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 999/1000 [03:07<00:00,  5.65it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [03:08<00:00,  4.76it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [03:08<00:00,  5.31it/s]
DONE (11.39s)
DONE (8.25s)
Computing criterion:   0%|          | 0/20 [00:00<?, ?it/s]Computing criterion:   5%|â–Œ         | 1/20 [00:20<06:23, 20.21s/it]Computing criterion:  10%|â–ˆ         | 2/20 [00:39<05:56, 19.82s/it]Computing criterion:  15%|â–ˆâ–Œ        | 3/20 [00:58<05:26, 19.18s/it]Computing criterion:  20%|â–ˆâ–ˆ        | 4/20 [01:16<05:04, 19.01s/it]Computing criterion:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [01:36<04:47, 19.14s/it]Computing criterion:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [01:54<04:24, 18.92s/it]Computing criterion:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [02:13<04:03, 18.70s/it]Computing criterion:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [02:31<03:42, 18.51s/it]Computing criterion:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [02:50<03:25, 18.64s/it]Computing criterion:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [03:10<03:11, 19.13s/it]Computing criterion:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [03:30<02:55, 19.49s/it]Computing criterion:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [03:50<02:36, 19.61s/it]Computing criterion:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [04:08<02:14, 19.17s/it]Computing criterion:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [04:27<01:53, 19.00s/it]Computing criterion:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [04:46<01:35, 19.00s/it]Computing criterion:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [05:04<01:15, 18.87s/it]Computing criterion:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [05:23<00:56, 18.88s/it]Computing criterion:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [05:42<00:37, 18.87s/it]Computing criterion:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [06:01<00:18, 18.80s/it]Computing criterion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [06:19<00:00, 18.64s/it]Computing criterion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [06:19<00:00, 18.97s/it]
Ref scores:   0%|          | 0/1000 [00:00<?, ?it/s]Ref scores:   0%|          | 1/1000 [00:02<48:40,  2.92s/it]Ref scores:   0%|          | 2/1000 [00:03<21:26,  1.29s/it]Ref scores:   0%|          | 3/1000 [00:03<12:44,  1.30it/s]Ref scores:   0%|          | 4/1000 [00:03<09:10,  1.81it/s]Ref scores:   0%|          | 5/1000 [00:03<07:05,  2.34it/s]Ref scores:   1%|          | 6/1000 [00:03<06:19,  2.62it/s]Ref scores:   1%|          | 7/1000 [00:04<05:03,  3.27it/s]Ref scores:   1%|          | 8/1000 [00:04<04:46,  3.46it/s]Ref scores:   1%|          | 9/1000 [00:04<04:24,  3.74it/s]Ref scores:   1%|          | 10/1000 [00:04<04:08,  3.98it/s]Ref scores:   1%|          | 11/1000 [00:04<03:58,  4.15it/s]Ref scores:   1%|          | 12/1000 [00:05<03:29,  4.71it/s]Ref scores:   1%|â–         | 13/1000 [00:05<03:10,  5.18it/s]Ref scores:   1%|â–         | 14/1000 [00:05<02:56,  5.58it/s]Ref scores:   2%|â–         | 15/1000 [00:05<02:35,  6.33it/s]Ref scores:   2%|â–         | 16/1000 [00:05<02:32,  6.45it/s]Ref scores:   2%|â–         | 17/1000 [00:05<02:52,  5.70it/s]Ref scores:   2%|â–         | 18/1000 [00:06<02:42,  6.04it/s]Ref scores:   2%|â–         | 19/1000 [00:06<02:56,  5.55it/s]Ref scores:   2%|â–         | 20/1000 [00:06<02:47,  5.84it/s]Ref scores:   2%|â–         | 21/1000 [00:06<02:39,  6.15it/s]Ref scores:   2%|â–         | 22/1000 [00:06<02:39,  6.12it/s]Ref scores:   2%|â–         | 23/1000 [00:06<02:35,  6.27it/s]Ref scores:   2%|â–         | 24/1000 [00:07<03:03,  5.32it/s]Ref scores:   2%|â–Ž         | 25/1000 [00:07<02:51,  5.68it/s]Ref scores:   3%|â–Ž         | 26/1000 [00:07<02:42,  5.98it/s]Ref scores:   3%|â–Ž         | 27/1000 [00:07<02:58,  5.45it/s]Ref scores:   3%|â–Ž         | 28/1000 [00:07<02:46,  5.84it/s]Ref scores:   3%|â–Ž         | 29/1000 [00:08<02:59,  5.40it/s]Ref scores:   3%|â–Ž         | 30/1000 [00:08<02:43,  5.94it/s]Ref scores:   3%|â–Ž         | 31/1000 [00:08<02:57,  5.46it/s]Ref scores:   3%|â–Ž         | 32/1000 [00:08<03:17,  4.91it/s]Ref scores:   3%|â–Ž         | 33/1000 [00:08<03:20,  4.83it/s]Ref scores:   3%|â–Ž         | 34/1000 [00:08<03:02,  5.29it/s]Ref scores:   4%|â–Ž         | 35/1000 [00:09<03:21,  4.78it/s]Ref scores:   4%|â–Ž         | 36/1000 [00:09<03:03,  5.25it/s]Ref scores:   4%|â–Ž         | 37/1000 [00:09<03:12,  5.00it/s]Ref scores:   4%|â–         | 38/1000 [00:09<03:01,  5.30it/s]Ref scores:   4%|â–         | 39/1000 [00:09<02:48,  5.69it/s]Ref scores:   4%|â–         | 40/1000 [00:10<02:39,  6.02it/s]Ref scores:   4%|â–         | 41/1000 [00:10<02:32,  6.28it/s]Ref scores:   4%|â–         | 42/1000 [00:10<02:30,  6.35it/s]Ref scores:   4%|â–         | 43/1000 [00:10<02:26,  6.54it/s]Ref scores:   4%|â–         | 44/1000 [00:10<02:24,  6.61it/s]Ref scores:   4%|â–         | 45/1000 [00:10<02:23,  6.65it/s]Ref scores:   5%|â–         | 46/1000 [00:10<02:23,  6.67it/s]Ref scores:   5%|â–         | 47/1000 [00:11<02:42,  5.87it/s]Ref scores:   5%|â–         | 48/1000 [00:11<02:34,  6.16it/s]Ref scores:   5%|â–         | 49/1000 [00:11<02:29,  6.36it/s]Ref scores:   5%|â–Œ         | 50/1000 [00:11<02:26,  6.48it/s]Ref scores:   5%|â–Œ         | 51/1000 [00:11<02:23,  6.61it/s]Ref scores:   5%|â–Œ         | 52/1000 [00:11<02:53,  5.46it/s]Ref scores:   5%|â–Œ         | 53/1000 [00:12<03:13,  4.88it/s]Ref scores:   5%|â–Œ         | 54/1000 [00:12<02:59,  5.28it/s]Ref scores:   6%|â–Œ         | 55/1000 [00:12<02:46,  5.69it/s]Ref scores:   6%|â–Œ         | 56/1000 [00:12<02:36,  6.03it/s]Ref scores:   6%|â–Œ         | 57/1000 [00:12<02:28,  6.33it/s]Ref scores:   6%|â–Œ         | 58/1000 [00:13<02:42,  5.78it/s]Ref scores:   6%|â–Œ         | 59/1000 [00:13<02:32,  6.16it/s]Ref scores:   6%|â–Œ         | 60/1000 [00:13<02:27,  6.39it/s]Ref scores:   6%|â–Œ         | 61/1000 [00:13<02:44,  5.71it/s]Ref scores:   6%|â–Œ         | 62/1000 [00:13<02:34,  6.06it/s]Ref scores:   6%|â–‹         | 63/1000 [00:13<02:28,  6.30it/s]Ref scores:   6%|â–‹         | 64/1000 [00:13<02:15,  6.89it/s]Ref scores:   6%|â–‹         | 65/1000 [00:14<02:35,  6.03it/s]Ref scores:   7%|â–‹         | 66/1000 [00:14<02:19,  6.68it/s]Ref scores:   7%|â–‹         | 67/1000 [00:14<02:16,  6.82it/s]Ref scores:   7%|â–‹         | 68/1000 [00:14<02:15,  6.90it/s]Ref scores:   7%|â–‹         | 69/1000 [00:14<02:44,  5.65it/s]Ref scores:   7%|â–‹         | 70/1000 [00:14<02:26,  6.36it/s]Ref scores:   7%|â–‹         | 71/1000 [00:15<02:53,  5.36it/s]Ref scores:   7%|â–‹         | 72/1000 [00:15<02:39,  5.82it/s]Ref scores:   7%|â–‹         | 73/1000 [00:15<02:48,  5.51it/s]Ref scores:   7%|â–‹         | 74/1000 [00:15<02:54,  5.29it/s]Ref scores:   8%|â–Š         | 75/1000 [00:15<02:42,  5.71it/s]Ref scores:   8%|â–Š         | 76/1000 [00:15<02:33,  6.02it/s]Ref scores:   8%|â–Š         | 77/1000 [00:16<02:26,  6.31it/s]Ref scores:   8%|â–Š         | 78/1000 [00:16<02:20,  6.55it/s]Ref scores:   8%|â–Š         | 79/1000 [00:16<02:35,  5.93it/s]Ref scores:   8%|â–Š         | 80/1000 [00:16<02:28,  6.20it/s]Ref scores:   8%|â–Š         | 81/1000 [00:16<02:43,  5.62it/s]Ref scores:   8%|â–Š         | 82/1000 [00:16<02:33,  5.99it/s]Ref scores:   8%|â–Š         | 83/1000 [00:17<02:26,  6.28it/s]Ref scores:   8%|â–Š         | 84/1000 [00:17<02:21,  6.50it/s]Ref scores:   8%|â–Š         | 85/1000 [00:17<02:39,  5.75it/s]Ref scores:   9%|â–Š         | 86/1000 [00:17<03:00,  5.05it/s]Ref scores:   9%|â–Š         | 87/1000 [00:17<02:42,  5.61it/s]Ref scores:   9%|â–‰         | 88/1000 [00:18<02:33,  5.96it/s]Ref scores:   9%|â–‰         | 89/1000 [00:18<02:56,  5.16it/s]Ref scores:   9%|â–‰         | 90/1000 [00:18<03:12,  4.73it/s]Ref scores:   9%|â–‰         | 91/1000 [00:18<02:53,  5.25it/s]Ref scores:   9%|â–‰         | 92/1000 [00:18<03:10,  4.75it/s]Ref scores:   9%|â–‰         | 93/1000 [00:19<03:13,  4.68it/s]Ref scores:   9%|â–‰         | 94/1000 [00:19<02:52,  5.26it/s]Ref scores:  10%|â–‰         | 95/1000 [00:19<02:39,  5.68it/s]Ref scores:  10%|â–‰         | 96/1000 [00:19<02:29,  6.04it/s]Ref scores:  10%|â–‰         | 97/1000 [00:19<02:15,  6.66it/s]Ref scores:  10%|â–‰         | 98/1000 [00:19<02:14,  6.72it/s]Ref scores:  10%|â–‰         | 99/1000 [00:19<02:17,  6.57it/s]Ref scores:  10%|â–ˆ         | 100/1000 [00:20<02:13,  6.72it/s]Ref scores:  10%|â–ˆ         | 101/1000 [00:20<02:16,  6.58it/s]Ref scores:  10%|â–ˆ         | 102/1000 [00:20<02:13,  6.71it/s]Ref scores:  10%|â–ˆ         | 103/1000 [00:20<02:42,  5.51it/s]Ref scores:  10%|â–ˆ         | 104/1000 [00:20<02:53,  5.17it/s]Ref scores:  10%|â–ˆ         | 105/1000 [00:21<02:59,  4.98it/s]Ref scores:  11%|â–ˆ         | 106/1000 [00:21<02:35,  5.75it/s]Ref scores:  11%|â–ˆ         | 107/1000 [00:21<02:26,  6.08it/s]Ref scores:  11%|â–ˆ         | 108/1000 [00:21<02:21,  6.31it/s]Ref scores:  11%|â–ˆ         | 109/1000 [00:21<02:16,  6.53it/s]Ref scores:  11%|â–ˆ         | 110/1000 [00:21<02:13,  6.65it/s]Ref scores:  11%|â–ˆ         | 111/1000 [00:21<02:15,  6.54it/s]Ref scores:  11%|â–ˆ         | 112/1000 [00:22<02:32,  5.84it/s]Ref scores:  11%|â–ˆâ–        | 113/1000 [00:22<02:28,  5.97it/s]Ref scores:  11%|â–ˆâ–        | 114/1000 [00:22<02:20,  6.29it/s]Ref scores:  12%|â–ˆâ–        | 115/1000 [00:22<02:15,  6.55it/s]Ref scores:  12%|â–ˆâ–        | 116/1000 [00:22<02:30,  5.87it/s]Ref scores:  12%|â–ˆâ–        | 117/1000 [00:23<02:43,  5.39it/s]Ref scores:  12%|â–ˆâ–        | 118/1000 [00:23<02:51,  5.15it/s]Ref scores:  12%|â–ˆâ–        | 119/1000 [00:23<02:37,  5.58it/s]Ref scores:  12%|â–ˆâ–        | 120/1000 [00:23<02:18,  6.35it/s]Ref scores:  12%|â–ˆâ–        | 121/1000 [00:23<02:34,  5.68it/s]Ref scores:  12%|â–ˆâ–        | 122/1000 [00:23<02:30,  5.82it/s]Ref scores:  12%|â–ˆâ–        | 123/1000 [00:24<02:23,  6.13it/s]Ref scores:  12%|â–ˆâ–        | 124/1000 [00:24<02:58,  4.90it/s]Ref scores:  12%|â–ˆâ–Ž        | 125/1000 [00:24<02:41,  5.40it/s]Ref scores:  13%|â–ˆâ–Ž        | 126/1000 [00:24<02:30,  5.79it/s]Ref scores:  13%|â–ˆâ–Ž        | 127/1000 [00:24<02:21,  6.15it/s]Ref scores:  13%|â–ˆâ–Ž        | 128/1000 [00:24<02:17,  6.35it/s]Ref scores:  13%|â–ˆâ–Ž        | 129/1000 [00:25<02:44,  5.30it/s]Ref scores:  13%|â–ˆâ–Ž        | 130/1000 [00:25<02:31,  5.75it/s]Ref scores:  13%|â–ˆâ–Ž        | 131/1000 [00:25<02:23,  6.06it/s]Ref scores:  13%|â–ˆâ–Ž        | 132/1000 [00:25<02:09,  6.70it/s]Ref scores:  13%|â–ˆâ–Ž        | 133/1000 [00:25<02:23,  6.02it/s]Ref scores:  13%|â–ˆâ–Ž        | 134/1000 [00:25<02:17,  6.28it/s]Ref scores:  14%|â–ˆâ–Ž        | 135/1000 [00:26<02:12,  6.55it/s]Ref scores:  14%|â–ˆâ–Ž        | 136/1000 [00:26<02:09,  6.68it/s]Ref scores:  14%|â–ˆâ–Ž        | 137/1000 [00:26<02:06,  6.85it/s]Ref scores:  14%|â–ˆâ–        | 138/1000 [00:26<02:24,  5.96it/s]Ref scores:  14%|â–ˆâ–        | 139/1000 [00:26<02:16,  6.29it/s]Ref scores:  14%|â–ˆâ–        | 140/1000 [00:26<02:40,  5.35it/s]Ref scores:  14%|â–ˆâ–        | 141/1000 [00:27<02:29,  5.76it/s]Ref scores:  14%|â–ˆâ–        | 142/1000 [00:27<02:20,  6.10it/s]Ref scores:  14%|â–ˆâ–        | 143/1000 [00:27<02:14,  6.36it/s]Ref scores:  14%|â–ˆâ–        | 144/1000 [00:27<02:29,  5.72it/s]Ref scores:  14%|â–ˆâ–        | 145/1000 [00:27<02:38,  5.40it/s]Ref scores:  15%|â–ˆâ–        | 146/1000 [00:27<02:27,  5.78it/s]Ref scores:  15%|â–ˆâ–        | 147/1000 [00:28<02:19,  6.13it/s]Ref scores:  15%|â–ˆâ–        | 148/1000 [00:28<02:06,  6.76it/s]Ref scores:  15%|â–ˆâ–        | 149/1000 [00:28<02:04,  6.84it/s]Ref scores:  15%|â–ˆâ–Œ        | 150/1000 [00:28<02:04,  6.85it/s]Ref scores:  15%|â–ˆâ–Œ        | 151/1000 [00:28<02:04,  6.83it/s]Ref scores:  15%|â–ˆâ–Œ        | 152/1000 [00:28<02:20,  6.04it/s]Ref scores:  15%|â–ˆâ–Œ        | 153/1000 [00:29<02:45,  5.11it/s]Ref scores:  15%|â–ˆâ–Œ        | 154/1000 [00:29<02:52,  4.89it/s]Ref scores:  16%|â–ˆâ–Œ        | 155/1000 [00:29<02:37,  5.35it/s]Ref scores:  16%|â–ˆâ–Œ        | 156/1000 [00:29<02:19,  6.05it/s]Ref scores:  16%|â–ˆâ–Œ        | 157/1000 [00:29<02:03,  6.81it/s]Ref scores:  16%|â–ˆâ–Œ        | 158/1000 [00:29<02:20,  6.00it/s]Ref scores:  16%|â–ˆâ–Œ        | 159/1000 [00:30<02:33,  5.48it/s]Ref scores:  16%|â–ˆâ–Œ        | 160/1000 [00:30<02:41,  5.19it/s]Ref scores:  16%|â–ˆâ–Œ        | 161/1000 [00:30<02:29,  5.62it/s]Ref scores:  16%|â–ˆâ–Œ        | 162/1000 [00:30<02:13,  6.28it/s]Ref scores:  16%|â–ˆâ–‹        | 163/1000 [00:30<02:08,  6.49it/s]Ref scores:  16%|â–ˆâ–‹        | 164/1000 [00:30<02:10,  6.40it/s]Ref scores:  16%|â–ˆâ–‹        | 165/1000 [00:31<02:34,  5.39it/s]Ref scores:  17%|â–ˆâ–‹        | 166/1000 [00:31<02:24,  5.78it/s]Ref scores:  17%|â–ˆâ–‹        | 167/1000 [00:31<02:35,  5.35it/s]Ref scores:  17%|â–ˆâ–‹        | 168/1000 [00:31<02:40,  5.17it/s]Ref scores:  17%|â–ˆâ–‹        | 169/1000 [00:31<02:48,  4.94it/s]Ref scores:  17%|â–ˆâ–‹        | 170/1000 [00:32<02:37,  5.25it/s]Ref scores:  17%|â–ˆâ–‹        | 171/1000 [00:32<03:02,  4.53it/s]Ref scores:  17%|â–ˆâ–‹        | 172/1000 [00:32<03:01,  4.57it/s]Ref scores:  17%|â–ˆâ–‹        | 173/1000 [00:32<02:58,  4.63it/s]Ref scores:  17%|â–ˆâ–‹        | 174/1000 [00:32<02:40,  5.13it/s]Ref scores:  18%|â–ˆâ–Š        | 175/1000 [00:33<02:45,  4.99it/s]Ref scores:  18%|â–ˆâ–Š        | 176/1000 [00:33<02:31,  5.43it/s]Ref scores:  18%|â–ˆâ–Š        | 177/1000 [00:33<02:22,  5.78it/s]Ref scores:  18%|â–ˆâ–Š        | 178/1000 [00:33<02:31,  5.44it/s]Ref scores:  18%|â–ˆâ–Š        | 179/1000 [00:33<02:38,  5.19it/s]Ref scores:  18%|â–ˆâ–Š        | 180/1000 [00:34<02:43,  5.00it/s]Ref scores:  18%|â–ˆâ–Š        | 181/1000 [00:34<02:30,  5.46it/s]Ref scores:  18%|â–ˆâ–Š        | 182/1000 [00:34<02:24,  5.66it/s]Ref scores:  18%|â–ˆâ–Š        | 183/1000 [00:34<02:16,  5.98it/s]Ref scores:  18%|â–ˆâ–Š        | 184/1000 [00:34<02:15,  6.04it/s]Ref scores:  18%|â–ˆâ–Š        | 185/1000 [00:34<02:37,  5.18it/s]Ref scores:  19%|â–ˆâ–Š        | 186/1000 [00:35<02:24,  5.62it/s]Ref scores:  19%|â–ˆâ–Š        | 187/1000 [00:35<02:16,  5.94it/s]Ref scores:  19%|â–ˆâ–‰        | 188/1000 [00:35<02:26,  5.54it/s]Ref scores:  19%|â–ˆâ–‰        | 189/1000 [00:35<02:53,  4.68it/s]Ref scores:  19%|â–ˆâ–‰        | 190/1000 [00:36<03:02,  4.43it/s]Ref scores:  19%|â–ˆâ–‰        | 191/1000 [00:36<02:43,  4.95it/s]Ref scores:  19%|â–ˆâ–‰        | 192/1000 [00:36<02:47,  4.81it/s]Ref scores:  19%|â–ˆâ–‰        | 193/1000 [00:36<02:51,  4.72it/s]Ref scores:  19%|â–ˆâ–‰        | 194/1000 [00:36<03:01,  4.44it/s]Ref scores:  20%|â–ˆâ–‰        | 195/1000 [00:37<02:58,  4.51it/s]Ref scores:  20%|â–ˆâ–‰        | 196/1000 [00:37<02:39,  5.04it/s]Ref scores:  20%|â–ˆâ–‰        | 197/1000 [00:37<02:30,  5.33it/s]Ref scores:  20%|â–ˆâ–‰        | 198/1000 [00:37<02:12,  6.04it/s]Ref scores:  20%|â–ˆâ–‰        | 199/1000 [00:37<02:24,  5.55it/s]Ref scores:  20%|â–ˆâ–ˆ        | 200/1000 [00:37<02:07,  6.26it/s]Ref scores:  20%|â–ˆâ–ˆ        | 201/1000 [00:38<02:30,  5.30it/s]Ref scores:  20%|â–ˆâ–ˆ        | 202/1000 [00:38<02:39,  5.01it/s]Ref scores:  20%|â–ˆâ–ˆ        | 203/1000 [00:38<02:52,  4.63it/s]Ref scores:  20%|â–ˆâ–ˆ        | 204/1000 [00:38<02:34,  5.16it/s]Ref scores:  20%|â–ˆâ–ˆ        | 205/1000 [00:38<02:37,  5.06it/s]Ref scores:  21%|â–ˆâ–ˆ        | 206/1000 [00:39<02:51,  4.64it/s]Ref scores:  21%|â–ˆâ–ˆ        | 207/1000 [00:39<02:50,  4.65it/s]Ref scores:  21%|â–ˆâ–ˆ        | 208/1000 [00:39<03:00,  4.38it/s]Ref scores:  21%|â–ˆâ–ˆ        | 209/1000 [00:39<02:38,  4.98it/s]Ref scores:  21%|â–ˆâ–ˆ        | 210/1000 [00:40<02:52,  4.59it/s]Ref scores:  21%|â–ˆâ–ˆ        | 211/1000 [00:40<02:30,  5.26it/s]Ref scores:  21%|â–ˆâ–ˆ        | 212/1000 [00:40<02:17,  5.71it/s]Ref scores:  21%|â–ˆâ–ˆâ–       | 213/1000 [00:40<02:53,  4.54it/s]Ref scores:  21%|â–ˆâ–ˆâ–       | 214/1000 [00:40<02:34,  5.08it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 215/1000 [00:40<02:37,  4.99it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 216/1000 [00:41<02:22,  5.49it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 217/1000 [00:41<02:31,  5.17it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 218/1000 [00:41<02:22,  5.49it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 219/1000 [00:41<02:39,  4.89it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 220/1000 [00:41<02:42,  4.81it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 221/1000 [00:42<02:26,  5.33it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 222/1000 [00:42<02:30,  5.16it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 223/1000 [00:42<02:17,  5.64it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 224/1000 [00:42<02:07,  6.06it/s]Ref scores:  22%|â–ˆâ–ˆâ–Ž       | 225/1000 [00:42<02:17,  5.64it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 226/1000 [00:42<02:08,  6.01it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 227/1000 [00:43<02:17,  5.61it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 228/1000 [00:43<02:09,  5.97it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 229/1000 [00:43<02:18,  5.58it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 230/1000 [00:43<02:24,  5.33it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 231/1000 [00:43<02:39,  4.82it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 232/1000 [00:44<02:23,  5.33it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 233/1000 [00:44<02:13,  5.75it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 234/1000 [00:44<02:23,  5.34it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 235/1000 [00:44<02:39,  4.80it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 236/1000 [00:44<02:23,  5.32it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 237/1000 [00:45<02:29,  5.10it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 238/1000 [00:45<02:20,  5.43it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 239/1000 [00:45<02:25,  5.25it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 240/1000 [00:45<02:12,  5.73it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 241/1000 [00:45<02:31,  5.00it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 242/1000 [00:46<02:32,  4.97it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 243/1000 [00:46<02:18,  5.48it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 244/1000 [00:46<02:24,  5.25it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 245/1000 [00:46<02:30,  5.01it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 246/1000 [00:46<02:43,  4.63it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 247/1000 [00:46<02:25,  5.18it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 248/1000 [00:47<02:17,  5.49it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 249/1000 [00:47<02:23,  5.22it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 250/1000 [00:47<02:12,  5.66it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 251/1000 [00:47<02:19,  5.38it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 252/1000 [00:47<02:09,  5.79it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 253/1000 [00:48<02:17,  5.42it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 254/1000 [00:48<02:25,  5.12it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 255/1000 [00:48<02:12,  5.61it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 256/1000 [00:48<02:19,  5.35it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 257/1000 [00:48<02:08,  5.80it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 258/1000 [00:48<02:00,  6.15it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 259/1000 [00:49<01:55,  6.44it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 260/1000 [00:49<02:18,  5.34it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 261/1000 [00:49<02:08,  5.77it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 262/1000 [00:49<02:15,  5.45it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 263/1000 [00:49<02:22,  5.18it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 264/1000 [00:50<02:11,  5.60it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 265/1000 [00:50<02:03,  5.96it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 266/1000 [00:50<01:58,  6.21it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 267/1000 [00:50<02:09,  5.65it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 268/1000 [00:50<02:17,  5.34it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 269/1000 [00:50<01:59,  6.12it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 270/1000 [00:51<02:12,  5.52it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 271/1000 [00:51<02:03,  5.90it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 272/1000 [00:51<02:15,  5.38it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 273/1000 [00:51<02:05,  5.78it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 274/1000 [00:51<01:59,  6.10it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 275/1000 [00:51<01:57,  6.15it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 276/1000 [00:52<01:53,  6.41it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 277/1000 [00:52<02:15,  5.32it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 278/1000 [00:52<02:06,  5.71it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 279/1000 [00:52<02:23,  5.01it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 280/1000 [00:52<02:03,  5.82it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 281/1000 [00:52<01:57,  6.09it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 282/1000 [00:53<01:56,  6.14it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 283/1000 [00:53<01:52,  6.38it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 284/1000 [00:53<01:48,  6.57it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 285/1000 [00:53<01:46,  6.73it/s]Ref scores:  29%|â–ˆâ–ˆâ–Š       | 286/1000 [00:53<02:00,  5.94it/s]Ref scores:  29%|â–ˆâ–ˆâ–Š       | 287/1000 [00:53<01:54,  6.23it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 288/1000 [00:54<02:14,  5.28it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 289/1000 [00:54<02:28,  4.77it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 290/1000 [00:54<02:14,  5.29it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 291/1000 [00:54<02:04,  5.68it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 292/1000 [00:54<02:01,  5.82it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 293/1000 [00:55<02:12,  5.33it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 294/1000 [00:55<02:17,  5.14it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 295/1000 [00:55<02:09,  5.44it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 296/1000 [00:55<02:24,  4.87it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 297/1000 [00:55<02:35,  4.53it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 298/1000 [00:56<02:17,  5.09it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 299/1000 [00:56<02:21,  4.97it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 300/1000 [00:56<02:09,  5.42it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 301/1000 [00:56<01:59,  5.83it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 302/1000 [00:56<01:53,  6.14it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 303/1000 [00:56<01:48,  6.41it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 304/1000 [00:56<01:38,  7.08it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 305/1000 [00:57<01:52,  6.19it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 306/1000 [00:57<01:47,  6.43it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 307/1000 [00:57<02:02,  5.68it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 308/1000 [00:57<01:55,  6.02it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 309/1000 [00:57<02:03,  5.58it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 310/1000 [00:58<01:55,  5.95it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 311/1000 [00:58<01:50,  6.24it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 312/1000 [00:58<01:46,  6.48it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆâ–      | 313/1000 [00:58<01:43,  6.63it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆâ–      | 314/1000 [00:58<01:41,  6.78it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 315/1000 [00:58<01:43,  6.61it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 316/1000 [00:58<01:57,  5.81it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 317/1000 [00:59<01:51,  6.12it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 318/1000 [00:59<02:00,  5.65it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 319/1000 [00:59<02:07,  5.33it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 320/1000 [00:59<01:57,  5.78it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 321/1000 [00:59<02:05,  5.43it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 322/1000 [01:00<02:12,  5.12it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 323/1000 [01:00<02:01,  5.56it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 324/1000 [01:00<01:53,  5.93it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 325/1000 [01:00<01:48,  6.21it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 326/1000 [01:00<01:44,  6.44it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 327/1000 [01:00<02:06,  5.32it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 328/1000 [01:01<02:10,  5.16it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 329/1000 [01:01<02:22,  4.71it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 330/1000 [01:01<02:08,  5.22it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 331/1000 [01:01<02:14,  4.97it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 332/1000 [01:01<02:02,  5.47it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 333/1000 [01:02<01:53,  5.87it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 334/1000 [01:02<01:45,  6.34it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 335/1000 [01:02<01:42,  6.51it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 336/1000 [01:02<01:39,  6.67it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 337/1000 [01:02<01:38,  6.72it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 338/1000 [01:02<01:50,  5.97it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 339/1000 [01:02<01:45,  6.24it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 340/1000 [01:03<02:04,  5.30it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 341/1000 [01:03<01:58,  5.56it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 342/1000 [01:03<01:51,  5.92it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 343/1000 [01:03<01:59,  5.51it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 344/1000 [01:03<02:07,  5.14it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 345/1000 [01:04<02:12,  4.93it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 346/1000 [01:04<02:01,  5.40it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 347/1000 [01:04<01:53,  5.77it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 348/1000 [01:04<02:00,  5.40it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 349/1000 [01:04<01:52,  5.77it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 350/1000 [01:05<02:01,  5.36it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 351/1000 [01:05<01:52,  5.79it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 352/1000 [01:05<01:45,  6.14it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 353/1000 [01:05<02:03,  5.25it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 354/1000 [01:05<01:53,  5.68it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 355/1000 [01:05<01:46,  6.03it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 356/1000 [01:06<01:42,  6.26it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 357/1000 [01:06<01:39,  6.44it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 358/1000 [01:06<01:37,  6.60it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 359/1000 [01:06<01:35,  6.73it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 360/1000 [01:06<01:49,  5.84it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 361/1000 [01:06<01:58,  5.39it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 362/1000 [01:07<02:12,  4.81it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 363/1000 [01:07<02:22,  4.47it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 364/1000 [01:07<02:05,  5.05it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 365/1000 [01:07<01:55,  5.50it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 366/1000 [01:07<01:47,  5.89it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 367/1000 [01:07<01:41,  6.22it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 368/1000 [01:08<01:38,  6.41it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 369/1000 [01:08<01:36,  6.55it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 370/1000 [01:08<01:34,  6.65it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 371/1000 [01:08<01:26,  7.28it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 372/1000 [01:08<01:27,  7.19it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 373/1000 [01:08<01:31,  6.87it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 374/1000 [01:08<01:30,  6.88it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 375/1000 [01:09<01:30,  6.92it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 376/1000 [01:09<01:50,  5.63it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 377/1000 [01:09<01:43,  6.01it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 378/1000 [01:09<01:51,  5.56it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 379/1000 [01:09<01:57,  5.29it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 380/1000 [01:10<01:48,  5.73it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 381/1000 [01:10<01:54,  5.39it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 382/1000 [01:10<01:46,  5.81it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 383/1000 [01:10<01:43,  5.94it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 384/1000 [01:10<01:39,  6.20it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 385/1000 [01:10<01:35,  6.42it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 386/1000 [01:11<01:35,  6.41it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 387/1000 [01:11<01:32,  6.60it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 388/1000 [01:11<01:30,  6.73it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 389/1000 [01:11<01:29,  6.85it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 390/1000 [01:11<01:39,  6.12it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 391/1000 [01:11<01:47,  5.66it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 392/1000 [01:12<01:53,  5.38it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 393/1000 [01:12<01:48,  5.61it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 394/1000 [01:12<01:53,  5.33it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 395/1000 [01:12<02:00,  5.03it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 396/1000 [01:12<01:43,  5.84it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 397/1000 [01:13<01:59,  5.06it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 398/1000 [01:13<02:01,  4.97it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 399/1000 [01:13<01:50,  5.46it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 400/1000 [01:13<01:42,  5.84it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 401/1000 [01:13<01:37,  6.17it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 402/1000 [01:13<01:47,  5.59it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 403/1000 [01:14<01:40,  5.94it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 404/1000 [01:14<01:47,  5.54it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 405/1000 [01:14<01:34,  6.31it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 406/1000 [01:14<01:31,  6.49it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 407/1000 [01:14<01:29,  6.63it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 408/1000 [01:14<01:27,  6.73it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 409/1000 [01:15<01:48,  5.47it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 410/1000 [01:15<01:40,  5.86it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 411/1000 [01:15<01:35,  6.19it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 412/1000 [01:15<01:52,  5.23it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 413/1000 [01:15<01:37,  6.03it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 414/1000 [01:15<01:33,  6.27it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 415/1000 [01:16<01:42,  5.69it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 416/1000 [01:16<01:36,  6.03it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 417/1000 [01:16<01:32,  6.31it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 418/1000 [01:16<01:23,  7.00it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 419/1000 [01:16<01:26,  6.74it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 420/1000 [01:16<01:36,  5.98it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 421/1000 [01:16<01:32,  6.29it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 422/1000 [01:17<01:28,  6.51it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 423/1000 [01:17<01:29,  6.47it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 424/1000 [01:17<01:39,  5.77it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 425/1000 [01:17<01:33,  6.16it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 426/1000 [01:17<01:50,  5.19it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 427/1000 [01:17<01:41,  5.67it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 428/1000 [01:18<01:46,  5.37it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 429/1000 [01:18<01:32,  6.15it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 430/1000 [01:18<01:42,  5.54it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 431/1000 [01:18<01:49,  5.21it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 432/1000 [01:18<01:52,  5.06it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 433/1000 [01:19<02:01,  4.66it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 434/1000 [01:19<02:02,  4.64it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 435/1000 [01:19<01:43,  5.46it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 436/1000 [01:19<01:36,  5.83it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 437/1000 [01:19<01:45,  5.36it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 438/1000 [01:20<01:37,  5.75it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 439/1000 [01:20<01:26,  6.51it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 440/1000 [01:20<01:35,  5.88it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 441/1000 [01:20<01:30,  6.15it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 442/1000 [01:20<01:21,  6.85it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 443/1000 [01:20<01:21,  6.87it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 444/1000 [01:20<01:20,  6.88it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 445/1000 [01:21<01:20,  6.89it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 446/1000 [01:21<01:19,  6.97it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 447/1000 [01:21<01:16,  7.21it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 448/1000 [01:21<01:16,  7.18it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 449/1000 [01:21<01:29,  6.13it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 450/1000 [01:21<01:26,  6.39it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 451/1000 [01:21<01:23,  6.60it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 452/1000 [01:22<01:33,  5.84it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 453/1000 [01:22<01:28,  6.15it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 454/1000 [01:22<01:44,  5.22it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 455/1000 [01:22<01:55,  4.74it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 456/1000 [01:23<01:54,  4.76it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 457/1000 [01:23<01:56,  4.68it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 458/1000 [01:23<01:44,  5.20it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 459/1000 [01:23<01:30,  6.00it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 460/1000 [01:23<01:37,  5.54it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 461/1000 [01:23<01:31,  5.89it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 462/1000 [01:24<01:26,  6.23it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 463/1000 [01:24<01:31,  5.85it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 464/1000 [01:24<01:21,  6.59it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 465/1000 [01:24<01:31,  5.82it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 466/1000 [01:24<01:27,  6.12it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 467/1000 [01:24<01:23,  6.35it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 468/1000 [01:24<01:21,  6.54it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 469/1000 [01:25<01:19,  6.69it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 470/1000 [01:25<01:18,  6.78it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 471/1000 [01:25<01:16,  6.88it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 472/1000 [01:25<01:16,  6.91it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 473/1000 [01:25<01:33,  5.62it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 474/1000 [01:25<01:30,  5.79it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 475/1000 [01:26<01:44,  5.05it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 476/1000 [01:26<01:53,  4.63it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 477/1000 [01:26<01:53,  4.62it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 478/1000 [01:26<01:41,  5.13it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 479/1000 [01:27<01:43,  5.03it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 480/1000 [01:27<01:51,  4.65it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 481/1000 [01:27<01:52,  4.61it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 482/1000 [01:27<01:40,  5.15it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 483/1000 [01:27<01:32,  5.59it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 484/1000 [01:28<01:44,  4.96it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 485/1000 [01:28<01:52,  4.59it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 486/1000 [01:28<01:53,  4.54it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 487/1000 [01:28<01:58,  4.33it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 488/1000 [01:28<01:45,  4.87it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 489/1000 [01:29<01:35,  5.36it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 490/1000 [01:29<01:28,  5.77it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 491/1000 [01:29<01:23,  6.10it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 492/1000 [01:29<01:18,  6.48it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 493/1000 [01:29<01:34,  5.37it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 494/1000 [01:29<01:39,  5.09it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 495/1000 [01:30<01:25,  5.89it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 496/1000 [01:30<01:32,  5.42it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 497/1000 [01:30<01:38,  5.13it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 498/1000 [01:30<01:41,  4.94it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 499/1000 [01:30<01:32,  5.44it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 500/1000 [01:31<01:25,  5.82it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 501/1000 [01:31<01:21,  6.10it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 502/1000 [01:31<01:35,  5.20it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 503/1000 [01:31<01:27,  5.66it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 504/1000 [01:31<01:34,  5.25it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 505/1000 [01:31<01:27,  5.67it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 506/1000 [01:32<01:22,  6.02it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 507/1000 [01:32<01:19,  6.18it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 508/1000 [01:32<01:19,  6.21it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 509/1000 [01:32<01:16,  6.43it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 510/1000 [01:32<01:12,  6.79it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 511/1000 [01:32<01:21,  6.03it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 512/1000 [01:33<01:28,  5.49it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 513/1000 [01:33<01:39,  4.88it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 514/1000 [01:33<01:40,  4.85it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 515/1000 [01:33<01:25,  5.66it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 516/1000 [01:33<01:31,  5.31it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 517/1000 [01:34<01:24,  5.72it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 518/1000 [01:34<01:20,  6.02it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 519/1000 [01:34<01:11,  6.73it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 520/1000 [01:34<01:22,  5.82it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 521/1000 [01:34<01:17,  6.17it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 522/1000 [01:34<01:26,  5.54it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 523/1000 [01:35<01:23,  5.73it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 524/1000 [01:35<01:18,  6.06it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 525/1000 [01:35<01:32,  5.16it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 526/1000 [01:35<01:27,  5.44it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 527/1000 [01:35<01:37,  4.83it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 528/1000 [01:35<01:28,  5.31it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 529/1000 [01:36<01:22,  5.72it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 530/1000 [01:36<01:27,  5.37it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 531/1000 [01:36<01:31,  5.14it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 532/1000 [01:36<01:18,  5.94it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 533/1000 [01:36<01:15,  6.19it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 534/1000 [01:36<01:13,  6.38it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 535/1000 [01:37<01:21,  5.69it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 536/1000 [01:37<01:37,  4.76it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 537/1000 [01:37<01:27,  5.28it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 538/1000 [01:37<01:31,  5.02it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 539/1000 [01:38<01:39,  4.62it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 540/1000 [01:38<01:40,  4.57it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 541/1000 [01:38<01:32,  4.98it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 542/1000 [01:38<01:24,  5.44it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 543/1000 [01:38<01:28,  5.19it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 544/1000 [01:39<01:31,  4.97it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 545/1000 [01:39<01:23,  5.46it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 546/1000 [01:39<01:17,  5.84it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 547/1000 [01:39<01:14,  6.11it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 548/1000 [01:39<01:26,  5.23it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 549/1000 [01:39<01:21,  5.54it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 550/1000 [01:40<01:15,  5.94it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 551/1000 [01:40<01:11,  6.26it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 552/1000 [01:40<01:09,  6.49it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 553/1000 [01:40<01:22,  5.42it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 554/1000 [01:40<01:16,  5.79it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 555/1000 [01:40<01:23,  5.31it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 556/1000 [01:41<01:17,  5.70it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 557/1000 [01:41<01:29,  4.96it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 558/1000 [01:41<01:21,  5.44it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 559/1000 [01:41<01:15,  5.83it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 560/1000 [01:41<01:11,  6.12it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 561/1000 [01:41<01:11,  6.16it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 562/1000 [01:42<01:10,  6.18it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 563/1000 [01:42<01:08,  6.41it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 564/1000 [01:42<01:16,  5.67it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 565/1000 [01:42<01:12,  5.98it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 566/1000 [01:42<01:09,  6.23it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 567/1000 [01:42<01:07,  6.41it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 568/1000 [01:43<01:07,  6.36it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 569/1000 [01:43<01:14,  5.78it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 570/1000 [01:43<01:20,  5.35it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 571/1000 [01:43<01:14,  5.76it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 572/1000 [01:43<01:10,  6.08it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 573/1000 [01:43<01:07,  6.33it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 574/1000 [01:44<01:13,  5.77it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 575/1000 [01:44<01:09,  6.11it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 576/1000 [01:44<01:24,  4.99it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 577/1000 [01:44<01:17,  5.49it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 578/1000 [01:44<01:12,  5.86it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 579/1000 [01:44<01:08,  6.15it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 580/1000 [01:45<01:05,  6.37it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 581/1000 [01:45<00:59,  7.01it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 582/1000 [01:45<01:07,  6.16it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 583/1000 [01:45<01:15,  5.54it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 584/1000 [01:45<01:20,  5.20it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 585/1000 [01:46<01:15,  5.49it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 586/1000 [01:46<01:18,  5.25it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 587/1000 [01:46<01:12,  5.69it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 588/1000 [01:46<01:08,  6.03it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 589/1000 [01:46<01:05,  6.29it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 590/1000 [01:46<01:03,  6.49it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 591/1000 [01:47<01:09,  5.86it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 592/1000 [01:47<01:20,  5.09it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 593/1000 [01:47<01:27,  4.67it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 594/1000 [01:47<01:18,  5.18it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 595/1000 [01:47<01:21,  4.98it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 596/1000 [01:48<01:21,  4.93it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 597/1000 [01:48<01:09,  5.80it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 598/1000 [01:48<01:05,  6.10it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 599/1000 [01:48<01:13,  5.49it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 600/1000 [01:48<01:08,  5.87it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 601/1000 [01:48<01:12,  5.50it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 602/1000 [01:49<01:07,  5.90it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 603/1000 [01:49<01:03,  6.25it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 604/1000 [01:49<00:57,  6.91it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 605/1000 [01:49<00:58,  6.70it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 606/1000 [01:49<00:54,  7.30it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 607/1000 [01:49<01:02,  6.31it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 608/1000 [01:50<01:08,  5.76it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 609/1000 [01:50<01:13,  5.34it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 610/1000 [01:50<01:07,  5.76it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 611/1000 [01:50<01:05,  5.90it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 612/1000 [01:50<01:12,  5.36it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 613/1000 [01:50<01:06,  5.81it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 614/1000 [01:51<01:04,  5.95it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 615/1000 [01:51<01:11,  5.41it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 616/1000 [01:51<01:05,  5.85it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 617/1000 [01:51<01:02,  6.16it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 618/1000 [01:51<00:55,  6.85it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 619/1000 [01:51<01:04,  5.95it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 620/1000 [01:51<00:57,  6.66it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 621/1000 [01:52<00:56,  6.76it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 622/1000 [01:52<01:03,  5.91it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 623/1000 [01:52<01:00,  6.20it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 624/1000 [01:52<00:58,  6.42it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 625/1000 [01:52<01:04,  5.81it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 626/1000 [01:52<01:01,  6.12it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 627/1000 [01:53<00:58,  6.35it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 628/1000 [01:53<00:53,  7.00it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 629/1000 [01:53<00:53,  6.99it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 630/1000 [01:53<00:52,  6.99it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 631/1000 [01:53<01:05,  5.60it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 632/1000 [01:54<01:09,  5.30it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 633/1000 [01:54<01:04,  5.73it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 634/1000 [01:54<01:00,  6.06it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 635/1000 [01:54<00:57,  6.34it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 636/1000 [01:54<00:57,  6.30it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 637/1000 [01:54<00:55,  6.53it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 638/1000 [01:54<00:56,  6.44it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 639/1000 [01:55<01:01,  5.82it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 640/1000 [01:55<00:58,  6.15it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 641/1000 [01:55<01:08,  5.21it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 642/1000 [01:55<01:10,  5.05it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 643/1000 [01:55<01:06,  5.37it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 644/1000 [01:56<01:01,  5.76it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 645/1000 [01:56<01:05,  5.41it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 646/1000 [01:56<01:09,  5.11it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 647/1000 [01:56<01:03,  5.58it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 648/1000 [01:56<01:01,  5.75it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 649/1000 [01:56<01:06,  5.28it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 650/1000 [01:57<00:57,  6.06it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 652/1000 [01:57<00:55,  6.31it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 653/1000 [01:57<00:55,  6.29it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 654/1000 [01:57<00:55,  6.28it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 655/1000 [01:57<00:53,  6.49it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 656/1000 [01:58<00:53,  6.41it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 657/1000 [01:58<00:52,  6.55it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 658/1000 [01:58<00:59,  5.79it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 659/1000 [01:58<00:56,  6.07it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 660/1000 [01:58<00:50,  6.76it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 661/1000 [01:58<00:49,  6.82it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 662/1000 [01:59<00:57,  5.89it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 663/1000 [01:59<00:54,  6.22it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 664/1000 [01:59<01:00,  5.59it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 665/1000 [01:59<00:56,  5.93it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 666/1000 [01:59<00:53,  6.21it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 667/1000 [01:59<01:03,  5.25it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 668/1000 [02:00<00:58,  5.65it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 669/1000 [02:00<00:55,  6.01it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 670/1000 [02:00<00:59,  5.55it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 671/1000 [02:00<01:02,  5.28it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 672/1000 [02:00<01:03,  5.14it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 673/1000 [02:01<01:06,  4.92it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 674/1000 [02:01<01:00,  5.40it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 675/1000 [02:01<01:03,  5.11it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 676/1000 [02:01<01:05,  4.98it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 677/1000 [02:01<01:11,  4.55it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 678/1000 [02:02<01:03,  5.09it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 679/1000 [02:02<00:58,  5.51it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 680/1000 [02:02<00:54,  5.89it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 681/1000 [02:02<00:51,  6.16it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 682/1000 [02:02<00:49,  6.40it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 683/1000 [02:02<00:59,  5.33it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 684/1000 [02:02<00:51,  6.11it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 685/1000 [02:03<00:48,  6.52it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 686/1000 [02:03<00:53,  5.83it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 687/1000 [02:03<00:50,  6.18it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 688/1000 [02:03<00:55,  5.67it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 689/1000 [02:03<00:59,  5.27it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 690/1000 [02:04<01:00,  5.09it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 691/1000 [02:04<00:55,  5.55it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 692/1000 [02:04<00:53,  5.78it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 693/1000 [02:04<00:56,  5.44it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 694/1000 [02:04<00:58,  5.25it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 695/1000 [02:04<00:53,  5.71it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 696/1000 [02:05<00:50,  6.07it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 697/1000 [02:05<00:47,  6.36it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 698/1000 [02:05<00:53,  5.62it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 699/1000 [02:05<00:50,  5.96it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 700/1000 [02:05<00:49,  6.05it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 701/1000 [02:05<00:48,  6.11it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 702/1000 [02:06<00:48,  6.16it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 703/1000 [02:06<00:46,  6.44it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 704/1000 [02:06<00:44,  6.59it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 705/1000 [02:06<00:40,  7.24it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 706/1000 [02:06<00:51,  5.71it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 707/1000 [02:06<00:54,  5.37it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 708/1000 [02:07<00:56,  5.17it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 709/1000 [02:07<00:51,  5.60it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 710/1000 [02:07<00:48,  5.95it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 711/1000 [02:07<00:53,  5.44it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 712/1000 [02:07<00:56,  5.13it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 713/1000 [02:08<00:51,  5.60it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 714/1000 [02:08<00:57,  4.97it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 715/1000 [02:08<00:52,  5.47it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 716/1000 [02:08<00:48,  5.82it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 717/1000 [02:08<00:51,  5.46it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 718/1000 [02:08<00:48,  5.83it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 719/1000 [02:09<00:51,  5.44it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 720/1000 [02:09<00:47,  5.95it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 721/1000 [02:09<00:46,  6.03it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 722/1000 [02:09<00:51,  5.43it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 723/1000 [02:09<00:44,  6.21it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 724/1000 [02:09<00:43,  6.41it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 725/1000 [02:10<00:41,  6.55it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 726/1000 [02:10<00:38,  7.18it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 727/1000 [02:10<00:44,  6.16it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 728/1000 [02:10<00:51,  5.26it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 729/1000 [02:10<00:59,  4.58it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 730/1000 [02:11<01:02,  4.34it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 731/1000 [02:11<00:54,  4.93it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 732/1000 [02:11<00:49,  5.40it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 733/1000 [02:11<00:46,  5.78it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 734/1000 [02:11<00:45,  5.91it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 735/1000 [02:11<00:49,  5.39it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 736/1000 [02:12<00:51,  5.10it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 737/1000 [02:12<00:47,  5.57it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 738/1000 [02:12<00:50,  5.22it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 739/1000 [02:12<00:52,  5.01it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 740/1000 [02:12<00:52,  4.94it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 741/1000 [02:13<00:48,  5.30it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 742/1000 [02:13<00:50,  5.15it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 743/1000 [02:13<00:54,  4.71it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 744/1000 [02:13<00:48,  5.24it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 745/1000 [02:13<00:42,  6.04it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 746/1000 [02:14<00:40,  6.26it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 747/1000 [02:14<00:39,  6.44it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 748/1000 [02:14<00:37,  6.64it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 749/1000 [02:14<00:37,  6.72it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 750/1000 [02:14<00:45,  5.48it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 751/1000 [02:14<00:48,  5.18it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 752/1000 [02:15<00:44,  5.61it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 753/1000 [02:15<00:41,  5.99it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 754/1000 [02:15<00:40,  6.10it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 755/1000 [02:15<00:46,  5.22it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 756/1000 [02:15<00:44,  5.45it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 757/1000 [02:15<00:41,  5.81it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 758/1000 [02:16<00:45,  5.34it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 759/1000 [02:16<00:47,  5.05it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 760/1000 [02:16<00:48,  4.90it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 761/1000 [02:16<00:44,  5.39it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 762/1000 [02:16<00:41,  5.80it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 763/1000 [02:17<00:38,  6.12it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 764/1000 [02:17<00:38,  6.17it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 765/1000 [02:17<00:45,  5.16it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 766/1000 [02:17<00:46,  5.05it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 767/1000 [02:17<00:43,  5.37it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 768/1000 [02:18<00:45,  5.15it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 769/1000 [02:18<00:38,  5.94it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 770/1000 [02:18<00:38,  6.03it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 771/1000 [02:18<00:36,  6.26it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 772/1000 [02:18<00:40,  5.62it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 773/1000 [02:18<00:42,  5.28it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 774/1000 [02:19<00:39,  5.76it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 775/1000 [02:19<00:37,  6.07it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 776/1000 [02:19<00:35,  6.37it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 777/1000 [02:19<00:41,  5.35it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 778/1000 [02:19<00:38,  5.77it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 779/1000 [02:19<00:41,  5.34it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 780/1000 [02:20<00:38,  5.76it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 781/1000 [02:20<00:37,  5.89it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 782/1000 [02:20<00:35,  6.21it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 783/1000 [02:20<00:34,  6.23it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 784/1000 [02:20<00:33,  6.41it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 785/1000 [02:20<00:37,  5.73it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 786/1000 [02:20<00:34,  6.24it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 787/1000 [02:21<00:37,  5.73it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 788/1000 [02:21<00:42,  5.04it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 789/1000 [02:21<00:38,  5.53it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 790/1000 [02:21<00:35,  5.91it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 791/1000 [02:21<00:33,  6.22it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 792/1000 [02:22<00:32,  6.44it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 793/1000 [02:22<00:38,  5.39it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 794/1000 [02:22<00:40,  5.15it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 795/1000 [02:22<00:37,  5.45it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 796/1000 [02:22<00:34,  5.84it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 797/1000 [02:22<00:32,  6.30it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 798/1000 [02:23<00:29,  6.96it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 799/1000 [02:23<00:35,  5.63it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 800/1000 [02:23<00:40,  4.96it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 801/1000 [02:23<00:37,  5.29it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 802/1000 [02:23<00:41,  4.76it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 803/1000 [02:24<00:41,  4.72it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 804/1000 [02:24<00:38,  5.10it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 805/1000 [02:24<00:39,  4.90it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 806/1000 [02:24<00:35,  5.40it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 807/1000 [02:24<00:33,  5.78it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 808/1000 [02:25<00:35,  5.44it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 809/1000 [02:25<00:33,  5.67it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 810/1000 [02:25<00:40,  4.72it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 811/1000 [02:25<00:40,  4.72it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 812/1000 [02:25<00:40,  4.66it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 813/1000 [02:26<00:36,  5.19it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 814/1000 [02:26<00:39,  4.75it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 815/1000 [02:26<00:36,  5.13it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 816/1000 [02:26<00:37,  4.91it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 817/1000 [02:26<00:37,  4.86it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 818/1000 [02:27<00:33,  5.37it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 819/1000 [02:27<00:34,  5.18it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 820/1000 [02:27<00:31,  5.64it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 821/1000 [02:27<00:29,  5.99it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 822/1000 [02:27<00:28,  6.24it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 823/1000 [02:27<00:31,  5.59it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 824/1000 [02:28<00:29,  5.98it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 825/1000 [02:28<00:34,  5.13it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 826/1000 [02:28<00:31,  5.56it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 827/1000 [02:28<00:29,  5.95it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 828/1000 [02:28<00:31,  5.44it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 829/1000 [02:29<00:35,  4.85it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 830/1000 [02:29<00:29,  5.67it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 831/1000 [02:29<00:31,  5.36it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 832/1000 [02:29<00:29,  5.79it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 833/1000 [02:29<00:35,  4.72it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 834/1000 [02:30<00:31,  5.23it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 835/1000 [02:30<00:29,  5.66it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 836/1000 [02:30<00:28,  5.82it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 837/1000 [02:30<00:29,  5.43it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 838/1000 [02:30<00:31,  5.20it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 839/1000 [02:30<00:34,  4.70it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 840/1000 [02:31<00:34,  4.67it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 841/1000 [02:31<00:33,  4.69it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 842/1000 [02:31<00:31,  5.07it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 843/1000 [02:31<00:28,  5.50it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 844/1000 [02:31<00:29,  5.26it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 845/1000 [02:32<00:32,  4.74it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 846/1000 [02:32<00:29,  5.27it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 847/1000 [02:32<00:26,  5.83it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 848/1000 [02:32<00:25,  5.93it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 849/1000 [02:32<00:24,  6.21it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 850/1000 [02:33<00:28,  5.30it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 851/1000 [02:33<00:26,  5.71it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 852/1000 [02:33<00:29,  5.00it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 853/1000 [02:33<00:26,  5.50it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 854/1000 [02:33<00:28,  5.16it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 855/1000 [02:33<00:24,  5.97it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 856/1000 [02:34<00:26,  5.52it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 857/1000 [02:34<00:27,  5.26it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 858/1000 [02:34<00:23,  6.05it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 859/1000 [02:34<00:22,  6.29it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 860/1000 [02:34<00:24,  5.64it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 861/1000 [02:34<00:25,  5.36it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 862/1000 [02:35<00:30,  4.57it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 863/1000 [02:35<00:25,  5.40it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 864/1000 [02:35<00:23,  5.80it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 866/1000 [02:35<00:19,  6.96it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 867/1000 [02:35<00:21,  6.12it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 868/1000 [02:36<00:24,  5.30it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 869/1000 [02:36<00:23,  5.69it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 870/1000 [02:36<00:24,  5.39it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 871/1000 [02:36<00:24,  5.17it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 872/1000 [02:37<00:25,  4.93it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 873/1000 [02:37<00:22,  5.71it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 874/1000 [02:37<00:25,  5.01it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 875/1000 [02:37<00:22,  5.49it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 876/1000 [02:37<00:21,  5.86it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 877/1000 [02:37<00:19,  6.18it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 878/1000 [02:38<00:24,  4.98it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 879/1000 [02:38<00:22,  5.46it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 880/1000 [02:38<00:20,  5.83it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 881/1000 [02:38<00:19,  6.15it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 882/1000 [02:38<00:18,  6.37it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 883/1000 [02:38<00:17,  6.56it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 884/1000 [02:38<00:16,  7.18it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 885/1000 [02:39<00:16,  6.77it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 886/1000 [02:39<00:17,  6.59it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 887/1000 [02:39<00:16,  6.74it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 888/1000 [02:39<00:16,  6.78it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 889/1000 [02:39<00:16,  6.62it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 890/1000 [02:39<00:20,  5.50it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 891/1000 [02:40<00:17,  6.27it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 892/1000 [02:40<00:19,  5.63it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 893/1000 [02:40<00:22,  4.74it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 894/1000 [02:40<00:23,  4.44it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 895/1000 [02:41<00:23,  4.46it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 896/1000 [02:41<00:20,  5.11it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 897/1000 [02:41<00:20,  4.99it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 898/1000 [02:41<00:18,  5.44it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 899/1000 [02:41<00:17,  5.86it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 900/1000 [02:41<00:16,  6.14it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 901/1000 [02:41<00:16,  6.16it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 902/1000 [02:42<00:17,  5.55it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 903/1000 [02:42<00:16,  5.97it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 904/1000 [02:42<00:14,  6.71it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 905/1000 [02:42<00:13,  7.01it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 906/1000 [02:42<00:16,  5.67it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 907/1000 [02:43<00:17,  5.38it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 908/1000 [02:43<00:17,  5.18it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 909/1000 [02:43<00:16,  5.65it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 910/1000 [02:43<00:15,  6.00it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 911/1000 [02:43<00:16,  5.51it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 912/1000 [02:44<00:17,  4.91it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 913/1000 [02:44<00:16,  5.40it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 914/1000 [02:44<00:14,  5.77it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 915/1000 [02:44<00:16,  5.30it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 916/1000 [02:44<00:14,  5.72it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 917/1000 [02:44<00:15,  5.28it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 918/1000 [02:45<00:14,  5.85it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 919/1000 [02:45<00:13,  6.19it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 920/1000 [02:45<00:12,  6.45it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 921/1000 [02:45<00:13,  5.72it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 922/1000 [02:45<00:15,  5.01it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 923/1000 [02:45<00:14,  5.46it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 924/1000 [02:46<00:15,  4.87it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 925/1000 [02:46<00:13,  5.69it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 926/1000 [02:46<00:12,  5.84it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 927/1000 [02:46<00:11,  6.16it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 928/1000 [02:46<00:12,  5.63it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 929/1000 [02:46<00:12,  5.81it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 930/1000 [02:47<00:11,  6.11it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 931/1000 [02:47<00:10,  6.35it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 932/1000 [02:47<00:10,  6.51it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 933/1000 [02:47<00:10,  6.64it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 934/1000 [02:47<00:09,  6.78it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 935/1000 [02:47<00:09,  6.80it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 936/1000 [02:47<00:09,  6.63it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 937/1000 [02:48<00:08,  7.24it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 938/1000 [02:48<00:10,  6.11it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 939/1000 [02:48<00:09,  6.36it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 940/1000 [02:48<00:09,  6.54it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 941/1000 [02:48<00:08,  6.65it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 942/1000 [02:48<00:10,  5.49it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 943/1000 [02:49<00:09,  5.88it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 944/1000 [02:49<00:08,  6.59it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 945/1000 [02:49<00:08,  6.71it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 946/1000 [02:49<00:07,  6.77it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 947/1000 [02:49<00:09,  5.50it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 948/1000 [02:49<00:08,  5.86it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 949/1000 [02:50<00:09,  5.42it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 950/1000 [02:50<00:09,  5.11it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 951/1000 [02:50<00:08,  5.92it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 952/1000 [02:50<00:07,  6.17it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 953/1000 [02:50<00:07,  6.38it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 954/1000 [02:50<00:07,  6.52it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 955/1000 [02:51<00:07,  6.42it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 956/1000 [02:51<00:06,  6.57it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 957/1000 [02:51<00:06,  6.46it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 958/1000 [02:51<00:06,  6.39it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 959/1000 [02:51<00:06,  6.52it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 960/1000 [02:51<00:06,  6.62it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 961/1000 [02:52<00:07,  5.45it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 962/1000 [02:52<00:06,  5.81it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 963/1000 [02:52<00:07,  5.07it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 964/1000 [02:52<00:06,  5.54it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 965/1000 [02:52<00:07,  4.93it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 966/1000 [02:53<00:06,  5.43it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 967/1000 [02:53<00:06,  4.82it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 968/1000 [02:53<00:06,  5.12it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 969/1000 [02:53<00:05,  5.91it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 970/1000 [02:53<00:04,  6.22it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 971/1000 [02:53<00:04,  6.88it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 972/1000 [02:54<00:04,  6.05it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 973/1000 [02:54<00:04,  5.50it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 974/1000 [02:54<00:05,  5.18it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 975/1000 [02:54<00:04,  5.61it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 976/1000 [02:54<00:04,  4.93it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 977/1000 [02:55<00:04,  4.79it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 978/1000 [02:55<00:04,  5.29it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 979/1000 [02:55<00:03,  5.69it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 980/1000 [02:55<00:03,  6.03it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 981/1000 [02:55<00:03,  6.32it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 982/1000 [02:55<00:02,  6.48it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 983/1000 [02:55<00:02,  6.40it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 984/1000 [02:56<00:02,  5.70it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 985/1000 [02:56<00:02,  5.87it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 986/1000 [02:56<00:02,  5.98it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 987/1000 [02:56<00:02,  6.25it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 988/1000 [02:56<00:01,  6.63it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 989/1000 [02:56<00:01,  6.75it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 990/1000 [02:57<00:01,  7.27it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 991/1000 [02:57<00:01,  5.77it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 992/1000 [02:57<00:01,  6.11it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 993/1000 [02:57<00:01,  6.35it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 994/1000 [02:57<00:00,  6.49it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 995/1000 [02:57<00:00,  6.61it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 996/1000 [02:58<00:00,  5.43it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 997/1000 [02:58<00:00,  4.83it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 998/1000 [02:58<00:00,  4.82it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 999/1000 [02:58<00:00,  5.32it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [02:58<00:00,  6.08it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [02:58<00:00,  5.59it/s]
DONE (2.66s)
DONE (7.57s)
loss_threshold ROC AUC: 0.508436, PR AUC: 0.5046450532654283, tpr_at_low_fpr: {0.001: 0.001, 0.01: 0.006}
min_k_threshold ROC AUC: 0.508149, PR AUC: 0.5011388212840311, tpr_at_low_fpr: {0.001: 0.001, 0.01: 0.007}
zlib_threshold ROC AUC: 0.504366, PR AUC: 0.5039016663774663, tpr_at_low_fpr: {0.001: 0.0, 0.01: 0.006}
ref-stablelm-base-alpha-3b-v2_threshold ROC AUC: 0.494775, PR AUC: 0.4940521949252168, tpr_at_low_fpr: {0.001: 0.0, 0.01: 0.016}
loss_threshold roc_auc: 0.508
min_k_threshold roc_auc: 0.508
zlib_threshold roc_auc: 0.504
ref-stablelm-base-alpha-3b-v2_threshold roc_auc: 0.495
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
results_new/neo125_github_experiment/meta-llama_Llama-2-70b-hf/dm_mathematics_ngram_13_<0.8_truncated
Saving results to absolute path: /home/chawins/mimir/tmp_results/neo125_github_experiment/meta-llama_Llama-2-70b-hf/dm_mathematics_ngram_13_<0.8_truncated
LOG: cache_dir is /home/chawins/.cache
Using cache dir /home/chawins/.cache
Loading BASE model meta-llama/Llama-2-70b-hf...
Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.embed_tokens.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:   7%|â–‹         | 1/15 [00:00<00:04,  3.27it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  13%|â–ˆâ–Ž        | 2/15 [00:00<00:03,  3.39it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 3/15 [00:00<00:03,  3.44it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:01<00:03,  3.46it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:01<00:02,  3.46it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:01<00:02,  3.46it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:02<00:02,  3.47it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:02<00:02,  3.47it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:02<00:01,  3.38it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:02<00:01,  3.30it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:03<00:01,  3.35it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:03<00:00,  3.39it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:03<00:00,  3.40it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:04<00:00,  3.42it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for lm_head.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:04<00:00,  3.48it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:04<00:00,  3.42it/s]
  0%|          | 0/57 [00:00<?, ?w/s]model.embed_tokens.weight:   0%|          | 0/57 [00:00<?, ?w/s, dev=0]model.embed_tokens.weight:   2%|â–         | 1/57 [00:00<00:09,  5.80w/s, dev=0]model.layers.0.input_layernorm.weight:   2%|â–         | 1/57 [00:00<00:09,  5.79w/s, dev=0]model.layers.0.mlp.down_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:04, 11.54w/s, dev=0]  model.layers.0.mlp.down_proj.weight:   5%|â–Œ         | 3/57 [00:00<00:05,  9.12w/s, dev=0]model.layers.0.mlp.gate_proj.weight:   5%|â–Œ         | 3/57 [00:00<00:05,  9.11w/s, dev=0]model.layers.0.mlp.up_proj.weight:   7%|â–‹         | 4/57 [00:00<00:06,  8.18w/s, dev=0]  model.layers.0.mlp.up_proj.weight:   9%|â–‰         | 5/57 [00:00<00:06,  7.75w/s, dev=0]model.layers.0.post_attention_layernorm.weight:   9%|â–‰         | 5/57 [00:00<00:06,  7.75w/s, dev=0]model.layers.0.self_attn.k_proj.weight:  11%|â–ˆ         | 6/57 [00:00<00:05,  9.29w/s, dev=0]        model.layers.0.self_attn.o_proj.weight:  12%|â–ˆâ–        | 7/57 [00:00<00:04, 10.74w/s, dev=0]model.layers.0.self_attn.q_proj.weight:  14%|â–ˆâ–        | 8/57 [00:00<00:04, 11.47w/s, dev=0]model.layers.0.self_attn.q_proj.weight:  16%|â–ˆâ–Œ        | 9/57 [00:00<00:03, 12.05w/s, dev=0]model.layers.0.self_attn.rotary_emb.inv_freq:  16%|â–ˆâ–Œ        | 9/57 [00:00<00:03, 12.04w/s, dev=0]model.layers.0.self_attn.v_proj.weight:  18%|â–ˆâ–Š        | 10/57 [00:00<00:03, 13.37w/s, dev=0]     model.layers.1.input_layernorm.weight:  19%|â–ˆâ–‰        | 11/57 [00:00<00:03, 14.58w/s, dev=0] model.layers.1.mlp.down_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:00<00:02, 15.89w/s, dev=0]  model.layers.1.mlp.down_proj.weight:  23%|â–ˆâ–ˆâ–Ž       | 13/57 [00:00<00:03, 14.23w/s, dev=0]model.layers.1.mlp.gate_proj.weight:  23%|â–ˆâ–ˆâ–Ž       | 13/57 [00:00<00:03, 14.23w/s, dev=0]model.layers.1.mlp.up_proj.weight:  25%|â–ˆâ–ˆâ–       | 14/57 [00:01<00:03, 13.08w/s, dev=0]  model.layers.1.post_attention_layernorm.weight:  26%|â–ˆâ–ˆâ–‹       | 15/57 [00:01<00:03, 12.18w/s, dev=0]model.layers.1.self_attn.k_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:01<00:03, 12.99w/s, dev=0]        model.layers.1.self_attn.k_proj.weight:  30%|â–ˆâ–ˆâ–‰       | 17/57 [00:01<00:02, 13.73w/s, dev=0]model.layers.1.self_attn.o_proj.weight:  30%|â–ˆâ–ˆâ–‰       | 17/57 [00:01<00:02, 13.73w/s, dev=0]model.layers.1.self_attn.q_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 18/57 [00:01<00:02, 14.01w/s, dev=0]model.layers.1.self_attn.rotary_emb.inv_freq:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 19/57 [00:01<00:02, 14.27w/s, dev=0]model.layers.1.self_attn.v_proj.weight:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 20/57 [00:01<00:02, 15.02w/s, dev=0]      model.layers.1.self_attn.v_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 21/57 [00:01<00:02, 15.68w/s, dev=0]model.layers.2.input_layernorm.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 21/57 [00:01<00:02, 15.68w/s, dev=0] model.layers.2.mlp.down_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:01<00:02, 16.42w/s, dev=0]  model.layers.2.mlp.gate_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 23/57 [00:01<00:02, 15.36w/s, dev=0]model.layers.2.mlp.up_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/57 [00:01<00:02, 14.48w/s, dev=0]  model.layers.2.mlp.up_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/57 [00:01<00:02, 13.78w/s, dev=0]model.layers.2.post_attention_layernorm.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/57 [00:01<00:02, 13.77w/s, dev=0]model.layers.2.self_attn.k_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 26/57 [00:01<00:02, 14.32w/s, dev=0]        model.layers.2.self_attn.o_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 27/57 [00:01<00:02, 14.81w/s, dev=0]model.layers.2.self_attn.q_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:01<00:01, 14.98w/s, dev=0]model.layers.2.self_attn.q_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 29/57 [00:01<00:01, 15.14w/s, dev=0]model.layers.2.self_attn.rotary_emb.inv_freq:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 29/57 [00:01<00:01, 15.14w/s, dev=0]model.layers.2.self_attn.v_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 30/57 [00:01<00:01, 15.66w/s, dev=0]      model.layers.3.input_layernorm.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 31/57 [00:01<00:01, 16.11w/s, dev=0] model.layers.3.mlp.down_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 32/57 [00:01<00:01, 16.63w/s, dev=0]  model.layers.3.mlp.down_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 33/57 [00:02<00:01, 15.74w/s, dev=0]model.layers.3.mlp.gate_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 33/57 [00:02<00:01, 15.74w/s, dev=0]model.layers.3.mlp.up_proj.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:02<00:01, 15.02w/s, dev=0]  model.layers.3.post_attention_layernorm.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/57 [00:02<00:01, 14.39w/s, dev=0]model.layers.3.self_attn.k_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 36/57 [00:02<00:01, 14.79w/s, dev=0]        model.layers.3.self_attn.k_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 37/57 [00:02<00:01, 15.15w/s, dev=0]model.layers.3.self_attn.o_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 37/57 [00:02<00:01, 15.14w/s, dev=0]model.layers.3.self_attn.q_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 38/57 [00:02<00:01, 15.23w/s, dev=0]model.layers.3.self_attn.rotary_emb.inv_freq:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 39/57 [00:02<00:01, 15.33w/s, dev=0]model.layers.3.self_attn.v_proj.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:02<00:01, 15.72w/s, dev=0]      model.layers.3.self_attn.v_proj.weight:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/57 [00:02<00:00, 16.07w/s, dev=0]model.layers.4.input_layernorm.weight:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/57 [00:02<00:00, 16.07w/s, dev=0] model.layers.4.mlp.down_proj.weight:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 42/57 [00:02<00:00, 16.46w/s, dev=0]  model.layers.4.mlp.gate_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 43/57 [00:02<00:00, 15.74w/s, dev=0]model.layers.4.mlp.up_proj.weight:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 44/57 [00:02<00:00, 15.08w/s, dev=0]  model.layers.4.mlp.up_proj.weight:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 45/57 [00:03<00:00, 14.55w/s, dev=0]model.layers.4.post_attention_layernorm.weight:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 45/57 [00:03<00:00, 14.55w/s, dev=0]model.layers.4.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:03<00:00, 14.87w/s, dev=0]        model.layers.4.self_attn.o_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 47/57 [00:03<00:00, 15.16w/s, dev=0]model.layers.4.self_attn.q_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/57 [00:03<00:00, 15.23w/s, dev=0]model.layers.4.self_attn.q_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 49/57 [00:03<00:00, 15.30w/s, dev=0]model.layers.4.self_attn.rotary_emb.inv_freq:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 49/57 [00:03<00:00, 15.30w/s, dev=0]model.layers.4.self_attn.v_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 50/57 [00:03<00:00, 15.61w/s, dev=0]      model.layers.5.mlp.gate_proj.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 51/57 [00:03<00:00, 15.88w/s, dev=0]   model.layers.5.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:03<00:00, 15.40w/s, dev=0]model.layers.5.self_attn.k_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 53/57 [00:03<00:00, 15.66w/s, dev=0]model.layers.5.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 53/57 [00:03<00:00, 15.66w/s, dev=0]model.layers.5.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/57 [00:03<00:00, 15.74w/s, dev=0]model.layers.5.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 55/57 [00:03<00:00, 15.82w/s, dev=0]model.layers.5.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 56/57 [00:03<00:00, 16.10w/s, dev=0]      model.layers.5.self_attn.v_proj.weight: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:03<00:00, 16.36w/s, dev=0]                                                                                               0%|          | 0/59 [00:00<?, ?w/s]model.layers.5.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=0]model.layers.5.mlp.down_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 1314.42w/s, dev=0]model.layers.5.mlp.down_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:05, 10.24w/s, dev=0]  model.layers.5.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:05, 10.22w/s, dev=0]  model.layers.5.post_attention_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:06,  8.34w/s, dev=0]model.layers.5.post_attention_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:04, 11.10w/s, dev=0]model.layers.6.input_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:04, 11.09w/s, dev=0]         model.layers.6.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 13.86w/s, dev=0]  model.layers.6.mlp.down_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:05,  9.12w/s, dev=0]model.layers.6.mlp.gate_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:05,  9.11w/s, dev=0]model.layers.6.mlp.up_proj.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:06,  7.71w/s, dev=0]  model.layers.6.mlp.up_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:01<00:07,  7.06w/s, dev=0]model.layers.6.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:01<00:07,  7.06w/s, dev=0]model.layers.6.self_attn.k_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:01<00:06,  7.94w/s, dev=0]        model.layers.6.self_attn.o_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:01<00:05,  8.21w/s, dev=0]model.layers.6.self_attn.o_proj.weight:  19%|â–ˆâ–Š        | 11/59 [00:01<00:05,  8.54w/s, dev=0]model.layers.6.self_attn.q_proj.weight:  19%|â–ˆâ–Š        | 11/59 [00:01<00:05,  8.53w/s, dev=0]model.layers.6.self_attn.rotary_emb.inv_freq:  20%|â–ˆâ–ˆ        | 12/59 [00:01<00:06,  7.66w/s, dev=0]model.layers.6.self_attn.v_proj.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:01<00:05,  8.29w/s, dev=0]      model.layers.6.self_attn.v_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:01<00:05,  8.84w/s, dev=0]model.layers.7.input_layernorm.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:01<00:05,  8.84w/s, dev=0] model.layers.7.mlp.down_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:01<00:04,  9.47w/s, dev=0]  model.layers.7.mlp.gate_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:04,  9.06w/s, dev=0]model.layers.7.mlp.gate_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:02<00:05,  8.17w/s, dev=0]model.layers.7.mlp.up_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:02<00:05,  8.17w/s, dev=0]  model.layers.7.post_attention_layernorm.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:02<00:05,  7.33w/s, dev=0]model.layers.7.self_attn.k_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:02<00:05,  7.74w/s, dev=0]        model.layers.7.self_attn.k_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:02<00:05,  7.63w/s, dev=0]model.layers.7.self_attn.o_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:02<00:05,  7.63w/s, dev=0]model.layers.7.self_attn.q_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:02<00:04,  7.85w/s, dev=0]model.layers.7.self_attn.rotary_emb.inv_freq:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:02<00:04,  8.09w/s, dev=0]model.layers.7.self_attn.rotary_emb.inv_freq:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:02<00:04,  8.45w/s, dev=0]model.layers.7.self_attn.v_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:02<00:04,  8.45w/s, dev=0]      model.layers.8.input_layernorm.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:02<00:03,  8.80w/s, dev=0] model.layers.8.mlp.down_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:02<00:03,  9.16w/s, dev=0]  model.layers.8.mlp.down_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:02<00:03,  9.00w/s, dev=0]model.layers.8.mlp.gate_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:02<00:03,  9.00w/s, dev=0]model.layers.8.mlp.up_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:03<00:03,  8.84w/s, dev=0]  model.layers.8.post_attention_layernorm.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:03<00:03,  8.72w/s, dev=0]model.layers.8.post_attention_layernorm.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:03<00:03,  9.03w/s, dev=0]model.layers.8.self_attn.k_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:03<00:03,  9.03w/s, dev=0]        model.layers.8.self_attn.o_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:03<00:03,  9.03w/s, dev=0]model.layers.8.self_attn.q_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:03<00:03,  9.20w/s, dev=0]model.layers.8.self_attn.q_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:03<00:02,  9.37w/s, dev=0]model.layers.8.self_attn.rotary_emb.inv_freq:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:03<00:02,  9.36w/s, dev=0]model.layers.8.self_attn.v_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:03<00:02,  9.66w/s, dev=0]      model.layers.9.input_layernorm.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:03<00:02,  9.91w/s, dev=0] model.layers.9.mlp.down_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:03<00:02, 10.20w/s, dev=0]  model.layers.9.mlp.down_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:03<00:02, 10.01w/s, dev=0]model.layers.9.mlp.gate_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:03<00:02, 10.01w/s, dev=0]model.layers.9.mlp.up_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:03<00:02,  9.39w/s, dev=0]  model.layers.9.post_attention_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:04<00:02,  9.27w/s, dev=0]model.layers.9.self_attn.k_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:04<00:02,  9.51w/s, dev=0]        model.layers.9.self_attn.k_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:04<00:01,  9.53w/s, dev=0]model.layers.9.self_attn.o_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:04<00:01,  9.53w/s, dev=0]model.layers.9.self_attn.q_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:04<00:01,  9.65w/s, dev=0]model.layers.9.self_attn.rotary_emb.inv_freq:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:04<00:01,  9.77w/s, dev=0]model.layers.9.self_attn.v_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:04<00:01, 10.00w/s, dev=0]      model.layers.9.self_attn.v_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:04<00:01, 10.20w/s, dev=0]model.layers.10.input_layernorm.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:04<00:01, 10.20w/s, dev=0]model.layers.10.mlp.down_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:04<00:01, 10.43w/s, dev=0]  model.layers.10.mlp.gate_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:04<00:01, 10.26w/s, dev=0]model.layers.10.mlp.up_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:04<00:01,  9.77w/s, dev=0]  model.layers.10.mlp.up_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:04<00:01,  9.62w/s, dev=0]model.layers.10.post_attention_layernorm.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:04<00:01,  9.62w/s, dev=0]model.layers.10.self_attn.k_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:04<00:01,  9.82w/s, dev=0]        model.layers.10.self_attn.o_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:05<00:00,  9.70w/s, dev=0]model.layers.10.self_attn.q_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:05<00:00,  9.80w/s, dev=0]model.layers.10.self_attn.q_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:05<00:00,  9.90w/s, dev=0]model.layers.10.self_attn.rotary_emb.inv_freq:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:05<00:00,  9.90w/s, dev=0]model.layers.10.self_attn.v_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:05<00:00, 10.09w/s, dev=0]      model.layers.11.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:05<00:00, 10.26w/s, dev=0]model.layers.11.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:05<00:00, 10.42w/s, dev=0]model.layers.11.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:05<00:00, 10.51w/s, dev=0]model.layers.11.self_attn.q_proj.weight:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:05<00:00, 10.58w/s, dev=0]model.layers.11.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:05<00:00, 10.58w/s, dev=0]model.layers.11.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:05<00:00, 10.77w/s, dev=0]                                                                                                      0%|          | 0/55 [00:00<?, ?w/s]model.layers.11.input_layernorm.weight:   0%|          | 0/55 [00:00<?, ?w/s, dev=0]model.layers.11.mlp.down_proj.weight:   2%|â–         | 1/55 [00:00<00:00, 1212.58w/s, dev=0]model.layers.11.mlp.down_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:10,  5.09w/s, dev=0]  model.layers.11.mlp.gate_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:10,  5.09w/s, dev=0]model.layers.11.mlp.up_proj.weight:   5%|â–Œ         | 3/55 [00:00<00:09,  5.32w/s, dev=0]  model.layers.11.mlp.up_proj.weight:   7%|â–‹         | 4/55 [00:00<00:09,  5.51w/s, dev=0]model.layers.11.post_attention_layernorm.weight:   7%|â–‹         | 4/55 [00:00<00:09,  5.51w/s, dev=0]model.layers.12.input_layernorm.weight:   9%|â–‰         | 5/55 [00:00<00:07,  6.88w/s, dev=0]         model.layers.12.mlp.down_proj.weight:  11%|â–ˆ         | 6/55 [00:00<00:05,  8.26w/s, dev=0]  model.layers.12.mlp.down_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:01<00:07,  6.79w/s, dev=0]model.layers.12.mlp.gate_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:01<00:07,  6.79w/s, dev=0]model.layers.12.mlp.up_proj.weight:  15%|â–ˆâ–        | 8/55 [00:01<00:08,  5.86w/s, dev=0]  model.layers.12.post_attention_layernorm.weight:  16%|â–ˆâ–‹        | 9/55 [00:01<00:07,  5.81w/s, dev=0]model.layers.12.post_attention_layernorm.weight:  18%|â–ˆâ–Š        | 10/55 [00:01<00:06,  6.45w/s, dev=0]model.layers.12.self_attn.k_proj.weight:  18%|â–ˆâ–Š        | 10/55 [00:01<00:06,  6.45w/s, dev=0]        model.layers.12.self_attn.o_proj.weight:  20%|â–ˆâ–ˆ        | 11/55 [00:01<00:06,  7.06w/s, dev=0]model.layers.12.self_attn.q_proj.weight:  22%|â–ˆâ–ˆâ–       | 12/55 [00:01<00:05,  7.45w/s, dev=0]model.layers.12.self_attn.q_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:01<00:05,  7.83w/s, dev=0]model.layers.12.self_attn.rotary_emb.inv_freq:  24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:01<00:05,  7.83w/s, dev=0]model.layers.12.self_attn.v_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 14/55 [00:01<00:04,  8.43w/s, dev=0]      model.layers.13.input_layernorm.weight:  27%|â–ˆâ–ˆâ–‹       | 15/55 [00:01<00:04,  8.34w/s, dev=0] model.layers.13.input_layernorm.weight:  29%|â–ˆâ–ˆâ–‰       | 16/55 [00:01<00:04,  8.89w/s, dev=0]model.layers.13.mlp.down_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 16/55 [00:01<00:04,  8.89w/s, dev=0]  model.layers.13.mlp.gate_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:01<00:04,  8.60w/s, dev=0]model.layers.13.mlp.up_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/55 [00:02<00:04,  8.40w/s, dev=0]  model.layers.13.mlp.up_proj.weight:  35%|â–ˆâ–ˆâ–ˆâ–      | 19/55 [00:02<00:04,  7.73w/s, dev=0]model.layers.13.post_attention_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–      | 19/55 [00:02<00:04,  7.73w/s, dev=0]model.layers.13.self_attn.k_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:02<00:04,  8.13w/s, dev=0]        model.layers.13.self_attn.o_proj.weight:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/55 [00:02<00:03,  8.51w/s, dev=0]model.layers.13.self_attn.q_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/55 [00:02<00:03,  8.73w/s, dev=0]model.layers.13.self_attn.q_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 23/55 [00:02<00:03,  8.95w/s, dev=0]model.layers.13.self_attn.rotary_emb.inv_freq:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 23/55 [00:02<00:03,  8.95w/s, dev=0]model.layers.13.self_attn.v_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 24/55 [00:02<00:03,  9.33w/s, dev=0]      model.layers.14.input_layernorm.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:02<00:03,  9.69w/s, dev=0] model.layers.14.mlp.down_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 26/55 [00:02<00:02, 10.08w/s, dev=0]  model.layers.14.mlp.down_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 27/55 [00:02<00:02,  9.79w/s, dev=0]model.layers.14.mlp.gate_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 27/55 [00:02<00:02,  9.79w/s, dev=0]model.layers.14.mlp.up_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 28/55 [00:03<00:02,  9.25w/s, dev=0]  model.layers.14.post_attention_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/55 [00:03<00:02,  9.07w/s, dev=0]model.layers.14.self_attn.k_proj.weight:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 30/55 [00:03<00:02,  9.38w/s, dev=0]        model.layers.14.self_attn.k_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/55 [00:03<00:02,  9.66w/s, dev=0]model.layers.14.self_attn.o_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/55 [00:03<00:02,  9.66w/s, dev=0]model.layers.14.self_attn.q_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 32/55 [00:03<00:02,  9.09w/s, dev=0]model.layers.14.self_attn.rotary_emb.inv_freq:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/55 [00:03<00:02,  9.24w/s, dev=0]model.layers.14.self_attn.v_proj.weight:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/55 [00:03<00:02,  9.52w/s, dev=0]      model.layers.14.self_attn.v_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:03<00:02,  9.78w/s, dev=0]model.layers.15.input_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:03<00:02,  9.78w/s, dev=0] model.layers.15.mlp.down_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 36/55 [00:03<00:01, 10.06w/s, dev=0]  model.layers.15.mlp.gate_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 37/55 [00:03<00:01,  9.89w/s, dev=0]model.layers.15.mlp.up_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 38/55 [00:03<00:01,  9.71w/s, dev=0]  model.layers.15.mlp.up_proj.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 39/55 [00:04<00:01,  9.19w/s, dev=0]model.layers.15.post_attention_layernorm.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 39/55 [00:04<00:01,  9.19w/s, dev=0]model.layers.15.self_attn.k_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 40/55 [00:04<00:01,  9.42w/s, dev=0]        model.layers.15.self_attn.o_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/55 [00:04<00:01,  9.63w/s, dev=0]model.layers.15.self_attn.q_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 42/55 [00:04<00:01,  9.74w/s, dev=0]model.layers.15.self_attn.q_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 43/55 [00:04<00:01,  9.84w/s, dev=0]model.layers.15.self_attn.rotary_emb.inv_freq:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 43/55 [00:04<00:01,  9.84w/s, dev=0]model.layers.15.self_attn.v_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 44/55 [00:04<00:01, 10.06w/s, dev=0]      model.layers.16.input_layernorm.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:04<00:00, 10.27w/s, dev=0] model.layers.16.mlp.down_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 46/55 [00:04<00:00, 10.50w/s, dev=0]  model.layers.16.mlp.down_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 47/55 [00:04<00:00, 10.31w/s, dev=0]model.layers.16.mlp.gate_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 47/55 [00:04<00:00, 10.30w/s, dev=0]model.layers.16.mlp.up_proj.weight:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 48/55 [00:04<00:00,  9.84w/s, dev=0]  model.layers.16.post_attention_layernorm.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 49/55 [00:05<00:00,  9.71w/s, dev=0]model.layers.16.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 50/55 [00:05<00:00,  9.91w/s, dev=0]        model.layers.16.self_attn.k_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 51/55 [00:05<00:00, 10.09w/s, dev=0]model.layers.16.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 51/55 [00:05<00:00, 10.09w/s, dev=0]model.layers.16.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 52/55 [00:05<00:00, 10.19w/s, dev=0]model.layers.16.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 53/55 [00:05<00:00, 10.29w/s, dev=0]model.layers.16.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 54/55 [00:05<00:00, 10.48w/s, dev=0]      model.layers.16.self_attn.v_proj.weight: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55/55 [00:05<00:00, 10.66w/s, dev=0]                                                                                                0%|          | 0/57 [00:00<?, ?w/s]model.layers.17.input_layernorm.weight:   0%|          | 0/57 [00:00<?, ?w/s, dev=0]model.layers.17.mlp.down_proj.weight:   2%|â–         | 1/57 [00:00<00:00, 1233.26w/s, dev=0]model.layers.17.mlp.down_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:04, 12.38w/s, dev=0]  model.layers.17.mlp.gate_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:04, 12.35w/s, dev=0]model.layers.17.mlp.up_proj.weight:   5%|â–Œ         | 3/57 [00:00<00:04, 11.56w/s, dev=0]  model.layers.17.mlp.up_proj.weight:   7%|â–‹         | 4/57 [00:00<00:04, 11.21w/s, dev=0]model.layers.17.post_attention_layernorm.weight:   7%|â–‹         | 4/57 [00:00<00:04, 11.20w/s, dev=0]model.layers.17.self_attn.k_proj.weight:   9%|â–‰         | 5/57 [00:00<00:03, 13.99w/s, dev=0]        model.layers.17.self_attn.o_proj.weight:  11%|â–ˆ         | 6/57 [00:00<00:03, 16.45w/s, dev=0]model.layers.17.self_attn.q_proj.weight:  12%|â–ˆâ–        | 7/57 [00:00<00:02, 17.33w/s, dev=0]model.layers.17.self_attn.rotary_emb.inv_freq:  14%|â–ˆâ–        | 8/57 [00:00<00:02, 17.98w/s, dev=0]model.layers.17.self_attn.v_proj.weight:  16%|â–ˆâ–Œ        | 9/57 [00:00<00:02, 20.20w/s, dev=0]      model.layers.18.input_layernorm.weight:  18%|â–ˆâ–Š        | 10/57 [00:00<00:02, 22.12w/s, dev=0]model.layers.18.mlp.down_proj.weight:  19%|â–ˆâ–‰        | 11/57 [00:00<00:01, 24.32w/s, dev=0]  model.layers.18.mlp.down_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:00<00:02, 21.73w/s, dev=0]model.layers.18.mlp.gate_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:00<00:02, 21.72w/s, dev=0]model.layers.18.mlp.up_proj.weight:  23%|â–ˆâ–ˆâ–Ž       | 13/57 [00:00<00:02, 19.82w/s, dev=0]  model.layers.18.post_attention_layernorm.weight:  25%|â–ˆâ–ˆâ–       | 14/57 [00:00<00:02, 18.30w/s, dev=0]model.layers.18.self_attn.k_proj.weight:  26%|â–ˆâ–ˆâ–‹       | 15/57 [00:00<00:02, 19.60w/s, dev=0]        model.layers.18.self_attn.o_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:00<00:01, 20.79w/s, dev=0]model.layers.18.self_attn.q_proj.weight:  30%|â–ˆâ–ˆâ–‰       | 17/57 [00:00<00:01, 21.34w/s, dev=0]model.layers.18.self_attn.rotary_emb.inv_freq:  32%|â–ˆâ–ˆâ–ˆâ–      | 18/57 [00:00<00:01, 21.58w/s, dev=0]model.layers.18.self_attn.v_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 19/57 [00:00<00:01, 22.77w/s, dev=0]      model.layers.18.self_attn.v_proj.weight:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 20/57 [00:00<00:01, 23.76w/s, dev=0]model.layers.19.input_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 20/57 [00:00<00:01, 23.76w/s, dev=0] model.layers.19.mlp.down_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 21/57 [00:00<00:01, 24.94w/s, dev=0]  model.layers.19.mlp.gate_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:00<00:01, 22.63w/s, dev=0]model.layers.19.mlp.up_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 23/57 [00:01<00:01, 21.07w/s, dev=0]  model.layers.19.post_attention_layernorm.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/57 [00:01<00:01, 19.89w/s, dev=0]model.layers.19.self_attn.k_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/57 [00:01<00:01, 20.71w/s, dev=0]        model.layers.19.self_attn.o_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 26/57 [00:01<00:01, 21.45w/s, dev=0]model.layers.19.self_attn.q_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 27/57 [00:01<00:01, 21.68w/s, dev=0]model.layers.19.self_attn.q_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:01<00:01, 21.91w/s, dev=0]model.layers.19.self_attn.rotary_emb.inv_freq:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:01<00:01, 21.90w/s, dev=0]model.layers.19.self_attn.v_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 29/57 [00:01<00:01, 22.68w/s, dev=0]      model.layers.20.input_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 30/57 [00:01<00:01, 20.28w/s, dev=1] model.layers.20.mlp.down_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 31/57 [00:01<00:01, 20.94w/s, dev=1]  model.layers.20.mlp.gate_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 32/57 [00:01<00:01, 19.92w/s, dev=1]model.layers.20.mlp.up_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 33/57 [00:01<00:01, 19.10w/s, dev=1]  model.layers.20.post_attention_layernorm.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:01<00:01, 18.34w/s, dev=1]model.layers.20.self_attn.k_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/57 [00:01<00:01, 18.88w/s, dev=1]        model.layers.20.self_attn.k_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 36/57 [00:01<00:01, 19.34w/s, dev=1]model.layers.20.self_attn.o_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 36/57 [00:01<00:01, 19.34w/s, dev=1]model.layers.20.self_attn.q_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 37/57 [00:01<00:01, 19.48w/s, dev=1]model.layers.20.self_attn.rotary_emb.inv_freq:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 38/57 [00:01<00:00, 19.64w/s, dev=1]model.layers.20.self_attn.v_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 39/57 [00:01<00:00, 20.15w/s, dev=1]      model.layers.21.input_layernorm.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:01<00:00, 20.61w/s, dev=1] model.layers.21.mlp.down_proj.weight:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/57 [00:01<00:00, 21.12w/s, dev=1]  model.layers.21.mlp.gate_proj.weight:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 42/57 [00:02<00:00, 20.31w/s, dev=1]model.layers.21.mlp.up_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 43/57 [00:02<00:00, 19.55w/s, dev=1]  model.layers.21.mlp.up_proj.weight:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 44/57 [00:02<00:00, 19.04w/s, dev=1]model.layers.21.post_attention_layernorm.weight:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 44/57 [00:02<00:00, 19.04w/s, dev=1]model.layers.21.self_attn.k_proj.weight:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 45/57 [00:02<00:00, 19.47w/s, dev=1]        model.layers.21.self_attn.o_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:02<00:00, 19.83w/s, dev=1]model.layers.21.self_attn.q_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 47/57 [00:02<00:00, 19.93w/s, dev=1]model.layers.21.self_attn.rotary_emb.inv_freq:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/57 [00:02<00:00, 19.97w/s, dev=1]model.layers.21.self_attn.v_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 49/57 [00:02<00:00, 20.38w/s, dev=1]      model.layers.22.mlp.down_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 50/57 [00:02<00:00, 20.73w/s, dev=1]   model.layers.22.mlp.gate_proj.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 51/57 [00:02<00:00, 20.07w/s, dev=1]model.layers.22.mlp.gate_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:02<00:00, 19.48w/s, dev=1]model.layers.22.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:02<00:00, 19.47w/s, dev=1]model.layers.22.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 53/57 [00:02<00:00, 19.81w/s, dev=1]model.layers.22.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/57 [00:02<00:00, 19.90w/s, dev=1]model.layers.22.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 55/57 [00:02<00:00, 19.99w/s, dev=1]model.layers.22.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 56/57 [00:02<00:00, 20.35w/s, dev=1]                                                                                                      0%|          | 0/59 [00:00<?, ?w/s]model.layers.22.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=1]model.layers.22.mlp.up_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 1242.39w/s, dev=1]model.layers.22.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:03, 16.67w/s, dev=1]  model.layers.22.post_attention_layernorm.weight:   3%|â–Ž         | 2/59 [00:00<00:03, 16.64w/s, dev=1]model.layers.23.input_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:02, 24.88w/s, dev=1]         model.layers.23.mlp.down_proj.weight:   7%|â–‹         | 4/59 [00:00<00:01, 33.11w/s, dev=1]  model.layers.23.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:02, 20.69w/s, dev=1]model.layers.23.mlp.gate_proj.weight:   8%|â–Š         | 5/59 [00:00<00:02, 20.67w/s, dev=1]model.layers.23.mlp.up_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:03, 16.75w/s, dev=1]  model.layers.23.post_attention_layernorm.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:03, 14.68w/s, dev=1]model.layers.23.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:03, 16.76w/s, dev=1]model.layers.23.self_attn.k_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:03, 16.75w/s, dev=1]        model.layers.23.self_attn.o_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:02, 18.65w/s, dev=1]model.layers.23.self_attn.q_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:02, 19.35w/s, dev=1]model.layers.23.self_attn.rotary_emb.inv_freq:  19%|â–ˆâ–Š        | 11/59 [00:00<00:02, 19.95w/s, dev=1]model.layers.23.self_attn.v_proj.weight:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:02, 21.75w/s, dev=1]      model.layers.24.input_layernorm.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:01, 23.37w/s, dev=1] model.layers.24.mlp.down_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:01, 25.15w/s, dev=1]  model.layers.24.mlp.down_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:01, 22.68w/s, dev=1]model.layers.24.mlp.gate_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:01, 22.67w/s, dev=1]model.layers.24.mlp.up_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:00<00:02, 20.32w/s, dev=1]  model.layers.24.post_attention_layernorm.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:00<00:02, 18.98w/s, dev=1]model.layers.24.self_attn.k_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:00<00:02, 20.08w/s, dev=1]        model.layers.24.self_attn.o_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:00<00:01, 21.07w/s, dev=1]model.layers.24.self_attn.q_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:00<00:01, 21.33w/s, dev=1]model.layers.24.self_attn.rotary_emb.inv_freq:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:00<00:01, 21.57w/s, dev=1]model.layers.24.self_attn.rotary_emb.inv_freq:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:00<00:01, 22.58w/s, dev=1]model.layers.24.self_attn.v_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:00<00:01, 22.58w/s, dev=1]      model.layers.25.input_layernorm.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:00<00:01, 23.48w/s, dev=1] model.layers.25.mlp.down_proj.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:00<00:01, 24.49w/s, dev=1]  model.layers.25.mlp.gate_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:01, 22.86w/s, dev=1]model.layers.25.mlp.up_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:01, 21.51w/s, dev=1]  model.layers.25.post_attention_layernorm.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:01<00:01, 20.32w/s, dev=1]model.layers.25.self_attn.k_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:01, 21.07w/s, dev=1]        model.layers.25.self_attn.k_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:01, 21.74w/s, dev=1]model.layers.25.self_attn.o_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:01, 21.74w/s, dev=1]model.layers.25.self_attn.q_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:01<00:01, 21.93w/s, dev=1]model.layers.25.self_attn.rotary_emb.inv_freq:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:01<00:01, 22.07w/s, dev=1]model.layers.25.self_attn.v_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:01<00:01, 22.77w/s, dev=1]      model.layers.26.input_layernorm.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:01<00:01, 23.40w/s, dev=1] model.layers.26.mlp.down_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:01<00:01, 24.10w/s, dev=1]  model.layers.26.mlp.gate_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:01<00:01, 22.79w/s, dev=1]model.layers.26.mlp.gate_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:01<00:01, 21.72w/s, dev=1]model.layers.26.mlp.up_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:01<00:01, 21.72w/s, dev=1]  model.layers.26.post_attention_layernorm.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:01<00:01, 20.78w/s, dev=1]model.layers.26.self_attn.k_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:01<00:00, 21.34w/s, dev=1]        model.layers.26.self_attn.o_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:01<00:00, 21.83w/s, dev=1]model.layers.26.self_attn.q_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:01<00:00, 21.94w/s, dev=1]model.layers.26.self_attn.rotary_emb.inv_freq:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:01<00:00, 22.04w/s, dev=1]model.layers.26.self_attn.v_proj.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:01<00:00, 22.57w/s, dev=1]      model.layers.26.self_attn.v_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:01<00:00, 23.04w/s, dev=1]model.layers.27.input_layernorm.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:01<00:00, 23.04w/s, dev=1] model.layers.27.mlp.down_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:01<00:00, 23.57w/s, dev=1]  model.layers.27.mlp.gate_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:01<00:00, 22.62w/s, dev=1]model.layers.27.mlp.up_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:02<00:00, 21.99w/s, dev=1]  model.layers.27.post_attention_layernorm.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:02<00:00, 21.27w/s, dev=1]model.layers.27.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:02<00:00, 21.72w/s, dev=1]        model.layers.27.self_attn.o_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:02<00:00, 22.10w/s, dev=1]model.layers.27.self_attn.o_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:02<00:00, 22.20w/s, dev=1]model.layers.27.self_attn.q_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:02<00:00, 22.20w/s, dev=1]model.layers.27.self_attn.rotary_emb.inv_freq:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:02<00:00, 22.29w/s, dev=1]model.layers.27.self_attn.v_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:02<00:00, 22.72w/s, dev=1]      model.layers.28.mlp.gate_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:02<00:00, 23.10w/s, dev=1]   model.layers.28.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:02<00:00, 22.30w/s, dev=1]model.layers.28.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:02<00:00, 22.66w/s, dev=1]model.layers.28.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:02<00:00, 22.72w/s, dev=1]model.layers.28.self_attn.q_proj.weight:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:02<00:00, 22.78w/s, dev=1]model.layers.28.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:02<00:00, 22.78w/s, dev=1]model.layers.28.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:02<00:00, 23.18w/s, dev=1]                                                                                                      0%|          | 0/59 [00:00<?, ?w/s]model.layers.28.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=1]model.layers.28.mlp.down_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 1205.61w/s, dev=1]model.layers.28.mlp.down_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:03, 15.60w/s, dev=1]  model.layers.28.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:03, 15.56w/s, dev=1]  model.layers.28.post_attention_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:04, 11.97w/s, dev=1]model.layers.28.post_attention_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:03, 15.93w/s, dev=1]model.layers.29.input_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:03, 15.92w/s, dev=1]         model.layers.29.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:02, 19.88w/s, dev=1]  model.layers.29.mlp.down_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:03, 16.96w/s, dev=1]model.layers.29.mlp.gate_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:03, 16.95w/s, dev=1]model.layers.29.mlp.up_proj.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:03, 15.04w/s, dev=1]  model.layers.29.mlp.up_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:03, 13.82w/s, dev=1]model.layers.29.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:03, 13.81w/s, dev=1]model.layers.29.self_attn.k_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:03, 15.53w/s, dev=1]        model.layers.29.self_attn.o_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:02, 17.05w/s, dev=1]model.layers.29.self_attn.q_proj.weight:  19%|â–ˆâ–Š        | 11/59 [00:00<00:02, 17.82w/s, dev=1]model.layers.29.self_attn.rotary_emb.inv_freq:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:02, 18.47w/s, dev=1]model.layers.29.self_attn.v_proj.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:02, 19.99w/s, dev=1]      model.layers.30.input_layernorm.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:02, 21.38w/s, dev=1] model.layers.30.mlp.down_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:01, 22.90w/s, dev=1]  model.layers.30.mlp.down_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:00<00:02, 21.15w/s, dev=1]model.layers.30.mlp.gate_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:00<00:02, 21.14w/s, dev=1]model.layers.30.mlp.up_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:00<00:02, 19.60w/s, dev=1]  model.layers.30.post_attention_layernorm.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:00<00:02, 18.40w/s, dev=1]model.layers.30.self_attn.k_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:00<00:02, 19.41w/s, dev=1]        model.layers.30.self_attn.o_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:00<00:01, 20.29w/s, dev=1]model.layers.30.self_attn.q_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:01<00:01, 20.61w/s, dev=1]model.layers.30.self_attn.rotary_emb.inv_freq:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:01, 20.88w/s, dev=1]model.layers.30.self_attn.v_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:01, 21.81w/s, dev=1]      model.layers.30.self_attn.v_proj.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:01, 22.63w/s, dev=1]model.layers.31.input_layernorm.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:01, 22.62w/s, dev=1] model.layers.31.mlp.down_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:01, 23.56w/s, dev=1]  model.layers.31.mlp.gate_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:01, 21.97w/s, dev=1]model.layers.31.mlp.up_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:01<00:01, 20.81w/s, dev=1]  model.layers.31.post_attention_layernorm.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:01, 19.69w/s, dev=1]model.layers.31.self_attn.k_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:01, 20.38w/s, dev=1]        model.layers.31.self_attn.o_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:01<00:01, 21.00w/s, dev=1]model.layers.31.self_attn.q_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:01<00:01, 21.19w/s, dev=1]model.layers.31.self_attn.q_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:01<00:01, 21.38w/s, dev=1]model.layers.31.self_attn.rotary_emb.inv_freq:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:01<00:01, 21.38w/s, dev=1]model.layers.31.self_attn.v_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:01<00:01, 22.04w/s, dev=1]      model.layers.32.input_layernorm.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:01<00:01, 22.60w/s, dev=1] model.layers.32.mlp.down_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:01<00:01, 23.26w/s, dev=1]  model.layers.32.mlp.gate_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:01<00:01, 22.06w/s, dev=1]model.layers.32.mlp.up_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:01<00:01, 21.20w/s, dev=1]  model.layers.32.post_attention_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:01<00:01, 20.37w/s, dev=1]model.layers.32.self_attn.k_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:01<00:00, 20.90w/s, dev=1]        model.layers.32.self_attn.k_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:01<00:00, 21.35w/s, dev=1]model.layers.32.self_attn.o_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:01<00:00, 21.35w/s, dev=1]model.layers.32.self_attn.q_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:01<00:00, 21.44w/s, dev=1]model.layers.32.self_attn.rotary_emb.inv_freq:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:01<00:00, 21.58w/s, dev=1]model.layers.32.self_attn.v_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:01<00:00, 22.08w/s, dev=1]      model.layers.33.input_layernorm.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:01<00:00, 22.54w/s, dev=1] model.layers.33.mlp.down_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:01<00:00, 23.05w/s, dev=1]  model.layers.33.mlp.gate_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:02<00:00, 22.19w/s, dev=1]model.layers.33.mlp.up_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:02<00:00, 21.47w/s, dev=1]  model.layers.33.mlp.up_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:02<00:00, 20.81w/s, dev=1]model.layers.33.post_attention_layernorm.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:02<00:00, 20.81w/s, dev=1]model.layers.33.self_attn.k_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:02<00:00, 21.24w/s, dev=1]        model.layers.33.self_attn.o_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:02<00:00, 21.63w/s, dev=1]model.layers.33.self_attn.q_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:02<00:00, 21.75w/s, dev=1]model.layers.33.self_attn.rotary_emb.inv_freq:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:02<00:00, 21.87w/s, dev=1]model.layers.33.self_attn.v_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:02<00:00, 22.29w/s, dev=1]      model.layers.34.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:02<00:00, 22.66w/s, dev=1]model.layers.34.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:02<00:00, 23.03w/s, dev=1]model.layers.34.self_attn.o_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:02<00:00, 23.13w/s, dev=1]model.layers.34.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:02<00:00, 23.13w/s, dev=1]model.layers.34.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:02<00:00, 23.19w/s, dev=1]model.layers.34.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:02<00:00, 23.60w/s, dev=1]                                                                                                      0%|          | 0/55 [00:00<?, ?w/s]model.layers.34.input_layernorm.weight:   0%|          | 0/55 [00:00<?, ?w/s, dev=1]model.layers.34.mlp.down_proj.weight:   2%|â–         | 1/55 [00:00<00:00, 1285.81w/s, dev=1]model.layers.34.mlp.down_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:02, 17.75w/s, dev=1]  model.layers.34.mlp.gate_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:02, 17.71w/s, dev=1]model.layers.34.mlp.up_proj.weight:   5%|â–Œ         | 3/55 [00:00<00:04, 12.77w/s, dev=1]  model.layers.34.mlp.up_proj.weight:   7%|â–‹         | 4/55 [00:00<00:04, 11.45w/s, dev=1]model.layers.34.post_attention_layernorm.weight:   7%|â–‹         | 4/55 [00:00<00:04, 11.44w/s, dev=1]model.layers.35.input_layernorm.weight:   9%|â–‰         | 5/55 [00:00<00:03, 14.29w/s, dev=1]         model.layers.35.mlp.down_proj.weight:  11%|â–ˆ         | 6/55 [00:00<00:02, 17.13w/s, dev=1]  model.layers.35.mlp.down_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:03, 14.81w/s, dev=1]model.layers.35.mlp.gate_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:03, 14.80w/s, dev=1]model.layers.35.mlp.up_proj.weight:  15%|â–ˆâ–        | 8/55 [00:00<00:03, 13.92w/s, dev=1]  model.layers.35.post_attention_layernorm.weight:  16%|â–ˆâ–‹        | 9/55 [00:00<00:03, 12.96w/s, dev=1]model.layers.35.post_attention_layernorm.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:03, 14.39w/s, dev=1]model.layers.35.self_attn.k_proj.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:03, 14.39w/s, dev=1]        model.layers.35.self_attn.o_proj.weight:  20%|â–ˆâ–ˆ        | 11/55 [00:00<00:02, 15.71w/s, dev=1]model.layers.35.self_attn.q_proj.weight:  22%|â–ˆâ–ˆâ–       | 12/55 [00:00<00:02, 16.29w/s, dev=1]model.layers.35.self_attn.rotary_emb.inv_freq:  24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:00<00:02, 16.78w/s, dev=1]model.layers.35.self_attn.v_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 14/55 [00:00<00:02, 18.06w/s, dev=1]      model.layers.36.input_layernorm.weight:  27%|â–ˆâ–ˆâ–‹       | 15/55 [00:00<00:02, 19.22w/s, dev=1] model.layers.36.mlp.down_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 16/55 [00:00<00:01, 20.49w/s, dev=1]  model.layers.36.mlp.down_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:00<00:02, 18.86w/s, dev=1]model.layers.36.mlp.gate_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:00<00:02, 18.85w/s, dev=1]model.layers.36.mlp.up_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/55 [00:01<00:02, 17.89w/s, dev=1]  model.layers.36.post_attention_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–      | 19/55 [00:01<00:02, 17.04w/s, dev=1]model.layers.36.self_attn.k_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:01<00:01, 17.93w/s, dev=1]        model.layers.36.self_attn.o_proj.weight:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/55 [00:01<00:01, 18.72w/s, dev=1]model.layers.36.self_attn.q_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/55 [00:01<00:01, 19.04w/s, dev=1]model.layers.36.self_attn.rotary_emb.inv_freq:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 23/55 [00:01<00:01, 19.36w/s, dev=1]model.layers.36.self_attn.rotary_emb.inv_freq:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 24/55 [00:01<00:01, 20.19w/s, dev=1]model.layers.36.self_attn.v_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 24/55 [00:01<00:01, 20.19w/s, dev=1]      model.layers.37.input_layernorm.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:01<00:01, 20.95w/s, dev=1] model.layers.37.mlp.down_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 26/55 [00:01<00:01, 21.79w/s, dev=1]  model.layers.37.mlp.gate_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 27/55 [00:01<00:01, 20.64w/s, dev=1]model.layers.37.mlp.up_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 28/55 [00:01<00:01, 19.70w/s, dev=1]  model.layers.37.post_attention_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/55 [00:01<00:01, 18.81w/s, dev=1]model.layers.37.self_attn.k_proj.weight:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 30/55 [00:01<00:01, 19.45w/s, dev=1]        model.layers.37.self_attn.k_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/55 [00:01<00:01, 20.04w/s, dev=1]model.layers.37.self_attn.o_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/55 [00:01<00:01, 20.04w/s, dev=1]model.layers.37.self_attn.q_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 32/55 [00:01<00:01, 20.27w/s, dev=1]model.layers.37.self_attn.rotary_emb.inv_freq:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/55 [00:01<00:01, 20.45w/s, dev=1]model.layers.37.self_attn.v_proj.weight:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/55 [00:01<00:00, 21.07w/s, dev=1]      model.layers.38.input_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:01<00:00, 21.61w/s, dev=1] model.layers.38.mlp.down_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 36/55 [00:01<00:00, 22.22w/s, dev=1]  model.layers.38.mlp.gate_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 37/55 [00:01<00:00, 21.41w/s, dev=1]model.layers.38.mlp.gate_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 38/55 [00:01<00:00, 20.54w/s, dev=1]model.layers.38.mlp.up_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 38/55 [00:01<00:00, 20.54w/s, dev=1]  model.layers.38.post_attention_layernorm.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 39/55 [00:01<00:00, 19.88w/s, dev=1]model.layers.38.self_attn.k_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 40/55 [00:01<00:00, 20.38w/s, dev=1]        model.layers.38.self_attn.o_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/55 [00:01<00:00, 20.83w/s, dev=1]model.layers.38.self_attn.q_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 42/55 [00:02<00:00, 21.00w/s, dev=1]model.layers.38.self_attn.rotary_emb.inv_freq:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 43/55 [00:02<00:00, 21.19w/s, dev=1]model.layers.38.self_attn.v_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 44/55 [00:02<00:00, 21.68w/s, dev=1]      model.layers.38.self_attn.v_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:02<00:00, 22.12w/s, dev=1]model.layers.39.input_layernorm.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:02<00:00, 22.12w/s, dev=1] model.layers.39.mlp.down_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 46/55 [00:02<00:00, 22.61w/s, dev=1]  model.layers.39.mlp.gate_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 47/55 [00:02<00:00, 21.86w/s, dev=1]model.layers.39.mlp.up_proj.weight:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 48/55 [00:02<00:00, 21.01w/s, dev=1]  model.layers.39.post_attention_layernorm.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 49/55 [00:02<00:00, 20.49w/s, dev=1]model.layers.39.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 50/55 [00:02<00:00, 20.90w/s, dev=1]        model.layers.39.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 51/55 [00:02<00:00, 21.26w/s, dev=1]model.layers.39.self_attn.o_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 52/55 [00:02<00:00, 21.39w/s, dev=1]model.layers.39.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 52/55 [00:02<00:00, 21.39w/s, dev=1]model.layers.39.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 53/55 [00:02<00:00, 21.49w/s, dev=1]model.layers.39.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 54/55 [00:02<00:00, 21.89w/s, dev=1]                                                                                                      0%|          | 0/57 [00:00<?, ?w/s]model.layers.40.input_layernorm.weight:   0%|          | 0/57 [00:00<?, ?w/s, dev=1]model.layers.40.mlp.down_proj.weight:   2%|â–         | 1/57 [00:00<00:00, 1198.03w/s, dev=1]model.layers.40.mlp.down_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:03, 16.61w/s, dev=1]  model.layers.40.mlp.gate_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:03, 16.57w/s, dev=1]model.layers.40.mlp.up_proj.weight:   5%|â–Œ         | 3/57 [00:00<00:03, 13.58w/s, dev=1]  model.layers.40.mlp.up_proj.weight:   7%|â–‹         | 4/57 [00:00<00:04, 12.64w/s, dev=1]model.layers.40.post_attention_layernorm.weight:   7%|â–‹         | 4/57 [00:00<00:04, 12.63w/s, dev=1]model.layers.40.self_attn.k_proj.weight:   9%|â–‰         | 5/57 [00:00<00:03, 15.76w/s, dev=1]        model.layers.40.self_attn.k_proj.weight:  11%|â–ˆ         | 6/57 [00:00<00:03, 13.29w/s, dev=1]model.layers.40.self_attn.o_proj.weight:  11%|â–ˆ         | 6/57 [00:00<00:03, 13.28w/s, dev=1]model.layers.40.self_attn.q_proj.weight:  12%|â–ˆâ–        | 7/57 [00:00<00:03, 14.39w/s, dev=1]model.layers.40.self_attn.rotary_emb.inv_freq:  14%|â–ˆâ–        | 8/57 [00:00<00:03, 15.37w/s, dev=1]model.layers.40.self_attn.v_proj.weight:  16%|â–ˆâ–Œ        | 9/57 [00:00<00:02, 17.28w/s, dev=1]      model.layers.41.input_layernorm.weight:  18%|â–ˆâ–Š        | 10/57 [00:00<00:02, 16.94w/s, dev=2]model.layers.41.input_layernorm.weight:  19%|â–ˆâ–‰        | 11/57 [00:00<00:02, 18.40w/s, dev=2]model.layers.41.mlp.down_proj.weight:  19%|â–ˆâ–‰        | 11/57 [00:00<00:02, 18.39w/s, dev=2]  model.layers.41.mlp.gate_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:00<00:02, 16.51w/s, dev=2]model.layers.41.mlp.up_proj.weight:  23%|â–ˆâ–ˆâ–Ž       | 13/57 [00:00<00:02, 15.35w/s, dev=2]  model.layers.41.post_attention_layernorm.weight:  25%|â–ˆâ–ˆâ–       | 14/57 [00:01<00:03, 13.21w/s, dev=2]model.layers.41.self_attn.k_proj.weight:  26%|â–ˆâ–ˆâ–‹       | 15/57 [00:01<00:02, 14.14w/s, dev=2]        model.layers.41.self_attn.k_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:01<00:02, 14.94w/s, dev=2]model.layers.41.self_attn.o_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:01<00:02, 14.94w/s, dev=2]model.layers.41.self_attn.q_proj.weight:  30%|â–ˆâ–ˆâ–‰       | 17/57 [00:01<00:02, 15.30w/s, dev=2]model.layers.41.self_attn.rotary_emb.inv_freq:  32%|â–ˆâ–ˆâ–ˆâ–      | 18/57 [00:01<00:02, 15.68w/s, dev=2]model.layers.41.self_attn.v_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 19/57 [00:01<00:02, 16.54w/s, dev=2]      model.layers.42.input_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 20/57 [00:01<00:02, 17.30w/s, dev=2] model.layers.42.mlp.down_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 21/57 [00:01<00:01, 18.16w/s, dev=2]  model.layers.42.mlp.down_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:01<00:02, 17.06w/s, dev=2]model.layers.42.mlp.gate_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:01<00:02, 17.05w/s, dev=2]model.layers.42.mlp.up_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 23/57 [00:01<00:02, 14.99w/s, dev=2]  model.layers.42.post_attention_layernorm.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/57 [00:01<00:02, 14.46w/s, dev=2]model.layers.42.self_attn.k_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/57 [00:01<00:02, 15.06w/s, dev=2]        model.layers.42.self_attn.o_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 26/57 [00:01<00:01, 15.58w/s, dev=2]model.layers.42.self_attn.q_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 27/57 [00:01<00:01, 15.83w/s, dev=2]model.layers.42.self_attn.q_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:01<00:01, 16.08w/s, dev=2]model.layers.42.self_attn.rotary_emb.inv_freq:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:01<00:01, 16.08w/s, dev=2]model.layers.42.self_attn.v_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 29/57 [00:01<00:01, 16.65w/s, dev=2]      model.layers.43.input_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 30/57 [00:01<00:01, 17.14w/s, dev=2] model.layers.43.mlp.down_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 31/57 [00:01<00:01, 17.71w/s, dev=2]  model.layers.43.mlp.gate_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 32/57 [00:01<00:01, 17.07w/s, dev=2]model.layers.43.mlp.up_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 33/57 [00:02<00:01, 15.33w/s, dev=2]  model.layers.43.mlp.up_proj.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:02<00:01, 14.87w/s, dev=2]model.layers.43.post_attention_layernorm.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:02<00:01, 14.87w/s, dev=2]model.layers.43.self_attn.k_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/57 [00:02<00:01, 15.31w/s, dev=2]        model.layers.43.self_attn.o_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 36/57 [00:02<00:01, 15.65w/s, dev=2]model.layers.43.self_attn.q_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 37/57 [00:02<00:01, 15.78w/s, dev=2]model.layers.43.self_attn.rotary_emb.inv_freq:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 38/57 [00:02<00:01, 15.94w/s, dev=2]model.layers.43.self_attn.v_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 39/57 [00:02<00:01, 16.35w/s, dev=2]      model.layers.43.self_attn.v_proj.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:02<00:01, 15.28w/s, dev=2]model.layers.44.input_layernorm.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:02<00:01, 15.28w/s, dev=2] model.layers.44.mlp.down_proj.weight:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/57 [00:02<00:01, 15.66w/s, dev=2]  model.layers.44.mlp.gate_proj.weight:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 42/57 [00:02<00:00, 15.27w/s, dev=2]model.layers.44.mlp.up_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 43/57 [00:02<00:00, 14.95w/s, dev=2]  model.layers.44.post_attention_layernorm.weight:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 44/57 [00:02<00:00, 14.70w/s, dev=2]model.layers.44.self_attn.k_proj.weight:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 45/57 [00:02<00:00, 15.03w/s, dev=2]        model.layers.44.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:03<00:00, 14.37w/s, dev=2]model.layers.44.self_attn.o_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:03<00:00, 14.37w/s, dev=2]model.layers.44.self_attn.q_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 47/57 [00:03<00:00, 14.51w/s, dev=2]model.layers.44.self_attn.rotary_emb.inv_freq:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/57 [00:03<00:00, 14.65w/s, dev=2]model.layers.44.self_attn.v_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 49/57 [00:03<00:00, 14.96w/s, dev=2]      model.layers.45.mlp.down_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 50/57 [00:03<00:00, 15.23w/s, dev=2]   model.layers.45.mlp.gate_proj.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 51/57 [00:03<00:00, 15.06w/s, dev=2]model.layers.45.mlp.gate_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:03<00:00, 14.85w/s, dev=2]model.layers.45.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:03<00:00, 14.85w/s, dev=2]model.layers.45.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 53/57 [00:03<00:00, 15.06w/s, dev=2]model.layers.45.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/57 [00:03<00:00, 15.10w/s, dev=2]model.layers.45.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 55/57 [00:03<00:00, 15.22w/s, dev=2]model.layers.45.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 56/57 [00:03<00:00, 15.50w/s, dev=2]                                                                                                      0%|          | 0/59 [00:00<?, ?w/s]model.layers.45.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=2]model.layers.45.mlp.up_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 1042.06w/s, dev=2]model.layers.45.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:05, 10.01w/s, dev=2]  model.layers.45.post_attention_layernorm.weight:   3%|â–Ž         | 2/59 [00:00<00:05, 10.00w/s, dev=2]model.layers.46.input_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:03, 14.97w/s, dev=2]         model.layers.46.mlp.down_proj.weight:   7%|â–‹         | 4/59 [00:00<00:02, 19.93w/s, dev=2]  model.layers.46.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 15.38w/s, dev=2]model.layers.46.mlp.gate_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 15.36w/s, dev=2]model.layers.46.mlp.up_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:03, 13.51w/s, dev=2]  model.layers.46.post_attention_layernorm.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:04, 12.57w/s, dev=2]model.layers.46.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:03, 14.35w/s, dev=2]model.layers.46.self_attn.k_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:03, 14.35w/s, dev=2]        model.layers.46.self_attn.o_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:03, 15.98w/s, dev=2]model.layers.46.self_attn.q_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:02, 16.73w/s, dev=2]model.layers.46.self_attn.rotary_emb.inv_freq:  19%|â–ˆâ–Š        | 11/59 [00:00<00:02, 17.43w/s, dev=2]model.layers.46.self_attn.v_proj.weight:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:02, 19.00w/s, dev=2]      model.layers.47.input_layernorm.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:02, 20.39w/s, dev=2] model.layers.47.mlp.down_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:02, 21.95w/s, dev=2]  model.layers.47.mlp.down_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:02, 19.87w/s, dev=2]model.layers.47.mlp.gate_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:02, 19.86w/s, dev=2]model.layers.47.mlp.up_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:00<00:02, 18.33w/s, dev=2]  model.layers.47.post_attention_layernorm.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:00<00:02, 17.18w/s, dev=2]model.layers.47.self_attn.k_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:00<00:02, 18.18w/s, dev=2]        model.layers.47.self_attn.o_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:00<00:02, 19.05w/s, dev=2]model.layers.47.self_attn.q_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:02, 19.38w/s, dev=2]model.layers.47.self_attn.rotary_emb.inv_freq:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:01<00:01, 19.66w/s, dev=2]model.layers.47.self_attn.rotary_emb.inv_freq:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:01, 20.59w/s, dev=2]model.layers.47.self_attn.v_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:01, 20.59w/s, dev=2]      model.layers.48.input_layernorm.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:01, 21.43w/s, dev=2] model.layers.48.mlp.down_proj.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:01, 22.35w/s, dev=2]  model.layers.48.mlp.gate_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:01, 21.05w/s, dev=2]model.layers.48.mlp.up_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:01, 20.04w/s, dev=2]  model.layers.48.post_attention_layernorm.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:01<00:01, 19.07w/s, dev=2]model.layers.48.self_attn.k_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:01, 19.77w/s, dev=2]        model.layers.48.self_attn.k_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:01, 20.37w/s, dev=2]model.layers.48.self_attn.o_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:01, 20.37w/s, dev=2]model.layers.48.self_attn.q_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:01<00:01, 20.57w/s, dev=2]model.layers.48.self_attn.rotary_emb.inv_freq:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:01<00:01, 20.77w/s, dev=2]model.layers.48.self_attn.v_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:01<00:01, 21.43w/s, dev=2]      model.layers.49.input_layernorm.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:01<00:01, 22.03w/s, dev=2] model.layers.49.mlp.down_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:01<00:01, 22.69w/s, dev=2]  model.layers.49.mlp.gate_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:01<00:01, 21.65w/s, dev=2]model.layers.49.mlp.gate_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:01<00:01, 20.79w/s, dev=2]model.layers.49.mlp.up_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:01<00:01, 20.79w/s, dev=2]  model.layers.49.post_attention_layernorm.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:01<00:01, 20.12w/s, dev=2]model.layers.49.self_attn.k_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:01<00:01, 20.65w/s, dev=2]        model.layers.49.self_attn.o_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:01<00:00, 21.14w/s, dev=2]model.layers.49.self_attn.q_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:01<00:00, 21.28w/s, dev=2]model.layers.49.self_attn.rotary_emb.inv_freq:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:01<00:00, 21.41w/s, dev=2]model.layers.49.self_attn.v_proj.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:01<00:00, 21.93w/s, dev=2]      model.layers.49.self_attn.v_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:01<00:00, 22.39w/s, dev=2]model.layers.50.input_layernorm.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:01<00:00, 22.39w/s, dev=2] model.layers.50.mlp.down_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:01<00:00, 22.91w/s, dev=2]  model.layers.50.mlp.gate_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:02<00:00, 22.11w/s, dev=2]model.layers.50.mlp.up_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:02<00:00, 21.50w/s, dev=2]  model.layers.50.post_attention_layernorm.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:02<00:00, 20.87w/s, dev=2]model.layers.50.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:02<00:00, 21.31w/s, dev=2]        model.layers.50.self_attn.o_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:02<00:00, 21.68w/s, dev=2]model.layers.50.self_attn.o_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:02<00:00, 21.80w/s, dev=2]model.layers.50.self_attn.q_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:02<00:00, 21.79w/s, dev=2]model.layers.50.self_attn.rotary_emb.inv_freq:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:02<00:00, 21.90w/s, dev=2]model.layers.50.self_attn.v_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:02<00:00, 22.32w/s, dev=2]      model.layers.51.mlp.gate_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:02<00:00, 22.69w/s, dev=2]   model.layers.51.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:02<00:00, 22.18w/s, dev=2]model.layers.51.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:02<00:00, 22.54w/s, dev=2]model.layers.51.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:02<00:00, 22.68w/s, dev=2]model.layers.51.self_attn.q_proj.weight:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:02<00:00, 22.75w/s, dev=2]model.layers.51.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:02<00:00, 22.75w/s, dev=2]model.layers.51.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:02<00:00, 23.15w/s, dev=2]                                                                                                      0%|          | 0/59 [00:00<?, ?w/s]model.layers.51.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=2]model.layers.51.mlp.down_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 1005.59w/s, dev=2]model.layers.51.mlp.down_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:03, 16.37w/s, dev=2]  model.layers.51.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:03, 16.34w/s, dev=2]  model.layers.51.post_attention_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:04, 12.55w/s, dev=2]model.layers.51.post_attention_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:03, 16.70w/s, dev=2]model.layers.52.input_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:03, 16.69w/s, dev=2]         model.layers.52.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:02, 20.85w/s, dev=2]  model.layers.52.mlp.gate_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:02, 17.69w/s, dev=2]model.layers.52.mlp.gate_proj.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:03, 16.00w/s, dev=2]model.layers.52.mlp.up_proj.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:03, 15.99w/s, dev=2]  model.layers.52.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:03, 14.67w/s, dev=2]model.layers.52.self_attn.k_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:03, 16.49w/s, dev=2]        model.layers.52.self_attn.k_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:02, 18.12w/s, dev=2]model.layers.52.self_attn.o_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:02, 18.12w/s, dev=2]model.layers.52.self_attn.q_proj.weight:  19%|â–ˆâ–Š        | 11/59 [00:00<00:02, 18.76w/s, dev=2]model.layers.52.self_attn.rotary_emb.inv_freq:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:02, 19.32w/s, dev=2]model.layers.52.self_attn.v_proj.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:02, 20.92w/s, dev=2]      model.layers.53.input_layernorm.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:02, 22.27w/s, dev=2] model.layers.53.mlp.down_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:01, 23.85w/s, dev=2]  model.layers.53.mlp.down_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:00<00:02, 21.38w/s, dev=2]model.layers.53.mlp.gate_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:00<00:02, 21.37w/s, dev=2]model.layers.53.mlp.up_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:00<00:02, 19.71w/s, dev=2]  model.layers.53.post_attention_layernorm.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:00<00:02, 18.38w/s, dev=2]model.layers.53.self_attn.k_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:00<00:02, 19.39w/s, dev=2]        model.layers.53.self_attn.o_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:00<00:01, 20.31w/s, dev=2]model.layers.53.self_attn.q_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:01<00:01, 20.61w/s, dev=2]model.layers.53.self_attn.q_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:01, 20.88w/s, dev=2]model.layers.53.self_attn.rotary_emb.inv_freq:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:01, 20.87w/s, dev=2]model.layers.53.self_attn.v_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:01, 21.81w/s, dev=2]      model.layers.54.input_layernorm.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:01, 22.61w/s, dev=2] model.layers.54.mlp.down_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:01, 23.55w/s, dev=2]  model.layers.54.mlp.gate_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:01, 22.07w/s, dev=2]model.layers.54.mlp.up_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:01<00:01, 20.94w/s, dev=2]  model.layers.54.mlp.up_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:01, 20.01w/s, dev=2]model.layers.54.post_attention_layernorm.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:01, 20.01w/s, dev=2]model.layers.54.self_attn.k_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:01, 20.72w/s, dev=2]        model.layers.54.self_attn.o_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:01<00:01, 21.34w/s, dev=2]model.layers.54.self_attn.q_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:01<00:01, 21.58w/s, dev=2]model.layers.54.self_attn.rotary_emb.inv_freq:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:01<00:01, 21.78w/s, dev=2]model.layers.54.self_attn.v_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:01<00:01, 22.45w/s, dev=2]      model.layers.55.input_layernorm.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:01<00:01, 23.03w/s, dev=2] model.layers.55.mlp.down_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:01<00:01, 23.71w/s, dev=2]  model.layers.55.mlp.down_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:01<00:01, 22.82w/s, dev=2]model.layers.55.mlp.gate_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:01<00:01, 22.82w/s, dev=2]model.layers.55.mlp.up_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:01<00:01, 21.88w/s, dev=2]  model.layers.55.post_attention_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:01<00:00, 21.02w/s, dev=2]model.layers.55.self_attn.k_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:01<00:00, 21.57w/s, dev=2]        model.layers.55.self_attn.o_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:01<00:00, 22.06w/s, dev=2]model.layers.55.self_attn.q_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:01<00:00, 22.18w/s, dev=2]model.layers.55.self_attn.rotary_emb.inv_freq:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:01<00:00, 22.33w/s, dev=2]model.layers.55.self_attn.v_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:01<00:00, 22.86w/s, dev=2]      model.layers.55.self_attn.v_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:01<00:00, 23.32w/s, dev=2]model.layers.56.input_layernorm.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:01<00:00, 23.32w/s, dev=2] model.layers.56.mlp.down_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:01<00:00, 23.84w/s, dev=2]  model.layers.56.mlp.gate_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:02<00:00, 22.98w/s, dev=2]model.layers.56.mlp.up_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:02<00:00, 22.33w/s, dev=2]  model.layers.56.post_attention_layernorm.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:02<00:00, 21.73w/s, dev=2]model.layers.56.self_attn.k_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:02<00:00, 22.18w/s, dev=2]        model.layers.56.self_attn.o_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:02<00:00, 22.58w/s, dev=2]model.layers.56.self_attn.q_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:02<00:00, 22.69w/s, dev=2]model.layers.56.self_attn.q_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:02<00:00, 22.79w/s, dev=2]model.layers.56.self_attn.rotary_emb.inv_freq:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:02<00:00, 22.79w/s, dev=2]model.layers.56.self_attn.v_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:02<00:00, 23.22w/s, dev=2]      model.layers.57.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:02<00:00, 23.61w/s, dev=2]model.layers.57.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:02<00:00, 24.00w/s, dev=2]model.layers.57.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:02<00:00, 24.09w/s, dev=2]model.layers.57.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:02<00:00, 24.14w/s, dev=2]model.layers.57.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:02<00:00, 24.56w/s, dev=2]                                                                                                      0%|          | 0/55 [00:00<?, ?w/s]model.layers.57.input_layernorm.weight:   0%|          | 0/55 [00:00<?, ?w/s, dev=2]model.layers.57.mlp.down_proj.weight:   2%|â–         | 1/55 [00:00<00:00, 1153.55w/s, dev=2]model.layers.57.mlp.down_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:03, 17.15w/s, dev=2]  model.layers.57.mlp.gate_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:03, 17.11w/s, dev=2]model.layers.57.mlp.up_proj.weight:   5%|â–Œ         | 3/55 [00:00<00:03, 13.15w/s, dev=2]  model.layers.57.mlp.up_proj.weight:   7%|â–‹         | 4/55 [00:00<00:04, 11.61w/s, dev=2]model.layers.57.post_attention_layernorm.weight:   7%|â–‹         | 4/55 [00:00<00:04, 11.60w/s, dev=2]model.layers.58.input_layernorm.weight:   9%|â–‰         | 5/55 [00:00<00:03, 14.48w/s, dev=2]         model.layers.58.mlp.down_proj.weight:  11%|â–ˆ         | 6/55 [00:00<00:02, 17.36w/s, dev=2]  model.layers.58.mlp.down_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:03, 15.16w/s, dev=2]model.layers.58.mlp.gate_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:03, 15.15w/s, dev=2]model.layers.58.mlp.up_proj.weight:  15%|â–ˆâ–        | 8/55 [00:00<00:03, 13.88w/s, dev=2]  model.layers.58.post_attention_layernorm.weight:  16%|â–ˆâ–‹        | 9/55 [00:00<00:03, 13.00w/s, dev=2]model.layers.58.post_attention_layernorm.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:03, 14.43w/s, dev=2]model.layers.58.self_attn.k_proj.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:03, 14.43w/s, dev=2]        model.layers.58.self_attn.o_proj.weight:  20%|â–ˆâ–ˆ        | 11/55 [00:00<00:02, 15.76w/s, dev=2]model.layers.58.self_attn.q_proj.weight:  22%|â–ˆâ–ˆâ–       | 12/55 [00:00<00:02, 16.40w/s, dev=2]model.layers.58.self_attn.rotary_emb.inv_freq:  24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:00<00:02, 16.97w/s, dev=2]model.layers.58.self_attn.v_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 14/55 [00:00<00:02, 18.26w/s, dev=2]      model.layers.59.input_layernorm.weight:  27%|â–ˆâ–ˆâ–‹       | 15/55 [00:00<00:02, 19.32w/s, dev=2] model.layers.59.mlp.down_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 16/55 [00:00<00:01, 20.60w/s, dev=2]  model.layers.59.mlp.down_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:00<00:01, 19.14w/s, dev=2]model.layers.59.mlp.gate_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:00<00:01, 19.13w/s, dev=2]model.layers.59.mlp.up_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/55 [00:00<00:02, 18.25w/s, dev=2]  model.layers.59.post_attention_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–      | 19/55 [00:01<00:02, 17.36w/s, dev=2]model.layers.59.self_attn.k_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:01<00:01, 18.27w/s, dev=2]        model.layers.59.self_attn.o_proj.weight:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/55 [00:01<00:01, 19.09w/s, dev=2]model.layers.59.self_attn.q_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/55 [00:01<00:01, 19.44w/s, dev=2]model.layers.59.self_attn.rotary_emb.inv_freq:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 23/55 [00:01<00:01, 19.77w/s, dev=2]model.layers.59.self_attn.rotary_emb.inv_freq:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 24/55 [00:01<00:01, 20.62w/s, dev=2]model.layers.59.self_attn.v_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 24/55 [00:01<00:01, 20.62w/s, dev=2]      model.layers.60.input_layernorm.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:01<00:01, 21.40w/s, dev=2] model.layers.60.mlp.down_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 26/55 [00:01<00:01, 22.25w/s, dev=2]  model.layers.60.mlp.gate_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 27/55 [00:01<00:01, 21.18w/s, dev=2]model.layers.60.mlp.up_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 28/55 [00:01<00:01, 20.27w/s, dev=2]  model.layers.60.post_attention_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/55 [00:01<00:01, 19.47w/s, dev=2]model.layers.60.self_attn.k_proj.weight:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 30/55 [00:01<00:01, 20.13w/s, dev=2]        model.layers.60.self_attn.k_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/55 [00:01<00:01, 20.74w/s, dev=2]model.layers.60.self_attn.o_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/55 [00:01<00:01, 20.73w/s, dev=2]model.layers.60.self_attn.q_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 32/55 [00:01<00:01, 20.96w/s, dev=2]model.layers.60.self_attn.rotary_emb.inv_freq:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/55 [00:01<00:01, 21.18w/s, dev=2]model.layers.60.self_attn.v_proj.weight:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/55 [00:01<00:00, 21.82w/s, dev=2]      model.layers.61.input_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:01<00:00, 22.39w/s, dev=2] model.layers.61.mlp.down_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 36/55 [00:01<00:00, 23.03w/s, dev=2]  model.layers.61.mlp.gate_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 37/55 [00:01<00:00, 22.09w/s, dev=2]model.layers.61.mlp.gate_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 38/55 [00:01<00:00, 21.20w/s, dev=2]model.layers.61.mlp.up_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 38/55 [00:01<00:00, 21.20w/s, dev=2]  model.layers.61.post_attention_layernorm.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 39/55 [00:01<00:00, 20.43w/s, dev=2]model.layers.61.self_attn.k_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 40/55 [00:01<00:00, 20.95w/s, dev=2]        model.layers.61.self_attn.o_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/55 [00:01<00:00, 21.40w/s, dev=2]model.layers.61.self_attn.q_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 42/55 [00:01<00:00, 21.52w/s, dev=2]model.layers.61.self_attn.rotary_emb.inv_freq:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 43/55 [00:01<00:00, 21.63w/s, dev=2]model.layers.61.self_attn.v_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 44/55 [00:01<00:00, 22.13w/s, dev=2]      model.layers.61.self_attn.v_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:01<00:00, 22.57w/s, dev=2]model.layers.62.input_layernorm.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:02<00:00, 19.53w/s, dev=3] model.layers.62.mlp.down_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 46/55 [00:02<00:00, 19.95w/s, dev=3]  model.layers.62.mlp.gate_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 47/55 [00:02<00:00, 19.43w/s, dev=3]model.layers.62.mlp.up_proj.weight:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 48/55 [00:02<00:00, 18.92w/s, dev=3]  model.layers.62.post_attention_layernorm.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 49/55 [00:02<00:00, 18.47w/s, dev=3]model.layers.62.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 50/55 [00:02<00:00, 18.84w/s, dev=3]        model.layers.62.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 51/55 [00:02<00:00, 19.18w/s, dev=3]model.layers.62.self_attn.o_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 52/55 [00:02<00:00, 19.32w/s, dev=3]model.layers.62.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 52/55 [00:02<00:00, 19.32w/s, dev=3]model.layers.62.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 53/55 [00:02<00:00, 19.44w/s, dev=3]model.layers.62.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 54/55 [00:02<00:00, 19.81w/s, dev=3]                                                                                                      0%|          | 0/57 [00:00<?, ?w/s]model.layers.63.input_layernorm.weight:   0%|          | 0/57 [00:00<?, ?w/s, dev=3]model.layers.63.mlp.down_proj.weight:   2%|â–         | 1/57 [00:00<00:00, 1001.03w/s, dev=3]model.layers.63.mlp.down_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:03, 17.31w/s, dev=3]  model.layers.63.mlp.gate_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:03, 17.27w/s, dev=3]model.layers.63.mlp.up_proj.weight:   5%|â–Œ         | 3/57 [00:00<00:04, 12.81w/s, dev=3]  model.layers.63.mlp.up_proj.weight:   7%|â–‹         | 4/57 [00:00<00:04, 11.36w/s, dev=3]model.layers.63.post_attention_layernorm.weight:   7%|â–‹         | 4/57 [00:00<00:04, 11.35w/s, dev=3]model.layers.63.self_attn.k_proj.weight:   9%|â–‰         | 5/57 [00:00<00:03, 14.17w/s, dev=3]        model.layers.63.self_attn.o_proj.weight:  11%|â–ˆ         | 6/57 [00:00<00:03, 16.68w/s, dev=3]model.layers.63.self_attn.q_proj.weight:  12%|â–ˆâ–        | 7/57 [00:00<00:02, 17.62w/s, dev=3]model.layers.63.self_attn.rotary_emb.inv_freq:  14%|â–ˆâ–        | 8/57 [00:00<00:02, 18.40w/s, dev=3]model.layers.63.self_attn.v_proj.weight:  16%|â–ˆâ–Œ        | 9/57 [00:00<00:02, 20.68w/s, dev=3]      model.layers.64.input_layernorm.weight:  18%|â–ˆâ–Š        | 10/57 [00:00<00:02, 22.63w/s, dev=3]model.layers.64.mlp.down_proj.weight:  19%|â–ˆâ–‰        | 11/57 [00:00<00:01, 24.88w/s, dev=3]  model.layers.64.mlp.down_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:00<00:02, 21.66w/s, dev=3]model.layers.64.mlp.gate_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:00<00:02, 21.65w/s, dev=3]model.layers.64.mlp.up_proj.weight:  23%|â–ˆâ–ˆâ–Ž       | 13/57 [00:00<00:02, 19.34w/s, dev=3]  model.layers.64.post_attention_layernorm.weight:  25%|â–ˆâ–ˆâ–       | 14/57 [00:00<00:02, 17.73w/s, dev=3]model.layers.64.self_attn.k_proj.weight:  26%|â–ˆâ–ˆâ–‹       | 15/57 [00:00<00:02, 18.99w/s, dev=3]        model.layers.64.self_attn.o_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:00<00:02, 20.10w/s, dev=3]model.layers.64.self_attn.q_proj.weight:  30%|â–ˆâ–ˆâ–‰       | 17/57 [00:00<00:01, 20.46w/s, dev=3]model.layers.64.self_attn.rotary_emb.inv_freq:  32%|â–ˆâ–ˆâ–ˆâ–      | 18/57 [00:00<00:01, 20.77w/s, dev=3]model.layers.64.self_attn.v_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 19/57 [00:00<00:01, 21.91w/s, dev=3]      model.layers.64.self_attn.v_proj.weight:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 20/57 [00:00<00:01, 22.89w/s, dev=3]model.layers.65.input_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 20/57 [00:00<00:01, 22.88w/s, dev=3] model.layers.65.mlp.down_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 21/57 [00:00<00:01, 24.02w/s, dev=3]  model.layers.65.mlp.gate_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:00<00:01, 22.10w/s, dev=3]model.layers.65.mlp.up_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 23/57 [00:01<00:01, 20.72w/s, dev=3]  model.layers.65.post_attention_layernorm.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/57 [00:01<00:01, 19.78w/s, dev=3]model.layers.65.self_attn.k_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/57 [00:01<00:01, 20.59w/s, dev=3]        model.layers.65.self_attn.o_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 26/57 [00:01<00:01, 21.33w/s, dev=3]model.layers.65.self_attn.q_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 27/57 [00:01<00:01, 21.62w/s, dev=3]model.layers.65.self_attn.q_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:01<00:01, 21.85w/s, dev=3]model.layers.65.self_attn.rotary_emb.inv_freq:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:01<00:01, 21.84w/s, dev=3]model.layers.65.self_attn.v_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 29/57 [00:01<00:01, 22.61w/s, dev=3]      model.layers.66.input_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 30/57 [00:01<00:01, 23.29w/s, dev=3] model.layers.66.mlp.down_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 31/57 [00:01<00:01, 24.06w/s, dev=3]  model.layers.66.mlp.gate_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 32/57 [00:01<00:01, 22.74w/s, dev=3]model.layers.66.mlp.up_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 33/57 [00:01<00:01, 21.70w/s, dev=3]  model.layers.66.post_attention_layernorm.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:01<00:01, 20.73w/s, dev=3]model.layers.66.self_attn.k_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/57 [00:01<00:01, 21.33w/s, dev=3]        model.layers.66.self_attn.k_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 36/57 [00:01<00:00, 21.87w/s, dev=3]model.layers.66.self_attn.o_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 36/57 [00:01<00:00, 21.87w/s, dev=3]model.layers.66.self_attn.q_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 37/57 [00:01<00:00, 22.04w/s, dev=3]model.layers.66.self_attn.rotary_emb.inv_freq:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 38/57 [00:01<00:00, 22.21w/s, dev=3]model.layers.66.self_attn.v_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 39/57 [00:01<00:00, 22.79w/s, dev=3]      model.layers.67.input_layernorm.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:01<00:00, 23.30w/s, dev=3] model.layers.67.mlp.down_proj.weight:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/57 [00:01<00:00, 23.88w/s, dev=3]  model.layers.67.mlp.gate_proj.weight:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 42/57 [00:01<00:00, 23.02w/s, dev=3]model.layers.67.mlp.up_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 43/57 [00:01<00:00, 22.32w/s, dev=3]  model.layers.67.mlp.up_proj.weight:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 44/57 [00:02<00:00, 21.59w/s, dev=3]model.layers.67.post_attention_layernorm.weight:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 44/57 [00:02<00:00, 21.59w/s, dev=3]model.layers.67.self_attn.k_proj.weight:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 45/57 [00:02<00:00, 22.08w/s, dev=3]        model.layers.67.self_attn.o_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:02<00:00, 22.51w/s, dev=3]model.layers.67.self_attn.q_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 47/57 [00:02<00:00, 22.62w/s, dev=3]model.layers.67.self_attn.rotary_emb.inv_freq:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/57 [00:02<00:00, 22.72w/s, dev=3]model.layers.67.self_attn.v_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 49/57 [00:02<00:00, 23.19w/s, dev=3]      model.layers.68.mlp.down_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 50/57 [00:02<00:00, 23.61w/s, dev=3]   model.layers.68.mlp.gate_proj.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 51/57 [00:02<00:00, 22.85w/s, dev=3]model.layers.68.mlp.gate_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:02<00:00, 22.26w/s, dev=3]model.layers.68.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:02<00:00, 22.25w/s, dev=3]model.layers.68.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 53/57 [00:02<00:00, 22.61w/s, dev=3]model.layers.68.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/57 [00:02<00:00, 22.74w/s, dev=3]model.layers.68.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 55/57 [00:02<00:00, 22.86w/s, dev=3]model.layers.68.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 56/57 [00:02<00:00, 23.27w/s, dev=3]                                                                                                      0%|          | 0/59 [00:00<?, ?w/s]model.layers.68.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=3]model.layers.68.mlp.up_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 880.60w/s, dev=3]model.layers.68.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:02, 19.73w/s, dev=3] model.layers.68.post_attention_layernorm.weight:   3%|â–Ž         | 2/59 [00:00<00:02, 19.67w/s, dev=3]model.layers.69.input_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:01, 29.40w/s, dev=3]         model.layers.69.mlp.down_proj.weight:   7%|â–‹         | 4/59 [00:00<00:01, 39.09w/s, dev=3]  model.layers.69.mlp.gate_proj.weight:   8%|â–Š         | 5/59 [00:00<00:02, 25.14w/s, dev=3]model.layers.69.mlp.gate_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:02, 19.08w/s, dev=3]model.layers.69.mlp.up_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:02, 19.06w/s, dev=3]  model.layers.69.post_attention_layernorm.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:03, 16.43w/s, dev=3]model.layers.69.self_attn.k_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:02, 18.75w/s, dev=3]        model.layers.69.self_attn.o_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:02, 20.76w/s, dev=3]model.layers.69.self_attn.o_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:02, 21.37w/s, dev=3]model.layers.69.self_attn.q_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:02, 21.35w/s, dev=3]model.layers.69.self_attn.rotary_emb.inv_freq:  19%|â–ˆâ–Š        | 11/59 [00:00<00:02, 21.84w/s, dev=3]model.layers.69.self_attn.v_proj.weight:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:01, 23.80w/s, dev=3]      model.layers.70.input_layernorm.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:01, 25.39w/s, dev=3] model.layers.70.mlp.down_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:01, 27.33w/s, dev=3]  model.layers.70.mlp.down_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:01, 23.91w/s, dev=3]model.layers.70.mlp.gate_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:01, 23.90w/s, dev=3]model.layers.70.mlp.up_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:00<00:01, 21.84w/s, dev=3]  model.layers.70.post_attention_layernorm.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:00<00:02, 20.32w/s, dev=3]model.layers.70.self_attn.k_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:00<00:01, 21.50w/s, dev=3]        model.layers.70.self_attn.o_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:00<00:01, 22.57w/s, dev=3]model.layers.70.self_attn.o_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:00<00:01, 22.93w/s, dev=3]model.layers.70.self_attn.q_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:00<00:01, 22.92w/s, dev=3]model.layers.70.self_attn.rotary_emb.inv_freq:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:00<00:01, 23.13w/s, dev=3]model.layers.70.self_attn.v_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:00<00:01, 24.22w/s, dev=3]      model.layers.71.input_layernorm.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:00<00:01, 25.13w/s, dev=3] model.layers.71.mlp.down_proj.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:00<00:01, 26.21w/s, dev=3]  model.layers.71.mlp.down_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:01, 24.49w/s, dev=3]model.layers.71.mlp.gate_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:01, 24.48w/s, dev=3]model.layers.71.mlp.up_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:01, 23.09w/s, dev=3]  model.layers.71.post_attention_layernorm.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:01<00:01, 21.95w/s, dev=3]model.layers.71.self_attn.k_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:01, 22.76w/s, dev=3]        model.layers.71.self_attn.o_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:01, 23.48w/s, dev=3]model.layers.71.self_attn.o_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:01<00:01, 23.71w/s, dev=3]model.layers.71.self_attn.q_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:01<00:01, 23.71w/s, dev=3]model.layers.71.self_attn.rotary_emb.inv_freq:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:01<00:01, 23.91w/s, dev=3]model.layers.71.self_attn.v_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:01<00:01, 24.68w/s, dev=3]      model.layers.72.input_layernorm.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:01<00:01, 25.34w/s, dev=3] model.layers.72.mlp.down_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:01<00:00, 26.10w/s, dev=3]  model.layers.72.mlp.down_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:01<00:00, 24.88w/s, dev=3]model.layers.72.mlp.gate_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:01<00:00, 24.87w/s, dev=3]model.layers.72.mlp.up_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:01<00:00, 23.75w/s, dev=3]  model.layers.72.post_attention_layernorm.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:01<00:00, 22.80w/s, dev=3]model.layers.72.self_attn.k_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:01<00:00, 23.41w/s, dev=3]        model.layers.72.self_attn.o_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:01<00:00, 23.96w/s, dev=3]model.layers.72.self_attn.o_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:01<00:00, 24.13w/s, dev=3]model.layers.72.self_attn.q_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:01<00:00, 24.12w/s, dev=3]model.layers.72.self_attn.rotary_emb.inv_freq:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:01<00:00, 24.28w/s, dev=3]model.layers.72.self_attn.v_proj.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:01<00:00, 24.87w/s, dev=3]      model.layers.73.input_layernorm.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:01<00:00, 25.39w/s, dev=3] model.layers.73.mlp.down_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:01<00:00, 25.98w/s, dev=3]  model.layers.73.mlp.down_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:01<00:00, 25.03w/s, dev=3]model.layers.73.mlp.gate_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:01<00:00, 25.03w/s, dev=3]model.layers.73.mlp.up_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:01<00:00, 24.17w/s, dev=3]  model.layers.73.post_attention_layernorm.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:02<00:00, 23.40w/s, dev=3]model.layers.73.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:02<00:00, 23.89w/s, dev=3]        model.layers.73.self_attn.o_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:02<00:00, 24.33w/s, dev=3]model.layers.73.self_attn.o_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:02<00:00, 24.47w/s, dev=3]model.layers.73.self_attn.q_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:02<00:00, 24.46w/s, dev=3]model.layers.73.self_attn.rotary_emb.inv_freq:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:02<00:00, 24.59w/s, dev=3]model.layers.73.self_attn.v_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:02<00:00, 25.07w/s, dev=3]      model.layers.74.mlp.gate_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:02<00:00, 25.49w/s, dev=3]   model.layers.74.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:02<00:00, 24.74w/s, dev=3]model.layers.74.self_attn.k_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:02<00:00, 25.15w/s, dev=3]model.layers.74.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:02<00:00, 25.14w/s, dev=3]model.layers.74.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:02<00:00, 25.26w/s, dev=3]model.layers.74.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:02<00:00, 25.36w/s, dev=3]model.layers.74.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:02<00:00, 25.80w/s, dev=3]                                                                                                      0%|          | 0/55 [00:00<?, ?w/s]model.layers.74.input_layernorm.weight:   0%|          | 0/55 [00:00<?, ?w/s, dev=3]model.layers.74.mlp.down_proj.weight:   2%|â–         | 1/55 [00:00<00:00, 947.87w/s, dev=3]model.layers.74.mlp.down_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:03, 16.83w/s, dev=3] model.layers.74.mlp.up_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:03, 16.79w/s, dev=3]  model.layers.74.post_attention_layernorm.weight:   5%|â–Œ         | 3/55 [00:00<00:03, 13.06w/s, dev=3]model.layers.74.post_attention_layernorm.weight:   7%|â–‹         | 4/55 [00:00<00:02, 17.37w/s, dev=3]model.layers.75.input_layernorm.weight:   7%|â–‹         | 4/55 [00:00<00:02, 17.36w/s, dev=3]         model.layers.75.mlp.down_proj.weight:   9%|â–‰         | 5/55 [00:00<00:02, 21.68w/s, dev=3]  model.layers.75.mlp.down_proj.weight:  11%|â–ˆ         | 6/55 [00:00<00:02, 17.64w/s, dev=3]model.layers.75.mlp.gate_proj.weight:  11%|â–ˆ         | 6/55 [00:00<00:02, 17.62w/s, dev=3]model.layers.75.mlp.up_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:03, 15.49w/s, dev=3]  model.layers.75.mlp.up_proj.weight:  15%|â–ˆâ–        | 8/55 [00:00<00:03, 14.36w/s, dev=3]model.layers.75.post_attention_layernorm.weight:  15%|â–ˆâ–        | 8/55 [00:00<00:03, 14.36w/s, dev=3]model.layers.75.self_attn.k_proj.weight:  16%|â–ˆâ–‹        | 9/55 [00:00<00:02, 16.14w/s, dev=3]        model.layers.75.self_attn.o_proj.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:02, 17.78w/s, dev=3]model.layers.75.self_attn.q_proj.weight:  20%|â–ˆâ–ˆ        | 11/55 [00:00<00:02, 18.53w/s, dev=3]model.layers.75.self_attn.rotary_emb.inv_freq:  22%|â–ˆâ–ˆâ–       | 12/55 [00:00<00:02, 19.20w/s, dev=3]model.layers.75.self_attn.v_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:00<00:02, 20.78w/s, dev=3]      model.layers.76.input_layernorm.weight:  25%|â–ˆâ–ˆâ–Œ       | 14/55 [00:00<00:01, 22.22w/s, dev=3] model.layers.76.mlp.down_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 15/55 [00:00<00:01, 23.79w/s, dev=3]  model.layers.76.mlp.down_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 16/55 [00:00<00:01, 21.69w/s, dev=3]model.layers.76.mlp.gate_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 16/55 [00:00<00:01, 21.68w/s, dev=3]model.layers.76.mlp.up_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:00<00:01, 20.07w/s, dev=3]  model.layers.76.post_attention_layernorm.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/55 [00:00<00:01, 18.85w/s, dev=3]model.layers.76.self_attn.k_proj.weight:  35%|â–ˆâ–ˆâ–ˆâ–      | 19/55 [00:00<00:01, 19.89w/s, dev=3]        model.layers.76.self_attn.o_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:00<00:01, 20.82w/s, dev=3]model.layers.76.self_attn.q_proj.weight:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/55 [00:00<00:01, 21.17w/s, dev=3]model.layers.76.self_attn.rotary_emb.inv_freq:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/55 [00:01<00:01, 21.49w/s, dev=3]model.layers.76.self_attn.v_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 23/55 [00:01<00:01, 22.46w/s, dev=3]      model.layers.76.self_attn.v_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 24/55 [00:01<00:01, 23.33w/s, dev=3]model.layers.77.input_layernorm.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 24/55 [00:01<00:01, 23.32w/s, dev=3] model.layers.77.mlp.down_proj.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:01<00:01, 24.29w/s, dev=3]  model.layers.77.mlp.gate_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 26/55 [00:01<00:01, 22.92w/s, dev=3]model.layers.77.mlp.up_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 27/55 [00:01<00:01, 21.64w/s, dev=3]  model.layers.77.post_attention_layernorm.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 28/55 [00:01<00:01, 20.55w/s, dev=3]model.layers.77.self_attn.k_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/55 [00:01<00:01, 21.27w/s, dev=3]        model.layers.77.self_attn.o_proj.weight:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 30/55 [00:01<00:01, 21.89w/s, dev=3]model.layers.77.self_attn.q_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/55 [00:01<00:01, 22.09w/s, dev=3]model.layers.77.self_attn.q_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 32/55 [00:01<00:01, 22.29w/s, dev=3]model.layers.77.self_attn.rotary_emb.inv_freq:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 32/55 [00:01<00:01, 22.29w/s, dev=3]model.layers.77.self_attn.v_proj.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/55 [00:01<00:00, 22.98w/s, dev=3]      model.layers.78.input_layernorm.weight:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/55 [00:01<00:00, 23.60w/s, dev=3] model.layers.78.mlp.down_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:01<00:00, 24.29w/s, dev=3]  model.layers.78.mlp.gate_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 36/55 [00:01<00:00, 23.16w/s, dev=3]model.layers.78.mlp.up_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 37/55 [00:01<00:00, 22.36w/s, dev=3]  model.layers.78.post_attention_layernorm.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 38/55 [00:01<00:00, 21.61w/s, dev=3]model.layers.78.self_attn.k_proj.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 39/55 [00:01<00:00, 22.17w/s, dev=3]        model.layers.78.self_attn.k_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 40/55 [00:01<00:00, 22.65w/s, dev=3]model.layers.78.self_attn.o_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 40/55 [00:01<00:00, 22.64w/s, dev=3]model.layers.78.self_attn.q_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/55 [00:01<00:00, 22.75w/s, dev=3]model.layers.78.self_attn.rotary_emb.inv_freq:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 42/55 [00:01<00:00, 22.88w/s, dev=3]model.layers.78.self_attn.v_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 43/55 [00:01<00:00, 23.42w/s, dev=3]      model.layers.79.input_layernorm.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 44/55 [00:01<00:00, 23.89w/s, dev=3] model.layers.79.mlp.down_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:01<00:00, 24.43w/s, dev=3]  model.layers.79.mlp.gate_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 46/55 [00:01<00:00, 23.45w/s, dev=3]model.layers.79.mlp.up_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 47/55 [00:02<00:00, 22.71w/s, dev=3]  model.layers.79.mlp.up_proj.weight:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 48/55 [00:02<00:00, 22.08w/s, dev=3]model.layers.79.post_attention_layernorm.weight:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 48/55 [00:02<00:00, 22.07w/s, dev=3]model.layers.79.self_attn.k_proj.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 49/55 [00:02<00:00, 22.53w/s, dev=3]        model.layers.79.self_attn.o_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 50/55 [00:02<00:00, 22.94w/s, dev=3]model.layers.79.self_attn.q_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 51/55 [00:02<00:00, 23.08w/s, dev=3]model.layers.79.self_attn.rotary_emb.inv_freq:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 52/55 [00:02<00:00, 23.21w/s, dev=3]model.layers.79.self_attn.v_proj.weight:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 53/55 [00:02<00:00, 23.65w/s, dev=3]      model.norm.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 54/55 [00:02<00:00, 24.04w/s, dev=3]                                                                                                0%|          | 0/1 [00:00<?, ?w/s]lm_head.weight:   0%|          | 0/1 [00:00<?, ?w/s, dev=3]lm_head.weight: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  8.72w/s, dev=3]                                                                   self.device: cuda:0
Loading BASE model stabilityai/stablelm-base-alpha-3b-v2...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:01<00:01,  1.56s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.11it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.00it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[2] MOVING BASE MODEL TO GPU.../home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for iamgroot42/mimir contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/iamgroot42/mimir
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
Loading dataset the_pile...
Loading from HuggingFace!
Downloading data:   0%|          | 0.00/929k [00:00<?, ?B/s]Downloading data:  32%|â–ˆâ–ˆâ–ˆâ–      | 296k/929k [00:00<00:00, 2.92MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 929k/929k [00:00<00:00, 6.67MB/s]
Downloading data:   0%|          | 0.00/104k [00:00<?, ?B/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104k/104k [00:00<00:00, 1.72MB/s]
Downloading data:   0%|          | 0.00/2.30M [00:00<?, ?B/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.30M/2.30M [00:00<00:00, 28.1MB/s]
Downloading data:   0%|          | 0.00/2.87M [00:00<?, ?B/s]Downloading data:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1.55M/2.87M [00:00<00:00, 15.5MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.87M/2.87M [00:00<00:00, 24.7MB/s]
Downloading data:   0%|          | 0.00/929k [00:00<?, ?B/s]Downloading data:  32%|â–ˆâ–ˆâ–ˆâ–      | 296k/929k [00:00<00:00, 2.92MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 929k/929k [00:00<00:00, 6.67MB/s]
Downloading data:   0%|          | 0.00/729k [00:00<?, ?B/s]Downloading data:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 296k/729k [00:00<00:00, 2.95MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 729k/729k [00:00<00:00, 5.52MB/s]
Downloading data:   0%|          | 0.00/19.5M [00:00<?, ?B/s]Downloading data:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 7.71M/19.5M [00:00<00:00, 77.1MB/s]Downloading data:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 17.5M/19.5M [00:00<00:00, 89.3MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19.5M/19.5M [00:00<00:00, 88.5MB/s]
Downloading data:   0%|          | 0.00/19.9M [00:00<?, ?B/s]Downloading data:   6%|â–Œ         | 1.20M/19.9M [00:00<00:01, 12.0MB/s]Downloading data:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 9.66M/19.9M [00:00<00:00, 54.7MB/s]Downloading data:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18.0M/19.9M [00:00<00:00, 67.8MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19.9M/19.9M [00:00<00:00, 61.6MB/s]
Downloading data:   0%|          | 0.00/929k [00:00<?, ?B/s]Downloading data:  24%|â–ˆâ–ˆâ–       | 222k/929k [00:00<00:00, 2.21MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 929k/929k [00:00<00:00, 5.87MB/s]
Downloading data:   0%|          | 0.00/921k [00:00<?, ?B/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 921k/921k [00:00<00:00, 18.6MB/s]
Downloading data:   0%|          | 0.00/25.1M [00:00<?, ?B/s]Downloading data:   6%|â–Œ         | 1.47M/25.1M [00:00<00:01, 14.7MB/s]Downloading data:  21%|â–ˆâ–ˆâ–       | 5.38M/25.1M [00:00<00:00, 29.0MB/s]Downloading data:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 9.28M/25.1M [00:00<00:00, 33.5MB/s]Downloading data:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 13.9M/25.1M [00:00<00:00, 38.7MB/s]Downloading data:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 19.6M/25.1M [00:00<00:00, 45.2MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25.1M/25.1M [00:00<00:00, 43.3MB/s]
Downloading data:   0%|          | 0.00/24.9M [00:00<?, ?B/s]Downloading data:   5%|â–         | 1.23M/24.9M [00:00<00:01, 12.3MB/s]Downloading data:  21%|â–ˆâ–ˆ        | 5.27M/24.9M [00:00<00:00, 27.4MB/s]Downloading data:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9.78M/24.9M [00:00<00:00, 35.3MB/s]Downloading data:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 15.1M/24.9M [00:00<00:00, 42.4MB/s]Downloading data:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20.1M/24.9M [00:00<00:00, 44.9MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24.9M/24.9M [00:00<00:00, 44.3MB/s]
Generating ngram_7_0.2 split: 0 examples [00:00, ? examples/s]Generating ngram_7_0.2 split: 89 examples [00:00, 789.43 examples/s]
Generating ngram_13_0.2 split: 0 examples [00:00, ? examples/s]Generating ngram_13_0.2 split: 113 examples [00:00, 1115.54 examples/s]Generating ngram_13_0.2 split: 229 examples [00:00, 1138.26 examples/s]Generating ngram_13_0.2 split: 343 examples [00:00, 1131.24 examples/s]Generating ngram_13_0.2 split: 460 examples [00:00, 1142.09 examples/s]Generating ngram_13_0.2 split: 577 examples [00:00, 1148.22 examples/s]Generating ngram_13_0.2 split: 748 examples [00:00, 1139.88 examples/s]Generating ngram_13_0.2 split: 777 examples [00:00, 913.90 examples/s] 
Generating ngram_13_0.8 split: 0 examples [00:00, ? examples/s]Generating ngram_13_0.8 split: 112 examples [00:00, 1106.35 examples/s]Generating ngram_13_0.8 split: 235 examples [00:00, 1173.99 examples/s]Generating ngram_13_0.8 split: 355 examples [00:00, 1180.47 examples/s]Generating ngram_13_0.8 split: 530 examples [00:00, 1167.56 examples/s]Generating ngram_13_0.8 split: 700 examples [00:00, 1151.64 examples/s]Generating ngram_13_0.8 split: 869 examples [00:00, 1137.87 examples/s]Generating ngram_13_0.8 split: 1000 examples [00:01, 734.78 examples/s]Generating ngram_13_0.8 split: 1000 examples [00:01, 915.89 examples/s]
Loading dataset the_pile...
Loading from HuggingFace!
Generating samples:   0%|          | 0/21 [00:00<?, ?it/s]Generating samples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:00<00:00, 1167.97it/s]
NEW N_SAMPLES IS  1000
Writing raw data to tmp_results/neo125_github_experiment/meta-llama_Llama-2-70b-hf/dm_mathematics_ngram_13_<0.8_truncated/raw_data.json
Writing raw data to tmp_results/neo125_github_experiment/meta-llama_Llama-2-70b-hf/dm_mathematics_ngram_13_<0.8_truncated/raw_data_lens.json
Computing criterion:   0%|          | 0/20 [00:00<?, ?it/s]Computing criterion:   5%|â–Œ         | 1/20 [00:35<11:09, 35.25s/it]Computing criterion:  10%|â–ˆ         | 2/20 [00:59<08:41, 28.95s/it]Computing criterion:  15%|â–ˆâ–Œ        | 3/20 [01:24<07:40, 27.10s/it]Computing criterion:  20%|â–ˆâ–ˆ        | 4/20 [01:48<06:52, 25.75s/it]Computing criterion:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [02:14<06:27, 25.83s/it]Computing criterion:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [02:40<06:01, 25.80s/it]Computing criterion:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [03:06<05:37, 25.96s/it]Computing criterion:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [03:31<05:09, 25.81s/it]Computing criterion:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [03:57<04:43, 25.77s/it]Computing criterion:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [04:23<04:18, 25.86s/it]Computing criterion:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [04:48<03:50, 25.60s/it]Computing criterion:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [05:14<03:25, 25.75s/it]Computing criterion:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [05:39<02:57, 25.38s/it]Computing criterion:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [06:05<02:33, 25.52s/it]Computing criterion:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [06:31<02:08, 25.68s/it]Computing criterion:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [06:55<01:41, 25.33s/it]Computing criterion:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [07:22<01:17, 25.78s/it]Computing criterion:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [07:48<00:51, 25.89s/it]Computing criterion:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [08:14<00:25, 25.99s/it]Computing criterion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [08:40<00:00, 25.76s/it]Computing criterion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [08:40<00:00, 26.00s/it]
Ref scores:   0%|          | 0/1000 [00:00<?, ?it/s]Ref scores:   0%|          | 1/1000 [00:11<3:04:19, 11.07s/it]Ref scores:   0%|          | 2/1000 [00:11<1:17:57,  4.69s/it]Ref scores:   0%|          | 3/1000 [00:11<44:34,  2.68s/it]  Ref scores:   0%|          | 4/1000 [00:11<28:11,  1.70s/it]Ref scores:   0%|          | 5/1000 [00:11<19:20,  1.17s/it]Ref scores:   1%|          | 6/1000 [00:12<13:59,  1.18it/s]Ref scores:   1%|          | 7/1000 [00:12<10:20,  1.60it/s]Ref scores:   1%|          | 8/1000 [00:12<08:23,  1.97it/s]Ref scores:   1%|          | 9/1000 [00:12<06:50,  2.42it/s]Ref scores:   1%|          | 10/1000 [00:13<05:46,  2.86it/s]Ref scores:   1%|          | 11/1000 [00:13<05:06,  3.23it/s]Ref scores:   1%|          | 12/1000 [00:13<04:37,  3.56it/s]Ref scores:   1%|â–         | 13/1000 [00:13<04:41,  3.50it/s]Ref scores:   1%|â–         | 14/1000 [00:14<04:43,  3.48it/s]Ref scores:   2%|â–         | 15/1000 [00:14<04:23,  3.73it/s]Ref scores:   2%|â–         | 16/1000 [00:14<04:30,  3.64it/s]Ref scores:   2%|â–         | 17/1000 [00:14<04:09,  3.93it/s]Ref scores:   2%|â–         | 18/1000 [00:15<04:20,  3.78it/s]Ref scores:   2%|â–         | 19/1000 [00:15<04:15,  3.84it/s]Ref scores:   2%|â–         | 20/1000 [00:15<04:23,  3.72it/s]Ref scores:   2%|â–         | 21/1000 [00:15<04:07,  3.95it/s]Ref scores:   2%|â–         | 22/1000 [00:16<04:20,  3.75it/s]Ref scores:   2%|â–         | 23/1000 [00:16<04:17,  3.79it/s]Ref scores:   2%|â–         | 24/1000 [00:16<03:59,  4.07it/s]Ref scores:   2%|â–Ž         | 25/1000 [00:16<03:47,  4.29it/s]Ref scores:   3%|â–Ž         | 26/1000 [00:17<03:41,  4.39it/s]Ref scores:   3%|â–Ž         | 27/1000 [00:17<03:34,  4.54it/s]Ref scores:   3%|â–Ž         | 28/1000 [00:17<03:52,  4.18it/s]Ref scores:   3%|â–Ž         | 29/1000 [00:17<03:46,  4.30it/s]Ref scores:   3%|â–Ž         | 30/1000 [00:17<03:52,  4.17it/s]Ref scores:   3%|â–Ž         | 31/1000 [00:18<03:42,  4.36it/s]Ref scores:   3%|â–Ž         | 32/1000 [00:18<03:50,  4.21it/s]Ref scores:   3%|â–Ž         | 33/1000 [00:18<03:34,  4.51it/s]Ref scores:   3%|â–Ž         | 34/1000 [00:18<03:30,  4.59it/s]Ref scores:   4%|â–Ž         | 35/1000 [00:19<03:39,  4.39it/s]Ref scores:   4%|â–Ž         | 36/1000 [00:19<03:34,  4.50it/s]Ref scores:   4%|â–Ž         | 37/1000 [00:19<03:44,  4.28it/s]Ref scores:   4%|â–         | 38/1000 [00:19<03:49,  4.19it/s]Ref scores:   4%|â–         | 39/1000 [00:20<03:42,  4.32it/s]Ref scores:   4%|â–         | 40/1000 [00:20<03:39,  4.38it/s]Ref scores:   4%|â–         | 41/1000 [00:20<03:46,  4.24it/s]Ref scores:   4%|â–         | 42/1000 [00:20<03:37,  4.40it/s]Ref scores:   4%|â–         | 43/1000 [00:20<03:25,  4.66it/s]Ref scores:   4%|â–         | 44/1000 [00:21<03:38,  4.37it/s]Ref scores:   4%|â–         | 45/1000 [00:21<03:37,  4.39it/s]Ref scores:   5%|â–         | 46/1000 [00:21<03:32,  4.49it/s]Ref scores:   5%|â–         | 47/1000 [00:21<03:32,  4.49it/s]Ref scores:   5%|â–         | 48/1000 [00:22<03:51,  4.12it/s]Ref scores:   5%|â–         | 49/1000 [00:22<03:56,  4.02it/s]Ref scores:   5%|â–Œ         | 50/1000 [00:22<03:59,  3.97it/s]Ref scores:   5%|â–Œ         | 51/1000 [00:22<03:47,  4.17it/s]Ref scores:   5%|â–Œ         | 52/1000 [00:23<03:52,  4.08it/s]Ref scores:   5%|â–Œ         | 53/1000 [00:23<03:54,  4.04it/s]Ref scores:   5%|â–Œ         | 54/1000 [00:23<03:45,  4.19it/s]Ref scores:   6%|â–Œ         | 55/1000 [00:23<03:30,  4.49it/s]Ref scores:   6%|â–Œ         | 56/1000 [00:23<03:30,  4.48it/s]Ref scores:   6%|â–Œ         | 57/1000 [00:24<03:27,  4.54it/s]Ref scores:   6%|â–Œ         | 58/1000 [00:24<03:47,  4.14it/s]Ref scores:   6%|â–Œ         | 59/1000 [00:24<04:03,  3.87it/s]Ref scores:   6%|â–Œ         | 60/1000 [00:25<04:02,  3.88it/s]Ref scores:   6%|â–Œ         | 61/1000 [00:25<04:01,  3.89it/s]Ref scores:   6%|â–Œ         | 62/1000 [00:25<03:49,  4.08it/s]Ref scores:   6%|â–‹         | 63/1000 [00:25<03:42,  4.21it/s]Ref scores:   6%|â–‹         | 64/1000 [00:25<03:38,  4.28it/s]Ref scores:   6%|â–‹         | 65/1000 [00:26<03:30,  4.44it/s]Ref scores:   7%|â–‹         | 66/1000 [00:26<03:18,  4.71it/s]Ref scores:   7%|â–‹         | 67/1000 [00:26<03:17,  4.73it/s]Ref scores:   7%|â–‹         | 68/1000 [00:26<03:19,  4.67it/s]Ref scores:   7%|â–‹         | 69/1000 [00:27<03:30,  4.43it/s]Ref scores:   7%|â–‹         | 70/1000 [00:27<03:26,  4.51it/s]Ref scores:   7%|â–‹         | 71/1000 [00:27<03:21,  4.61it/s]Ref scores:   7%|â–‹         | 72/1000 [00:27<03:18,  4.66it/s]Ref scores:   7%|â–‹         | 73/1000 [00:27<03:16,  4.72it/s]Ref scores:   7%|â–‹         | 74/1000 [00:28<03:30,  4.41it/s]Ref scores:   8%|â–Š         | 75/1000 [00:28<03:39,  4.22it/s]Ref scores:   8%|â–Š         | 76/1000 [00:28<03:44,  4.12it/s]Ref scores:   8%|â–Š         | 77/1000 [00:28<03:37,  4.23it/s]Ref scores:   8%|â–Š         | 78/1000 [00:29<03:33,  4.33it/s]Ref scores:   8%|â–Š         | 79/1000 [00:29<03:40,  4.18it/s]Ref scores:   8%|â–Š         | 80/1000 [00:29<03:30,  4.36it/s]Ref scores:   8%|â–Š         | 81/1000 [00:29<03:24,  4.49it/s]Ref scores:   8%|â–Š         | 82/1000 [00:29<03:20,  4.57it/s]Ref scores:   8%|â–Š         | 83/1000 [00:30<03:40,  4.15it/s]Ref scores:   8%|â–Š         | 84/1000 [00:30<03:30,  4.36it/s]Ref scores:   8%|â–Š         | 85/1000 [00:30<03:24,  4.47it/s]Ref scores:   9%|â–Š         | 86/1000 [00:30<03:32,  4.29it/s]Ref scores:   9%|â–Š         | 87/1000 [00:31<03:38,  4.17it/s]Ref scores:   9%|â–‰         | 88/1000 [00:31<03:29,  4.35it/s]Ref scores:   9%|â–‰         | 89/1000 [00:31<03:25,  4.44it/s]Ref scores:   9%|â–‰         | 90/1000 [00:31<03:19,  4.55it/s]Ref scores:   9%|â–‰         | 91/1000 [00:31<03:10,  4.77it/s]Ref scores:   9%|â–‰         | 92/1000 [00:32<03:22,  4.49it/s]Ref scores:   9%|â–‰         | 93/1000 [00:32<03:31,  4.28it/s]Ref scores:   9%|â–‰         | 94/1000 [00:32<03:23,  4.44it/s]Ref scores:  10%|â–‰         | 95/1000 [00:32<03:19,  4.53it/s]Ref scores:  10%|â–‰         | 96/1000 [00:33<03:19,  4.52it/s]Ref scores:  10%|â–‰         | 97/1000 [00:33<03:19,  4.52it/s]Ref scores:  10%|â–‰         | 98/1000 [00:33<03:18,  4.53it/s]Ref scores:  10%|â–‰         | 99/1000 [00:33<03:19,  4.52it/s]Ref scores:  10%|â–ˆ         | 100/1000 [00:34<03:18,  4.55it/s]Ref scores:  10%|â–ˆ         | 101/1000 [00:34<03:38,  4.12it/s]Ref scores:  10%|â–ˆ         | 102/1000 [00:34<03:23,  4.40it/s]Ref scores:  10%|â–ˆ         | 103/1000 [00:34<03:13,  4.63it/s]Ref scores:  10%|â–ˆ         | 104/1000 [00:34<03:11,  4.67it/s]Ref scores:  10%|â–ˆ         | 105/1000 [00:35<03:05,  4.82it/s]Ref scores:  11%|â–ˆ         | 106/1000 [00:35<03:18,  4.51it/s]Ref scores:  11%|â–ˆ         | 107/1000 [00:35<03:27,  4.30it/s]Ref scores:  11%|â–ˆ         | 108/1000 [00:35<03:34,  4.17it/s]Ref scores:  11%|â–ˆ         | 109/1000 [00:36<03:29,  4.26it/s]Ref scores:  11%|â–ˆ         | 110/1000 [00:36<03:23,  4.38it/s]Ref scores:  11%|â–ˆ         | 111/1000 [00:36<03:17,  4.51it/s]Ref scores:  11%|â–ˆ         | 112/1000 [00:36<03:35,  4.11it/s]Ref scores:  11%|â–ˆâ–        | 113/1000 [00:37<03:27,  4.28it/s]Ref scores:  11%|â–ˆâ–        | 114/1000 [00:37<03:24,  4.34it/s]Ref scores:  12%|â–ˆâ–        | 115/1000 [00:37<03:12,  4.59it/s]Ref scores:  12%|â–ˆâ–        | 116/1000 [00:37<03:12,  4.59it/s]Ref scores:  12%|â–ˆâ–        | 117/1000 [00:37<03:22,  4.36it/s]Ref scores:  12%|â–ˆâ–        | 118/1000 [00:38<03:20,  4.39it/s]Ref scores:  12%|â–ˆâ–        | 119/1000 [00:38<03:10,  4.63it/s]Ref scores:  12%|â–ˆâ–        | 120/1000 [00:38<03:07,  4.70it/s]Ref scores:  12%|â–ˆâ–        | 121/1000 [00:38<03:00,  4.86it/s]Ref scores:  12%|â–ˆâ–        | 122/1000 [00:38<03:03,  4.78it/s]Ref scores:  12%|â–ˆâ–        | 123/1000 [00:39<03:26,  4.25it/s]Ref scores:  12%|â–ˆâ–        | 124/1000 [00:39<03:32,  4.12it/s]Ref scores:  12%|â–ˆâ–Ž        | 125/1000 [00:39<03:26,  4.25it/s]Ref scores:  13%|â–ˆâ–Ž        | 126/1000 [00:39<03:39,  3.98it/s]Ref scores:  13%|â–ˆâ–Ž        | 127/1000 [00:40<03:26,  4.22it/s]Ref scores:  13%|â–ˆâ–Ž        | 128/1000 [00:40<03:19,  4.38it/s]Ref scores:  13%|â–ˆâ–Ž        | 129/1000 [00:40<03:35,  4.04it/s]Ref scores:  13%|â–ˆâ–Ž        | 130/1000 [00:40<03:38,  3.98it/s]Ref scores:  13%|â–ˆâ–Ž        | 131/1000 [00:41<03:39,  3.95it/s]Ref scores:  13%|â–ˆâ–Ž        | 132/1000 [00:41<03:32,  4.09it/s]Ref scores:  13%|â–ˆâ–Ž        | 133/1000 [00:41<03:35,  4.03it/s]Ref scores:  13%|â–ˆâ–Ž        | 134/1000 [00:41<03:45,  3.85it/s]Ref scores:  14%|â–ˆâ–Ž        | 135/1000 [00:42<03:32,  4.06it/s]Ref scores:  14%|â–ˆâ–Ž        | 136/1000 [00:42<03:44,  3.84it/s]Ref scores:  14%|â–ˆâ–Ž        | 137/1000 [00:42<03:30,  4.11it/s]Ref scores:  14%|â–ˆâ–        | 138/1000 [00:42<03:43,  3.85it/s]Ref scores:  14%|â–ˆâ–        | 139/1000 [00:43<03:31,  4.08it/s]Ref scores:  14%|â–ˆâ–        | 140/1000 [00:43<03:23,  4.23it/s]Ref scores:  14%|â–ˆâ–        | 141/1000 [00:43<03:18,  4.32it/s]Ref scores:  14%|â–ˆâ–        | 142/1000 [00:43<03:14,  4.41it/s]Ref scores:  14%|â–ˆâ–        | 143/1000 [00:44<03:21,  4.25it/s]Ref scores:  14%|â–ˆâ–        | 144/1000 [00:44<03:15,  4.38it/s]Ref scores:  14%|â–ˆâ–        | 145/1000 [00:44<03:21,  4.23it/s]Ref scores:  15%|â–ˆâ–        | 146/1000 [00:44<03:25,  4.15it/s]Ref scores:  15%|â–ˆâ–        | 147/1000 [00:45<03:48,  3.73it/s]Ref scores:  15%|â–ˆâ–        | 148/1000 [00:45<03:45,  3.78it/s]Ref scores:  15%|â–ˆâ–        | 149/1000 [00:45<03:30,  4.04it/s]Ref scores:  15%|â–ˆâ–Œ        | 150/1000 [00:45<03:21,  4.21it/s]Ref scores:  15%|â–ˆâ–Œ        | 151/1000 [00:46<03:16,  4.32it/s]Ref scores:  15%|â–ˆâ–Œ        | 152/1000 [00:46<03:13,  4.39it/s]Ref scores:  15%|â–ˆâ–Œ        | 153/1000 [00:46<03:07,  4.52it/s]Ref scores:  15%|â–ˆâ–Œ        | 154/1000 [00:46<03:11,  4.41it/s]Ref scores:  16%|â–ˆâ–Œ        | 155/1000 [00:46<03:11,  4.41it/s]Ref scores:  16%|â–ˆâ–Œ        | 156/1000 [00:47<03:13,  4.35it/s]Ref scores:  16%|â–ˆâ–Œ        | 157/1000 [00:47<03:07,  4.49it/s]Ref scores:  16%|â–ˆâ–Œ        | 158/1000 [00:47<03:09,  4.44it/s]Ref scores:  16%|â–ˆâ–Œ        | 159/1000 [00:47<03:07,  4.48it/s]Ref scores:  16%|â–ˆâ–Œ        | 160/1000 [00:48<03:07,  4.47it/s]Ref scores:  16%|â–ˆâ–Œ        | 161/1000 [00:48<02:58,  4.69it/s]Ref scores:  16%|â–ˆâ–Œ        | 162/1000 [00:48<02:59,  4.68it/s]Ref scores:  16%|â–ˆâ–‹        | 163/1000 [00:48<03:12,  4.36it/s]Ref scores:  16%|â–ˆâ–‹        | 164/1000 [00:48<03:08,  4.44it/s]Ref scores:  16%|â–ˆâ–‹        | 165/1000 [00:49<03:00,  4.62it/s]Ref scores:  17%|â–ˆâ–‹        | 166/1000 [00:49<02:59,  4.65it/s]Ref scores:  17%|â–ˆâ–‹        | 167/1000 [00:49<03:08,  4.42it/s]Ref scores:  17%|â–ˆâ–‹        | 168/1000 [00:49<03:03,  4.53it/s]Ref scores:  17%|â–ˆâ–‹        | 169/1000 [00:50<03:11,  4.33it/s]Ref scores:  17%|â–ˆâ–‹        | 170/1000 [00:50<03:17,  4.20it/s]Ref scores:  17%|â–ˆâ–‹        | 171/1000 [00:50<03:09,  4.37it/s]Ref scores:  17%|â–ˆâ–‹        | 172/1000 [00:50<03:15,  4.24it/s]Ref scores:  17%|â–ˆâ–‹        | 173/1000 [00:50<03:02,  4.53it/s]Ref scores:  17%|â–ˆâ–‹        | 174/1000 [00:51<03:10,  4.34it/s]Ref scores:  18%|â–ˆâ–Š        | 175/1000 [00:51<03:04,  4.46it/s]Ref scores:  18%|â–ˆâ–Š        | 176/1000 [00:51<03:01,  4.55it/s]Ref scores:  18%|â–ˆâ–Š        | 177/1000 [00:51<02:59,  4.58it/s]Ref scores:  18%|â–ˆâ–Š        | 178/1000 [00:52<02:50,  4.81it/s]Ref scores:  18%|â–ˆâ–Š        | 179/1000 [00:52<02:50,  4.82it/s]Ref scores:  18%|â–ˆâ–Š        | 180/1000 [00:52<02:50,  4.80it/s]Ref scores:  18%|â–ˆâ–Š        | 181/1000 [00:52<02:49,  4.83it/s]Ref scores:  18%|â–ˆâ–Š        | 182/1000 [00:52<02:53,  4.72it/s]Ref scores:  18%|â–ˆâ–Š        | 183/1000 [00:53<02:56,  4.64it/s]Ref scores:  18%|â–ˆâ–Š        | 184/1000 [00:53<03:05,  4.41it/s]Ref scores:  18%|â–ˆâ–Š        | 185/1000 [00:53<03:00,  4.52it/s]Ref scores:  19%|â–ˆâ–Š        | 186/1000 [00:53<02:59,  4.53it/s]Ref scores:  19%|â–ˆâ–Š        | 187/1000 [00:53<02:57,  4.57it/s]Ref scores:  19%|â–ˆâ–‰        | 188/1000 [00:54<02:55,  4.62it/s]Ref scores:  19%|â–ˆâ–‰        | 189/1000 [00:54<02:53,  4.68it/s]Ref scores:  19%|â–ˆâ–‰        | 190/1000 [00:54<02:50,  4.75it/s]Ref scores:  19%|â–ˆâ–‰        | 191/1000 [00:54<02:51,  4.72it/s]Ref scores:  19%|â–ˆâ–‰        | 192/1000 [00:55<02:50,  4.75it/s]Ref scores:  19%|â–ˆâ–‰        | 193/1000 [00:55<02:49,  4.77it/s]Ref scores:  19%|â–ˆâ–‰        | 194/1000 [00:55<03:01,  4.45it/s]Ref scores:  20%|â–ˆâ–‰        | 195/1000 [00:55<02:59,  4.49it/s]Ref scores:  20%|â–ˆâ–‰        | 196/1000 [00:55<03:07,  4.29it/s]Ref scores:  20%|â–ˆâ–‰        | 197/1000 [00:56<02:56,  4.56it/s]Ref scores:  20%|â–ˆâ–‰        | 198/1000 [00:56<02:53,  4.63it/s]Ref scores:  20%|â–ˆâ–‰        | 199/1000 [00:56<03:11,  4.19it/s]Ref scores:  20%|â–ˆâ–ˆ        | 200/1000 [00:56<02:58,  4.48it/s]Ref scores:  20%|â–ˆâ–ˆ        | 201/1000 [00:57<03:14,  4.11it/s]Ref scores:  20%|â–ˆâ–ˆ        | 202/1000 [00:57<03:16,  4.06it/s]Ref scores:  20%|â–ˆâ–ˆ        | 203/1000 [00:57<03:07,  4.26it/s]Ref scores:  20%|â–ˆâ–ˆ        | 204/1000 [00:57<03:03,  4.35it/s]Ref scores:  20%|â–ˆâ–ˆ        | 205/1000 [00:58<03:01,  4.39it/s]Ref scores:  21%|â–ˆâ–ˆ        | 206/1000 [00:58<02:58,  4.46it/s]Ref scores:  21%|â–ˆâ–ˆ        | 207/1000 [00:58<03:15,  4.06it/s]Ref scores:  21%|â–ˆâ–ˆ        | 208/1000 [00:58<03:17,  4.00it/s]Ref scores:  21%|â–ˆâ–ˆ        | 209/1000 [00:59<03:02,  4.32it/s]Ref scores:  21%|â–ˆâ–ˆ        | 210/1000 [00:59<03:09,  4.17it/s]Ref scores:  21%|â–ˆâ–ˆ        | 211/1000 [00:59<03:00,  4.37it/s]Ref scores:  21%|â–ˆâ–ˆ        | 212/1000 [00:59<02:49,  4.66it/s]Ref scores:  21%|â–ˆâ–ˆâ–       | 213/1000 [00:59<03:07,  4.20it/s]Ref scores:  21%|â–ˆâ–ˆâ–       | 214/1000 [01:00<02:55,  4.48it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 215/1000 [01:00<03:02,  4.30it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 216/1000 [01:00<02:49,  4.63it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 217/1000 [01:00<03:05,  4.22it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 218/1000 [01:01<03:09,  4.13it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 219/1000 [01:01<03:03,  4.26it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 220/1000 [01:01<03:16,  3.97it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 221/1000 [01:01<03:08,  4.13it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 222/1000 [01:02<03:19,  3.89it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 223/1000 [01:02<03:26,  3.76it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 224/1000 [01:02<03:32,  3.64it/s]Ref scores:  22%|â–ˆâ–ˆâ–Ž       | 225/1000 [01:02<03:27,  3.73it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 226/1000 [01:03<03:33,  3.62it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 227/1000 [01:03<03:20,  3.85it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 228/1000 [01:03<03:20,  3.84it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 229/1000 [01:03<03:11,  4.02it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 230/1000 [01:04<03:02,  4.21it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 231/1000 [01:04<02:59,  4.29it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 232/1000 [01:04<02:56,  4.36it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 233/1000 [01:04<03:02,  4.20it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 234/1000 [01:05<02:50,  4.49it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 235/1000 [01:05<02:58,  4.28it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 236/1000 [01:05<02:51,  4.46it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 237/1000 [01:05<02:48,  4.53it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 238/1000 [01:05<02:47,  4.56it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 239/1000 [01:06<03:02,  4.16it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 240/1000 [01:06<03:05,  4.10it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 241/1000 [01:06<02:57,  4.27it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 242/1000 [01:06<03:02,  4.16it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 243/1000 [01:07<03:06,  4.06it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 244/1000 [01:07<03:18,  3.81it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 245/1000 [01:07<03:00,  4.18it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 246/1000 [01:07<03:04,  4.08it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 247/1000 [01:08<03:07,  4.02it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 248/1000 [01:08<03:08,  3.98it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 249/1000 [01:08<03:00,  4.17it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 250/1000 [01:08<02:51,  4.36it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 251/1000 [01:09<02:50,  4.40it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 252/1000 [01:09<02:56,  4.23it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 253/1000 [01:09<02:52,  4.32it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 254/1000 [01:09<03:07,  3.98it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 255/1000 [01:10<03:16,  3.80it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 256/1000 [01:10<03:05,  4.01it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 257/1000 [01:10<02:55,  4.22it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 258/1000 [01:10<02:48,  4.39it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 259/1000 [01:11<02:46,  4.44it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 260/1000 [01:11<02:45,  4.48it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 261/1000 [01:11<02:44,  4.50it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 262/1000 [01:11<02:43,  4.51it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 263/1000 [01:12<03:07,  3.94it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 264/1000 [01:12<02:58,  4.12it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 265/1000 [01:12<02:54,  4.22it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 266/1000 [01:12<02:43,  4.50it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 267/1000 [01:12<02:49,  4.32it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 268/1000 [01:13<02:37,  4.64it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 269/1000 [01:13<02:32,  4.80it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 270/1000 [01:13<02:35,  4.68it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 271/1000 [01:13<02:53,  4.20it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 272/1000 [01:14<02:56,  4.13it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 273/1000 [01:14<02:47,  4.34it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 274/1000 [01:14<03:01,  4.01it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 275/1000 [01:14<02:47,  4.33it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 276/1000 [01:14<02:45,  4.38it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 277/1000 [01:15<02:58,  4.04it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 278/1000 [01:15<03:08,  3.84it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 279/1000 [01:15<03:13,  3.73it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 280/1000 [01:16<02:58,  4.02it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 281/1000 [01:16<03:07,  3.83it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 282/1000 [01:16<03:12,  3.72it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 283/1000 [01:16<02:58,  4.01it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 284/1000 [01:17<03:00,  3.97it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 285/1000 [01:17<02:53,  4.12it/s]Ref scores:  29%|â–ˆâ–ˆâ–Š       | 286/1000 [01:17<03:04,  3.86it/s]Ref scores:  29%|â–ˆâ–ˆâ–Š       | 287/1000 [01:17<02:49,  4.22it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 288/1000 [01:18<02:53,  4.10it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 289/1000 [01:18<02:56,  4.03it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 290/1000 [01:18<03:13,  3.67it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 291/1000 [01:18<03:10,  3.71it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 292/1000 [01:19<02:59,  3.94it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 293/1000 [01:19<03:15,  3.62it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 294/1000 [01:19<03:01,  3.89it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 295/1000 [01:19<02:53,  4.06it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 296/1000 [01:20<02:54,  4.03it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 297/1000 [01:20<02:55,  4.00it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 298/1000 [01:20<02:48,  4.16it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 299/1000 [01:20<02:43,  4.28it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 300/1000 [01:21<02:47,  4.18it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 301/1000 [01:21<02:59,  3.89it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 302/1000 [01:21<02:51,  4.08it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 303/1000 [01:21<02:43,  4.26it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 304/1000 [01:22<02:48,  4.14it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 305/1000 [01:22<02:43,  4.25it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 306/1000 [01:22<02:40,  4.32it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 307/1000 [01:22<02:35,  4.45it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 308/1000 [01:22<02:42,  4.27it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 309/1000 [01:23<03:01,  3.81it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 310/1000 [01:23<02:51,  4.02it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 311/1000 [01:23<02:44,  4.18it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 312/1000 [01:23<02:54,  3.95it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆâ–      | 313/1000 [01:24<02:54,  3.95it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆâ–      | 314/1000 [01:24<03:01,  3.77it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 315/1000 [01:24<02:42,  4.21it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 316/1000 [01:24<02:46,  4.10it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 317/1000 [01:25<02:42,  4.21it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 318/1000 [01:25<02:35,  4.38it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 319/1000 [01:25<02:48,  4.03it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 320/1000 [01:25<02:50,  4.00it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 321/1000 [01:26<02:34,  4.39it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 322/1000 [01:26<02:41,  4.21it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 323/1000 [01:26<02:50,  3.97it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 324/1000 [01:26<02:43,  4.14it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 325/1000 [01:27<02:45,  4.08it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 326/1000 [01:27<02:47,  4.02it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 327/1000 [01:27<02:40,  4.19it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 328/1000 [01:27<02:36,  4.30it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 329/1000 [01:28<02:24,  4.66it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 330/1000 [01:28<02:38,  4.23it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 331/1000 [01:28<02:49,  3.96it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 332/1000 [01:28<02:35,  4.30it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 333/1000 [01:28<02:29,  4.45it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 334/1000 [01:29<02:36,  4.26it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 335/1000 [01:29<02:32,  4.35it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 336/1000 [01:29<02:38,  4.18it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 337/1000 [01:29<02:35,  4.26it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 338/1000 [01:30<02:32,  4.35it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 339/1000 [01:30<02:27,  4.48it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 340/1000 [01:30<02:40,  4.12it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 341/1000 [01:30<02:32,  4.33it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 342/1000 [01:31<02:28,  4.42it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 343/1000 [01:31<02:35,  4.23it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 344/1000 [01:31<02:28,  4.43it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 345/1000 [01:31<02:24,  4.55it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 346/1000 [01:31<02:21,  4.63it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 347/1000 [01:32<02:19,  4.68it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 348/1000 [01:32<02:20,  4.64it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 349/1000 [01:32<02:35,  4.17it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 350/1000 [01:32<02:22,  4.55it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 351/1000 [01:33<02:37,  4.12it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 352/1000 [01:33<02:33,  4.22it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 353/1000 [01:33<02:30,  4.30it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 354/1000 [01:33<02:27,  4.39it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 355/1000 [01:34<02:25,  4.42it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 356/1000 [01:34<02:31,  4.24it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 357/1000 [01:34<02:20,  4.57it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 358/1000 [01:34<02:33,  4.18it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 359/1000 [01:35<02:42,  3.95it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 360/1000 [01:35<02:43,  3.91it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 361/1000 [01:35<02:34,  4.14it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 362/1000 [01:35<02:27,  4.32it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 363/1000 [01:35<02:33,  4.16it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 364/1000 [01:36<02:29,  4.26it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 365/1000 [01:36<02:26,  4.34it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 366/1000 [01:36<02:31,  4.19it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 367/1000 [01:36<02:35,  4.08it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 368/1000 [01:37<02:29,  4.22it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 369/1000 [01:37<02:33,  4.10it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 370/1000 [01:37<02:41,  3.89it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 371/1000 [01:37<02:32,  4.14it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 372/1000 [01:38<02:34,  4.06it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 373/1000 [01:38<02:36,  4.01it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 374/1000 [01:38<02:29,  4.17it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 375/1000 [01:38<02:23,  4.34it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 376/1000 [01:39<02:15,  4.60it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 377/1000 [01:39<02:13,  4.66it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 378/1000 [01:39<02:13,  4.65it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 379/1000 [01:39<02:28,  4.19it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 380/1000 [01:40<02:38,  3.91it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 381/1000 [01:40<02:31,  4.08it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 382/1000 [01:40<02:26,  4.22it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 383/1000 [01:40<02:29,  4.11it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 384/1000 [01:40<02:25,  4.23it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 385/1000 [01:41<02:21,  4.35it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 386/1000 [01:41<02:19,  4.41it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 387/1000 [01:41<02:15,  4.52it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 388/1000 [01:41<02:21,  4.34it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 389/1000 [01:42<02:25,  4.20it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 390/1000 [01:42<02:22,  4.27it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 391/1000 [01:42<02:26,  4.15it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 392/1000 [01:42<02:20,  4.33it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 393/1000 [01:43<02:16,  4.45it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 394/1000 [01:43<02:08,  4.71it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 395/1000 [01:43<02:23,  4.21it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 396/1000 [01:43<02:18,  4.36it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 397/1000 [01:43<02:13,  4.52it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 398/1000 [01:44<02:13,  4.51it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 399/1000 [01:44<02:12,  4.55it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 400/1000 [01:44<02:08,  4.66it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 401/1000 [01:44<02:16,  4.39it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 402/1000 [01:45<02:14,  4.46it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 403/1000 [01:45<02:19,  4.29it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 404/1000 [01:45<02:16,  4.37it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 405/1000 [01:45<02:11,  4.53it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 406/1000 [01:45<02:16,  4.35it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 407/1000 [01:46<02:20,  4.22it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 408/1000 [01:46<02:23,  4.13it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 409/1000 [01:46<02:18,  4.26it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 410/1000 [01:46<02:28,  3.98it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 411/1000 [01:47<02:20,  4.19it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 412/1000 [01:47<02:22,  4.11it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 413/1000 [01:47<02:16,  4.30it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 414/1000 [01:47<02:13,  4.37it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 415/1000 [01:48<02:06,  4.62it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 416/1000 [01:48<02:07,  4.58it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 417/1000 [01:48<02:13,  4.38it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 418/1000 [01:48<02:11,  4.44it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 419/1000 [01:48<02:08,  4.51it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 420/1000 [01:49<02:20,  4.13it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 421/1000 [01:49<02:15,  4.26it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 422/1000 [01:49<02:05,  4.59it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 423/1000 [01:49<02:12,  4.35it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 424/1000 [01:50<02:11,  4.38it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 425/1000 [01:50<02:15,  4.24it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 426/1000 [01:50<02:11,  4.37it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 427/1000 [01:50<02:09,  4.41it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 428/1000 [01:51<02:14,  4.24it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 429/1000 [01:51<02:25,  3.93it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 430/1000 [01:51<02:13,  4.27it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 431/1000 [01:51<02:11,  4.34it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 432/1000 [01:51<02:06,  4.48it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 433/1000 [01:52<02:05,  4.51it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 434/1000 [01:52<02:11,  4.30it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 435/1000 [01:52<02:03,  4.56it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 436/1000 [01:52<02:03,  4.57it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 437/1000 [01:53<02:02,  4.58it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 438/1000 [01:53<02:16,  4.12it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 439/1000 [01:53<02:25,  3.86it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 440/1000 [01:53<02:29,  3.74it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 441/1000 [01:54<02:32,  3.66it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 442/1000 [01:54<02:29,  3.72it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 443/1000 [01:54<02:27,  3.77it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 444/1000 [01:54<02:19,  3.98it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 445/1000 [01:55<02:14,  4.13it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 446/1000 [01:55<02:07,  4.33it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 447/1000 [01:55<02:05,  4.40it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 448/1000 [01:55<02:16,  4.03it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 449/1000 [01:56<02:17,  4.00it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 450/1000 [01:56<02:18,  3.96it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 451/1000 [01:56<02:13,  4.11it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 452/1000 [01:56<02:16,  4.03it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 453/1000 [01:57<02:09,  4.23it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 454/1000 [01:57<02:01,  4.50it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 455/1000 [01:57<02:00,  4.51it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 456/1000 [01:57<02:01,  4.49it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 457/1000 [01:57<01:59,  4.53it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 458/1000 [01:58<02:05,  4.32it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 459/1000 [01:58<02:02,  4.41it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 460/1000 [01:58<02:01,  4.44it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 461/1000 [01:58<02:00,  4.46it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 462/1000 [01:59<02:06,  4.24it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 463/1000 [01:59<02:10,  4.12it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 464/1000 [01:59<02:11,  4.08it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 465/1000 [01:59<02:01,  4.40it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 466/1000 [02:00<01:58,  4.52it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 467/1000 [02:00<02:08,  4.14it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 468/1000 [02:00<02:05,  4.25it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 469/1000 [02:00<02:12,  4.00it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 470/1000 [02:01<02:08,  4.13it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 471/1000 [02:01<02:03,  4.27it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 472/1000 [02:01<01:55,  4.56it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 473/1000 [02:01<02:06,  4.16it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 474/1000 [02:02<02:13,  3.94it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 475/1000 [02:02<02:19,  3.78it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 476/1000 [02:02<02:16,  3.84it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 477/1000 [02:02<02:15,  3.87it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 478/1000 [02:03<02:08,  4.05it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 479/1000 [02:03<02:03,  4.23it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 480/1000 [02:03<02:11,  3.96it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 481/1000 [02:03<02:11,  3.93it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 482/1000 [02:04<02:04,  4.16it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 483/1000 [02:04<01:55,  4.49it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 484/1000 [02:04<01:53,  4.56it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 485/1000 [02:04<01:58,  4.35it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 486/1000 [02:04<02:03,  4.18it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 487/1000 [02:05<02:10,  3.92it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 488/1000 [02:05<02:05,  4.09it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 489/1000 [02:05<02:11,  3.87it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 490/1000 [02:05<02:05,  4.07it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 491/1000 [02:06<01:59,  4.27it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 492/1000 [02:06<02:07,  3.98it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 493/1000 [02:06<02:13,  3.80it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 494/1000 [02:06<02:04,  4.07it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 495/1000 [02:07<01:57,  4.29it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 496/1000 [02:07<02:07,  3.96it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 497/1000 [02:07<02:06,  3.97it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 498/1000 [02:08<02:13,  3.75it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 499/1000 [02:08<02:22,  3.52it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 500/1000 [02:08<02:18,  3.61it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 501/1000 [02:08<02:15,  3.68it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 502/1000 [02:09<02:07,  3.92it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 503/1000 [02:09<02:06,  3.92it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 504/1000 [02:09<02:01,  4.10it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 505/1000 [02:09<01:57,  4.21it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 506/1000 [02:10<02:04,  3.96it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 507/1000 [02:10<01:57,  4.19it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 508/1000 [02:10<02:00,  4.09it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 509/1000 [02:10<01:55,  4.26it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 510/1000 [02:10<01:57,  4.17it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 511/1000 [02:11<01:54,  4.27it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 512/1000 [02:11<01:51,  4.37it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 513/1000 [02:11<01:48,  4.48it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 514/1000 [02:11<01:53,  4.29it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 515/1000 [02:12<01:51,  4.34it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 516/1000 [02:12<02:00,  4.02it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 517/1000 [02:12<02:05,  3.84it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 518/1000 [02:12<01:59,  4.02it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 519/1000 [02:13<01:55,  4.16it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 520/1000 [02:13<01:52,  4.27it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 521/1000 [02:13<01:50,  4.34it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 522/1000 [02:13<01:54,  4.18it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 523/1000 [02:14<01:56,  4.08it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 524/1000 [02:14<02:03,  3.84it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 525/1000 [02:14<01:57,  4.03it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 526/1000 [02:14<01:52,  4.21it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 527/1000 [02:14<01:43,  4.57it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 528/1000 [02:15<01:40,  4.68it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 529/1000 [02:15<01:40,  4.69it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 530/1000 [02:15<01:39,  4.71it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 531/1000 [02:15<01:45,  4.43it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 532/1000 [02:16<01:39,  4.73it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 533/1000 [02:16<01:44,  4.45it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 534/1000 [02:16<01:43,  4.50it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 535/1000 [02:16<01:42,  4.52it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 536/1000 [02:16<01:42,  4.54it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 537/1000 [02:17<01:41,  4.55it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 538/1000 [02:17<01:41,  4.57it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 539/1000 [02:17<01:50,  4.16it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 540/1000 [02:17<01:56,  3.94it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 541/1000 [02:18<01:51,  4.13it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 542/1000 [02:18<01:52,  4.06it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 543/1000 [02:18<01:44,  4.37it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 544/1000 [02:18<01:41,  4.48it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 545/1000 [02:19<01:36,  4.71it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 546/1000 [02:19<01:42,  4.44it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 547/1000 [02:19<01:55,  3.91it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 548/1000 [02:19<02:00,  3.76it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 549/1000 [02:20<01:58,  3.80it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 550/1000 [02:20<01:48,  4.17it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 551/1000 [02:20<01:49,  4.09it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 552/1000 [02:20<01:45,  4.24it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 553/1000 [02:21<01:42,  4.37it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 554/1000 [02:21<01:45,  4.23it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 555/1000 [02:21<01:42,  4.34it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 556/1000 [02:21<01:36,  4.62it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 557/1000 [02:21<01:34,  4.71it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 558/1000 [02:22<01:39,  4.43it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 559/1000 [02:22<01:47,  4.08it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 560/1000 [02:22<01:49,  4.01it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 561/1000 [02:22<01:46,  4.14it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 562/1000 [02:23<01:47,  4.07it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 563/1000 [02:23<01:48,  4.01it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 564/1000 [02:23<01:53,  3.83it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 565/1000 [02:23<01:52,  3.87it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 566/1000 [02:24<01:45,  4.10it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 567/1000 [02:24<01:40,  4.30it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 568/1000 [02:24<01:48,  4.00it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 569/1000 [02:24<01:53,  3.81it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 570/1000 [02:25<01:57,  3.67it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 571/1000 [02:25<01:49,  3.92it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 572/1000 [02:25<01:49,  3.91it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 573/1000 [02:25<01:44,  4.09it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 574/1000 [02:26<01:45,  4.05it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 575/1000 [02:26<01:41,  4.19it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 576/1000 [02:26<01:42,  4.13it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 577/1000 [02:26<01:39,  4.25it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 578/1000 [02:27<01:45,  3.99it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 579/1000 [02:27<01:40,  4.21it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 580/1000 [02:27<01:35,  4.40it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 581/1000 [02:27<01:38,  4.26it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 582/1000 [02:28<01:30,  4.60it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 583/1000 [02:28<01:30,  4.59it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 584/1000 [02:28<01:24,  4.90it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 585/1000 [02:28<01:31,  4.54it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 586/1000 [02:28<01:30,  4.55it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 587/1000 [02:29<01:26,  4.75it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 588/1000 [02:29<01:27,  4.70it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 589/1000 [02:29<01:37,  4.20it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 590/1000 [02:29<01:34,  4.33it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 591/1000 [02:30<01:41,  4.03it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 592/1000 [02:30<01:37,  4.19it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 593/1000 [02:30<01:43,  3.94it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 594/1000 [02:30<01:36,  4.19it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 595/1000 [02:31<01:34,  4.30it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 596/1000 [02:31<01:31,  4.40it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 597/1000 [02:31<01:34,  4.27it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 598/1000 [02:31<01:31,  4.37it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 599/1000 [02:31<01:28,  4.51it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 600/1000 [02:32<01:27,  4.58it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 601/1000 [02:32<01:31,  4.35it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 602/1000 [02:32<01:29,  4.43it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 603/1000 [02:32<01:24,  4.67it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 604/1000 [02:33<01:29,  4.42it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 605/1000 [02:33<01:32,  4.26it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 606/1000 [02:33<01:35,  4.13it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 607/1000 [02:33<01:31,  4.28it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 608/1000 [02:34<01:33,  4.19it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 609/1000 [02:34<01:30,  4.31it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 610/1000 [02:34<01:28,  4.42it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 611/1000 [02:34<01:30,  4.28it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 612/1000 [02:34<01:28,  4.37it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 613/1000 [02:35<01:23,  4.61it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 614/1000 [02:35<01:19,  4.84it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 615/1000 [02:35<01:25,  4.52it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 616/1000 [02:35<01:28,  4.33it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 617/1000 [02:36<01:31,  4.19it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 618/1000 [02:36<01:25,  4.49it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 619/1000 [02:36<01:28,  4.31it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 620/1000 [02:36<01:24,  4.47it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 621/1000 [02:36<01:24,  4.49it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 622/1000 [02:37<01:32,  4.10it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 623/1000 [02:37<01:27,  4.29it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 624/1000 [02:37<01:30,  4.17it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 625/1000 [02:37<01:27,  4.30it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 626/1000 [02:38<01:25,  4.35it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 627/1000 [02:38<01:24,  4.43it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 628/1000 [02:38<01:26,  4.30it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 629/1000 [02:38<01:20,  4.58it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 630/1000 [02:39<01:28,  4.17it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 631/1000 [02:39<01:26,  4.28it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 632/1000 [02:39<01:28,  4.18it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 633/1000 [02:39<01:23,  4.39it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 634/1000 [02:39<01:26,  4.25it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 635/1000 [02:40<01:20,  4.56it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 636/1000 [02:40<01:26,  4.19it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 637/1000 [02:40<01:19,  4.56it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 638/1000 [02:40<01:22,  4.39it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 639/1000 [02:41<01:24,  4.25it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 640/1000 [02:41<01:22,  4.37it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 641/1000 [02:41<01:20,  4.47it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 642/1000 [02:41<01:16,  4.70it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 643/1000 [02:41<01:16,  4.65it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 644/1000 [02:42<01:17,  4.60it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 645/1000 [02:42<01:24,  4.20it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 646/1000 [02:42<01:26,  4.08it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 647/1000 [02:42<01:22,  4.26it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 648/1000 [02:43<01:17,  4.54it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 649/1000 [02:43<01:15,  4.62it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 650/1000 [02:43<01:16,  4.60it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 651/1000 [02:43<01:19,  4.38it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 652/1000 [02:44<01:22,  4.22it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 653/1000 [02:44<01:20,  4.34it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 654/1000 [02:44<01:17,  4.45it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 655/1000 [02:44<01:15,  4.56it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 656/1000 [02:44<01:23,  4.14it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 657/1000 [02:45<01:28,  3.88it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 658/1000 [02:45<01:22,  4.12it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 659/1000 [02:45<01:19,  4.29it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 660/1000 [02:45<01:16,  4.45it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 661/1000 [02:46<01:19,  4.27it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 662/1000 [02:46<01:21,  4.16it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 663/1000 [02:46<01:15,  4.49it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 664/1000 [02:46<01:17,  4.31it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 665/1000 [02:47<01:12,  4.59it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 666/1000 [02:47<01:20,  4.17it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 667/1000 [02:47<01:17,  4.28it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 668/1000 [02:47<01:22,  4.01it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 669/1000 [02:48<01:18,  4.20it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 670/1000 [02:48<01:16,  4.31it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 671/1000 [02:48<01:14,  4.41it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 672/1000 [02:48<01:13,  4.45it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 673/1000 [02:48<01:12,  4.48it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 674/1000 [02:49<01:15,  4.29it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 675/1000 [02:49<01:12,  4.47it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 676/1000 [02:49<01:12,  4.48it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 677/1000 [02:49<01:10,  4.60it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 678/1000 [02:49<01:08,  4.70it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 679/1000 [02:50<01:08,  4.68it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 680/1000 [02:50<01:15,  4.21it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 681/1000 [02:50<01:12,  4.40it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 682/1000 [02:50<01:18,  4.06it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 683/1000 [02:51<01:14,  4.27it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 684/1000 [02:51<01:12,  4.35it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 685/1000 [02:51<01:10,  4.48it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 686/1000 [02:51<01:08,  4.58it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 687/1000 [02:52<01:12,  4.34it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 688/1000 [02:52<01:10,  4.41it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 689/1000 [02:52<01:09,  4.49it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 690/1000 [02:52<01:11,  4.32it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 691/1000 [02:52<01:10,  4.41it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 692/1000 [02:53<01:05,  4.70it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 693/1000 [02:53<01:12,  4.26it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 694/1000 [02:53<01:09,  4.38it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 695/1000 [02:53<01:11,  4.24it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 696/1000 [02:54<01:13,  4.11it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 697/1000 [02:54<01:14,  4.08it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 698/1000 [02:54<01:17,  3.89it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 699/1000 [02:54<01:17,  3.89it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 700/1000 [02:55<01:17,  3.88it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 701/1000 [02:55<01:16,  3.92it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 702/1000 [02:55<01:12,  4.10it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 703/1000 [02:55<01:10,  4.24it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 704/1000 [02:56<01:14,  3.96it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 705/1000 [02:56<01:17,  3.80it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 706/1000 [02:56<01:19,  3.69it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 707/1000 [02:56<01:13,  3.99it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 708/1000 [02:57<01:09,  4.22it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 709/1000 [02:57<01:06,  4.39it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 710/1000 [02:57<01:05,  4.44it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 711/1000 [02:57<01:03,  4.56it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 712/1000 [02:58<01:06,  4.35it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 713/1000 [02:58<01:11,  4.00it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 714/1000 [02:58<01:07,  4.23it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 715/1000 [02:58<01:03,  4.52it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 716/1000 [02:58<01:03,  4.51it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 717/1000 [02:59<01:08,  4.13it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 718/1000 [02:59<01:03,  4.47it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 719/1000 [02:59<00:59,  4.74it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 720/1000 [02:59<01:05,  4.27it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 721/1000 [03:00<01:03,  4.41it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 722/1000 [03:00<01:08,  4.07it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 723/1000 [03:00<01:02,  4.44it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 724/1000 [03:00<01:04,  4.29it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 725/1000 [03:01<01:00,  4.57it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 726/1000 [03:01<00:57,  4.79it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 727/1000 [03:01<00:57,  4.73it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 728/1000 [03:01<01:01,  4.45it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 729/1000 [03:01<01:03,  4.29it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 730/1000 [03:02<01:04,  4.19it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 731/1000 [03:02<00:59,  4.48it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 732/1000 [03:02<00:56,  4.72it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 733/1000 [03:02<01:02,  4.25it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 734/1000 [03:03<01:04,  4.13it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 735/1000 [03:03<01:05,  4.07it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 736/1000 [03:03<01:05,  4.01it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 737/1000 [03:03<01:02,  4.22it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 738/1000 [03:04<00:58,  4.50it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 739/1000 [03:04<00:57,  4.55it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 740/1000 [03:04<00:56,  4.58it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 741/1000 [03:04<00:56,  4.58it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 742/1000 [03:04<00:59,  4.37it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 743/1000 [03:05<01:03,  4.03it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 744/1000 [03:05<01:06,  3.84it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 745/1000 [03:05<01:11,  3.59it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 746/1000 [03:06<01:11,  3.53it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 747/1000 [03:06<01:09,  3.62it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 748/1000 [03:06<01:02,  4.04it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 749/1000 [03:06<01:02,  4.02it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 750/1000 [03:07<01:02,  3.99it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 751/1000 [03:07<00:59,  4.20it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 752/1000 [03:07<00:56,  4.37it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 753/1000 [03:07<00:55,  4.45it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 754/1000 [03:07<00:54,  4.50it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 755/1000 [03:08<00:51,  4.74it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 756/1000 [03:08<00:54,  4.44it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 757/1000 [03:08<00:53,  4.53it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 758/1000 [03:08<00:55,  4.34it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 759/1000 [03:09<00:57,  4.22it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 760/1000 [03:09<00:58,  4.12it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 761/1000 [03:09<00:58,  4.07it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 762/1000 [03:09<00:56,  4.21it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 763/1000 [03:10<00:54,  4.34it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 764/1000 [03:10<00:52,  4.46it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 765/1000 [03:10<00:58,  4.05it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 766/1000 [03:10<00:53,  4.37it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 767/1000 [03:10<00:51,  4.51it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 768/1000 [03:11<00:56,  4.08it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 769/1000 [03:11<00:57,  4.04it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 770/1000 [03:11<00:54,  4.20it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 771/1000 [03:11<00:53,  4.31it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 772/1000 [03:12<00:49,  4.58it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 773/1000 [03:12<00:49,  4.59it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 774/1000 [03:12<00:49,  4.60it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 775/1000 [03:12<00:54,  4.16it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 776/1000 [03:13<00:50,  4.46it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 777/1000 [03:13<00:47,  4.70it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 778/1000 [03:13<00:47,  4.68it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 779/1000 [03:13<00:47,  4.64it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 780/1000 [03:13<00:50,  4.37it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 781/1000 [03:14<00:47,  4.65it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 782/1000 [03:14<00:46,  4.68it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 783/1000 [03:14<00:44,  4.87it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 784/1000 [03:14<00:47,  4.57it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 785/1000 [03:14<00:44,  4.78it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 786/1000 [03:15<00:44,  4.82it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 787/1000 [03:15<00:45,  4.71it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 788/1000 [03:15<00:43,  4.89it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 789/1000 [03:15<00:43,  4.87it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 790/1000 [03:15<00:42,  4.90it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 791/1000 [03:16<00:47,  4.39it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 792/1000 [03:16<00:49,  4.20it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 793/1000 [03:16<00:48,  4.30it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 794/1000 [03:16<00:46,  4.41it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 795/1000 [03:17<00:45,  4.46it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 796/1000 [03:17<00:45,  4.47it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 797/1000 [03:17<00:44,  4.51it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 798/1000 [03:17<00:46,  4.30it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 799/1000 [03:18<00:50,  4.01it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 800/1000 [03:18<00:45,  4.36it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 801/1000 [03:18<00:44,  4.43it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 802/1000 [03:18<00:46,  4.24it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 803/1000 [03:19<00:49,  3.98it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 804/1000 [03:19<00:51,  3.79it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 805/1000 [03:19<00:51,  3.82it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 806/1000 [03:19<00:48,  4.04it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 807/1000 [03:20<00:46,  4.17it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 808/1000 [03:20<00:49,  3.88it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 809/1000 [03:20<00:50,  3.77it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 810/1000 [03:20<00:47,  3.97it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 811/1000 [03:21<00:49,  3.81it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 812/1000 [03:21<00:48,  3.85it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 813/1000 [03:21<00:48,  3.89it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 814/1000 [03:21<00:45,  4.09it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 815/1000 [03:22<00:41,  4.48it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 816/1000 [03:22<00:45,  4.06it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 817/1000 [03:22<00:47,  3.85it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 818/1000 [03:22<00:45,  4.04it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 819/1000 [03:23<00:43,  4.17it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 820/1000 [03:23<00:40,  4.46it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 821/1000 [03:23<00:41,  4.31it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 822/1000 [03:23<00:39,  4.48it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 823/1000 [03:23<00:43,  4.09it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 824/1000 [03:24<00:39,  4.42it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 825/1000 [03:24<00:39,  4.47it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 826/1000 [03:24<00:38,  4.51it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 827/1000 [03:24<00:38,  4.51it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 828/1000 [03:25<00:41,  4.15it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 829/1000 [03:25<00:42,  4.05it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 830/1000 [03:25<00:42,  4.01it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 831/1000 [03:25<00:38,  4.36it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 832/1000 [03:26<00:39,  4.22it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 833/1000 [03:26<00:38,  4.36it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 834/1000 [03:26<00:36,  4.54it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 835/1000 [03:26<00:36,  4.57it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 836/1000 [03:26<00:35,  4.60it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 837/1000 [03:27<00:35,  4.66it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 838/1000 [03:27<00:38,  4.20it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 839/1000 [03:27<00:38,  4.13it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 840/1000 [03:27<00:40,  3.92it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 841/1000 [03:28<00:44,  3.61it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 842/1000 [03:28<00:40,  3.86it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 843/1000 [03:28<00:40,  3.85it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 844/1000 [03:29<00:40,  3.88it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 845/1000 [03:29<00:38,  4.07it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 846/1000 [03:29<00:36,  4.22it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 847/1000 [03:29<00:34,  4.39it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 848/1000 [03:29<00:37,  4.06it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 849/1000 [03:30<00:39,  3.85it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 850/1000 [03:30<00:37,  4.02it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 851/1000 [03:30<00:35,  4.22it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 852/1000 [03:30<00:32,  4.53it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 853/1000 [03:31<00:31,  4.60it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 854/1000 [03:31<00:33,  4.40it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 855/1000 [03:31<00:35,  4.04it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 856/1000 [03:31<00:37,  3.84it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 857/1000 [03:32<00:37,  3.85it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 858/1000 [03:32<00:36,  3.88it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 859/1000 [03:32<00:34,  4.05it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 860/1000 [03:32<00:34,  4.01it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 861/1000 [03:33<00:34,  3.98it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 862/1000 [03:33<00:36,  3.79it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 863/1000 [03:33<00:33,  4.05it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 864/1000 [03:33<00:32,  4.20it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 865/1000 [03:34<00:30,  4.37it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 866/1000 [03:34<00:30,  4.42it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 867/1000 [03:34<00:32,  4.07it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 868/1000 [03:34<00:30,  4.28it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 869/1000 [03:35<00:31,  4.16it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 870/1000 [03:35<00:31,  4.09it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 871/1000 [03:35<00:33,  3.88it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 872/1000 [03:35<00:32,  3.90it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 873/1000 [03:36<00:31,  4.08it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 874/1000 [03:36<00:32,  3.85it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 875/1000 [03:36<00:30,  4.11it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 876/1000 [03:36<00:30,  4.04it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 877/1000 [03:37<00:30,  3.99it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 878/1000 [03:37<00:29,  4.20it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 879/1000 [03:37<00:26,  4.50it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 880/1000 [03:37<00:25,  4.73it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 881/1000 [03:37<00:25,  4.66it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 882/1000 [03:38<00:25,  4.67it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 883/1000 [03:38<00:24,  4.71it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 884/1000 [03:38<00:25,  4.62it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 885/1000 [03:38<00:24,  4.67it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 886/1000 [03:38<00:26,  4.38it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 887/1000 [03:39<00:28,  4.02it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 888/1000 [03:39<00:29,  3.83it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 889/1000 [03:39<00:28,  3.84it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 890/1000 [03:40<00:26,  4.08it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 891/1000 [03:40<00:28,  3.85it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 892/1000 [03:40<00:26,  4.10it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 893/1000 [03:40<00:26,  4.05it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 894/1000 [03:41<00:26,  4.00it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 895/1000 [03:41<00:25,  4.16it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 896/1000 [03:41<00:25,  4.10it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 897/1000 [03:41<00:26,  3.90it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 898/1000 [03:42<00:24,  4.08it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 899/1000 [03:42<00:23,  4.32it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 900/1000 [03:42<00:22,  4.50it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 901/1000 [03:42<00:22,  4.31it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 902/1000 [03:42<00:22,  4.41it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 903/1000 [03:43<00:21,  4.46it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 904/1000 [03:43<00:23,  4.12it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 905/1000 [03:43<00:22,  4.29it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 906/1000 [03:43<00:22,  4.19it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 907/1000 [03:44<00:22,  4.10it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 908/1000 [03:44<00:22,  4.06it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 909/1000 [03:44<00:21,  4.18it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 910/1000 [03:44<00:20,  4.32it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 911/1000 [03:45<00:21,  4.17it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 912/1000 [03:45<00:21,  4.07it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 913/1000 [03:45<00:22,  3.85it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 914/1000 [03:45<00:22,  3.85it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 915/1000 [03:46<00:21,  3.87it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 916/1000 [03:46<00:23,  3.57it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 917/1000 [03:46<00:21,  3.87it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 918/1000 [03:46<00:20,  4.04it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 919/1000 [03:47<00:20,  3.98it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 920/1000 [03:47<00:18,  4.22it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 921/1000 [03:47<00:18,  4.31it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 922/1000 [03:47<00:17,  4.37it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 923/1000 [03:48<00:17,  4.41it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 924/1000 [03:48<00:17,  4.45it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 925/1000 [03:48<00:17,  4.24it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 926/1000 [03:48<00:17,  4.31it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 927/1000 [03:49<00:18,  3.98it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 928/1000 [03:49<00:17,  4.18it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 929/1000 [03:49<00:16,  4.26it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 930/1000 [03:49<00:15,  4.42it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 931/1000 [03:49<00:16,  4.07it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 932/1000 [03:50<00:16,  4.19it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 933/1000 [03:50<00:17,  3.91it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 934/1000 [03:50<00:15,  4.19it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 935/1000 [03:50<00:15,  4.29it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 936/1000 [03:51<00:14,  4.42it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 937/1000 [03:51<00:14,  4.23it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 938/1000 [03:51<00:14,  4.34it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 939/1000 [03:51<00:13,  4.44it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 940/1000 [03:52<00:13,  4.29it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 941/1000 [03:52<00:12,  4.56it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 942/1000 [03:52<00:13,  4.35it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 943/1000 [03:52<00:13,  4.21it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 944/1000 [03:52<00:12,  4.33it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 945/1000 [03:53<00:12,  4.42it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 946/1000 [03:53<00:11,  4.57it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 947/1000 [03:53<00:12,  4.14it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 948/1000 [03:53<00:12,  4.09it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 949/1000 [03:54<00:12,  4.22it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 950/1000 [03:54<00:11,  4.34it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 951/1000 [03:54<00:11,  4.42it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 952/1000 [03:54<00:10,  4.45it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 953/1000 [03:54<00:10,  4.68it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 954/1000 [03:55<00:09,  4.73it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 955/1000 [03:55<00:09,  4.71it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 956/1000 [03:55<00:09,  4.76it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 957/1000 [03:55<00:09,  4.45it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 958/1000 [03:56<00:09,  4.48it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 959/1000 [03:56<00:09,  4.53it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 960/1000 [03:56<00:08,  4.54it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 961/1000 [03:56<00:08,  4.79it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 962/1000 [03:56<00:08,  4.73it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 963/1000 [03:57<00:07,  4.70it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 964/1000 [03:57<00:08,  4.26it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 965/1000 [03:57<00:08,  4.16it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 966/1000 [03:57<00:07,  4.29it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 967/1000 [03:58<00:07,  4.41it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 968/1000 [03:58<00:07,  4.46it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 969/1000 [03:58<00:06,  4.47it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 970/1000 [03:58<00:06,  4.77it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 971/1000 [03:58<00:06,  4.82it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 972/1000 [03:59<00:05,  4.70it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 973/1000 [03:59<00:05,  4.67it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 974/1000 [03:59<00:05,  4.76it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 975/1000 [03:59<00:05,  4.50it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 976/1000 [04:00<00:05,  4.51it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 977/1000 [04:00<00:04,  4.63it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 978/1000 [04:00<00:04,  4.63it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 979/1000 [04:00<00:04,  4.61it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 980/1000 [04:00<00:04,  4.40it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 981/1000 [04:01<00:04,  4.54it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 982/1000 [04:01<00:03,  4.57it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 983/1000 [04:01<00:03,  4.59it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 984/1000 [04:01<00:03,  4.61it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 985/1000 [04:01<00:03,  4.63it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 986/1000 [04:02<00:03,  4.61it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 987/1000 [04:02<00:03,  4.17it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 988/1000 [04:02<00:02,  4.34it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 989/1000 [04:02<00:02,  4.20it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 990/1000 [04:03<00:02,  4.30it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 991/1000 [04:03<00:01,  4.65it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 992/1000 [04:03<00:01,  4.63it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 993/1000 [04:03<00:01,  4.03it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 994/1000 [04:04<00:01,  4.25it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 995/1000 [04:04<00:01,  4.14it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 996/1000 [04:04<00:01,  3.93it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 997/1000 [04:04<00:00,  4.07it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 998/1000 [04:05<00:00,  4.21it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 999/1000 [04:05<00:00,  4.32it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [04:05<00:00,  4.20it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [04:05<00:00,  4.07it/s]
DONE (10.80s)
DONE (8.05s)
Computing criterion:   0%|          | 0/20 [00:00<?, ?it/s]Computing criterion:   5%|â–Œ         | 1/20 [00:25<08:04, 25.52s/it]Computing criterion:  10%|â–ˆ         | 2/20 [00:51<07:46, 25.93s/it]Computing criterion:  15%|â–ˆâ–Œ        | 3/20 [01:16<07:13, 25.50s/it]Computing criterion:  20%|â–ˆâ–ˆ        | 4/20 [01:42<06:47, 25.44s/it]Computing criterion:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [02:08<06:25, 25.70s/it]Computing criterion:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [02:33<05:59, 25.66s/it]Computing criterion:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [02:59<05:32, 25.58s/it]Computing criterion:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [03:25<05:10, 25.85s/it]Computing criterion:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [03:50<04:40, 25.51s/it]Computing criterion:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [04:15<04:13, 25.36s/it]Computing criterion:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [04:41<03:48, 25.44s/it]Computing criterion:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [05:06<03:23, 25.48s/it]Computing criterion:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [05:32<03:00, 25.74s/it]Computing criterion:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [05:59<02:35, 25.83s/it]Computing criterion:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [06:24<02:08, 25.76s/it]Computing criterion:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [06:48<01:41, 25.32s/it]Computing criterion:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [07:15<01:17, 25.70s/it]Computing criterion:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [07:41<00:51, 25.79s/it]Computing criterion:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [08:06<00:25, 25.54s/it]Computing criterion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [08:32<00:00, 25.66s/it]Computing criterion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [08:32<00:00, 25.62s/it]
Ref scores:   0%|          | 0/1000 [00:00<?, ?it/s]Ref scores:   0%|          | 1/1000 [00:02<33:41,  2.02s/it]Ref scores:   0%|          | 2/1000 [00:02<16:42,  1.00s/it]Ref scores:   0%|          | 3/1000 [00:02<11:00,  1.51it/s]Ref scores:   0%|          | 4/1000 [00:02<07:43,  2.15it/s]Ref scores:   0%|          | 5/1000 [00:02<05:55,  2.80it/s]Ref scores:   1%|          | 6/1000 [00:03<04:41,  3.54it/s]Ref scores:   1%|          | 7/1000 [00:03<04:01,  4.11it/s]Ref scores:   1%|          | 8/1000 [00:03<03:36,  4.58it/s]Ref scores:   1%|          | 9/1000 [00:03<03:58,  4.15it/s]Ref scores:   1%|          | 10/1000 [00:03<04:04,  4.06it/s]Ref scores:   1%|          | 11/1000 [00:04<04:07,  4.00it/s]Ref scores:   1%|          | 12/1000 [00:04<04:09,  3.96it/s]Ref scores:   1%|â–         | 13/1000 [00:04<03:42,  4.45it/s]Ref scores:   1%|â–         | 14/1000 [00:04<03:51,  4.27it/s]Ref scores:   2%|â–         | 15/1000 [00:05<03:29,  4.70it/s]Ref scores:   2%|â–         | 16/1000 [00:05<03:14,  5.06it/s]Ref scores:   2%|â–         | 17/1000 [00:05<03:03,  5.35it/s]Ref scores:   2%|â–         | 18/1000 [00:05<03:19,  4.93it/s]Ref scores:   2%|â–         | 19/1000 [00:05<03:30,  4.67it/s]Ref scores:   2%|â–         | 20/1000 [00:06<03:43,  4.39it/s]Ref scores:   2%|â–         | 21/1000 [00:06<03:51,  4.23it/s]Ref scores:   2%|â–         | 22/1000 [00:06<03:29,  4.66it/s]Ref scores:   2%|â–         | 23/1000 [00:06<03:07,  5.21it/s]Ref scores:   2%|â–         | 24/1000 [00:06<02:58,  5.46it/s]Ref scores:   2%|â–Ž         | 25/1000 [00:07<03:20,  4.87it/s]Ref scores:   3%|â–Ž         | 26/1000 [00:07<03:45,  4.33it/s]Ref scores:   3%|â–Ž         | 27/1000 [00:07<03:23,  4.79it/s]Ref scores:   3%|â–Ž         | 28/1000 [00:07<03:56,  4.10it/s]Ref scores:   3%|â–Ž         | 29/1000 [00:08<04:10,  3.87it/s]Ref scores:   3%|â–Ž         | 30/1000 [00:08<03:42,  4.36it/s]Ref scores:   3%|â–Ž         | 31/1000 [00:08<04:01,  4.01it/s]Ref scores:   3%|â–Ž         | 32/1000 [00:08<04:14,  3.80it/s]Ref scores:   3%|â–Ž         | 33/1000 [00:09<03:45,  4.29it/s]Ref scores:   3%|â–Ž         | 34/1000 [00:09<03:50,  4.18it/s]Ref scores:   4%|â–Ž         | 35/1000 [00:09<03:55,  4.09it/s]Ref scores:   4%|â–Ž         | 36/1000 [00:09<04:08,  3.89it/s]Ref scores:   4%|â–Ž         | 37/1000 [00:10<04:06,  3.90it/s]Ref scores:   4%|â–         | 38/1000 [00:10<04:05,  3.92it/s]Ref scores:   4%|â–         | 39/1000 [00:10<03:36,  4.43it/s]Ref scores:   4%|â–         | 40/1000 [00:10<03:57,  4.04it/s]Ref scores:   4%|â–         | 41/1000 [00:11<04:00,  3.99it/s]Ref scores:   4%|â–         | 42/1000 [00:11<03:34,  4.46it/s]Ref scores:   4%|â–         | 43/1000 [00:11<03:53,  4.10it/s]Ref scores:   4%|â–         | 44/1000 [00:11<03:29,  4.57it/s]Ref scores:   4%|â–         | 45/1000 [00:11<03:05,  5.15it/s]Ref scores:   5%|â–         | 46/1000 [00:11<02:56,  5.42it/s]Ref scores:   5%|â–         | 47/1000 [00:12<02:50,  5.59it/s]Ref scores:   5%|â–         | 48/1000 [00:12<02:45,  5.75it/s]Ref scores:   5%|â–         | 49/1000 [00:12<02:39,  5.95it/s]Ref scores:   5%|â–Œ         | 50/1000 [00:12<02:38,  6.01it/s]Ref scores:   5%|â–Œ         | 51/1000 [00:12<02:30,  6.32it/s]Ref scores:   5%|â–Œ         | 52/1000 [00:13<03:07,  5.05it/s]Ref scores:   5%|â–Œ         | 53/1000 [00:13<03:06,  5.08it/s]Ref scores:   5%|â–Œ         | 54/1000 [00:13<03:23,  4.65it/s]Ref scores:   6%|â–Œ         | 55/1000 [00:13<03:44,  4.22it/s]Ref scores:   6%|â–Œ         | 56/1000 [00:13<03:20,  4.72it/s]Ref scores:   6%|â–Œ         | 57/1000 [00:14<03:07,  5.03it/s]Ref scores:   6%|â–Œ         | 58/1000 [00:14<02:55,  5.37it/s]Ref scores:   6%|â–Œ         | 59/1000 [00:14<02:48,  5.57it/s]Ref scores:   6%|â–Œ         | 60/1000 [00:14<03:10,  4.93it/s]Ref scores:   6%|â–Œ         | 61/1000 [00:14<03:24,  4.59it/s]Ref scores:   6%|â–Œ         | 62/1000 [00:15<03:08,  4.97it/s]Ref scores:   6%|â–‹         | 63/1000 [00:15<03:25,  4.56it/s]Ref scores:   6%|â–‹         | 64/1000 [00:15<03:43,  4.19it/s]Ref scores:   6%|â–‹         | 65/1000 [00:15<03:57,  3.93it/s]Ref scores:   7%|â–‹         | 66/1000 [00:16<04:06,  3.79it/s]Ref scores:   7%|â–‹         | 67/1000 [00:16<03:38,  4.27it/s]Ref scores:   7%|â–‹         | 68/1000 [00:16<03:18,  4.69it/s]Ref scores:   7%|â–‹         | 69/1000 [00:16<03:02,  5.09it/s]Ref scores:   7%|â–‹         | 70/1000 [00:16<02:53,  5.36it/s]Ref scores:   7%|â–‹         | 71/1000 [00:17<02:44,  5.63it/s]Ref scores:   7%|â–‹         | 72/1000 [00:17<03:16,  4.73it/s]Ref scores:   7%|â–‹         | 73/1000 [00:17<03:28,  4.45it/s]Ref scores:   7%|â–‹         | 74/1000 [00:17<03:46,  4.09it/s]Ref scores:   8%|â–Š         | 75/1000 [00:18<03:49,  4.04it/s]Ref scores:   8%|â–Š         | 76/1000 [00:18<03:51,  3.99it/s]Ref scores:   8%|â–Š         | 77/1000 [00:18<04:00,  3.83it/s]Ref scores:   8%|â–Š         | 78/1000 [00:18<03:34,  4.30it/s]Ref scores:   8%|â–Š         | 79/1000 [00:18<03:08,  4.89it/s]Ref scores:   8%|â–Š         | 80/1000 [00:19<02:57,  5.19it/s]Ref scores:   8%|â–Š         | 81/1000 [00:19<03:16,  4.68it/s]Ref scores:   8%|â–Š         | 82/1000 [00:19<03:01,  5.05it/s]Ref scores:   8%|â–Š         | 83/1000 [00:19<02:52,  5.32it/s]Ref scores:   8%|â–Š         | 84/1000 [00:19<02:47,  5.46it/s]Ref scores:   8%|â–Š         | 85/1000 [00:20<03:08,  4.85it/s]Ref scores:   9%|â–Š         | 86/1000 [00:20<02:56,  5.16it/s]Ref scores:   9%|â–Š         | 87/1000 [00:20<03:14,  4.71it/s]Ref scores:   9%|â–‰         | 88/1000 [00:20<03:00,  5.04it/s]Ref scores:   9%|â–‰         | 89/1000 [00:21<03:25,  4.42it/s]Ref scores:   9%|â–‰         | 90/1000 [00:21<03:01,  5.01it/s]Ref scores:   9%|â–‰         | 91/1000 [00:21<02:44,  5.53it/s]Ref scores:   9%|â–‰         | 92/1000 [00:21<02:39,  5.68it/s]Ref scores:   9%|â–‰         | 93/1000 [00:21<02:35,  5.82it/s]Ref scores:   9%|â–‰         | 94/1000 [00:21<02:41,  5.60it/s]Ref scores:  10%|â–‰         | 95/1000 [00:21<02:37,  5.74it/s]Ref scores:  10%|â–‰         | 96/1000 [00:22<02:34,  5.85it/s]Ref scores:  10%|â–‰         | 97/1000 [00:22<02:30,  6.00it/s]Ref scores:  10%|â–‰         | 98/1000 [00:22<03:02,  4.94it/s]Ref scores:  10%|â–‰         | 99/1000 [00:22<03:25,  4.39it/s]Ref scores:  10%|â–ˆ         | 100/1000 [00:23<03:06,  4.84it/s]Ref scores:  10%|â–ˆ         | 101/1000 [00:23<02:47,  5.37it/s]Ref scores:  10%|â–ˆ         | 102/1000 [00:23<03:05,  4.85it/s]Ref scores:  10%|â–ˆ         | 103/1000 [00:23<03:01,  4.94it/s]Ref scores:  10%|â–ˆ         | 104/1000 [00:23<02:51,  5.23it/s]Ref scores:  10%|â–ˆ         | 105/1000 [00:23<02:43,  5.47it/s]Ref scores:  11%|â–ˆ         | 106/1000 [00:24<03:04,  4.86it/s]Ref scores:  11%|â–ˆ         | 107/1000 [00:24<02:52,  5.17it/s]Ref scores:  11%|â–ˆ         | 108/1000 [00:24<02:37,  5.66it/s]Ref scores:  11%|â–ˆ         | 109/1000 [00:24<02:32,  5.86it/s]Ref scores:  11%|â–ˆ         | 110/1000 [00:24<02:30,  5.92it/s]Ref scores:  11%|â–ˆ         | 111/1000 [00:25<03:05,  4.78it/s]Ref scores:  11%|â–ˆ         | 112/1000 [00:25<02:53,  5.11it/s]Ref scores:  11%|â–ˆâ–        | 113/1000 [00:25<02:44,  5.40it/s]Ref scores:  11%|â–ˆâ–        | 114/1000 [00:25<02:38,  5.58it/s]Ref scores:  12%|â–ˆâ–        | 115/1000 [00:25<02:28,  5.98it/s]Ref scores:  12%|â–ˆâ–        | 116/1000 [00:25<02:19,  6.33it/s]Ref scores:  12%|â–ˆâ–        | 117/1000 [00:26<02:19,  6.34it/s]Ref scores:  12%|â–ˆâ–        | 118/1000 [00:26<03:04,  4.77it/s]Ref scores:  12%|â–ˆâ–        | 119/1000 [00:26<03:00,  4.88it/s]Ref scores:  12%|â–ˆâ–        | 120/1000 [00:26<03:13,  4.55it/s]Ref scores:  12%|â–ˆâ–        | 121/1000 [00:27<03:22,  4.34it/s]Ref scores:  12%|â–ˆâ–        | 122/1000 [00:27<03:04,  4.76it/s]Ref scores:  12%|â–ˆâ–        | 123/1000 [00:27<02:52,  5.09it/s]Ref scores:  12%|â–ˆâ–        | 124/1000 [00:27<02:41,  5.42it/s]Ref scores:  12%|â–ˆâ–Ž        | 125/1000 [00:27<03:01,  4.82it/s]Ref scores:  13%|â–ˆâ–Ž        | 126/1000 [00:27<02:48,  5.19it/s]Ref scores:  13%|â–ˆâ–Ž        | 127/1000 [00:28<02:40,  5.43it/s]Ref scores:  13%|â–ˆâ–Ž        | 128/1000 [00:28<03:08,  4.63it/s]Ref scores:  13%|â–ˆâ–Ž        | 129/1000 [00:28<02:52,  5.04it/s]Ref scores:  13%|â–ˆâ–Ž        | 130/1000 [00:28<03:07,  4.64it/s]Ref scores:  13%|â–ˆâ–Ž        | 131/1000 [00:29<02:53,  5.01it/s]Ref scores:  13%|â–ˆâ–Ž        | 132/1000 [00:29<02:41,  5.37it/s]Ref scores:  13%|â–ˆâ–Ž        | 133/1000 [00:29<03:09,  4.59it/s]Ref scores:  13%|â–ˆâ–Ž        | 134/1000 [00:29<03:19,  4.34it/s]Ref scores:  14%|â–ˆâ–Ž        | 135/1000 [00:29<03:26,  4.19it/s]Ref scores:  14%|â–ˆâ–Ž        | 136/1000 [00:30<03:00,  4.79it/s]Ref scores:  14%|â–ˆâ–Ž        | 137/1000 [00:30<02:46,  5.19it/s]Ref scores:  14%|â–ˆâ–        | 138/1000 [00:30<02:36,  5.51it/s]Ref scores:  14%|â–ˆâ–        | 139/1000 [00:30<02:39,  5.40it/s]Ref scores:  14%|â–ˆâ–        | 140/1000 [00:30<02:33,  5.60it/s]Ref scores:  14%|â–ˆâ–        | 141/1000 [00:30<02:28,  5.77it/s]Ref scores:  14%|â–ˆâ–        | 142/1000 [00:31<02:26,  5.86it/s]Ref scores:  14%|â–ˆâ–        | 143/1000 [00:31<02:48,  5.10it/s]Ref scores:  14%|â–ˆâ–        | 144/1000 [00:31<03:11,  4.47it/s]Ref scores:  14%|â–ˆâ–        | 145/1000 [00:31<02:54,  4.90it/s]Ref scores:  15%|â–ˆâ–        | 146/1000 [00:31<02:38,  5.40it/s]Ref scores:  15%|â–ˆâ–        | 147/1000 [00:32<02:55,  4.86it/s]Ref scores:  15%|â–ˆâ–        | 148/1000 [00:32<02:37,  5.39it/s]Ref scores:  15%|â–ˆâ–        | 149/1000 [00:32<02:52,  4.94it/s]Ref scores:  15%|â–ˆâ–Œ        | 150/1000 [00:32<02:42,  5.23it/s]Ref scores:  15%|â–ˆâ–Œ        | 151/1000 [00:32<02:35,  5.46it/s]Ref scores:  15%|â–ˆâ–Œ        | 152/1000 [00:33<02:29,  5.69it/s]Ref scores:  15%|â–ˆâ–Œ        | 153/1000 [00:33<02:50,  4.96it/s]Ref scores:  15%|â–ˆâ–Œ        | 154/1000 [00:33<03:05,  4.57it/s]Ref scores:  16%|â–ˆâ–Œ        | 155/1000 [00:33<02:50,  4.95it/s]Ref scores:  16%|â–ˆâ–Œ        | 156/1000 [00:34<03:04,  4.58it/s]Ref scores:  16%|â–ˆâ–Œ        | 157/1000 [00:34<03:21,  4.18it/s]Ref scores:  16%|â–ˆâ–Œ        | 158/1000 [00:34<03:25,  4.10it/s]Ref scores:  16%|â–ˆâ–Œ        | 159/1000 [00:34<03:27,  4.06it/s]Ref scores:  16%|â–ˆâ–Œ        | 160/1000 [00:34<02:59,  4.67it/s]Ref scores:  16%|â–ˆâ–Œ        | 161/1000 [00:35<03:18,  4.24it/s]Ref scores:  16%|â–ˆâ–Œ        | 162/1000 [00:35<03:22,  4.15it/s]Ref scores:  16%|â–ˆâ–‹        | 163/1000 [00:35<03:34,  3.91it/s]Ref scores:  16%|â–ˆâ–‹        | 164/1000 [00:35<03:08,  4.44it/s]Ref scores:  16%|â–ˆâ–‹        | 165/1000 [00:36<02:50,  4.88it/s]Ref scores:  17%|â–ˆâ–‹        | 166/1000 [00:36<02:41,  5.17it/s]Ref scores:  17%|â–ˆâ–‹        | 167/1000 [00:36<02:31,  5.51it/s]Ref scores:  17%|â–ˆâ–‹        | 168/1000 [00:36<02:24,  5.78it/s]Ref scores:  17%|â–ˆâ–‹        | 169/1000 [00:36<02:53,  4.80it/s]Ref scores:  17%|â–ˆâ–‹        | 170/1000 [00:37<02:41,  5.12it/s]Ref scores:  17%|â–ˆâ–‹        | 171/1000 [00:37<02:56,  4.70it/s]Ref scores:  17%|â–ˆâ–‹        | 172/1000 [00:37<02:43,  5.06it/s]Ref scores:  17%|â–ˆâ–‹        | 173/1000 [00:37<02:33,  5.40it/s]Ref scores:  17%|â–ˆâ–‹        | 174/1000 [00:37<02:28,  5.58it/s]Ref scores:  18%|â–ˆâ–Š        | 175/1000 [00:37<02:24,  5.72it/s]Ref scores:  18%|â–ˆâ–Š        | 176/1000 [00:38<02:21,  5.82it/s]Ref scores:  18%|â–ˆâ–Š        | 177/1000 [00:38<02:19,  5.90it/s]Ref scores:  18%|â–ˆâ–Š        | 178/1000 [00:38<02:41,  5.09it/s]Ref scores:  18%|â–ˆâ–Š        | 179/1000 [00:38<02:32,  5.37it/s]Ref scores:  18%|â–ˆâ–Š        | 180/1000 [00:38<02:26,  5.58it/s]Ref scores:  18%|â–ˆâ–Š        | 181/1000 [00:39<02:22,  5.74it/s]Ref scores:  18%|â–ˆâ–Š        | 182/1000 [00:39<02:17,  5.94it/s]Ref scores:  18%|â–ˆâ–Š        | 183/1000 [00:39<02:14,  6.07it/s]Ref scores:  18%|â–ˆâ–Š        | 184/1000 [00:39<02:45,  4.94it/s]Ref scores:  18%|â–ˆâ–Š        | 185/1000 [00:39<02:33,  5.29it/s]Ref scores:  19%|â–ˆâ–Š        | 186/1000 [00:39<02:21,  5.73it/s]Ref scores:  19%|â–ˆâ–Š        | 187/1000 [00:40<02:18,  5.85it/s]Ref scores:  19%|â–ˆâ–‰        | 188/1000 [00:40<02:48,  4.81it/s]Ref scores:  19%|â–ˆâ–‰        | 189/1000 [00:40<02:59,  4.52it/s]Ref scores:  19%|â–ˆâ–‰        | 190/1000 [00:40<03:08,  4.30it/s]Ref scores:  19%|â–ˆâ–‰        | 191/1000 [00:41<02:51,  4.71it/s]Ref scores:  19%|â–ˆâ–‰        | 192/1000 [00:41<02:37,  5.13it/s]Ref scores:  19%|â–ˆâ–‰        | 193/1000 [00:41<02:27,  5.47it/s]Ref scores:  19%|â–ˆâ–‰        | 194/1000 [00:41<02:45,  4.87it/s]Ref scores:  20%|â–ˆâ–‰        | 195/1000 [00:41<02:35,  5.18it/s]Ref scores:  20%|â–ˆâ–‰        | 196/1000 [00:42<03:00,  4.46it/s]Ref scores:  20%|â–ˆâ–‰        | 197/1000 [00:42<02:47,  4.79it/s]Ref scores:  20%|â–ˆâ–‰        | 198/1000 [00:42<03:08,  4.26it/s]Ref scores:  20%|â–ˆâ–‰        | 199/1000 [00:42<02:50,  4.69it/s]Ref scores:  20%|â–ˆâ–ˆ        | 200/1000 [00:42<02:38,  5.06it/s]Ref scores:  20%|â–ˆâ–ˆ        | 201/1000 [00:43<02:29,  5.33it/s]Ref scores:  20%|â–ˆâ–ˆ        | 202/1000 [00:43<02:21,  5.62it/s]Ref scores:  20%|â–ˆâ–ˆ        | 203/1000 [00:43<02:49,  4.71it/s]Ref scores:  20%|â–ˆâ–ˆ        | 204/1000 [00:43<02:44,  4.83it/s]Ref scores:  20%|â–ˆâ–ˆ        | 205/1000 [00:43<02:32,  5.21it/s]Ref scores:  21%|â–ˆâ–ˆ        | 206/1000 [00:44<02:47,  4.73it/s]Ref scores:  21%|â–ˆâ–ˆ        | 207/1000 [00:44<02:34,  5.12it/s]Ref scores:  21%|â–ˆâ–ˆ        | 208/1000 [00:44<02:29,  5.29it/s]Ref scores:  21%|â–ˆâ–ˆ        | 209/1000 [00:44<02:53,  4.56it/s]Ref scores:  21%|â–ˆâ–ˆ        | 210/1000 [00:44<02:37,  5.01it/s]Ref scores:  21%|â–ˆâ–ˆ        | 211/1000 [00:45<02:50,  4.63it/s]Ref scores:  21%|â–ˆâ–ˆ        | 212/1000 [00:45<02:44,  4.78it/s]Ref scores:  21%|â–ˆâ–ˆâ–       | 213/1000 [00:45<02:40,  4.90it/s]Ref scores:  21%|â–ˆâ–ˆâ–       | 214/1000 [00:45<03:02,  4.32it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 215/1000 [00:45<02:43,  4.80it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 216/1000 [00:46<03:03,  4.28it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 217/1000 [00:46<02:46,  4.70it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 218/1000 [00:46<03:06,  4.19it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 219/1000 [00:46<02:48,  4.63it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 220/1000 [00:47<03:06,  4.18it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 221/1000 [00:47<02:50,  4.57it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 222/1000 [00:47<03:05,  4.19it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 223/1000 [00:47<02:42,  4.79it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 224/1000 [00:47<02:31,  5.13it/s]Ref scores:  22%|â–ˆâ–ˆâ–Ž       | 225/1000 [00:48<02:23,  5.38it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 226/1000 [00:48<02:41,  4.81it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 227/1000 [00:48<02:51,  4.51it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 228/1000 [00:48<02:37,  4.89it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 229/1000 [00:48<02:28,  5.21it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 230/1000 [00:49<02:43,  4.71it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 231/1000 [00:49<02:31,  5.07it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 232/1000 [00:49<02:45,  4.64it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 233/1000 [00:49<02:54,  4.39it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 234/1000 [00:50<02:39,  4.80it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 235/1000 [00:50<02:29,  5.13it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 236/1000 [00:50<02:42,  4.69it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 237/1000 [00:50<03:00,  4.22it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 238/1000 [00:50<02:43,  4.65it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 239/1000 [00:51<02:26,  5.19it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 240/1000 [00:51<02:41,  4.70it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 241/1000 [00:51<02:30,  5.03it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 242/1000 [00:51<02:51,  4.41it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 243/1000 [00:52<03:10,  3.98it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 244/1000 [00:52<03:20,  3.78it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 245/1000 [00:52<02:56,  4.27it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 246/1000 [00:52<02:40,  4.71it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 247/1000 [00:52<02:35,  4.84it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 248/1000 [00:53<02:43,  4.61it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 249/1000 [00:53<02:29,  5.04it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 250/1000 [00:53<02:20,  5.32it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 251/1000 [00:53<02:36,  4.80it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 252/1000 [00:53<02:23,  5.21it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 253/1000 [00:54<02:15,  5.50it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 254/1000 [00:54<02:09,  5.75it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 255/1000 [00:54<02:05,  5.94it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 256/1000 [00:54<02:04,  5.99it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 257/1000 [00:54<02:23,  5.18it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 258/1000 [00:54<02:36,  4.74it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 259/1000 [00:55<02:46,  4.45it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 260/1000 [00:55<02:33,  4.83it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 261/1000 [00:55<02:43,  4.51it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 262/1000 [00:55<02:30,  4.89it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 263/1000 [00:55<02:21,  5.21it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 264/1000 [00:56<02:44,  4.49it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 265/1000 [00:56<02:50,  4.31it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 266/1000 [00:56<02:54,  4.20it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 267/1000 [00:56<02:36,  4.69it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 268/1000 [00:57<02:25,  5.03it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 269/1000 [00:57<02:17,  5.32it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 270/1000 [00:57<02:06,  5.77it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 271/1000 [00:57<02:04,  5.86it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 272/1000 [00:57<02:00,  6.03it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 273/1000 [00:58<02:28,  4.91it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 274/1000 [00:58<02:19,  5.20it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 275/1000 [00:58<02:33,  4.72it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 276/1000 [00:58<02:42,  4.45it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 277/1000 [00:58<02:48,  4.30it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 278/1000 [00:59<02:33,  4.72it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 279/1000 [00:59<02:51,  4.21it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 280/1000 [00:59<02:55,  4.11it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 281/1000 [00:59<02:37,  4.56it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 282/1000 [00:59<02:23,  5.01it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 283/1000 [01:00<02:15,  5.30it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 284/1000 [01:00<02:38,  4.53it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 285/1000 [01:00<02:44,  4.34it/s]Ref scores:  29%|â–ˆâ–ˆâ–Š       | 286/1000 [01:00<02:36,  4.56it/s]Ref scores:  29%|â–ˆâ–ˆâ–Š       | 287/1000 [01:01<02:23,  4.98it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 288/1000 [01:01<02:41,  4.42it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 289/1000 [01:01<02:46,  4.27it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 290/1000 [01:01<02:58,  3.99it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 291/1000 [01:02<03:06,  3.79it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 292/1000 [01:02<02:45,  4.28it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 293/1000 [01:02<02:30,  4.69it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 294/1000 [01:02<02:48,  4.20it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 295/1000 [01:02<02:27,  4.79it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 296/1000 [01:03<02:15,  5.20it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 297/1000 [01:03<02:07,  5.49it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 298/1000 [01:03<02:03,  5.66it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 299/1000 [01:03<01:59,  5.85it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 300/1000 [01:03<02:18,  5.05it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 301/1000 [01:04<02:11,  5.31it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 302/1000 [01:04<02:06,  5.54it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 303/1000 [01:04<02:02,  5.71it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 304/1000 [01:04<02:26,  4.74it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 305/1000 [01:04<02:36,  4.45it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 306/1000 [01:05<02:22,  4.88it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 307/1000 [01:05<02:33,  4.51it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 308/1000 [01:05<02:19,  4.95it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 309/1000 [01:05<02:11,  5.24it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 310/1000 [01:05<02:22,  4.86it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 311/1000 [01:06<02:11,  5.25it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 312/1000 [01:06<02:25,  4.72it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆâ–      | 313/1000 [01:06<02:33,  4.49it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆâ–      | 314/1000 [01:06<02:40,  4.28it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 315/1000 [01:07<02:51,  4.00it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 316/1000 [01:07<02:33,  4.46it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 317/1000 [01:07<02:19,  4.91it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 318/1000 [01:07<02:37,  4.33it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 319/1000 [01:07<02:18,  4.92it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 320/1000 [01:07<02:10,  5.23it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 321/1000 [01:08<02:30,  4.51it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 322/1000 [01:08<02:44,  4.12it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 323/1000 [01:08<02:26,  4.61it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 324/1000 [01:08<02:13,  5.05it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 325/1000 [01:09<02:04,  5.41it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 326/1000 [01:09<02:00,  5.61it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 327/1000 [01:09<01:56,  5.76it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 328/1000 [01:09<01:54,  5.87it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 329/1000 [01:09<01:51,  6.04it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 330/1000 [01:09<02:17,  4.87it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 331/1000 [01:10<02:09,  5.18it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 332/1000 [01:10<02:21,  4.72it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 333/1000 [01:10<02:37,  4.24it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 334/1000 [01:10<02:42,  4.11it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 335/1000 [01:11<02:23,  4.64it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 336/1000 [01:11<02:10,  5.11it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 337/1000 [01:11<01:57,  5.65it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 338/1000 [01:11<01:53,  5.83it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 339/1000 [01:11<02:10,  5.05it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 340/1000 [01:11<02:00,  5.46it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 341/1000 [01:12<01:55,  5.71it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 342/1000 [01:12<01:51,  5.90it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 343/1000 [01:12<01:44,  6.30it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 344/1000 [01:12<01:41,  6.44it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 345/1000 [01:12<01:41,  6.44it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 346/1000 [01:12<01:41,  6.43it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 347/1000 [01:13<02:01,  5.39it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 348/1000 [01:13<02:15,  4.81it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 349/1000 [01:13<02:03,  5.26it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 350/1000 [01:13<01:56,  5.56it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 351/1000 [01:13<02:12,  4.91it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 352/1000 [01:14<01:57,  5.50it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 353/1000 [01:14<02:12,  4.87it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 354/1000 [01:14<02:28,  4.35it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 355/1000 [01:14<02:47,  3.84it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 356/1000 [01:15<02:46,  3.86it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 357/1000 [01:15<02:53,  3.70it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 358/1000 [01:15<02:50,  3.76it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 359/1000 [01:16<02:56,  3.63it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 360/1000 [01:16<02:53,  3.69it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 361/1000 [01:16<02:50,  3.74it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 362/1000 [01:16<02:27,  4.31it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 363/1000 [01:16<02:32,  4.17it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 364/1000 [01:17<02:42,  3.93it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 365/1000 [01:17<02:21,  4.49it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 366/1000 [01:17<02:08,  4.93it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 367/1000 [01:17<01:55,  5.50it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 368/1000 [01:17<01:48,  5.81it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 369/1000 [01:18<02:04,  5.08it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 370/1000 [01:18<01:55,  5.48it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 371/1000 [01:18<01:48,  5.79it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 372/1000 [01:18<01:45,  5.96it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 373/1000 [01:18<02:09,  4.86it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 374/1000 [01:19<01:59,  5.24it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 375/1000 [01:19<01:52,  5.55it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 376/1000 [01:19<01:48,  5.78it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 377/1000 [01:19<02:08,  4.84it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 378/1000 [01:19<01:54,  5.42it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 379/1000 [01:19<01:49,  5.67it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 380/1000 [01:20<02:11,  4.72it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 381/1000 [01:20<02:00,  5.12it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 382/1000 [01:20<02:12,  4.67it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 383/1000 [01:20<02:01,  5.09it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 384/1000 [01:20<01:53,  5.43it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 385/1000 [01:21<02:07,  4.83it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 386/1000 [01:21<02:13,  4.61it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 387/1000 [01:21<02:02,  5.02it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 388/1000 [01:21<01:52,  5.42it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 389/1000 [01:21<02:05,  4.86it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 390/1000 [01:22<02:15,  4.52it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 391/1000 [01:22<02:01,  5.00it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 392/1000 [01:22<02:11,  4.61it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 393/1000 [01:22<01:59,  5.08it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 394/1000 [01:22<01:50,  5.49it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 395/1000 [01:23<02:03,  4.89it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 396/1000 [01:23<02:18,  4.37it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 397/1000 [01:23<02:04,  4.83it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 398/1000 [01:23<01:54,  5.28it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 399/1000 [01:24<02:12,  4.53it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 400/1000 [01:24<02:19,  4.31it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 401/1000 [01:24<02:05,  4.78it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 402/1000 [01:24<02:20,  4.25it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 403/1000 [01:24<02:06,  4.72it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 404/1000 [01:25<01:55,  5.16it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 405/1000 [01:25<02:08,  4.64it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 406/1000 [01:25<01:53,  5.25it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 407/1000 [01:25<01:43,  5.75it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 408/1000 [01:25<01:39,  5.97it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 409/1000 [01:25<01:36,  6.15it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 410/1000 [01:26<01:33,  6.28it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 411/1000 [01:26<01:58,  4.97it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 412/1000 [01:26<02:15,  4.35it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 413/1000 [01:26<02:27,  3.99it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 414/1000 [01:27<02:10,  4.50it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 415/1000 [01:27<01:58,  4.93it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 416/1000 [01:27<01:46,  5.47it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 417/1000 [01:27<01:38,  5.92it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 418/1000 [01:27<01:53,  5.11it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 419/1000 [01:27<01:42,  5.65it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 420/1000 [01:28<01:56,  4.97it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 421/1000 [01:28<02:12,  4.38it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 422/1000 [01:28<01:55,  5.00it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 423/1000 [01:28<01:43,  5.55it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 424/1000 [01:29<02:03,  4.67it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 425/1000 [01:29<01:52,  5.12it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 426/1000 [01:29<01:46,  5.41it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 427/1000 [01:29<01:41,  5.67it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 428/1000 [01:29<01:36,  5.92it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 429/1000 [01:29<01:58,  4.81it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 430/1000 [01:30<01:49,  5.19it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 431/1000 [01:30<01:39,  5.74it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 432/1000 [01:30<01:53,  5.00it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 433/1000 [01:30<01:46,  5.33it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 434/1000 [01:30<02:04,  4.55it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 435/1000 [01:31<02:17,  4.12it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 436/1000 [01:31<02:20,  4.02it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 437/1000 [01:31<02:21,  3.98it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 438/1000 [01:31<02:04,  4.53it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 439/1000 [01:32<01:52,  4.98it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 440/1000 [01:32<01:45,  5.33it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 441/1000 [01:32<01:44,  5.34it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 442/1000 [01:32<01:38,  5.67it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 443/1000 [01:32<01:34,  5.88it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 444/1000 [01:32<01:31,  6.10it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 445/1000 [01:33<01:29,  6.17it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 446/1000 [01:33<01:46,  5.20it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 447/1000 [01:33<02:09,  4.27it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 448/1000 [01:33<01:52,  4.90it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 449/1000 [01:33<01:44,  5.29it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 450/1000 [01:34<01:37,  5.63it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 451/1000 [01:34<01:58,  4.64it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 452/1000 [01:34<01:44,  5.24it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 453/1000 [01:34<01:55,  4.72it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 454/1000 [01:34<01:45,  5.15it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 455/1000 [01:35<01:40,  5.44it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 456/1000 [01:35<01:31,  5.92it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 457/1000 [01:35<01:46,  5.09it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 458/1000 [01:35<01:39,  5.47it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 459/1000 [01:35<01:35,  5.69it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 460/1000 [01:35<01:30,  5.95it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 461/1000 [01:36<01:51,  4.83it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 462/1000 [01:36<02:00,  4.48it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 463/1000 [01:36<01:49,  4.91it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 464/1000 [01:36<01:40,  5.33it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 465/1000 [01:37<01:35,  5.60it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 466/1000 [01:37<01:48,  4.91it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 467/1000 [01:37<02:03,  4.33it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 468/1000 [01:37<01:51,  4.79it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 469/1000 [01:37<01:42,  5.16it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 470/1000 [01:38<01:58,  4.49it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 471/1000 [01:38<01:46,  4.96it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 472/1000 [01:38<01:38,  5.35it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 473/1000 [01:38<01:33,  5.66it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 474/1000 [01:38<01:26,  6.08it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 475/1000 [01:39<01:41,  5.20it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 476/1000 [01:39<01:51,  4.71it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 477/1000 [01:39<01:38,  5.28it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 478/1000 [01:39<01:33,  5.57it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 479/1000 [01:39<01:30,  5.77it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 480/1000 [01:39<01:44,  5.00it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 481/1000 [01:40<01:33,  5.56it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 482/1000 [01:40<01:28,  5.85it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 483/1000 [01:40<01:25,  6.03it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 484/1000 [01:40<01:45,  4.90it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 485/1000 [01:41<01:58,  4.34it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 486/1000 [01:41<02:08,  4.00it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 487/1000 [01:41<01:53,  4.54it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 488/1000 [01:41<01:42,  4.99it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 489/1000 [01:41<01:32,  5.52it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 490/1000 [01:41<01:28,  5.74it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 491/1000 [01:42<01:25,  5.96it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 492/1000 [01:42<01:43,  4.91it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 493/1000 [01:42<01:32,  5.46it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 494/1000 [01:42<01:25,  5.92it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 495/1000 [01:42<01:23,  6.04it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 496/1000 [01:43<01:42,  4.92it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 497/1000 [01:43<01:55,  4.36it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 498/1000 [01:43<01:59,  4.19it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 499/1000 [01:43<01:47,  4.66it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 500/1000 [01:44<01:54,  4.38it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 501/1000 [01:44<01:43,  4.83it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 502/1000 [01:44<01:50,  4.49it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 503/1000 [01:44<01:45,  4.71it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 504/1000 [01:44<01:51,  4.43it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 505/1000 [01:45<01:40,  4.93it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 506/1000 [01:45<01:33,  5.30it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 507/1000 [01:45<01:28,  5.58it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 508/1000 [01:45<01:45,  4.68it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 509/1000 [01:45<01:35,  5.14it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 510/1000 [01:45<01:28,  5.51it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 511/1000 [01:46<01:25,  5.74it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 512/1000 [01:46<01:22,  5.91it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 513/1000 [01:46<01:40,  4.86it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 514/1000 [01:46<01:47,  4.53it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 515/1000 [01:46<01:36,  5.01it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 516/1000 [01:47<01:30,  5.33it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 517/1000 [01:47<01:25,  5.67it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 518/1000 [01:47<01:37,  4.97it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 519/1000 [01:47<01:30,  5.32it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 520/1000 [01:47<01:25,  5.58it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 521/1000 [01:48<01:37,  4.90it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 522/1000 [01:48<01:30,  5.25it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 523/1000 [01:48<01:26,  5.53it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 524/1000 [01:48<01:38,  4.86it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 525/1000 [01:48<01:30,  5.26it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 526/1000 [01:49<01:25,  5.52it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 527/1000 [01:49<01:36,  4.91it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 528/1000 [01:49<01:29,  5.26it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 529/1000 [01:49<01:23,  5.61it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 530/1000 [01:49<01:21,  5.80it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 531/1000 [01:49<01:32,  5.08it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 532/1000 [01:50<01:25,  5.46it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 533/1000 [01:50<01:41,  4.61it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 534/1000 [01:50<01:32,  5.02it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 535/1000 [01:50<01:40,  4.61it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 536/1000 [01:51<01:46,  4.37it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 537/1000 [01:51<01:34,  4.87it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 538/1000 [01:51<01:27,  5.30it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 539/1000 [01:51<01:21,  5.63it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 540/1000 [01:51<01:33,  4.92it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 541/1000 [01:51<01:27,  5.27it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 542/1000 [01:52<01:21,  5.60it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 543/1000 [01:52<01:22,  5.51it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 544/1000 [01:52<01:18,  5.80it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 545/1000 [01:52<01:30,  5.04it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 546/1000 [01:52<01:23,  5.42it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 547/1000 [01:53<01:38,  4.61it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 548/1000 [01:53<01:27,  5.19it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 549/1000 [01:53<01:26,  5.22it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 550/1000 [01:53<01:20,  5.58it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 551/1000 [01:53<01:31,  4.92it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 552/1000 [01:54<01:24,  5.27it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 553/1000 [01:54<01:38,  4.54it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 554/1000 [01:54<01:43,  4.30it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 555/1000 [01:54<01:32,  4.81it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 556/1000 [01:55<01:39,  4.46it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 557/1000 [01:55<01:43,  4.26it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 558/1000 [01:55<01:29,  4.92it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 559/1000 [01:55<01:20,  5.45it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 560/1000 [01:55<01:17,  5.68it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 561/1000 [01:55<01:14,  5.92it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 562/1000 [01:56<01:30,  4.84it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 563/1000 [01:56<01:41,  4.30it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 564/1000 [01:56<01:45,  4.15it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 565/1000 [01:56<01:32,  4.68it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 566/1000 [01:57<01:38,  4.42it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 567/1000 [01:57<01:29,  4.86it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 568/1000 [01:57<01:36,  4.47it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 569/1000 [01:57<01:27,  4.90it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 570/1000 [01:57<01:21,  5.25it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 571/1000 [01:58<01:30,  4.74it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 572/1000 [01:58<01:40,  4.24it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 573/1000 [01:58<01:27,  4.89it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 574/1000 [01:58<01:21,  5.23it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 575/1000 [01:58<01:16,  5.57it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 576/1000 [01:59<01:13,  5.76it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 577/1000 [01:59<01:11,  5.91it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 578/1000 [01:59<01:23,  5.07it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 579/1000 [01:59<01:17,  5.45it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 580/1000 [01:59<01:14,  5.67it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 581/1000 [02:00<01:28,  4.71it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 582/1000 [02:00<01:39,  4.21it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 583/1000 [02:00<01:29,  4.68it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 584/1000 [02:00<01:39,  4.19it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 585/1000 [02:00<01:26,  4.81it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 586/1000 [02:01<01:19,  5.24it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 587/1000 [02:01<01:14,  5.52it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 588/1000 [02:01<01:10,  5.81it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 589/1000 [02:01<01:08,  6.03it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 590/1000 [02:01<01:06,  6.20it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 591/1000 [02:01<01:04,  6.34it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 592/1000 [02:02<01:04,  6.31it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 593/1000 [02:02<01:03,  6.40it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 594/1000 [02:02<01:03,  6.36it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 595/1000 [02:02<01:01,  6.63it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 596/1000 [02:02<01:18,  5.17it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 597/1000 [02:03<01:30,  4.48it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 598/1000 [02:03<01:41,  3.94it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 599/1000 [02:03<01:27,  4.60it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 600/1000 [02:03<01:19,  5.06it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 601/1000 [02:03<01:25,  4.65it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 602/1000 [02:04<01:18,  5.05it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 603/1000 [02:04<01:26,  4.61it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 604/1000 [02:04<01:30,  4.37it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 605/1000 [02:04<01:33,  4.22it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 606/1000 [02:05<01:27,  4.51it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 607/1000 [02:05<01:19,  4.91it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 608/1000 [02:05<01:14,  5.25it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 609/1000 [02:05<01:21,  4.77it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 610/1000 [02:05<01:31,  4.25it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 611/1000 [02:06<01:19,  4.90it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 612/1000 [02:06<01:29,  4.33it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 613/1000 [02:06<01:20,  4.79it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 614/1000 [02:06<01:18,  4.93it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 615/1000 [02:06<01:12,  5.29it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 616/1000 [02:07<01:21,  4.74it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 617/1000 [02:07<01:14,  5.12it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 618/1000 [02:07<01:09,  5.49it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 619/1000 [02:07<01:21,  4.67it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 620/1000 [02:07<01:18,  4.85it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 621/1000 [02:08<01:24,  4.48it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 622/1000 [02:08<01:32,  4.11it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 623/1000 [02:08<01:21,  4.63it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 624/1000 [02:08<01:29,  4.19it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 625/1000 [02:09<01:20,  4.67it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 626/1000 [02:09<01:13,  5.06it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 627/1000 [02:09<01:24,  4.40it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 628/1000 [02:09<01:16,  4.83it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 629/1000 [02:09<01:08,  5.38it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 630/1000 [02:09<01:05,  5.64it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 631/1000 [02:10<01:14,  4.93it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 632/1000 [02:10<01:09,  5.27it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 633/1000 [02:10<01:06,  5.53it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 634/1000 [02:10<01:03,  5.73it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 635/1000 [02:10<00:59,  6.14it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 636/1000 [02:10<00:58,  6.27it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 637/1000 [02:11<00:57,  6.29it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 638/1000 [02:11<01:08,  5.29it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 639/1000 [02:11<01:19,  4.53it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 640/1000 [02:11<01:27,  4.10it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 641/1000 [02:12<01:17,  4.64it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 642/1000 [02:12<01:08,  5.22it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 643/1000 [02:12<01:04,  5.50it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 644/1000 [02:12<01:01,  5.79it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 645/1000 [02:12<01:15,  4.73it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 646/1000 [02:13<01:23,  4.23it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 647/1000 [02:13<01:29,  3.96it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 648/1000 [02:13<01:29,  3.92it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 649/1000 [02:14<01:33,  3.74it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 650/1000 [02:14<01:35,  3.65it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 651/1000 [02:14<01:26,  4.04it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 652/1000 [02:14<01:15,  4.60it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 653/1000 [02:14<01:09,  5.00it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 654/1000 [02:15<01:19,  4.34it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 655/1000 [02:15<01:09,  4.98it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 656/1000 [02:15<01:18,  4.40it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 657/1000 [02:15<01:21,  4.22it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 658/1000 [02:15<01:12,  4.70it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 659/1000 [02:16<01:16,  4.44it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 660/1000 [02:16<01:09,  4.88it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 661/1000 [02:16<01:04,  5.25it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 662/1000 [02:16<01:00,  5.60it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 663/1000 [02:16<01:11,  4.68it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 664/1000 [02:17<01:03,  5.29it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 665/1000 [02:17<01:12,  4.59it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 666/1000 [02:17<01:16,  4.38it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 667/1000 [02:17<01:19,  4.20it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 668/1000 [02:18<01:10,  4.68it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 669/1000 [02:18<01:04,  5.12it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 670/1000 [02:18<00:58,  5.64it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 671/1000 [02:18<00:56,  5.83it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 672/1000 [02:18<00:54,  5.98it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 673/1000 [02:18<00:51,  6.35it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 674/1000 [02:19<01:01,  5.30it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 675/1000 [02:19<01:11,  4.52it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 676/1000 [02:19<01:04,  5.02it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 677/1000 [02:19<01:10,  4.59it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 678/1000 [02:19<01:14,  4.32it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 679/1000 [02:20<01:07,  4.79it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 680/1000 [02:20<00:59,  5.35it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 681/1000 [02:20<00:56,  5.69it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 682/1000 [02:20<00:52,  6.10it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 683/1000 [02:20<01:01,  5.16it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 684/1000 [02:21<01:10,  4.45it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 685/1000 [02:21<01:13,  4.27it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 686/1000 [02:21<01:09,  4.54it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 687/1000 [02:21<01:12,  4.31it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 688/1000 [02:21<01:05,  4.77it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 689/1000 [02:22<01:09,  4.45it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 690/1000 [02:22<01:02,  4.93it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 691/1000 [02:22<00:56,  5.47it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 692/1000 [02:22<00:53,  5.71it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 693/1000 [02:22<01:01,  4.95it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 694/1000 [02:23<01:00,  5.06it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 695/1000 [02:23<00:56,  5.35it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 696/1000 [02:23<01:03,  4.77it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 697/1000 [02:23<00:58,  5.15it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 698/1000 [02:23<00:54,  5.51it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 699/1000 [02:24<00:55,  5.45it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 700/1000 [02:24<00:52,  5.74it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 701/1000 [02:24<00:50,  5.90it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 702/1000 [02:24<00:49,  5.99it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 703/1000 [02:24<00:57,  5.15it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 704/1000 [02:24<00:53,  5.52it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 705/1000 [02:25<00:51,  5.74it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 706/1000 [02:25<00:47,  6.14it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 707/1000 [02:25<00:56,  5.17it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 708/1000 [02:25<00:53,  5.45it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 709/1000 [02:25<00:51,  5.68it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 710/1000 [02:25<00:49,  5.85it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 711/1000 [02:26<00:47,  6.07it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 712/1000 [02:26<00:46,  6.14it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 713/1000 [02:26<00:45,  6.28it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 714/1000 [02:26<00:53,  5.30it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 715/1000 [02:26<00:59,  4.76it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 716/1000 [02:27<01:06,  4.25it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 717/1000 [02:27<00:59,  4.75it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 718/1000 [02:27<00:52,  5.35it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 719/1000 [02:27<00:49,  5.67it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 720/1000 [02:27<00:56,  4.95it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 721/1000 [02:28<00:51,  5.37it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 722/1000 [02:28<01:00,  4.57it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 723/1000 [02:28<01:04,  4.31it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 724/1000 [02:28<00:58,  4.72it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 725/1000 [02:29<01:02,  4.40it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 726/1000 [02:29<01:04,  4.24it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 727/1000 [02:29<00:58,  4.70it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 728/1000 [02:29<00:51,  5.27it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 729/1000 [02:29<00:56,  4.77it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 730/1000 [02:30<00:50,  5.34it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 731/1000 [02:30<01:02,  4.32it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 732/1000 [02:30<00:54,  4.90it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 733/1000 [02:30<00:58,  4.54it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 734/1000 [02:30<00:53,  5.00it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 735/1000 [02:31<00:49,  5.34it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 736/1000 [02:31<00:47,  5.58it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 737/1000 [02:31<00:53,  4.93it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 738/1000 [02:31<01:03,  4.13it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 739/1000 [02:31<00:54,  4.77it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 740/1000 [02:32<00:58,  4.42it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 741/1000 [02:32<00:53,  4.85it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 742/1000 [02:32<00:49,  5.22it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 743/1000 [02:32<00:57,  4.47it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 744/1000 [02:33<01:02,  4.08it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 745/1000 [02:33<00:55,  4.62it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 746/1000 [02:33<00:50,  5.06it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 747/1000 [02:33<00:46,  5.43it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 748/1000 [02:33<00:44,  5.73it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 749/1000 [02:33<00:42,  5.86it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 750/1000 [02:34<00:39,  6.29it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 751/1000 [02:34<00:39,  6.30it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 752/1000 [02:34<00:46,  5.33it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 753/1000 [02:34<00:54,  4.57it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 754/1000 [02:35<00:59,  4.13it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 755/1000 [02:35<00:53,  4.61it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 756/1000 [02:35<00:48,  5.05it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 757/1000 [02:35<00:43,  5.57it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 758/1000 [02:35<00:40,  6.02it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 759/1000 [02:35<00:39,  6.11it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 760/1000 [02:36<00:46,  5.20it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 761/1000 [02:36<00:42,  5.56it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 762/1000 [02:36<00:39,  6.02it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 763/1000 [02:36<00:37,  6.37it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 764/1000 [02:36<00:44,  5.36it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 765/1000 [02:36<00:48,  4.82it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 766/1000 [02:37<00:51,  4.52it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 767/1000 [02:37<00:45,  5.11it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 768/1000 [02:37<00:41,  5.63it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 769/1000 [02:37<00:38,  6.07it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 770/1000 [02:37<00:37,  6.20it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 771/1000 [02:37<00:36,  6.23it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 772/1000 [02:38<00:36,  6.25it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 773/1000 [02:38<00:43,  5.26it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 774/1000 [02:38<00:47,  4.76it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 775/1000 [02:38<00:41,  5.37it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 776/1000 [02:38<00:39,  5.69it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 777/1000 [02:39<00:37,  5.91it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 778/1000 [02:39<00:35,  6.27it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 779/1000 [02:39<00:33,  6.58it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 780/1000 [02:39<00:33,  6.51it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 781/1000 [02:39<00:33,  6.51it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 782/1000 [02:39<00:42,  5.09it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 783/1000 [02:40<00:39,  5.46it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 784/1000 [02:40<00:37,  5.74it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 785/1000 [02:40<00:42,  5.02it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 786/1000 [02:40<00:39,  5.40it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 787/1000 [02:40<00:37,  5.64it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 788/1000 [02:40<00:36,  5.82it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 789/1000 [02:41<00:35,  5.96it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 790/1000 [02:41<00:34,  6.12it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 791/1000 [02:41<00:40,  5.21it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 792/1000 [02:41<00:44,  4.69it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 793/1000 [02:42<00:49,  4.16it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 794/1000 [02:42<00:43,  4.69it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 795/1000 [02:42<00:49,  4.17it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 796/1000 [02:42<00:43,  4.66it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 797/1000 [02:42<00:46,  4.39it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 798/1000 [02:43<00:41,  4.83it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 799/1000 [02:43<00:38,  5.27it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 800/1000 [02:43<00:35,  5.61it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 801/1000 [02:43<00:40,  4.92it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 802/1000 [02:43<00:43,  4.56it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 803/1000 [02:44<00:39,  5.04it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 804/1000 [02:44<00:44,  4.38it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 805/1000 [02:44<00:39,  4.88it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 806/1000 [02:44<00:42,  4.53it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 807/1000 [02:44<00:38,  5.01it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 808/1000 [02:45<00:34,  5.54it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 809/1000 [02:45<00:38,  4.92it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 810/1000 [02:45<00:37,  5.03it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 811/1000 [02:45<00:43,  4.39it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 812/1000 [02:45<00:38,  4.90it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 813/1000 [02:46<00:35,  5.26it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 814/1000 [02:46<00:33,  5.56it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 815/1000 [02:46<00:33,  5.48it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 816/1000 [02:46<00:31,  5.79it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 817/1000 [02:46<00:36,  5.06it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 818/1000 [02:47<00:33,  5.43it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 819/1000 [02:47<00:31,  5.66it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 820/1000 [02:47<00:30,  5.92it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 821/1000 [02:47<00:35,  5.10it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 822/1000 [02:47<00:32,  5.46it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 823/1000 [02:48<00:36,  4.85it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 824/1000 [02:48<00:40,  4.30it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 825/1000 [02:48<00:42,  4.14it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 826/1000 [02:48<00:45,  3.86it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 827/1000 [02:49<00:38,  4.44it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 828/1000 [02:49<00:35,  4.91it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 829/1000 [02:49<00:31,  5.49it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 830/1000 [02:49<00:29,  5.71it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 831/1000 [02:49<00:33,  4.98it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 832/1000 [02:50<00:38,  4.37it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 833/1000 [02:50<00:34,  4.86it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 834/1000 [02:50<00:31,  5.24it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 835/1000 [02:50<00:29,  5.58it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 836/1000 [02:50<00:36,  4.44it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 837/1000 [02:51<00:38,  4.26it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 838/1000 [02:51<00:41,  3.95it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 839/1000 [02:51<00:35,  4.49it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 840/1000 [02:51<00:39,  4.09it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 841/1000 [02:51<00:34,  4.58it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 842/1000 [02:52<00:36,  4.36it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 843/1000 [02:52<00:32,  4.82it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 844/1000 [02:52<00:30,  5.19it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 845/1000 [02:52<00:34,  4.48it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 846/1000 [02:53<00:31,  4.95it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 847/1000 [02:53<00:29,  5.28it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 848/1000 [02:53<00:33,  4.52it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 849/1000 [02:53<00:36,  4.13it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 850/1000 [02:54<00:36,  4.06it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 851/1000 [02:54<00:37,  4.01it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 852/1000 [02:54<00:32,  4.54it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 853/1000 [02:54<00:33,  4.33it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 854/1000 [02:54<00:30,  4.79it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 855/1000 [02:54<00:28,  5.15it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 856/1000 [02:55<00:33,  4.27it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 857/1000 [02:55<00:35,  4.00it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 858/1000 [02:55<00:32,  4.33it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 859/1000 [02:56<00:33,  4.19it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 860/1000 [02:56<00:29,  4.71it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 861/1000 [02:56<00:27,  5.10it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 862/1000 [02:56<00:25,  5.41it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 863/1000 [02:56<00:27,  4.95it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 864/1000 [02:56<00:25,  5.34it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 865/1000 [02:57<00:24,  5.59it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 866/1000 [02:57<00:27,  4.92it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 867/1000 [02:57<00:29,  4.53it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 868/1000 [02:57<00:33,  3.95it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 869/1000 [02:58<00:33,  3.94it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 870/1000 [02:58<00:29,  4.45it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 871/1000 [02:58<00:31,  4.06it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 872/1000 [02:58<00:28,  4.42it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 873/1000 [02:58<00:25,  4.92it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 874/1000 [02:59<00:28,  4.37it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 875/1000 [02:59<00:25,  4.83it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 876/1000 [02:59<00:22,  5.40it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 877/1000 [02:59<00:26,  4.61it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 878/1000 [03:00<00:27,  4.38it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 879/1000 [03:00<00:28,  4.31it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 880/1000 [03:00<00:24,  4.82it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 881/1000 [03:00<00:22,  5.19it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 882/1000 [03:00<00:21,  5.56it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 883/1000 [03:01<00:24,  4.69it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 884/1000 [03:01<00:26,  4.42it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 885/1000 [03:01<00:22,  5.04it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 886/1000 [03:01<00:21,  5.36it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 887/1000 [03:01<00:24,  4.58it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 888/1000 [03:02<00:22,  5.05it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 889/1000 [03:02<00:24,  4.61it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 890/1000 [03:02<00:26,  4.18it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 891/1000 [03:02<00:22,  4.80it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 892/1000 [03:03<00:24,  4.47it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 893/1000 [03:03<00:21,  5.07it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 894/1000 [03:03<00:24,  4.41it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 895/1000 [03:03<00:21,  4.85it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 896/1000 [03:03<00:24,  4.26it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 897/1000 [03:04<00:21,  4.76it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 898/1000 [03:04<00:19,  5.13it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 899/1000 [03:04<00:17,  5.66it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 900/1000 [03:04<00:21,  4.72it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 901/1000 [03:04<00:19,  5.10it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 902/1000 [03:04<00:17,  5.47it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 903/1000 [03:05<00:17,  5.42it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 904/1000 [03:05<00:16,  5.73it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 905/1000 [03:05<00:18,  5.01it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 906/1000 [03:05<00:17,  5.34it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 907/1000 [03:05<00:16,  5.60it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 908/1000 [03:06<00:15,  6.04it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 909/1000 [03:06<00:17,  5.14it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 910/1000 [03:06<00:20,  4.48it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 911/1000 [03:06<00:21,  4.13it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 912/1000 [03:07<00:21,  4.03it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 913/1000 [03:07<00:19,  4.57it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 914/1000 [03:07<00:17,  5.03it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 915/1000 [03:07<00:18,  4.60it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 916/1000 [03:07<00:16,  5.01it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 917/1000 [03:08<00:18,  4.57it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 918/1000 [03:08<00:16,  4.99it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 919/1000 [03:08<00:14,  5.53it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 920/1000 [03:08<00:13,  5.75it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 921/1000 [03:08<00:13,  5.95it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 922/1000 [03:08<00:14,  5.25it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 923/1000 [03:09<00:16,  4.77it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 924/1000 [03:09<00:14,  5.16it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 925/1000 [03:09<00:15,  4.69it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 926/1000 [03:09<00:17,  4.23it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 927/1000 [03:10<00:15,  4.71it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 928/1000 [03:10<00:14,  5.09it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 929/1000 [03:10<00:13,  5.41it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 930/1000 [03:10<00:11,  5.90it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 931/1000 [03:10<00:11,  6.02it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 932/1000 [03:10<00:11,  5.78it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 933/1000 [03:10<00:10,  6.19it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 934/1000 [03:11<00:10,  6.21it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 935/1000 [03:11<00:10,  6.22it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 936/1000 [03:11<00:10,  6.25it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 937/1000 [03:11<00:12,  5.24it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 938/1000 [03:11<00:11,  5.60it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 939/1000 [03:12<00:10,  5.78it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 940/1000 [03:12<00:11,  5.04it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 941/1000 [03:12<00:10,  5.38it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 942/1000 [03:12<00:12,  4.79it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 943/1000 [03:12<00:10,  5.21it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 944/1000 [03:13<00:10,  5.47it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 945/1000 [03:13<00:09,  5.72it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 946/1000 [03:13<00:10,  5.03it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 947/1000 [03:13<00:11,  4.67it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 948/1000 [03:13<00:10,  5.08it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 949/1000 [03:14<00:10,  4.64it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 950/1000 [03:14<00:11,  4.40it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 951/1000 [03:14<00:12,  4.06it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 952/1000 [03:14<00:10,  4.56it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 953/1000 [03:14<00:09,  5.02it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 954/1000 [03:15<00:09,  5.10it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 955/1000 [03:15<00:08,  5.43it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 956/1000 [03:15<00:09,  4.64it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 957/1000 [03:15<00:10,  4.16it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 958/1000 [03:16<00:08,  4.69it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 959/1000 [03:16<00:09,  4.39it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 960/1000 [03:16<00:09,  4.23it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 961/1000 [03:16<00:09,  3.94it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 962/1000 [03:17<00:08,  4.48it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 963/1000 [03:17<00:09,  4.09it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 964/1000 [03:17<00:07,  4.57it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 965/1000 [03:17<00:07,  4.97it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 966/1000 [03:17<00:06,  5.53it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 967/1000 [03:17<00:05,  5.75it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 968/1000 [03:18<00:05,  5.91it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 969/1000 [03:18<00:06,  5.10it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 970/1000 [03:18<00:05,  5.42it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 971/1000 [03:18<00:04,  5.90it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 972/1000 [03:18<00:04,  6.08it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 973/1000 [03:18<00:04,  5.82it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 974/1000 [03:19<00:05,  4.79it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 975/1000 [03:19<00:05,  4.45it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 976/1000 [03:19<00:04,  4.90it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 977/1000 [03:19<00:05,  4.32it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 978/1000 [03:20<00:05,  4.18it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 979/1000 [03:20<00:04,  4.64it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 980/1000 [03:20<00:04,  4.21it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 981/1000 [03:20<00:04,  3.94it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 982/1000 [03:21<00:04,  4.45it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 983/1000 [03:21<00:03,  4.27it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 984/1000 [03:21<00:03,  4.07it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 985/1000 [03:21<00:03,  3.85it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 986/1000 [03:22<00:03,  4.35it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 987/1000 [03:22<00:03,  4.04it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 988/1000 [03:22<00:03,  3.84it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 989/1000 [03:22<00:02,  3.86it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 990/1000 [03:23<00:02,  4.51it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 991/1000 [03:23<00:02,  4.30it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 992/1000 [03:23<00:01,  4.56it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 993/1000 [03:23<00:01,  4.98it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 994/1000 [03:23<00:01,  5.55it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 995/1000 [03:23<00:00,  5.82it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 996/1000 [03:24<00:00,  6.04it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 997/1000 [03:24<00:00,  6.11it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 998/1000 [03:24<00:00,  6.15it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 999/1000 [03:24<00:00,  6.25it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [03:24<00:00,  6.28it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [03:24<00:00,  4.88it/s]
DONE (1.86s)
DONE (8.16s)
loss_threshold ROC AUC: 0.49252300000000004, PR AUC: 0.5056191188317454, tpr_at_low_fpr: {0.001: 0.0, 0.01: 0.005}
min_k_threshold ROC AUC: 0.490638, PR AUC: 0.4857122459121052, tpr_at_low_fpr: {0.001: 0.0, 0.01: 0.006}
zlib_threshold ROC AUC: 0.474851, PR AUC: 0.48130957766961, tpr_at_low_fpr: {0.001: 0.0, 0.01: 0.007}
ref-stablelm-base-alpha-3b-v2_threshold ROC AUC: 0.496717, PR AUC: 0.5021883385650764, tpr_at_low_fpr: {0.001: 0.0, 0.01: 0.013}
loss_threshold roc_auc: 0.493
min_k_threshold roc_auc: 0.491
zlib_threshold roc_auc: 0.475
ref-stablelm-base-alpha-3b-v2_threshold roc_auc: 0.497
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
results_new/neo125_github_experiment/meta-llama_Llama-2-70b-hf/hackernews_ngram_13_<0.8_truncated
Saving results to absolute path: /home/chawins/mimir/tmp_results/neo125_github_experiment/meta-llama_Llama-2-70b-hf/hackernews_ngram_13_<0.8_truncated
LOG: cache_dir is /home/chawins/.cache
Using cache dir /home/chawins/.cache
Loading BASE model meta-llama/Llama-2-70b-hf...
Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.embed_tokens.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:   7%|â–‹         | 1/15 [00:00<00:02,  6.13it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  13%|â–ˆâ–Ž        | 2/15 [00:00<00:02,  6.33it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 3/15 [00:00<00:01,  6.36it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:00<00:01,  6.36it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:00<00:01,  6.34it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:00<00:01,  6.34it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:01<00:01,  6.26it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:01<00:01,  6.21it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:01<00:00,  6.22it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:01<00:00,  6.19it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:01<00:00,  6.20it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:01<00:00,  6.18it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:02<00:00,  6.20it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:02<00:00,  6.27it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for lm_head.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:02<00:00,  6.46it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:02<00:00,  6.30it/s]
  0%|          | 0/57 [00:00<?, ?w/s]model.embed_tokens.weight:   0%|          | 0/57 [00:00<?, ?w/s, dev=0]model.embed_tokens.weight:   2%|â–         | 1/57 [00:00<00:10,  5.28w/s, dev=0]model.layers.0.input_layernorm.weight:   2%|â–         | 1/57 [00:00<00:10,  5.27w/s, dev=0]model.layers.0.mlp.down_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:05, 10.53w/s, dev=0]  model.layers.0.mlp.down_proj.weight:   5%|â–Œ         | 3/57 [00:00<00:06,  8.51w/s, dev=0]model.layers.0.mlp.gate_proj.weight:   5%|â–Œ         | 3/57 [00:00<00:06,  8.50w/s, dev=0]model.layers.0.mlp.up_proj.weight:   7%|â–‹         | 4/57 [00:00<00:06,  7.70w/s, dev=0]  model.layers.0.mlp.up_proj.weight:   9%|â–‰         | 5/57 [00:00<00:07,  7.40w/s, dev=0]model.layers.0.post_attention_layernorm.weight:   9%|â–‰         | 5/57 [00:00<00:07,  7.40w/s, dev=0]model.layers.0.self_attn.k_proj.weight:  11%|â–ˆ         | 6/57 [00:00<00:05,  8.88w/s, dev=0]        model.layers.0.self_attn.o_proj.weight:  12%|â–ˆâ–        | 7/57 [00:00<00:04, 10.22w/s, dev=0]model.layers.0.self_attn.q_proj.weight:  14%|â–ˆâ–        | 8/57 [00:00<00:04, 10.86w/s, dev=0]model.layers.0.self_attn.q_proj.weight:  16%|â–ˆâ–Œ        | 9/57 [00:00<00:04, 11.38w/s, dev=0]model.layers.0.self_attn.rotary_emb.inv_freq:  16%|â–ˆâ–Œ        | 9/57 [00:00<00:04, 11.38w/s, dev=0]model.layers.0.self_attn.v_proj.weight:  18%|â–ˆâ–Š        | 10/57 [00:00<00:03, 12.64w/s, dev=0]     model.layers.1.input_layernorm.weight:  19%|â–ˆâ–‰        | 11/57 [00:00<00:03, 13.79w/s, dev=0] model.layers.1.mlp.down_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:00<00:02, 15.03w/s, dev=0]  model.layers.1.mlp.down_proj.weight:  23%|â–ˆâ–ˆâ–Ž       | 13/57 [00:00<00:03, 13.39w/s, dev=0]model.layers.1.mlp.gate_proj.weight:  23%|â–ˆâ–ˆâ–Ž       | 13/57 [00:00<00:03, 13.39w/s, dev=0]model.layers.1.mlp.up_proj.weight:  25%|â–ˆâ–ˆâ–       | 14/57 [00:01<00:03, 12.22w/s, dev=0]  model.layers.1.post_attention_layernorm.weight:  26%|â–ˆâ–ˆâ–‹       | 15/57 [00:01<00:03, 11.36w/s, dev=0]model.layers.1.self_attn.k_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:01<00:03, 12.12w/s, dev=0]        model.layers.1.self_attn.k_proj.weight:  30%|â–ˆâ–ˆâ–‰       | 17/57 [00:01<00:03, 12.81w/s, dev=0]model.layers.1.self_attn.o_proj.weight:  30%|â–ˆâ–ˆâ–‰       | 17/57 [00:01<00:03, 12.81w/s, dev=0]model.layers.1.self_attn.q_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 18/57 [00:01<00:02, 13.11w/s, dev=0]model.layers.1.self_attn.rotary_emb.inv_freq:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 19/57 [00:01<00:02, 13.41w/s, dev=0]model.layers.1.self_attn.v_proj.weight:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 20/57 [00:01<00:02, 14.11w/s, dev=0]      model.layers.2.input_layernorm.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 21/57 [00:01<00:02, 14.75w/s, dev=0] model.layers.2.mlp.down_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:01<00:02, 15.45w/s, dev=0]  model.layers.2.mlp.down_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 23/57 [00:01<00:02, 14.56w/s, dev=0]model.layers.2.mlp.gate_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 23/57 [00:01<00:02, 14.56w/s, dev=0]model.layers.2.mlp.up_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/57 [00:01<00:02, 12.72w/s, dev=0]  model.layers.2.post_attention_layernorm.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/57 [00:02<00:02, 11.66w/s, dev=0]model.layers.2.self_attn.k_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 26/57 [00:02<00:02, 12.13w/s, dev=0]        model.layers.2.self_attn.o_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 27/57 [00:02<00:02, 12.22w/s, dev=0]model.layers.2.self_attn.q_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:02<00:02, 12.41w/s, dev=0]model.layers.2.self_attn.q_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 29/57 [00:02<00:02, 11.75w/s, dev=0]model.layers.2.self_attn.rotary_emb.inv_freq:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 29/57 [00:02<00:02, 11.75w/s, dev=0]model.layers.2.self_attn.v_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 30/57 [00:02<00:02, 12.15w/s, dev=0]      model.layers.3.input_layernorm.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 31/57 [00:02<00:02, 12.51w/s, dev=0] model.layers.3.mlp.down_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 32/57 [00:02<00:01, 12.91w/s, dev=0]  model.layers.3.mlp.gate_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 33/57 [00:02<00:01, 12.51w/s, dev=0]model.layers.3.mlp.up_proj.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:02<00:01, 11.69w/s, dev=0]  model.layers.3.mlp.up_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/57 [00:03<00:02, 10.83w/s, dev=0]model.layers.3.post_attention_layernorm.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/57 [00:03<00:02, 10.83w/s, dev=0]model.layers.3.self_attn.k_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 36/57 [00:03<00:01, 11.14w/s, dev=0]        model.layers.3.self_attn.o_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 37/57 [00:03<00:01, 10.86w/s, dev=0]model.layers.3.self_attn.q_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 38/57 [00:03<00:01, 10.97w/s, dev=0]model.layers.3.self_attn.rotary_emb.inv_freq:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 39/57 [00:03<00:01, 11.11w/s, dev=0]model.layers.3.self_attn.v_proj.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:03<00:01, 11.39w/s, dev=0]      model.layers.3.self_attn.v_proj.weight:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/57 [00:03<00:01, 10.96w/s, dev=0]model.layers.4.input_layernorm.weight:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/57 [00:03<00:01, 10.96w/s, dev=0] model.layers.4.mlp.down_proj.weight:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 42/57 [00:03<00:01, 11.23w/s, dev=0]  model.layers.4.mlp.gate_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 43/57 [00:03<00:01, 10.94w/s, dev=0]model.layers.4.mlp.up_proj.weight:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 44/57 [00:04<00:01, 10.72w/s, dev=0]  model.layers.4.post_attention_layernorm.weight:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 45/57 [00:04<00:01, 10.55w/s, dev=0]model.layers.4.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:04<00:01, 10.78w/s, dev=0]        model.layers.4.self_attn.k_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 47/57 [00:04<00:00, 10.77w/s, dev=0]model.layers.4.self_attn.o_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 47/57 [00:04<00:00, 10.77w/s, dev=0]model.layers.4.self_attn.q_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/57 [00:04<00:00, 10.88w/s, dev=0]model.layers.4.self_attn.rotary_emb.inv_freq:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 49/57 [00:04<00:00, 10.99w/s, dev=0]model.layers.4.self_attn.v_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 50/57 [00:04<00:00, 11.21w/s, dev=0]      model.layers.5.mlp.gate_proj.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 51/57 [00:04<00:00, 11.41w/s, dev=0]   model.layers.5.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:04<00:00, 11.20w/s, dev=0]model.layers.5.self_attn.k_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 53/57 [00:04<00:00, 11.05w/s, dev=0]model.layers.5.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 53/57 [00:04<00:00, 11.05w/s, dev=0]model.layers.5.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/57 [00:04<00:00, 11.14w/s, dev=0]model.layers.5.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 55/57 [00:04<00:00, 11.22w/s, dev=0]model.layers.5.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 56/57 [00:04<00:00, 11.42w/s, dev=0]                                                                                                     0%|          | 0/59 [00:00<?, ?w/s]model.layers.5.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=0]model.layers.5.mlp.down_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 1473.75w/s, dev=0]model.layers.5.mlp.down_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:06,  8.86w/s, dev=0]  model.layers.5.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:06,  8.85w/s, dev=0]  model.layers.5.post_attention_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:07,  7.44w/s, dev=0]model.layers.5.post_attention_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:05,  9.90w/s, dev=0]model.layers.6.input_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:05,  9.90w/s, dev=0]         model.layers.6.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:04, 12.37w/s, dev=0]  model.layers.6.mlp.down_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:05,  9.09w/s, dev=0]model.layers.6.mlp.gate_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:05,  9.09w/s, dev=0]model.layers.6.mlp.up_proj.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:06,  8.47w/s, dev=0]  model.layers.6.mlp.up_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:01<00:07,  6.73w/s, dev=0]model.layers.6.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:01<00:07,  6.73w/s, dev=0]model.layers.6.self_attn.k_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:01<00:06,  7.57w/s, dev=0]        model.layers.6.self_attn.o_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:01<00:05,  8.31w/s, dev=0]model.layers.6.self_attn.q_proj.weight:  19%|â–ˆâ–Š        | 11/59 [00:01<00:05,  8.76w/s, dev=0]model.layers.6.self_attn.q_proj.weight:  20%|â–ˆâ–ˆ        | 12/59 [00:01<00:05,  9.18w/s, dev=0]model.layers.6.self_attn.rotary_emb.inv_freq:  20%|â–ˆâ–ˆ        | 12/59 [00:01<00:05,  9.18w/s, dev=0]model.layers.6.self_attn.v_proj.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:01<00:04,  9.94w/s, dev=0]      model.layers.7.input_layernorm.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:01<00:04, 10.46w/s, dev=0] model.layers.7.mlp.down_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:01<00:03, 11.20w/s, dev=0]  model.layers.7.mlp.down_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:04,  9.28w/s, dev=0]model.layers.7.mlp.gate_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:04,  9.27w/s, dev=0]model.layers.7.mlp.up_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:01<00:04,  9.05w/s, dev=0]  model.layers.7.post_attention_layernorm.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:02<00:04,  8.71w/s, dev=0]model.layers.7.self_attn.k_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:02<00:04,  9.19w/s, dev=0]        model.layers.7.self_attn.k_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:02<00:04,  8.68w/s, dev=0]model.layers.7.self_attn.o_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:02<00:04,  8.68w/s, dev=0]model.layers.7.self_attn.q_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:02<00:04,  8.89w/s, dev=0]model.layers.7.self_attn.rotary_emb.inv_freq:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:02<00:04,  9.13w/s, dev=0]model.layers.7.self_attn.v_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:02<00:03,  9.54w/s, dev=0]      model.layers.7.self_attn.v_proj.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:02<00:03,  9.92w/s, dev=0]model.layers.8.input_layernorm.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:02<00:03,  9.92w/s, dev=0] model.layers.8.mlp.down_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:02<00:03, 10.33w/s, dev=0]  model.layers.8.mlp.gate_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:02<00:03, 10.04w/s, dev=0]model.layers.8.mlp.up_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:02<00:03,  9.87w/s, dev=0]  model.layers.8.mlp.up_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:03<00:03,  9.26w/s, dev=0]model.layers.8.post_attention_layernorm.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:03<00:03,  9.26w/s, dev=0]model.layers.8.self_attn.k_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:03<00:03,  9.59w/s, dev=0]        model.layers.8.self_attn.o_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:03<00:02,  9.89w/s, dev=0]model.layers.8.self_attn.q_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:03<00:02, 10.08w/s, dev=0]model.layers.8.self_attn.rotary_emb.inv_freq:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:03<00:02, 10.27w/s, dev=0]model.layers.8.self_attn.v_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:03<00:02, 10.59w/s, dev=0]      model.layers.9.input_layernorm.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:03<00:02, 10.88w/s, dev=0] model.layers.9.mlp.down_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:03<00:02, 11.20w/s, dev=0]  model.layers.9.mlp.down_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:03<00:02, 11.01w/s, dev=0]model.layers.9.mlp.gate_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:03<00:02, 11.01w/s, dev=0]model.layers.9.mlp.up_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:03<00:02, 10.46w/s, dev=0]  model.layers.9.post_attention_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:03<00:02, 10.33w/s, dev=0]model.layers.9.self_attn.k_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:03<00:01, 10.60w/s, dev=0]        model.layers.9.self_attn.o_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:03<00:01, 10.85w/s, dev=0]model.layers.9.self_attn.q_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:03<00:01, 10.98w/s, dev=0]model.layers.9.self_attn.rotary_emb.inv_freq:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:03<00:01, 10.63w/s, dev=0]model.layers.9.self_attn.v_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:03<00:01, 10.88w/s, dev=0]      model.layers.9.self_attn.v_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:03<00:01, 11.11w/s, dev=0]model.layers.10.input_layernorm.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:03<00:01, 11.11w/s, dev=0]model.layers.10.mlp.down_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:03<00:01, 11.36w/s, dev=0]  model.layers.10.mlp.gate_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:04<00:01, 11.18w/s, dev=0]model.layers.10.mlp.up_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:04<00:01, 11.02w/s, dev=0]  model.layers.10.post_attention_layernorm.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:04<00:01, 10.38w/s, dev=0]model.layers.10.self_attn.k_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:04<00:00, 10.60w/s, dev=0]        model.layers.10.self_attn.o_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:04<00:00, 10.80w/s, dev=0]model.layers.10.self_attn.q_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:04<00:00, 10.89w/s, dev=0]model.layers.10.self_attn.q_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:04<00:00, 10.98w/s, dev=0]model.layers.10.self_attn.rotary_emb.inv_freq:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:04<00:00, 10.98w/s, dev=0]model.layers.10.self_attn.v_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:04<00:00, 11.19w/s, dev=0]      model.layers.11.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:04<00:00, 11.38w/s, dev=0]model.layers.11.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:04<00:00, 11.58w/s, dev=0]model.layers.11.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:04<00:00, 11.68w/s, dev=0]model.layers.11.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:04<00:00, 11.78w/s, dev=0]model.layers.11.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:04<00:00, 11.98w/s, dev=0]                                                                                                      0%|          | 0/55 [00:00<?, ?w/s]model.layers.11.input_layernorm.weight:   0%|          | 0/55 [00:00<?, ?w/s, dev=0]model.layers.11.mlp.down_proj.weight:   2%|â–         | 1/55 [00:00<00:00, 1608.86w/s, dev=0]model.layers.11.mlp.down_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:05,  9.68w/s, dev=0]  model.layers.11.mlp.gate_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:05,  9.67w/s, dev=0]model.layers.11.mlp.up_proj.weight:   5%|â–Œ         | 3/55 [00:00<00:06,  8.33w/s, dev=0]  model.layers.11.mlp.up_proj.weight:   7%|â–‹         | 4/55 [00:00<00:06,  7.89w/s, dev=0]model.layers.11.post_attention_layernorm.weight:   7%|â–‹         | 4/55 [00:00<00:06,  7.88w/s, dev=0]model.layers.12.input_layernorm.weight:   9%|â–‰         | 5/55 [00:00<00:05,  9.85w/s, dev=0]         model.layers.12.mlp.down_proj.weight:  11%|â–ˆ         | 6/55 [00:00<00:04, 11.81w/s, dev=0]  model.layers.12.mlp.down_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:05,  8.31w/s, dev=0]model.layers.12.mlp.gate_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:05,  8.31w/s, dev=0]model.layers.12.mlp.up_proj.weight:  15%|â–ˆâ–        | 8/55 [00:01<00:05,  7.95w/s, dev=0]  model.layers.12.post_attention_layernorm.weight:  16%|â–ˆâ–‹        | 9/55 [00:01<00:06,  7.66w/s, dev=0]model.layers.12.post_attention_layernorm.weight:  18%|â–ˆâ–Š        | 10/55 [00:01<00:05,  8.50w/s, dev=0]model.layers.12.self_attn.k_proj.weight:  18%|â–ˆâ–Š        | 10/55 [00:01<00:05,  8.50w/s, dev=0]        model.layers.12.self_attn.o_proj.weight:  20%|â–ˆâ–ˆ        | 11/55 [00:01<00:04,  9.30w/s, dev=0]model.layers.12.self_attn.q_proj.weight:  22%|â–ˆâ–ˆâ–       | 12/55 [00:01<00:04,  9.77w/s, dev=0]model.layers.12.self_attn.rotary_emb.inv_freq:  24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:01<00:04, 10.20w/s, dev=0]model.layers.12.self_attn.v_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 14/55 [00:01<00:03, 10.98w/s, dev=0]      model.layers.12.self_attn.v_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 15/55 [00:01<00:03, 11.70w/s, dev=0]model.layers.13.input_layernorm.weight:  27%|â–ˆâ–ˆâ–‹       | 15/55 [00:01<00:03, 11.70w/s, dev=0] model.layers.13.mlp.down_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 16/55 [00:01<00:03, 12.48w/s, dev=0]  model.layers.13.mlp.gate_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:01<00:03, 11.80w/s, dev=0]model.layers.13.mlp.up_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/55 [00:01<00:03, 11.22w/s, dev=0]  model.layers.13.post_attention_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–      | 19/55 [00:01<00:03, 10.84w/s, dev=0]model.layers.13.post_attention_layernorm.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:01<00:03, 11.40w/s, dev=0]model.layers.13.self_attn.k_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:01<00:03, 11.40w/s, dev=0]        model.layers.13.self_attn.o_proj.weight:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/55 [00:01<00:02, 11.93w/s, dev=0]model.layers.13.self_attn.q_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/55 [00:01<00:02, 12.18w/s, dev=0]model.layers.13.self_attn.rotary_emb.inv_freq:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 23/55 [00:01<00:02, 12.43w/s, dev=0]model.layers.13.self_attn.v_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 24/55 [00:01<00:02, 12.97w/s, dev=0]      model.layers.13.self_attn.v_proj.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:01<00:02, 13.46w/s, dev=0]model.layers.14.input_layernorm.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:01<00:02, 13.45w/s, dev=0] model.layers.14.mlp.down_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 26/55 [00:01<00:02, 13.99w/s, dev=0]  model.layers.14.mlp.gate_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 27/55 [00:02<00:02, 13.38w/s, dev=0]model.layers.14.mlp.up_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 28/55 [00:02<00:02, 12.84w/s, dev=0]  model.layers.14.post_attention_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/55 [00:02<00:02, 12.43w/s, dev=0]model.layers.14.post_attention_layernorm.weight:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 30/55 [00:02<00:01, 12.85w/s, dev=0]model.layers.14.self_attn.k_proj.weight:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 30/55 [00:02<00:01, 12.85w/s, dev=0]        model.layers.14.self_attn.o_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/55 [00:02<00:01, 13.24w/s, dev=0]model.layers.14.self_attn.q_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 32/55 [00:02<00:01, 13.41w/s, dev=0]model.layers.14.self_attn.rotary_emb.inv_freq:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/55 [00:02<00:01, 13.57w/s, dev=0]model.layers.14.self_attn.v_proj.weight:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/55 [00:02<00:01, 13.98w/s, dev=0]      model.layers.14.self_attn.v_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:02<00:01, 14.34w/s, dev=0]model.layers.15.input_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:02<00:01, 14.34w/s, dev=0] model.layers.15.mlp.down_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 36/55 [00:02<00:01, 14.75w/s, dev=0]  model.layers.15.mlp.gate_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 37/55 [00:02<00:01, 14.32w/s, dev=0]model.layers.15.mlp.up_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 38/55 [00:02<00:01, 13.99w/s, dev=0]  model.layers.15.post_attention_layernorm.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 39/55 [00:02<00:01, 13.61w/s, dev=0]model.layers.15.post_attention_layernorm.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 40/55 [00:02<00:01, 13.96w/s, dev=0]model.layers.15.self_attn.k_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 40/55 [00:02<00:01, 13.96w/s, dev=0]        model.layers.15.self_attn.o_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/55 [00:02<00:00, 14.26w/s, dev=0]model.layers.15.self_attn.q_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 42/55 [00:02<00:00, 14.39w/s, dev=0]model.layers.15.self_attn.rotary_emb.inv_freq:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 43/55 [00:02<00:00, 14.52w/s, dev=0]model.layers.15.self_attn.v_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 44/55 [00:02<00:00, 14.85w/s, dev=0]      model.layers.15.self_attn.v_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:02<00:00, 15.16w/s, dev=0]model.layers.16.input_layernorm.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:02<00:00, 15.16w/s, dev=0] model.layers.16.mlp.down_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 46/55 [00:02<00:00, 15.49w/s, dev=0]  model.layers.16.mlp.gate_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 47/55 [00:03<00:00, 15.09w/s, dev=0]model.layers.16.mlp.up_proj.weight:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 48/55 [00:03<00:00, 14.72w/s, dev=0]  model.layers.16.post_attention_layernorm.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 49/55 [00:03<00:00, 14.34w/s, dev=0]model.layers.16.post_attention_layernorm.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 50/55 [00:03<00:00, 14.64w/s, dev=0]model.layers.16.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 50/55 [00:03<00:00, 14.63w/s, dev=0]        model.layers.16.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 51/55 [00:03<00:00, 14.89w/s, dev=0]model.layers.16.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 52/55 [00:03<00:00, 14.98w/s, dev=0]model.layers.16.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 53/55 [00:03<00:00, 15.08w/s, dev=0]model.layers.16.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 54/55 [00:03<00:00, 15.37w/s, dev=0]      model.layers.16.self_attn.v_proj.weight: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55/55 [00:03<00:00, 15.62w/s, dev=0]                                                                                                0%|          | 0/57 [00:00<?, ?w/s]model.layers.17.input_layernorm.weight:   0%|          | 0/57 [00:00<?, ?w/s, dev=0]model.layers.17.mlp.down_proj.weight:   2%|â–         | 1/57 [00:00<00:00, 1513.10w/s, dev=0]model.layers.17.mlp.down_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:04, 13.50w/s, dev=0]  model.layers.17.mlp.gate_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:04, 13.48w/s, dev=0]model.layers.17.mlp.up_proj.weight:   5%|â–Œ         | 3/57 [00:00<00:05, 10.14w/s, dev=0]  model.layers.17.mlp.up_proj.weight:   7%|â–‹         | 4/57 [00:00<00:05,  9.03w/s, dev=0]model.layers.17.post_attention_layernorm.weight:   7%|â–‹         | 4/57 [00:00<00:05,  9.03w/s, dev=0]model.layers.17.self_attn.k_proj.weight:   9%|â–‰         | 5/57 [00:00<00:04, 11.27w/s, dev=0]        model.layers.17.self_attn.o_proj.weight:  11%|â–ˆ         | 6/57 [00:00<00:03, 13.33w/s, dev=0]model.layers.17.self_attn.q_proj.weight:  12%|â–ˆâ–        | 7/57 [00:00<00:03, 14.03w/s, dev=0]model.layers.17.self_attn.q_proj.weight:  14%|â–ˆâ–        | 8/57 [00:00<00:03, 14.59w/s, dev=0]model.layers.17.self_attn.rotary_emb.inv_freq:  14%|â–ˆâ–        | 8/57 [00:00<00:03, 14.58w/s, dev=0]model.layers.17.self_attn.v_proj.weight:  16%|â–ˆâ–Œ        | 9/57 [00:00<00:02, 16.39w/s, dev=0]      model.layers.18.input_layernorm.weight:  18%|â–ˆâ–Š        | 10/57 [00:00<00:02, 17.99w/s, dev=0]model.layers.18.mlp.down_proj.weight:  19%|â–ˆâ–‰        | 11/57 [00:00<00:02, 19.78w/s, dev=0]  model.layers.18.mlp.down_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:00<00:02, 17.15w/s, dev=0]model.layers.18.mlp.gate_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:00<00:02, 17.15w/s, dev=0]model.layers.18.mlp.up_proj.weight:  23%|â–ˆâ–ˆâ–Ž       | 13/57 [00:00<00:02, 15.29w/s, dev=0]  model.layers.18.post_attention_layernorm.weight:  25%|â–ˆâ–ˆâ–       | 14/57 [00:01<00:03, 13.99w/s, dev=0]model.layers.18.self_attn.k_proj.weight:  26%|â–ˆâ–ˆâ–‹       | 15/57 [00:01<00:02, 14.98w/s, dev=0]        model.layers.18.self_attn.k_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:01<00:02, 15.88w/s, dev=0]model.layers.18.self_attn.o_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:01<00:02, 15.88w/s, dev=0]model.layers.18.self_attn.q_proj.weight:  30%|â–ˆâ–ˆâ–‰       | 17/57 [00:01<00:02, 16.24w/s, dev=0]model.layers.18.self_attn.rotary_emb.inv_freq:  32%|â–ˆâ–ˆâ–ˆâ–      | 18/57 [00:01<00:02, 16.42w/s, dev=0]model.layers.18.self_attn.v_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 19/57 [00:01<00:02, 17.32w/s, dev=0]      model.layers.19.input_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 20/57 [00:01<00:02, 18.10w/s, dev=0] model.layers.19.mlp.down_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 21/57 [00:01<00:01, 19.00w/s, dev=0]  model.layers.19.mlp.down_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:01<00:02, 17.21w/s, dev=0]model.layers.19.mlp.gate_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:01<00:02, 17.21w/s, dev=0]model.layers.19.mlp.up_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 23/57 [00:01<00:02, 15.98w/s, dev=0]  model.layers.19.post_attention_layernorm.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/57 [00:01<00:02, 15.04w/s, dev=0]model.layers.19.self_attn.k_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/57 [00:01<00:02, 15.66w/s, dev=0]        model.layers.19.self_attn.o_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 26/57 [00:01<00:01, 16.21w/s, dev=0]model.layers.19.self_attn.q_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 27/57 [00:01<00:01, 16.37w/s, dev=0]model.layers.19.self_attn.q_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:01<00:01, 16.52w/s, dev=0]model.layers.19.self_attn.rotary_emb.inv_freq:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:01<00:01, 16.52w/s, dev=0]model.layers.19.self_attn.v_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 29/57 [00:01<00:01, 17.10w/s, dev=0]      model.layers.20.input_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 30/57 [00:01<00:01, 15.80w/s, dev=1] model.layers.20.mlp.down_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 31/57 [00:01<00:01, 16.31w/s, dev=1]  model.layers.20.mlp.gate_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 32/57 [00:02<00:01, 15.49w/s, dev=1]model.layers.20.mlp.up_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 33/57 [00:02<00:01, 14.83w/s, dev=1]  model.layers.20.mlp.up_proj.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:02<00:01, 14.22w/s, dev=1]model.layers.20.post_attention_layernorm.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:02<00:01, 14.22w/s, dev=1]model.layers.20.self_attn.k_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/57 [00:02<00:01, 14.63w/s, dev=1]        model.layers.20.self_attn.o_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 36/57 [00:02<00:01, 15.01w/s, dev=1]model.layers.20.self_attn.q_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 37/57 [00:02<00:01, 15.13w/s, dev=1]model.layers.20.self_attn.rotary_emb.inv_freq:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 38/57 [00:02<00:01, 15.25w/s, dev=1]model.layers.20.self_attn.v_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 39/57 [00:02<00:01, 15.65w/s, dev=1]      model.layers.20.self_attn.v_proj.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:02<00:01, 16.01w/s, dev=1]model.layers.21.input_layernorm.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:02<00:01, 16.01w/s, dev=1] model.layers.21.mlp.down_proj.weight:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/57 [00:02<00:00, 16.40w/s, dev=1]  model.layers.21.mlp.gate_proj.weight:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 42/57 [00:02<00:00, 15.74w/s, dev=1]model.layers.21.mlp.up_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 43/57 [00:02<00:00, 15.11w/s, dev=1]  model.layers.21.post_attention_layernorm.weight:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 44/57 [00:03<00:00, 14.66w/s, dev=1]model.layers.21.self_attn.k_proj.weight:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 45/57 [00:03<00:00, 14.99w/s, dev=1]        model.layers.21.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:03<00:00, 15.28w/s, dev=1]model.layers.21.self_attn.o_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:03<00:00, 15.28w/s, dev=1]model.layers.21.self_attn.q_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 47/57 [00:03<00:00, 15.36w/s, dev=1]model.layers.21.self_attn.rotary_emb.inv_freq:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/57 [00:03<00:00, 15.42w/s, dev=1]model.layers.21.self_attn.v_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 49/57 [00:03<00:00, 15.74w/s, dev=1]      model.layers.22.mlp.down_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 50/57 [00:03<00:00, 16.02w/s, dev=1]   model.layers.22.mlp.gate_proj.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 51/57 [00:03<00:00, 15.48w/s, dev=1]model.layers.22.mlp.gate_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:03<00:00, 15.01w/s, dev=1]model.layers.22.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:03<00:00, 15.01w/s, dev=1]model.layers.22.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 53/57 [00:03<00:00, 15.25w/s, dev=1]model.layers.22.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/57 [00:03<00:00, 15.30w/s, dev=1]model.layers.22.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 55/57 [00:03<00:00, 15.37w/s, dev=1]model.layers.22.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 56/57 [00:03<00:00, 15.65w/s, dev=1]                                                                                                      0%|          | 0/59 [00:00<?, ?w/s]model.layers.22.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=1]model.layers.22.mlp.up_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 1294.54w/s, dev=1]model.layers.22.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 12.49w/s, dev=1]  model.layers.22.post_attention_layernorm.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 12.47w/s, dev=1]model.layers.23.input_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:03, 18.66w/s, dev=1]         model.layers.23.mlp.down_proj.weight:   7%|â–‹         | 4/59 [00:00<00:02, 24.85w/s, dev=1]  model.layers.23.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 15.43w/s, dev=1]model.layers.23.mlp.gate_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 15.42w/s, dev=1]model.layers.23.mlp.up_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:04, 12.55w/s, dev=1]  model.layers.23.post_attention_layernorm.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:04, 11.05w/s, dev=1]model.layers.23.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:04, 12.61w/s, dev=1]model.layers.23.self_attn.k_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:04, 12.61w/s, dev=1]        model.layers.23.self_attn.o_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:03, 14.01w/s, dev=1]model.layers.23.self_attn.q_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:03, 14.51w/s, dev=1]model.layers.23.self_attn.q_proj.weight:  19%|â–ˆâ–Š        | 11/59 [00:00<00:03, 14.98w/s, dev=1]model.layers.23.self_attn.rotary_emb.inv_freq:  19%|â–ˆâ–Š        | 11/59 [00:00<00:03, 14.97w/s, dev=1]model.layers.23.self_attn.v_proj.weight:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:02, 16.32w/s, dev=1]      model.layers.24.input_layernorm.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:02, 17.54w/s, dev=1] model.layers.24.mlp.down_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:02, 18.88w/s, dev=1]  model.layers.24.mlp.down_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:02, 16.84w/s, dev=1]model.layers.24.mlp.gate_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:02, 16.83w/s, dev=1]model.layers.24.mlp.up_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:02, 15.13w/s, dev=1]  model.layers.24.post_attention_layernorm.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:01<00:02, 14.01w/s, dev=1]model.layers.24.self_attn.k_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:02, 14.83w/s, dev=1]        model.layers.24.self_attn.k_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:02, 15.57w/s, dev=1]model.layers.24.self_attn.o_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:02, 15.57w/s, dev=1]model.layers.24.self_attn.q_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:02, 15.77w/s, dev=1]model.layers.24.self_attn.rotary_emb.inv_freq:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:01<00:02, 15.95w/s, dev=1]model.layers.24.self_attn.v_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:02, 16.71w/s, dev=1]      model.layers.24.self_attn.v_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:02, 17.38w/s, dev=1]model.layers.25.input_layernorm.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:02, 17.38w/s, dev=1] model.layers.25.mlp.down_proj.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:01, 18.13w/s, dev=1]  model.layers.25.mlp.gate_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:02, 16.87w/s, dev=1]model.layers.25.mlp.up_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:02, 15.88w/s, dev=1]  model.layers.25.mlp.up_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:01<00:02, 15.01w/s, dev=1]model.layers.25.post_attention_layernorm.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:01<00:02, 15.00w/s, dev=1]model.layers.25.self_attn.k_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:01, 15.56w/s, dev=1]        model.layers.25.self_attn.o_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:01, 16.05w/s, dev=1]model.layers.25.self_attn.q_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:01<00:01, 16.18w/s, dev=1]model.layers.25.self_attn.q_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:01<00:01, 16.29w/s, dev=1]model.layers.25.self_attn.rotary_emb.inv_freq:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:01<00:01, 16.29w/s, dev=1]model.layers.25.self_attn.v_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:01<00:01, 16.81w/s, dev=1]      model.layers.26.input_layernorm.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:01<00:01, 17.26w/s, dev=1] model.layers.26.mlp.down_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:01<00:01, 17.78w/s, dev=1]  model.layers.26.mlp.down_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:02<00:01, 16.85w/s, dev=1]model.layers.26.mlp.gate_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:02<00:01, 16.85w/s, dev=1]model.layers.26.mlp.up_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:02<00:01, 16.08w/s, dev=1]  model.layers.26.post_attention_layernorm.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:02<00:01, 15.43w/s, dev=1]model.layers.26.self_attn.k_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:02<00:01, 15.85w/s, dev=1]        model.layers.26.self_attn.k_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:02<00:01, 16.21w/s, dev=1]model.layers.26.self_attn.o_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:02<00:01, 16.20w/s, dev=1]model.layers.26.self_attn.q_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:02<00:01, 16.28w/s, dev=1]model.layers.26.self_attn.rotary_emb.inv_freq:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:02<00:01, 16.35w/s, dev=1]model.layers.26.self_attn.v_proj.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:02<00:01, 16.75w/s, dev=1]      model.layers.26.self_attn.v_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:02<00:00, 17.10w/s, dev=1]model.layers.27.input_layernorm.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:02<00:00, 17.10w/s, dev=1] model.layers.27.mlp.down_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:02<00:00, 17.49w/s, dev=1]  model.layers.27.mlp.gate_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:02<00:00, 16.80w/s, dev=1]model.layers.27.mlp.up_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:02<00:00, 16.27w/s, dev=1]  model.layers.27.mlp.up_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:02<00:00, 15.74w/s, dev=1]model.layers.27.post_attention_layernorm.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:02<00:00, 15.74w/s, dev=1]model.layers.27.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:02<00:00, 16.07w/s, dev=1]        model.layers.27.self_attn.o_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:02<00:00, 16.37w/s, dev=1]model.layers.27.self_attn.q_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:03<00:00, 16.47w/s, dev=1]model.layers.27.self_attn.rotary_emb.inv_freq:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:03<00:00, 16.54w/s, dev=1]model.layers.27.self_attn.v_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:03<00:00, 16.86w/s, dev=1]      model.layers.27.self_attn.v_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:03<00:00, 17.15w/s, dev=1]model.layers.28.mlp.gate_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:03<00:00, 17.15w/s, dev=1]   model.layers.28.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:03<00:00, 16.59w/s, dev=1]model.layers.28.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:03<00:00, 16.86w/s, dev=1]model.layers.28.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:03<00:00, 16.91w/s, dev=1]model.layers.28.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:03<00:00, 16.96w/s, dev=1]model.layers.28.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:03<00:00, 17.25w/s, dev=1]      model.layers.28.self_attn.v_proj.weight: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 59/59 [00:03<00:00, 17.51w/s, dev=1]                                                                                                0%|          | 0/59 [00:00<?, ?w/s]model.layers.28.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=1]model.layers.28.mlp.down_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 1244.23w/s, dev=1]model.layers.28.mlp.down_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 11.99w/s, dev=1]  model.layers.28.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 11.97w/s, dev=1]  model.layers.28.post_attention_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:06,  9.14w/s, dev=1]model.layers.28.post_attention_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:04, 12.16w/s, dev=1]model.layers.29.input_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:04, 12.16w/s, dev=1]         model.layers.29.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 15.19w/s, dev=1]  model.layers.29.mlp.down_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:04, 12.53w/s, dev=1]model.layers.29.mlp.gate_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:04, 12.52w/s, dev=1]model.layers.29.mlp.up_proj.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:04, 11.00w/s, dev=1]  model.layers.29.mlp.up_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:05, 10.12w/s, dev=1]model.layers.29.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:05, 10.12w/s, dev=1]model.layers.29.self_attn.k_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:04, 11.38w/s, dev=1]        model.layers.29.self_attn.o_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:03, 12.54w/s, dev=1]model.layers.29.self_attn.q_proj.weight:  19%|â–ˆâ–Š        | 11/59 [00:00<00:03, 13.09w/s, dev=1]model.layers.29.self_attn.rotary_emb.inv_freq:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:03, 13.54w/s, dev=1]model.layers.29.self_attn.v_proj.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:03, 14.66w/s, dev=1]      model.layers.29.self_attn.v_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:02, 15.65w/s, dev=1]model.layers.30.input_layernorm.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:02, 15.65w/s, dev=1] model.layers.30.mlp.down_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:02, 16.76w/s, dev=1]  model.layers.30.mlp.gate_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:02, 15.33w/s, dev=1]model.layers.30.mlp.up_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:01<00:02, 14.23w/s, dev=1]  model.layers.30.post_attention_layernorm.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:03, 13.36w/s, dev=1]model.layers.30.self_attn.k_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:02, 14.10w/s, dev=1]        model.layers.30.self_attn.k_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:02, 14.76w/s, dev=1]model.layers.30.self_attn.o_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:02, 14.75w/s, dev=1]model.layers.30.self_attn.q_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:01<00:02, 15.01w/s, dev=1]model.layers.30.self_attn.rotary_emb.inv_freq:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:02, 15.23w/s, dev=1]model.layers.30.self_attn.v_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:02, 15.92w/s, dev=1]      model.layers.31.input_layernorm.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:02, 16.53w/s, dev=1] model.layers.31.mlp.down_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:01, 17.22w/s, dev=1]  model.layers.31.mlp.down_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:02, 16.11w/s, dev=1]model.layers.31.mlp.gate_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:02, 16.11w/s, dev=1]model.layers.31.mlp.up_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:01<00:02, 15.28w/s, dev=1]  model.layers.31.post_attention_layernorm.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:02, 14.50w/s, dev=1]model.layers.31.self_attn.k_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:01, 15.01w/s, dev=1]        model.layers.31.self_attn.o_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:01<00:01, 15.46w/s, dev=1]model.layers.31.self_attn.q_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:01<00:01, 15.60w/s, dev=1]model.layers.31.self_attn.q_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:02<00:01, 15.74w/s, dev=1]model.layers.31.self_attn.rotary_emb.inv_freq:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:02<00:01, 15.73w/s, dev=1]model.layers.31.self_attn.v_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:02<00:01, 16.22w/s, dev=1]      model.layers.32.input_layernorm.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:02<00:01, 16.64w/s, dev=1] model.layers.32.mlp.down_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:02<00:01, 17.13w/s, dev=1]  model.layers.32.mlp.gate_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:02<00:01, 16.30w/s, dev=1]model.layers.32.mlp.up_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:02<00:01, 15.67w/s, dev=1]  model.layers.32.mlp.up_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:02<00:01, 15.07w/s, dev=1]model.layers.32.post_attention_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:02<00:01, 15.07w/s, dev=1]model.layers.32.self_attn.k_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:02<00:01, 15.46w/s, dev=1]        model.layers.32.self_attn.o_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:02<00:01, 15.81w/s, dev=1]model.layers.32.self_attn.q_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:02<00:01, 15.91w/s, dev=1]model.layers.32.self_attn.rotary_emb.inv_freq:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:02<00:01, 16.01w/s, dev=1]model.layers.32.self_attn.v_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:02<00:00, 16.39w/s, dev=1]      model.layers.32.self_attn.v_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:02<00:00, 16.73w/s, dev=1]model.layers.33.input_layernorm.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:02<00:00, 16.73w/s, dev=1] model.layers.33.mlp.down_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:02<00:00, 17.11w/s, dev=1]  model.layers.33.mlp.gate_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:02<00:00, 16.49w/s, dev=1]model.layers.33.mlp.up_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:02<00:00, 15.95w/s, dev=1]  model.layers.33.post_attention_layernorm.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:03<00:00, 15.47w/s, dev=1]model.layers.33.self_attn.k_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:03<00:00, 15.79w/s, dev=1]        model.layers.33.self_attn.k_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:03<00:00, 16.08w/s, dev=1]model.layers.33.self_attn.o_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:03<00:00, 16.07w/s, dev=1]model.layers.33.self_attn.q_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:03<00:00, 16.16w/s, dev=1]model.layers.33.self_attn.rotary_emb.inv_freq:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:03<00:00, 16.25w/s, dev=1]model.layers.33.self_attn.v_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:03<00:00, 16.56w/s, dev=1]      model.layers.34.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:03<00:00, 16.84w/s, dev=1]model.layers.34.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:03<00:00, 17.12w/s, dev=1]model.layers.34.self_attn.o_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:03<00:00, 17.19w/s, dev=1]model.layers.34.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:03<00:00, 17.19w/s, dev=1]model.layers.34.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:03<00:00, 17.23w/s, dev=1]model.layers.34.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:03<00:00, 17.53w/s, dev=1]                                                                                                      0%|          | 0/55 [00:00<?, ?w/s]model.layers.34.input_layernorm.weight:   0%|          | 0/55 [00:00<?, ?w/s, dev=1]model.layers.34.mlp.down_proj.weight:   2%|â–         | 1/55 [00:00<00:00, 1246.45w/s, dev=1]model.layers.34.mlp.down_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:04, 13.00w/s, dev=1]  model.layers.34.mlp.gate_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:04, 12.98w/s, dev=1]model.layers.34.mlp.up_proj.weight:   5%|â–Œ         | 3/55 [00:00<00:05,  9.52w/s, dev=1]  model.layers.34.mlp.up_proj.weight:   7%|â–‹         | 4/55 [00:00<00:06,  8.46w/s, dev=1]model.layers.34.post_attention_layernorm.weight:   7%|â–‹         | 4/55 [00:00<00:06,  8.46w/s, dev=1]model.layers.35.input_layernorm.weight:   9%|â–‰         | 5/55 [00:00<00:04, 10.56w/s, dev=1]         model.layers.35.mlp.down_proj.weight:  11%|â–ˆ         | 6/55 [00:00<00:03, 12.67w/s, dev=1]  model.layers.35.mlp.down_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:04, 10.92w/s, dev=1]model.layers.35.mlp.gate_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:04, 10.91w/s, dev=1]model.layers.35.mlp.up_proj.weight:  15%|â–ˆâ–        | 8/55 [00:00<00:04, 10.14w/s, dev=1]  model.layers.35.post_attention_layernorm.weight:  16%|â–ˆâ–‹        | 9/55 [00:00<00:04,  9.49w/s, dev=1]model.layers.35.post_attention_layernorm.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:04, 10.54w/s, dev=1]model.layers.35.self_attn.k_proj.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:04, 10.53w/s, dev=1]        model.layers.35.self_attn.o_proj.weight:  20%|â–ˆâ–ˆ        | 11/55 [00:00<00:03, 11.48w/s, dev=1]model.layers.35.self_attn.q_proj.weight:  22%|â–ˆâ–ˆâ–       | 12/55 [00:01<00:03, 11.90w/s, dev=1]model.layers.35.self_attn.q_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:01<00:03, 12.29w/s, dev=1]model.layers.35.self_attn.rotary_emb.inv_freq:  24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:01<00:03, 12.29w/s, dev=1]model.layers.35.self_attn.v_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 14/55 [00:01<00:03, 13.23w/s, dev=1]      model.layers.36.input_layernorm.weight:  27%|â–ˆâ–ˆâ–‹       | 15/55 [00:01<00:02, 14.08w/s, dev=1] model.layers.36.mlp.down_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 16/55 [00:01<00:02, 15.01w/s, dev=1]  model.layers.36.mlp.down_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:01<00:02, 13.85w/s, dev=1]model.layers.36.mlp.gate_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:01<00:02, 13.85w/s, dev=1]model.layers.36.mlp.up_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/55 [00:01<00:02, 13.08w/s, dev=1]  model.layers.36.post_attention_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–      | 19/55 [00:01<00:02, 12.43w/s, dev=1]model.layers.36.self_attn.k_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:01<00:02, 13.08w/s, dev=1]        model.layers.36.self_attn.k_proj.weight:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/55 [00:01<00:02, 13.68w/s, dev=1]model.layers.36.self_attn.o_proj.weight:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/55 [00:01<00:02, 13.68w/s, dev=1]model.layers.36.self_attn.q_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/55 [00:01<00:02, 13.92w/s, dev=1]model.layers.36.self_attn.rotary_emb.inv_freq:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 23/55 [00:01<00:02, 14.14w/s, dev=1]model.layers.36.self_attn.v_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 24/55 [00:01<00:02, 14.75w/s, dev=1]      model.layers.37.input_layernorm.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:01<00:01, 15.31w/s, dev=1] model.layers.37.mlp.down_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 26/55 [00:01<00:01, 15.92w/s, dev=1]  model.layers.37.mlp.down_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 27/55 [00:01<00:01, 15.09w/s, dev=1]model.layers.37.mlp.gate_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 27/55 [00:01<00:01, 15.09w/s, dev=1]model.layers.37.mlp.up_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 28/55 [00:01<00:01, 14.41w/s, dev=1]  model.layers.37.post_attention_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/55 [00:02<00:01, 13.77w/s, dev=1]model.layers.37.self_attn.k_proj.weight:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 30/55 [00:02<00:01, 14.24w/s, dev=1]        model.layers.37.self_attn.o_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/55 [00:02<00:01, 14.67w/s, dev=1]model.layers.37.self_attn.q_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 32/55 [00:02<00:01, 14.83w/s, dev=1]model.layers.37.self_attn.q_proj.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/55 [00:02<00:01, 14.98w/s, dev=1]model.layers.37.self_attn.rotary_emb.inv_freq:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/55 [00:02<00:01, 14.98w/s, dev=1]model.layers.37.self_attn.v_proj.weight:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/55 [00:02<00:01, 15.43w/s, dev=1]      model.layers.38.input_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:02<00:01, 15.84w/s, dev=1] model.layers.38.mlp.down_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 36/55 [00:02<00:01, 16.29w/s, dev=1]  model.layers.38.mlp.gate_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 37/55 [00:02<00:01, 15.66w/s, dev=1]model.layers.38.mlp.up_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 38/55 [00:02<00:01, 15.03w/s, dev=1]  model.layers.38.mlp.up_proj.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 39/55 [00:02<00:01, 14.52w/s, dev=1]model.layers.38.post_attention_layernorm.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 39/55 [00:02<00:01, 14.52w/s, dev=1]model.layers.38.self_attn.k_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 40/55 [00:02<00:01, 14.89w/s, dev=1]        model.layers.38.self_attn.o_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/55 [00:02<00:00, 15.22w/s, dev=1]model.layers.38.self_attn.q_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 42/55 [00:02<00:00, 15.33w/s, dev=1]model.layers.38.self_attn.rotary_emb.inv_freq:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 43/55 [00:02<00:00, 15.45w/s, dev=1]model.layers.38.self_attn.v_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 44/55 [00:02<00:00, 15.80w/s, dev=1]      model.layers.38.self_attn.v_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:02<00:00, 16.12w/s, dev=1]model.layers.39.input_layernorm.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:02<00:00, 16.12w/s, dev=1] model.layers.39.mlp.down_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 46/55 [00:02<00:00, 16.47w/s, dev=1]  model.layers.39.mlp.gate_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 47/55 [00:02<00:00, 15.92w/s, dev=1]model.layers.39.mlp.up_proj.weight:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 48/55 [00:03<00:00, 15.34w/s, dev=1]  model.layers.39.post_attention_layernorm.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 49/55 [00:03<00:00, 14.95w/s, dev=1]model.layers.39.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 50/55 [00:03<00:00, 15.25w/s, dev=1]        model.layers.39.self_attn.k_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 51/55 [00:03<00:00, 15.52w/s, dev=1]model.layers.39.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 51/55 [00:03<00:00, 15.52w/s, dev=1]model.layers.39.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 52/55 [00:03<00:00, 15.61w/s, dev=1]model.layers.39.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 53/55 [00:03<00:00, 15.68w/s, dev=1]model.layers.39.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 54/55 [00:03<00:00, 15.98w/s, dev=1]                                                                                                      0%|          | 0/57 [00:00<?, ?w/s]model.layers.40.input_layernorm.weight:   0%|          | 0/57 [00:00<?, ?w/s, dev=1]model.layers.40.mlp.down_proj.weight:   2%|â–         | 1/57 [00:00<00:00, 1291.75w/s, dev=1]model.layers.40.mlp.down_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:04, 12.30w/s, dev=1]  model.layers.40.mlp.gate_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:04, 12.28w/s, dev=1]model.layers.40.mlp.up_proj.weight:   5%|â–Œ         | 3/57 [00:00<00:05,  9.71w/s, dev=1]  model.layers.40.mlp.up_proj.weight:   7%|â–‹         | 4/57 [00:00<00:06,  8.75w/s, dev=1]model.layers.40.post_attention_layernorm.weight:   7%|â–‹         | 4/57 [00:00<00:06,  8.75w/s, dev=1]model.layers.40.self_attn.k_proj.weight:   9%|â–‰         | 5/57 [00:00<00:04, 10.93w/s, dev=1]        model.layers.40.self_attn.o_proj.weight:  11%|â–ˆ         | 6/57 [00:00<00:03, 12.93w/s, dev=1]model.layers.40.self_attn.q_proj.weight:  12%|â–ˆâ–        | 7/57 [00:00<00:03, 13.80w/s, dev=1]model.layers.40.self_attn.rotary_emb.inv_freq:  14%|â–ˆâ–        | 8/57 [00:00<00:03, 14.54w/s, dev=1]model.layers.40.self_attn.v_proj.weight:  16%|â–ˆâ–Œ        | 9/57 [00:00<00:02, 16.34w/s, dev=1]      model.layers.40.self_attn.v_proj.weight:  18%|â–ˆâ–Š        | 10/57 [00:00<00:02, 17.95w/s, dev=1]model.layers.41.input_layernorm.weight:  18%|â–ˆâ–Š        | 10/57 [00:00<00:02, 16.09w/s, dev=2] model.layers.41.mlp.down_proj.weight:  19%|â–ˆâ–‰        | 11/57 [00:00<00:02, 17.64w/s, dev=2]  model.layers.41.mlp.gate_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:00<00:02, 15.12w/s, dev=2]model.layers.41.mlp.up_proj.weight:  23%|â–ˆâ–ˆâ–Ž       | 13/57 [00:00<00:03, 13.56w/s, dev=2]  model.layers.41.post_attention_layernorm.weight:  25%|â–ˆâ–ˆâ–       | 14/57 [00:01<00:03, 12.55w/s, dev=2]model.layers.41.self_attn.k_proj.weight:  26%|â–ˆâ–ˆâ–‹       | 15/57 [00:01<00:03, 13.44w/s, dev=2]        model.layers.41.self_attn.k_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:01<00:02, 14.24w/s, dev=2]model.layers.41.self_attn.o_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:01<00:02, 14.24w/s, dev=2]model.layers.41.self_attn.q_proj.weight:  30%|â–ˆâ–ˆâ–‰       | 17/57 [00:01<00:02, 14.53w/s, dev=2]model.layers.41.self_attn.rotary_emb.inv_freq:  32%|â–ˆâ–ˆâ–ˆâ–      | 18/57 [00:01<00:02, 14.79w/s, dev=2]model.layers.41.self_attn.v_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 19/57 [00:01<00:02, 15.60w/s, dev=2]      model.layers.42.input_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 20/57 [00:01<00:02, 16.31w/s, dev=2] model.layers.42.mlp.down_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 21/57 [00:01<00:02, 17.12w/s, dev=2]  model.layers.42.mlp.down_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:01<00:02, 15.84w/s, dev=2]model.layers.42.mlp.gate_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:01<00:02, 15.84w/s, dev=2]model.layers.42.mlp.up_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 23/57 [00:01<00:02, 14.81w/s, dev=2]  model.layers.42.post_attention_layernorm.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/57 [00:01<00:02, 14.01w/s, dev=2]model.layers.42.self_attn.k_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/57 [00:01<00:02, 14.58w/s, dev=2]        model.layers.42.self_attn.o_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 26/57 [00:01<00:02, 15.09w/s, dev=2]model.layers.42.self_attn.q_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 27/57 [00:01<00:01, 15.26w/s, dev=2]model.layers.42.self_attn.q_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:01<00:01, 15.42w/s, dev=2]model.layers.42.self_attn.rotary_emb.inv_freq:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:01<00:01, 15.42w/s, dev=2]model.layers.42.self_attn.v_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 29/57 [00:01<00:01, 15.97w/s, dev=2]      model.layers.43.input_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 30/57 [00:01<00:01, 16.46w/s, dev=2] model.layers.43.mlp.down_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 31/57 [00:01<00:01, 17.01w/s, dev=2]  model.layers.43.mlp.gate_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 32/57 [00:01<00:01, 16.18w/s, dev=2]model.layers.43.mlp.up_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 33/57 [00:02<00:01, 15.40w/s, dev=2]  model.layers.43.mlp.up_proj.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:02<00:01, 14.67w/s, dev=2]model.layers.43.post_attention_layernorm.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:02<00:01, 14.67w/s, dev=2]model.layers.43.self_attn.k_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/57 [00:02<00:01, 15.10w/s, dev=2]        model.layers.43.self_attn.o_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 36/57 [00:02<00:01, 15.48w/s, dev=2]model.layers.43.self_attn.q_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 37/57 [00:02<00:01, 15.58w/s, dev=2]model.layers.43.self_attn.rotary_emb.inv_freq:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 38/57 [00:02<00:01, 15.67w/s, dev=2]model.layers.43.self_attn.v_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 39/57 [00:02<00:01, 16.07w/s, dev=2]      model.layers.43.self_attn.v_proj.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:02<00:01, 16.44w/s, dev=2]model.layers.44.input_layernorm.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:02<00:01, 16.44w/s, dev=2] model.layers.44.mlp.down_proj.weight:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/57 [00:02<00:00, 16.85w/s, dev=2]  model.layers.44.mlp.gate_proj.weight:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 42/57 [00:02<00:00, 16.12w/s, dev=2]model.layers.44.mlp.up_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 43/57 [00:02<00:00, 15.54w/s, dev=2]  model.layers.44.post_attention_layernorm.weight:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 44/57 [00:02<00:00, 15.05w/s, dev=2]model.layers.44.self_attn.k_proj.weight:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 45/57 [00:02<00:00, 15.39w/s, dev=2]        model.layers.44.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:02<00:00, 15.70w/s, dev=2]model.layers.44.self_attn.o_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:02<00:00, 15.70w/s, dev=2]model.layers.44.self_attn.q_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 47/57 [00:02<00:00, 15.79w/s, dev=2]model.layers.44.self_attn.rotary_emb.inv_freq:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/57 [00:03<00:00, 15.89w/s, dev=2]model.layers.44.self_attn.v_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 49/57 [00:03<00:00, 16.22w/s, dev=2]      model.layers.45.mlp.down_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 50/57 [00:03<00:00, 16.51w/s, dev=2]   model.layers.45.mlp.gate_proj.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 51/57 [00:03<00:00, 16.05w/s, dev=2]model.layers.45.mlp.gate_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:03<00:00, 15.59w/s, dev=2]model.layers.45.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:03<00:00, 15.58w/s, dev=2]model.layers.45.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 53/57 [00:03<00:00, 15.85w/s, dev=2]model.layers.45.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/57 [00:03<00:00, 15.89w/s, dev=2]model.layers.45.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 55/57 [00:03<00:00, 15.95w/s, dev=2]model.layers.45.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 56/57 [00:03<00:00, 16.24w/s, dev=2]                                                                                                      0%|          | 0/59 [00:00<?, ?w/s]model.layers.45.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=2]model.layers.45.mlp.up_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 1036.91w/s, dev=2]model.layers.45.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 13.04w/s, dev=2]  model.layers.45.post_attention_layernorm.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 13.02w/s, dev=2]model.layers.46.input_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:02, 19.47w/s, dev=2]         model.layers.46.mlp.down_proj.weight:   7%|â–‹         | 4/59 [00:00<00:02, 25.93w/s, dev=2]  model.layers.46.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 16.07w/s, dev=2]model.layers.46.mlp.gate_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 16.06w/s, dev=2]model.layers.46.mlp.up_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:04, 12.85w/s, dev=2]  model.layers.46.post_attention_layernorm.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:04, 11.27w/s, dev=2]model.layers.46.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:03, 12.87w/s, dev=2]model.layers.46.self_attn.k_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:03, 12.86w/s, dev=2]        model.layers.46.self_attn.o_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:03, 14.31w/s, dev=2]model.layers.46.self_attn.q_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:03, 14.83w/s, dev=2]model.layers.46.self_attn.q_proj.weight:  19%|â–ˆâ–Š        | 11/59 [00:00<00:03, 15.24w/s, dev=2]model.layers.46.self_attn.rotary_emb.inv_freq:  19%|â–ˆâ–Š        | 11/59 [00:00<00:03, 15.23w/s, dev=2]model.layers.46.self_attn.v_proj.weight:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:02, 16.61w/s, dev=2]      model.layers.47.input_layernorm.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:02, 17.79w/s, dev=2] model.layers.47.mlp.down_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:02, 19.15w/s, dev=2]  model.layers.47.mlp.down_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:02, 16.87w/s, dev=2]model.layers.47.mlp.gate_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:02, 16.87w/s, dev=2]model.layers.47.mlp.up_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:02, 15.31w/s, dev=2]  model.layers.47.post_attention_layernorm.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:01<00:02, 14.17w/s, dev=2]model.layers.47.self_attn.k_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:02, 15.00w/s, dev=2]        model.layers.47.self_attn.k_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:02, 15.73w/s, dev=2]model.layers.47.self_attn.o_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:02, 15.72w/s, dev=2]model.layers.47.self_attn.q_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:02, 15.95w/s, dev=2]model.layers.47.self_attn.rotary_emb.inv_freq:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:01<00:02, 16.17w/s, dev=2]model.layers.47.self_attn.v_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:02, 16.93w/s, dev=2]      model.layers.48.input_layernorm.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:02, 17.61w/s, dev=2] model.layers.48.mlp.down_proj.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:01, 18.38w/s, dev=2]  model.layers.48.mlp.down_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:01, 17.13w/s, dev=2]model.layers.48.mlp.gate_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:01, 17.13w/s, dev=2]model.layers.48.mlp.up_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:02, 16.18w/s, dev=2]  model.layers.48.post_attention_layernorm.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:01<00:02, 15.27w/s, dev=2]model.layers.48.self_attn.k_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:01, 15.83w/s, dev=2]        model.layers.48.self_attn.o_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:01, 16.33w/s, dev=2]model.layers.48.self_attn.q_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:01<00:01, 16.48w/s, dev=2]model.layers.48.self_attn.q_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:01<00:01, 16.61w/s, dev=2]model.layers.48.self_attn.rotary_emb.inv_freq:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:01<00:01, 16.61w/s, dev=2]model.layers.48.self_attn.v_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:01<00:01, 17.14w/s, dev=2]      model.layers.49.input_layernorm.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:01<00:01, 17.61w/s, dev=2] model.layers.49.mlp.down_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:01<00:01, 18.15w/s, dev=2]  model.layers.49.mlp.gate_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:02<00:01, 17.19w/s, dev=2]model.layers.49.mlp.up_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:02<00:01, 16.42w/s, dev=2]  model.layers.49.mlp.up_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:02<00:01, 15.78w/s, dev=2]model.layers.49.post_attention_layernorm.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:02<00:01, 15.78w/s, dev=2]model.layers.49.self_attn.k_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:02<00:01, 16.20w/s, dev=2]        model.layers.49.self_attn.o_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:02<00:01, 16.58w/s, dev=2]model.layers.49.self_attn.q_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:02<00:01, 16.67w/s, dev=2]model.layers.49.self_attn.rotary_emb.inv_freq:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:02<00:01, 16.76w/s, dev=2]model.layers.49.self_attn.v_proj.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:02<00:00, 17.17w/s, dev=2]      model.layers.49.self_attn.v_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:02<00:00, 17.53w/s, dev=2]model.layers.50.input_layernorm.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:02<00:00, 17.53w/s, dev=2] model.layers.50.mlp.down_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:02<00:00, 17.93w/s, dev=2]  model.layers.50.mlp.gate_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:02<00:00, 17.21w/s, dev=2]model.layers.50.mlp.up_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:02<00:00, 16.62w/s, dev=2]  model.layers.50.post_attention_layernorm.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:02<00:00, 16.08w/s, dev=2]model.layers.50.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:02<00:00, 16.42w/s, dev=2]        model.layers.50.self_attn.k_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:02<00:00, 16.72w/s, dev=2]model.layers.50.self_attn.o_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:02<00:00, 16.72w/s, dev=2]model.layers.50.self_attn.q_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:02<00:00, 16.80w/s, dev=2]model.layers.50.self_attn.rotary_emb.inv_freq:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:03<00:00, 16.87w/s, dev=2]model.layers.50.self_attn.v_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:03<00:00, 17.20w/s, dev=2]      model.layers.51.mlp.gate_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:03<00:00, 17.49w/s, dev=2]   model.layers.51.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:03<00:00, 16.99w/s, dev=2]model.layers.51.self_attn.k_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:03<00:00, 17.26w/s, dev=2]model.layers.51.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:03<00:00, 17.26w/s, dev=2]model.layers.51.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:03<00:00, 17.34w/s, dev=2]model.layers.51.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:03<00:00, 17.39w/s, dev=2]model.layers.51.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:03<00:00, 17.70w/s, dev=2]                                                                                                      0%|          | 0/59 [00:00<?, ?w/s]model.layers.51.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=2]model.layers.51.mlp.down_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 1081.84w/s, dev=2]model.layers.51.mlp.down_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:07,  7.74w/s, dev=2]  model.layers.51.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:07,  7.73w/s, dev=2]  model.layers.51.post_attention_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:07,  7.27w/s, dev=2]model.layers.51.post_attention_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:05,  9.68w/s, dev=2]model.layers.52.input_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:05,  9.68w/s, dev=2]         model.layers.52.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:04, 12.09w/s, dev=2]  model.layers.52.mlp.down_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:06,  8.24w/s, dev=2]model.layers.52.mlp.gate_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:06,  8.24w/s, dev=2]model.layers.52.mlp.up_proj.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:06,  7.99w/s, dev=2]  model.layers.52.mlp.up_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:01<00:07,  6.78w/s, dev=2]model.layers.52.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:01<00:07,  6.78w/s, dev=2]model.layers.52.self_attn.k_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:01<00:06,  7.62w/s, dev=2]        model.layers.52.self_attn.o_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:01<00:05,  8.41w/s, dev=2]model.layers.52.self_attn.q_proj.weight:  19%|â–ˆâ–Š        | 11/59 [00:01<00:05,  8.89w/s, dev=2]model.layers.52.self_attn.q_proj.weight:  20%|â–ˆâ–ˆ        | 12/59 [00:01<00:05,  9.33w/s, dev=2]model.layers.52.self_attn.rotary_emb.inv_freq:  20%|â–ˆâ–ˆ        | 12/59 [00:01<00:05,  9.33w/s, dev=2]model.layers.52.self_attn.v_proj.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:01<00:04, 10.10w/s, dev=2]      model.layers.53.input_layernorm.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:01<00:04, 10.77w/s, dev=2] model.layers.53.mlp.down_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:01<00:03, 11.53w/s, dev=2]  model.layers.53.mlp.down_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:03, 10.96w/s, dev=2]model.layers.53.mlp.gate_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:03, 10.96w/s, dev=2]model.layers.53.mlp.up_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:01<00:04,  9.81w/s, dev=2]  model.layers.53.post_attention_layernorm.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:04,  9.51w/s, dev=2]model.layers.53.self_attn.k_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:03, 10.04w/s, dev=2]        model.layers.53.self_attn.k_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:02<00:04,  9.39w/s, dev=2]model.layers.53.self_attn.o_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:02<00:04,  9.39w/s, dev=2]model.layers.53.self_attn.q_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:02<00:03,  9.62w/s, dev=2]model.layers.53.self_attn.rotary_emb.inv_freq:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:02<00:03,  9.83w/s, dev=2]model.layers.53.self_attn.v_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:02<00:03, 10.27w/s, dev=2]      model.layers.53.self_attn.v_proj.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:02<00:03, 10.67w/s, dev=2]model.layers.54.input_layernorm.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:02<00:03, 10.67w/s, dev=2] model.layers.54.mlp.down_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:02<00:03, 11.12w/s, dev=2]  model.layers.54.mlp.gate_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:02<00:03, 10.78w/s, dev=2]model.layers.54.mlp.up_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:02<00:03, 10.45w/s, dev=2]  model.layers.54.mlp.up_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:02<00:03,  9.56w/s, dev=2]model.layers.54.post_attention_layernorm.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:02<00:03,  9.56w/s, dev=2]model.layers.54.self_attn.k_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:02<00:03,  9.90w/s, dev=2]        model.layers.54.self_attn.o_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:02<00:02, 10.21w/s, dev=2]model.layers.54.self_attn.q_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:02<00:02, 10.39w/s, dev=2]model.layers.54.self_attn.rotary_emb.inv_freq:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:03<00:02, 10.56w/s, dev=2]model.layers.54.self_attn.rotary_emb.inv_freq:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:03<00:02, 10.89w/s, dev=2]model.layers.54.self_attn.v_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:03<00:02, 10.89w/s, dev=2]      model.layers.55.input_layernorm.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:03<00:02, 11.18w/s, dev=2] model.layers.55.mlp.down_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:03<00:02, 11.51w/s, dev=2]  model.layers.55.mlp.gate_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:03<00:02, 11.28w/s, dev=2]model.layers.55.mlp.up_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:03<00:02, 10.43w/s, dev=2]  model.layers.55.mlp.up_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:03<00:02, 10.26w/s, dev=2]model.layers.55.post_attention_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:03<00:02, 10.26w/s, dev=2]model.layers.55.self_attn.k_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:03<00:01, 10.52w/s, dev=2]        model.layers.55.self_attn.o_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:03<00:01, 10.73w/s, dev=2]model.layers.55.self_attn.q_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:03<00:01, 10.86w/s, dev=2]model.layers.55.self_attn.rotary_emb.inv_freq:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:04<00:01, 10.36w/s, dev=2]model.layers.55.self_attn.rotary_emb.inv_freq:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:04<00:01, 10.61w/s, dev=2]model.layers.55.self_attn.v_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:04<00:01, 10.61w/s, dev=2]      model.layers.56.input_layernorm.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:04<00:01, 10.83w/s, dev=2] model.layers.56.mlp.down_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:04<00:01, 11.08w/s, dev=2]  model.layers.56.mlp.gate_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:04<00:01, 10.90w/s, dev=2]model.layers.56.mlp.up_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:04<00:01, 10.76w/s, dev=2]  model.layers.56.mlp.up_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:04<00:01, 10.62w/s, dev=2]model.layers.56.post_attention_layernorm.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:04<00:01, 10.62w/s, dev=2]model.layers.56.self_attn.k_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:04<00:00, 10.84w/s, dev=2]        model.layers.56.self_attn.o_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:04<00:00, 11.05w/s, dev=2]model.layers.56.self_attn.q_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:04<00:00, 11.16w/s, dev=2]model.layers.56.self_attn.rotary_emb.inv_freq:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:04<00:00, 11.27w/s, dev=2]model.layers.56.self_attn.v_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:04<00:00, 11.48w/s, dev=2]      model.layers.56.self_attn.v_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:04<00:00, 11.68w/s, dev=2]model.layers.57.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:04<00:00, 11.68w/s, dev=2]model.layers.57.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:04<00:00, 11.88w/s, dev=2]model.layers.57.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:04<00:00, 11.98w/s, dev=2]model.layers.57.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:04<00:00, 12.07w/s, dev=2]model.layers.57.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:04<00:00, 12.28w/s, dev=2]                                                                                                      0%|          | 0/55 [00:00<?, ?w/s]model.layers.57.input_layernorm.weight:   0%|          | 0/55 [00:00<?, ?w/s, dev=2]model.layers.57.mlp.down_proj.weight:   2%|â–         | 1/55 [00:00<00:00, 1126.29w/s, dev=2]model.layers.57.mlp.down_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:04, 12.67w/s, dev=2]  model.layers.57.mlp.gate_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:04, 12.64w/s, dev=2]model.layers.57.mlp.up_proj.weight:   5%|â–Œ         | 3/55 [00:00<00:05,  9.64w/s, dev=2]  model.layers.57.mlp.up_proj.weight:   7%|â–‹         | 4/55 [00:00<00:05,  8.53w/s, dev=2]model.layers.57.post_attention_layernorm.weight:   7%|â–‹         | 4/55 [00:00<00:05,  8.52w/s, dev=2]model.layers.58.input_layernorm.weight:   9%|â–‰         | 5/55 [00:00<00:04, 10.65w/s, dev=2]         model.layers.58.mlp.down_proj.weight:  11%|â–ˆ         | 6/55 [00:00<00:03, 12.77w/s, dev=2]  model.layers.58.mlp.down_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:04, 11.18w/s, dev=2]model.layers.58.mlp.gate_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:04, 11.17w/s, dev=2]model.layers.58.mlp.up_proj.weight:  15%|â–ˆâ–        | 8/55 [00:00<00:04, 10.26w/s, dev=2]  model.layers.58.post_attention_layernorm.weight:  16%|â–ˆâ–‹        | 9/55 [00:00<00:04,  9.59w/s, dev=2]model.layers.58.post_attention_layernorm.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:04, 10.65w/s, dev=2]model.layers.58.self_attn.k_proj.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:04, 10.65w/s, dev=2]        model.layers.58.self_attn.o_proj.weight:  20%|â–ˆâ–ˆ        | 11/55 [00:00<00:03, 11.63w/s, dev=2]model.layers.58.self_attn.q_proj.weight:  22%|â–ˆâ–ˆâ–       | 12/55 [00:00<00:03, 12.11w/s, dev=2]model.layers.58.self_attn.rotary_emb.inv_freq:  24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:01<00:03, 12.54w/s, dev=2]model.layers.58.self_attn.v_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 14/55 [00:01<00:03, 13.50w/s, dev=2]      model.layers.58.self_attn.v_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 15/55 [00:01<00:02, 14.37w/s, dev=2]model.layers.59.input_layernorm.weight:  27%|â–ˆâ–ˆâ–‹       | 15/55 [00:01<00:02, 14.37w/s, dev=2] model.layers.59.mlp.down_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 16/55 [00:01<00:02, 15.33w/s, dev=2]  model.layers.59.mlp.gate_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:01<00:02, 14.22w/s, dev=2]model.layers.59.mlp.up_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/55 [00:01<00:02, 13.40w/s, dev=2]  model.layers.59.post_attention_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–      | 19/55 [00:01<00:02, 12.76w/s, dev=2]model.layers.59.post_attention_layernorm.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:01<00:02, 13.42w/s, dev=2]model.layers.59.self_attn.k_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:01<00:02, 13.42w/s, dev=2]        model.layers.59.self_attn.o_proj.weight:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/55 [00:01<00:02, 14.04w/s, dev=2]model.layers.59.self_attn.q_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/55 [00:01<00:02, 14.29w/s, dev=2]model.layers.59.self_attn.rotary_emb.inv_freq:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 23/55 [00:01<00:02, 14.53w/s, dev=2]model.layers.59.self_attn.v_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 24/55 [00:01<00:02, 15.16w/s, dev=2]      model.layers.60.input_layernorm.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:01<00:01, 15.73w/s, dev=2] model.layers.60.mlp.down_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 26/55 [00:01<00:01, 16.35w/s, dev=2]  model.layers.60.mlp.down_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 27/55 [00:01<00:01, 15.53w/s, dev=2]model.layers.60.mlp.gate_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 27/55 [00:01<00:01, 15.53w/s, dev=2]model.layers.60.mlp.up_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 28/55 [00:01<00:01, 14.82w/s, dev=2]  model.layers.60.post_attention_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/55 [00:02<00:01, 14.22w/s, dev=2]model.layers.60.self_attn.k_proj.weight:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 30/55 [00:02<00:01, 14.71w/s, dev=2]        model.layers.60.self_attn.o_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/55 [00:02<00:01, 15.15w/s, dev=2]model.layers.60.self_attn.q_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 32/55 [00:02<00:01, 15.32w/s, dev=2]model.layers.60.self_attn.rotary_emb.inv_freq:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/55 [00:02<00:01, 15.48w/s, dev=2]model.layers.60.self_attn.rotary_emb.inv_freq:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/55 [00:02<00:01, 15.94w/s, dev=2]model.layers.60.self_attn.v_proj.weight:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/55 [00:02<00:01, 15.94w/s, dev=2]      model.layers.61.input_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:02<00:01, 16.36w/s, dev=2] model.layers.61.mlp.down_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 36/55 [00:02<00:01, 16.82w/s, dev=2]  model.layers.61.mlp.gate_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 37/55 [00:02<00:01, 16.14w/s, dev=2]model.layers.61.mlp.up_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 38/55 [00:02<00:01, 15.53w/s, dev=2]  model.layers.61.post_attention_layernorm.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 39/55 [00:02<00:01, 15.00w/s, dev=2]model.layers.61.self_attn.k_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 40/55 [00:02<00:00, 15.39w/s, dev=2]        model.layers.61.self_attn.k_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/55 [00:02<00:00, 15.73w/s, dev=2]model.layers.61.self_attn.o_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/55 [00:02<00:00, 15.73w/s, dev=2]model.layers.61.self_attn.q_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 42/55 [00:02<00:00, 15.84w/s, dev=2]model.layers.61.self_attn.rotary_emb.inv_freq:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 43/55 [00:02<00:00, 15.93w/s, dev=2]model.layers.61.self_attn.v_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 44/55 [00:02<00:00, 16.30w/s, dev=2]      model.layers.62.input_layernorm.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:03<00:00, 14.88w/s, dev=3] model.layers.62.mlp.down_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 46/55 [00:03<00:00, 15.20w/s, dev=3]  model.layers.62.mlp.gate_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 47/55 [00:03<00:00, 14.79w/s, dev=3]model.layers.62.mlp.gate_proj.weight:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 48/55 [00:03<00:00, 14.39w/s, dev=3]model.layers.62.mlp.up_proj.weight:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 48/55 [00:03<00:00, 14.39w/s, dev=3]  model.layers.62.post_attention_layernorm.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 49/55 [00:03<00:00, 14.04w/s, dev=3]model.layers.62.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 50/55 [00:03<00:00, 14.33w/s, dev=3]        model.layers.62.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 51/55 [00:03<00:00, 14.59w/s, dev=3]model.layers.62.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 52/55 [00:03<00:00, 14.68w/s, dev=3]model.layers.62.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 53/55 [00:03<00:00, 14.78w/s, dev=3]model.layers.62.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 54/55 [00:03<00:00, 15.05w/s, dev=3]      model.layers.62.self_attn.v_proj.weight: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55/55 [00:03<00:00, 15.30w/s, dev=3]                                                                                                0%|          | 0/57 [00:00<?, ?w/s]model.layers.63.input_layernorm.weight:   0%|          | 0/57 [00:00<?, ?w/s, dev=3]model.layers.63.mlp.down_proj.weight:   2%|â–         | 1/57 [00:00<00:00, 990.62w/s, dev=3]model.layers.63.mlp.down_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:04, 12.84w/s, dev=3] model.layers.63.mlp.gate_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:04, 12.82w/s, dev=3]model.layers.63.mlp.up_proj.weight:   5%|â–Œ         | 3/57 [00:00<00:05,  9.50w/s, dev=3]  model.layers.63.mlp.up_proj.weight:   7%|â–‹         | 4/57 [00:00<00:06,  8.37w/s, dev=3]model.layers.63.post_attention_layernorm.weight:   7%|â–‹         | 4/57 [00:00<00:06,  8.37w/s, dev=3]model.layers.63.self_attn.k_proj.weight:   9%|â–‰         | 5/57 [00:00<00:04, 10.45w/s, dev=3]        model.layers.63.self_attn.o_proj.weight:  11%|â–ˆ         | 6/57 [00:00<00:04, 12.37w/s, dev=3]model.layers.63.self_attn.q_proj.weight:  12%|â–ˆâ–        | 7/57 [00:00<00:03, 13.14w/s, dev=3]model.layers.63.self_attn.q_proj.weight:  14%|â–ˆâ–        | 8/57 [00:00<00:03, 13.81w/s, dev=3]model.layers.63.self_attn.rotary_emb.inv_freq:  14%|â–ˆâ–        | 8/57 [00:00<00:03, 13.80w/s, dev=3]model.layers.63.self_attn.v_proj.weight:  16%|â–ˆâ–Œ        | 9/57 [00:00<00:03, 15.51w/s, dev=3]      model.layers.64.input_layernorm.weight:  18%|â–ˆâ–Š        | 10/57 [00:00<00:02, 17.05w/s, dev=3]model.layers.64.mlp.down_proj.weight:  19%|â–ˆâ–‰        | 11/57 [00:00<00:02, 18.75w/s, dev=3]  model.layers.64.mlp.down_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:00<00:02, 16.24w/s, dev=3]model.layers.64.mlp.gate_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:00<00:02, 16.23w/s, dev=3]model.layers.64.mlp.up_proj.weight:  23%|â–ˆâ–ˆâ–Ž       | 13/57 [00:00<00:03, 14.44w/s, dev=3]  model.layers.64.post_attention_layernorm.weight:  25%|â–ˆâ–ˆâ–       | 14/57 [00:01<00:03, 13.21w/s, dev=3]model.layers.64.self_attn.k_proj.weight:  26%|â–ˆâ–ˆâ–‹       | 15/57 [00:01<00:02, 14.15w/s, dev=3]        model.layers.64.self_attn.k_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:01<00:02, 15.00w/s, dev=3]model.layers.64.self_attn.o_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:01<00:02, 15.00w/s, dev=3]model.layers.64.self_attn.q_proj.weight:  30%|â–ˆâ–ˆâ–‰       | 17/57 [00:01<00:02, 15.28w/s, dev=3]model.layers.64.self_attn.rotary_emb.inv_freq:  32%|â–ˆâ–ˆâ–ˆâ–      | 18/57 [00:01<00:02, 15.53w/s, dev=3]model.layers.64.self_attn.v_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 19/57 [00:01<00:02, 16.39w/s, dev=3]      model.layers.65.input_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 20/57 [00:01<00:02, 17.16w/s, dev=3] model.layers.65.mlp.down_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 21/57 [00:01<00:01, 18.01w/s, dev=3]  model.layers.65.mlp.down_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:01<00:02, 16.60w/s, dev=3]model.layers.65.mlp.gate_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:01<00:02, 16.59w/s, dev=3]model.layers.65.mlp.up_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 23/57 [00:01<00:02, 15.50w/s, dev=3]  model.layers.65.post_attention_layernorm.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/57 [00:01<00:02, 14.69w/s, dev=3]model.layers.65.self_attn.k_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/57 [00:01<00:02, 15.30w/s, dev=3]        model.layers.65.self_attn.o_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 26/57 [00:01<00:01, 15.85w/s, dev=3]model.layers.65.self_attn.q_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 27/57 [00:01<00:01, 16.03w/s, dev=3]model.layers.65.self_attn.q_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:01<00:01, 16.20w/s, dev=3]model.layers.65.self_attn.rotary_emb.inv_freq:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:01<00:01, 16.20w/s, dev=3]model.layers.65.self_attn.v_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 29/57 [00:01<00:01, 16.77w/s, dev=3]      model.layers.66.input_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 30/57 [00:01<00:01, 17.28w/s, dev=3] model.layers.66.mlp.down_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 31/57 [00:01<00:01, 17.85w/s, dev=3]  model.layers.66.mlp.gate_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 32/57 [00:01<00:01, 16.89w/s, dev=3]model.layers.66.mlp.up_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 33/57 [00:02<00:01, 16.10w/s, dev=3]  model.layers.66.mlp.up_proj.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:02<00:01, 15.36w/s, dev=3]model.layers.66.post_attention_layernorm.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:02<00:01, 15.36w/s, dev=3]model.layers.66.self_attn.k_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/57 [00:02<00:01, 15.81w/s, dev=3]        model.layers.66.self_attn.o_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 36/57 [00:02<00:01, 16.20w/s, dev=3]model.layers.66.self_attn.q_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 37/57 [00:02<00:01, 16.30w/s, dev=3]model.layers.66.self_attn.rotary_emb.inv_freq:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 38/57 [00:02<00:01, 16.40w/s, dev=3]model.layers.66.self_attn.v_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 39/57 [00:02<00:01, 16.83w/s, dev=3]      model.layers.66.self_attn.v_proj.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:02<00:00, 17.20w/s, dev=3]model.layers.67.input_layernorm.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:02<00:00, 17.20w/s, dev=3] model.layers.67.mlp.down_proj.weight:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/57 [00:02<00:00, 17.62w/s, dev=3]  model.layers.67.mlp.gate_proj.weight:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 42/57 [00:02<00:00, 16.96w/s, dev=3]model.layers.67.mlp.up_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 43/57 [00:02<00:00, 16.39w/s, dev=3]  model.layers.67.post_attention_layernorm.weight:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 44/57 [00:02<00:00, 15.84w/s, dev=3]model.layers.67.self_attn.k_proj.weight:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 45/57 [00:02<00:00, 16.19w/s, dev=3]        model.layers.67.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:02<00:00, 16.51w/s, dev=3]model.layers.67.self_attn.o_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:02<00:00, 16.50w/s, dev=3]model.layers.67.self_attn.q_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 47/57 [00:02<00:00, 16.58w/s, dev=3]model.layers.67.self_attn.rotary_emb.inv_freq:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/57 [00:02<00:00, 16.66w/s, dev=3]model.layers.67.self_attn.v_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 49/57 [00:02<00:00, 17.00w/s, dev=3]      model.layers.68.mlp.down_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 50/57 [00:02<00:00, 17.31w/s, dev=3]   model.layers.68.mlp.gate_proj.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 51/57 [00:03<00:00, 16.77w/s, dev=3]model.layers.68.mlp.gate_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:03<00:00, 16.31w/s, dev=3]model.layers.68.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:03<00:00, 16.31w/s, dev=3]model.layers.68.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 53/57 [00:03<00:00, 16.58w/s, dev=3]model.layers.68.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/57 [00:03<00:00, 16.67w/s, dev=3]model.layers.68.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 55/57 [00:03<00:00, 16.75w/s, dev=3]model.layers.68.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 56/57 [00:03<00:00, 17.05w/s, dev=3]                                                                                                      0%|          | 0/59 [00:00<?, ?w/s]model.layers.68.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=3]model.layers.68.mlp.up_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 982.96w/s, dev=3]model.layers.68.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 13.74w/s, dev=3] model.layers.68.post_attention_layernorm.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 13.72w/s, dev=3]model.layers.69.input_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:02, 20.52w/s, dev=3]         model.layers.69.mlp.down_proj.weight:   7%|â–‹         | 4/59 [00:00<00:02, 27.32w/s, dev=3]  model.layers.69.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 17.22w/s, dev=3]model.layers.69.mlp.gate_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 17.20w/s, dev=3]model.layers.69.mlp.up_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:03, 13.52w/s, dev=3]  model.layers.69.post_attention_layernorm.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:04, 11.80w/s, dev=3]model.layers.69.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:03, 13.47w/s, dev=3]model.layers.69.self_attn.k_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:03, 13.47w/s, dev=3]        model.layers.69.self_attn.o_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:03, 14.97w/s, dev=3]model.layers.69.self_attn.q_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:03, 15.45w/s, dev=3]model.layers.69.self_attn.rotary_emb.inv_freq:  19%|â–ˆâ–Š        | 11/59 [00:00<00:03, 15.87w/s, dev=3]model.layers.69.self_attn.v_proj.weight:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:02, 17.30w/s, dev=3]      model.layers.69.self_attn.v_proj.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:02, 18.53w/s, dev=3]model.layers.70.input_layernorm.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:02, 18.53w/s, dev=3] model.layers.70.mlp.down_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:02, 19.95w/s, dev=3]  model.layers.70.mlp.gate_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:02, 17.54w/s, dev=3]model.layers.70.mlp.up_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:00<00:02, 16.01w/s, dev=3]  model.layers.70.post_attention_layernorm.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:01<00:02, 14.86w/s, dev=3]model.layers.70.post_attention_layernorm.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:02, 15.72w/s, dev=3]model.layers.70.self_attn.k_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:02, 15.72w/s, dev=3]        model.layers.70.self_attn.o_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:02, 16.50w/s, dev=3]model.layers.70.self_attn.q_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:02, 16.74w/s, dev=3]model.layers.70.self_attn.rotary_emb.inv_freq:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:01<00:02, 16.92w/s, dev=3]model.layers.70.self_attn.v_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:02, 17.72w/s, dev=3]      model.layers.70.self_attn.v_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:01, 18.40w/s, dev=3]model.layers.71.input_layernorm.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:01, 18.40w/s, dev=3] model.layers.71.mlp.down_proj.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:01, 19.19w/s, dev=3]  model.layers.71.mlp.gate_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:01, 17.92w/s, dev=3]model.layers.71.mlp.up_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:01, 16.89w/s, dev=3]  model.layers.71.post_attention_layernorm.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:01<00:01, 16.06w/s, dev=3]model.layers.71.post_attention_layernorm.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:01, 16.65w/s, dev=3]model.layers.71.self_attn.k_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:01, 16.64w/s, dev=3]        model.layers.71.self_attn.o_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:01, 17.16w/s, dev=3]model.layers.71.self_attn.q_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:01<00:01, 17.31w/s, dev=3]model.layers.71.self_attn.rotary_emb.inv_freq:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:01<00:01, 17.44w/s, dev=3]model.layers.71.self_attn.v_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:01<00:01, 17.99w/s, dev=3]      model.layers.71.self_attn.v_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:01<00:01, 18.49w/s, dev=3]model.layers.72.input_layernorm.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:01<00:01, 18.49w/s, dev=3] model.layers.72.mlp.down_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:01<00:01, 19.05w/s, dev=3]  model.layers.72.mlp.gate_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:01<00:01, 18.16w/s, dev=3]model.layers.72.mlp.up_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:02<00:01, 17.38w/s, dev=3]  model.layers.72.post_attention_layernorm.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:02<00:01, 16.71w/s, dev=3]model.layers.72.post_attention_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:02<00:01, 17.16w/s, dev=3]model.layers.72.self_attn.k_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:02<00:01, 17.15w/s, dev=3]        model.layers.72.self_attn.o_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:02<00:01, 17.54w/s, dev=3]model.layers.72.self_attn.q_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:02<00:01, 17.65w/s, dev=3]model.layers.72.self_attn.rotary_emb.inv_freq:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:02<00:01, 17.76w/s, dev=3]model.layers.72.self_attn.v_proj.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:02<00:00, 18.18w/s, dev=3]      model.layers.72.self_attn.v_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:02<00:00, 18.57w/s, dev=3]model.layers.73.input_layernorm.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:02<00:00, 18.57w/s, dev=3] model.layers.73.mlp.down_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:02<00:00, 19.00w/s, dev=3]  model.layers.73.mlp.gate_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:02<00:00, 18.30w/s, dev=3]model.layers.73.mlp.up_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:02<00:00, 17.67w/s, dev=3]  model.layers.73.post_attention_layernorm.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:02<00:00, 17.09w/s, dev=3]model.layers.73.post_attention_layernorm.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:02<00:00, 17.45w/s, dev=3]model.layers.73.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:02<00:00, 17.45w/s, dev=3]        model.layers.73.self_attn.o_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:02<00:00, 17.77w/s, dev=3]model.layers.73.self_attn.q_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:02<00:00, 17.85w/s, dev=3]model.layers.73.self_attn.rotary_emb.inv_freq:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:02<00:00, 17.93w/s, dev=3]model.layers.73.self_attn.v_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:02<00:00, 18.28w/s, dev=3]      model.layers.73.self_attn.v_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:02<00:00, 18.59w/s, dev=3]model.layers.74.mlp.gate_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:02<00:00, 18.59w/s, dev=3]   model.layers.74.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:02<00:00, 18.02w/s, dev=3]model.layers.74.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:03<00:00, 18.30w/s, dev=3]model.layers.74.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:03<00:00, 18.36w/s, dev=3]model.layers.74.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:03<00:00, 18.43w/s, dev=3]model.layers.74.self_attn.rotary_emb.inv_freq:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:03<00:00, 18.75w/s, dev=3]model.layers.74.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:03<00:00, 18.75w/s, dev=3]                                                                                                      0%|          | 0/55 [00:00<?, ?w/s]model.layers.74.input_layernorm.weight:   0%|          | 0/55 [00:00<?, ?w/s, dev=3]model.layers.74.mlp.down_proj.weight:   2%|â–         | 1/55 [00:00<00:00, 1015.57w/s, dev=3]model.layers.74.mlp.down_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:04, 12.59w/s, dev=3]  model.layers.74.mlp.up_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:04, 12.57w/s, dev=3]  model.layers.74.post_attention_layernorm.weight:   5%|â–Œ         | 3/55 [00:00<00:05,  9.72w/s, dev=3]model.layers.74.post_attention_layernorm.weight:   7%|â–‹         | 4/55 [00:00<00:03, 12.94w/s, dev=3]model.layers.75.input_layernorm.weight:   7%|â–‹         | 4/55 [00:00<00:03, 12.94w/s, dev=3]         model.layers.75.mlp.down_proj.weight:   9%|â–‰         | 5/55 [00:00<00:03, 16.16w/s, dev=3]  model.layers.75.mlp.down_proj.weight:  11%|â–ˆ         | 6/55 [00:00<00:03, 13.12w/s, dev=3]model.layers.75.mlp.gate_proj.weight:  11%|â–ˆ         | 6/55 [00:00<00:03, 13.12w/s, dev=3]model.layers.75.mlp.up_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:04, 11.52w/s, dev=3]  model.layers.75.mlp.up_proj.weight:  15%|â–ˆâ–        | 8/55 [00:00<00:04, 10.58w/s, dev=3]model.layers.75.post_attention_layernorm.weight:  15%|â–ˆâ–        | 8/55 [00:00<00:04, 10.58w/s, dev=3]model.layers.75.self_attn.k_proj.weight:  16%|â–ˆâ–‹        | 9/55 [00:00<00:03, 11.89w/s, dev=3]        model.layers.75.self_attn.o_proj.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:03, 13.11w/s, dev=3]model.layers.75.self_attn.q_proj.weight:  20%|â–ˆâ–ˆ        | 11/55 [00:00<00:03, 13.65w/s, dev=3]model.layers.75.self_attn.rotary_emb.inv_freq:  22%|â–ˆâ–ˆâ–       | 12/55 [00:00<00:03, 14.13w/s, dev=3]model.layers.75.self_attn.v_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:00<00:02, 15.30w/s, dev=3]      model.layers.76.input_layernorm.weight:  25%|â–ˆâ–ˆâ–Œ       | 14/55 [00:00<00:02, 16.36w/s, dev=3] model.layers.76.mlp.down_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 15/55 [00:00<00:02, 17.52w/s, dev=3]  model.layers.76.mlp.down_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 16/55 [00:01<00:02, 15.96w/s, dev=3]model.layers.76.mlp.gate_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 16/55 [00:01<00:02, 15.95w/s, dev=3]model.layers.76.mlp.up_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:01<00:02, 14.74w/s, dev=3]  model.layers.76.post_attention_layernorm.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/55 [00:01<00:02, 13.86w/s, dev=3]model.layers.76.self_attn.k_proj.weight:  35%|â–ˆâ–ˆâ–ˆâ–      | 19/55 [00:01<00:02, 14.63w/s, dev=3]        model.layers.76.self_attn.o_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:01<00:02, 15.32w/s, dev=3]model.layers.76.self_attn.q_proj.weight:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/55 [00:01<00:02, 15.59w/s, dev=3]model.layers.76.self_attn.rotary_emb.inv_freq:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/55 [00:01<00:02, 15.83w/s, dev=3]model.layers.76.self_attn.v_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 23/55 [00:01<00:01, 16.55w/s, dev=3]      model.layers.76.self_attn.v_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 24/55 [00:01<00:01, 17.20w/s, dev=3]model.layers.77.input_layernorm.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 24/55 [00:01<00:01, 17.19w/s, dev=3] model.layers.77.mlp.down_proj.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:01<00:01, 17.91w/s, dev=3]  model.layers.77.mlp.gate_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 26/55 [00:01<00:01, 16.85w/s, dev=3]model.layers.77.mlp.up_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 27/55 [00:01<00:01, 15.90w/s, dev=3]  model.layers.77.post_attention_layernorm.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 28/55 [00:01<00:01, 15.11w/s, dev=3]model.layers.77.self_attn.k_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/55 [00:01<00:01, 15.65w/s, dev=3]        model.layers.77.self_attn.o_proj.weight:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 30/55 [00:01<00:01, 16.13w/s, dev=3]model.layers.77.self_attn.q_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/55 [00:01<00:01, 16.29w/s, dev=3]model.layers.77.self_attn.q_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 32/55 [00:01<00:01, 16.44w/s, dev=3]model.layers.77.self_attn.rotary_emb.inv_freq:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 32/55 [00:01<00:01, 16.44w/s, dev=3]model.layers.77.self_attn.v_proj.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/55 [00:01<00:01, 16.95w/s, dev=3]      model.layers.78.input_layernorm.weight:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/55 [00:01<00:01, 17.40w/s, dev=3] model.layers.78.mlp.down_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:01<00:01, 17.91w/s, dev=3]  model.layers.78.mlp.gate_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 36/55 [00:02<00:01, 17.08w/s, dev=3]model.layers.78.mlp.up_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 37/55 [00:02<00:01, 16.40w/s, dev=3]  model.layers.78.post_attention_layernorm.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 38/55 [00:02<00:01, 15.79w/s, dev=3]model.layers.78.self_attn.k_proj.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 39/55 [00:02<00:00, 16.20w/s, dev=3]        model.layers.78.self_attn.k_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 40/55 [00:02<00:00, 16.58w/s, dev=3]model.layers.78.self_attn.o_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 40/55 [00:02<00:00, 16.58w/s, dev=3]model.layers.78.self_attn.q_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/55 [00:02<00:00, 16.68w/s, dev=3]model.layers.78.self_attn.rotary_emb.inv_freq:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 42/55 [00:02<00:00, 16.77w/s, dev=3]model.layers.78.self_attn.v_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 43/55 [00:02<00:00, 17.17w/s, dev=3]      model.layers.79.input_layernorm.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 44/55 [00:02<00:00, 17.51w/s, dev=3] model.layers.79.mlp.down_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:02<00:00, 17.91w/s, dev=3]  model.layers.79.mlp.gate_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 46/55 [00:02<00:00, 17.21w/s, dev=3]model.layers.79.mlp.up_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 47/55 [00:02<00:00, 16.67w/s, dev=3]  model.layers.79.mlp.up_proj.weight:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 48/55 [00:02<00:00, 16.20w/s, dev=3]model.layers.79.post_attention_layernorm.weight:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 48/55 [00:02<00:00, 16.19w/s, dev=3]model.layers.79.self_attn.k_proj.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 49/55 [00:02<00:00, 16.53w/s, dev=3]        model.layers.79.self_attn.o_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 50/55 [00:02<00:00, 16.82w/s, dev=3]model.layers.79.self_attn.q_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 51/55 [00:03<00:00, 16.92w/s, dev=3]model.layers.79.self_attn.rotary_emb.inv_freq:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 52/55 [00:03<00:00, 17.00w/s, dev=3]model.layers.79.self_attn.v_proj.weight:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 53/55 [00:03<00:00, 17.33w/s, dev=3]      model.norm.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 54/55 [00:03<00:00, 17.61w/s, dev=3]                                                                                                0%|          | 0/1 [00:00<?, ?w/s]lm_head.weight:   0%|          | 0/1 [00:00<?, ?w/s, dev=3]lm_head.weight: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  6.30w/s, dev=3]                                                                   self.device: cuda:0
Loading BASE model stabilityai/stablelm-base-alpha-3b-v2...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:01<00:01,  1.51s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.22it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.08it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[2] MOVING BASE MODEL TO GPU.../home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for iamgroot42/mimir contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/iamgroot42/mimir
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
Loading dataset the_pile...
Loading from HuggingFace!
Downloading data:   0%|          | 0.00/1.24M [00:00<?, ?B/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.24M/1.24M [00:00<00:00, 20.2MB/s]
Downloading data:   0%|          | 0.00/780k [00:00<?, ?B/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 780k/780k [00:00<00:00, 9.48MB/s]
Downloading data:   0%|          | 0.00/21.4M [00:00<?, ?B/s]Downloading data:   7%|â–‹         | 1.46M/21.4M [00:00<00:01, 14.6MB/s]Downloading data:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10.8M/21.4M [00:00<00:00, 60.9MB/s]Downloading data:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 17.9M/21.4M [00:00<00:00, 65.8MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21.4M/21.4M [00:00<00:00, 36.5MB/s]
Downloading data:   0%|          | 0.00/20.9M [00:00<?, ?B/s]Downloading data:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 7.88M/20.9M [00:00<00:00, 78.8MB/s]Downloading data:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 17.7M/20.9M [00:00<00:00, 90.3MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20.9M/20.9M [00:00<00:00, 89.7MB/s]
Downloading data:   0%|          | 0.00/1.24M [00:00<?, ?B/s]Downloading data:  11%|â–ˆ         | 132k/1.24M [00:00<00:00, 1.29MB/s]Downloading data:  21%|â–ˆâ–ˆ        | 262k/1.24M [00:00<00:00, 1.14MB/s]Downloading data:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 493k/1.24M [00:00<00:00, 1.22MB/s]Downloading data:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 755k/1.24M [00:00<00:00, 1.62MB/s]Downloading data:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 926k/1.24M [00:00<00:00, 1.49MB/s]Downloading data:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1.16M/1.24M [00:00<00:00, 1.72MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.24M/1.24M [00:00<00:00, 1.63MB/s]
Downloading data:   0%|          | 0.00/1.18M [00:00<?, ?B/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.18M/1.18M [00:00<00:00, 13.4MB/s]
Downloading data:   0%|          | 0.00/31.7M [00:00<?, ?B/s]Downloading data:   4%|â–         | 1.34M/31.7M [00:00<00:02, 13.3MB/s]Downloading data:  16%|â–ˆâ–‹        | 5.22M/31.7M [00:00<00:00, 28.3MB/s]Downloading data:  31%|â–ˆâ–ˆâ–ˆ       | 9.85M/31.7M [00:00<00:00, 36.5MB/s]Downloading data:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 15.4M/31.7M [00:00<00:00, 43.0MB/s]Downloading data:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22.0M/31.7M [00:00<00:00, 51.1MB/s]Downloading data:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 30.4M/31.7M [00:00<00:00, 62.2MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31.7M/31.7M [00:00<00:00, 51.2MB/s]
Downloading data:   0%|          | 0.00/31.5M [00:00<?, ?B/s]Downloading data:   4%|â–         | 1.38M/31.5M [00:00<00:02, 13.8MB/s]Downloading data:  25%|â–ˆâ–ˆâ–Œ       | 8.01M/31.5M [00:00<00:00, 44.6MB/s]Downloading data:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 13.8M/31.5M [00:00<00:00, 50.5MB/s]Downloading data:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 20.9M/31.5M [00:00<00:00, 58.9MB/s]Downloading data:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 29.4M/31.5M [00:00<00:00, 68.3MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31.5M/31.5M [00:00<00:00, 60.3MB/s]
Downloading data:   0%|          | 0.00/1.24M [00:00<?, ?B/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.24M/1.24M [00:00<00:00, 13.2MB/s]
Downloading data:   0%|          | 0.00/1.24M [00:00<?, ?B/s]Downloading data:  27%|â–ˆâ–ˆâ–‹       | 337k/1.24M [00:00<00:00, 3.36MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.24M/1.24M [00:00<00:00, 7.74MB/s]
Downloading data:   0%|          | 0.00/33.1M [00:00<?, ?B/s]Downloading data:  24%|â–ˆâ–ˆâ–       | 7.94M/33.1M [00:00<00:00, 79.4MB/s]Downloading data:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 17.7M/33.1M [00:00<00:00, 90.0MB/s]Downloading data:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 27.4M/33.1M [00:00<00:00, 93.1MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33.1M/33.1M [00:00<00:00, 92.5MB/s]
Downloading data:   0%|          | 0.00/33.3M [00:00<?, ?B/s]Downloading data:   4%|â–         | 1.49M/33.3M [00:00<00:02, 14.9MB/s]Downloading data:  32%|â–ˆâ–ˆâ–ˆâ–      | 10.5M/33.3M [00:00<00:00, 59.2MB/s]Downloading data:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 20.1M/33.3M [00:00<00:00, 76.1MB/s]Downloading data:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 29.6M/33.3M [00:00<00:00, 83.5MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33.3M/33.3M [00:00<00:00, 76.0MB/s]
Generating ngram_7_0.2 split: 0 examples [00:00, ? examples/s]Generating ngram_7_0.2 split: 77 examples [00:00, 757.87 examples/s]Generating ngram_7_0.2 split: 155 examples [00:00, 767.91 examples/s]Generating ngram_7_0.2 split: 268 examples [00:00, 753.85 examples/s]Generating ngram_7_0.2 split: 344 examples [00:00, 755.18 examples/s]Generating ngram_7_0.2 split: 425 examples [00:00, 770.98 examples/s]Generating ngram_7_0.2 split: 504 examples [00:00, 775.83 examples/s]Generating ngram_7_0.2 split: 583 examples [00:00, 775.79 examples/s]Generating ngram_7_0.2 split: 646 examples [00:01, 609.65 examples/s]
Generating ngram_13_0.2 split: 0 examples [00:00, ? examples/s]Generating ngram_13_0.2 split: 78 examples [00:00, 765.30 examples/s]Generating ngram_13_0.2 split: 157 examples [00:00, 774.14 examples/s]Generating ngram_13_0.2 split: 237 examples [00:00, 782.35 examples/s]Generating ngram_13_0.2 split: 355 examples [00:00, 780.96 examples/s]Generating ngram_13_0.2 split: 436 examples [00:00, 788.40 examples/s]Generating ngram_13_0.2 split: 549 examples [00:00, 770.20 examples/s]Generating ngram_13_0.2 split: 664 examples [00:00, 764.99 examples/s]Generating ngram_13_0.2 split: 775 examples [00:01, 752.96 examples/s]Generating ngram_13_0.2 split: 855 examples [00:01, 762.99 examples/s]Generating ngram_13_0.2 split: 937 examples [00:01, 774.58 examples/s]Generating ngram_13_0.2 split: 955 examples [00:01, 636.27 examples/s]
Generating ngram_13_0.8 split: 0 examples [00:00, ? examples/s]Generating ngram_13_0.8 split: 81 examples [00:00, 803.97 examples/s]Generating ngram_13_0.8 split: 202 examples [00:00, 803.90 examples/s]Generating ngram_13_0.8 split: 319 examples [00:00, 789.21 examples/s]Generating ngram_13_0.8 split: 400 examples [00:00, 792.43 examples/s]Generating ngram_13_0.8 split: 481 examples [00:00, 796.32 examples/s]Generating ngram_13_0.8 split: 565 examples [00:00, 804.52 examples/s]Generating ngram_13_0.8 split: 680 examples [00:00, 786.28 examples/s]Generating ngram_13_0.8 split: 791 examples [00:01, 765.27 examples/s]Generating ngram_13_0.8 split: 907 examples [00:01, 765.16 examples/s]Generating ngram_13_0.8 split: 988 examples [00:01, 774.41 examples/s]Generating ngram_13_0.8 split: 1000 examples [00:01, 640.97 examples/s]
Loading dataset the_pile...
Loading from HuggingFace!
Generating samples:   0%|          | 0/21 [00:00<?, ?it/s]Generating samples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:00<00:00, 1022.09it/s]
NEW N_SAMPLES IS  1000
Writing raw data to tmp_results/neo125_github_experiment/meta-llama_Llama-2-70b-hf/hackernews_ngram_13_<0.8_truncated/raw_data.json
Writing raw data to tmp_results/neo125_github_experiment/meta-llama_Llama-2-70b-hf/hackernews_ngram_13_<0.8_truncated/raw_data_lens.json
Computing criterion:   0%|          | 0/20 [00:00<?, ?it/s]Computing criterion:   5%|â–Œ         | 1/20 [00:22<07:13, 22.80s/it]Computing criterion:  10%|â–ˆ         | 2/20 [00:40<05:58, 19.94s/it]Computing criterion:  15%|â–ˆâ–Œ        | 3/20 [00:57<05:17, 18.70s/it]Computing criterion:  20%|â–ˆâ–ˆ        | 4/20 [01:15<04:53, 18.34s/it]Computing criterion:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [01:33<04:34, 18.28s/it]Computing criterion:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [01:51<04:13, 18.11s/it]Computing criterion:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [02:09<03:55, 18.14s/it]Computing criterion:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [02:27<03:34, 17.83s/it]Computing criterion:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [02:44<03:14, 17.65s/it]Computing criterion:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [03:02<02:57, 17.75s/it]Computing criterion:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [03:20<02:40, 17.78s/it]Computing criterion:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [03:39<02:25, 18.24s/it]Computing criterion:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [03:58<02:08, 18.35s/it]Computing criterion:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [04:17<01:51, 18.60s/it]Computing criterion:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [04:35<01:31, 18.36s/it]Computing criterion:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [04:52<01:12, 18.22s/it]Computing criterion:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [05:10<00:54, 18.09s/it]Computing criterion:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [05:29<00:36, 18.22s/it]Computing criterion:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [05:46<00:18, 18.03s/it]Computing criterion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [06:03<00:00, 17.69s/it]Computing criterion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [06:03<00:00, 18.19s/it]
Ref scores:   0%|          | 0/1000 [00:00<?, ?it/s]Ref scores:   0%|          | 1/1000 [00:09<2:40:06,  9.62s/it]Ref scores:   0%|          | 2/1000 [00:09<1:08:23,  4.11s/it]Ref scores:   0%|          | 3/1000 [00:10<38:05,  2.29s/it]  Ref scores:   0%|          | 4/1000 [00:10<23:59,  1.45s/it]Ref scores:   0%|          | 5/1000 [00:10<16:39,  1.00s/it]Ref scores:   1%|          | 6/1000 [00:10<12:39,  1.31it/s]Ref scores:   1%|          | 7/1000 [00:10<09:19,  1.78it/s]Ref scores:   1%|          | 8/1000 [00:10<07:08,  2.31it/s]Ref scores:   1%|          | 9/1000 [00:11<05:40,  2.91it/s]Ref scores:   1%|          | 10/1000 [00:11<04:40,  3.53it/s]Ref scores:   1%|          | 11/1000 [00:11<03:58,  4.14it/s]Ref scores:   1%|          | 12/1000 [00:11<03:30,  4.69it/s]Ref scores:   1%|â–         | 13/1000 [00:11<03:00,  5.46it/s]Ref scores:   1%|â–         | 14/1000 [00:11<02:49,  5.80it/s]Ref scores:   2%|â–         | 15/1000 [00:12<03:15,  5.05it/s]Ref scores:   2%|â–         | 16/1000 [00:12<02:58,  5.52it/s]Ref scores:   2%|â–         | 17/1000 [00:12<02:47,  5.86it/s]Ref scores:   2%|â–         | 18/1000 [00:12<03:11,  5.13it/s]Ref scores:   2%|â–         | 19/1000 [00:12<03:15,  5.01it/s]Ref scores:   2%|â–         | 20/1000 [00:13<03:19,  4.91it/s]Ref scores:   2%|â–         | 21/1000 [00:13<02:54,  5.61it/s]Ref scores:   2%|â–         | 22/1000 [00:13<02:44,  5.94it/s]Ref scores:   2%|â–         | 24/1000 [00:13<02:10,  7.46it/s]Ref scores:   2%|â–Ž         | 25/1000 [00:13<02:31,  6.45it/s]Ref scores:   3%|â–Ž         | 26/1000 [00:13<02:28,  6.54it/s]Ref scores:   3%|â–Ž         | 27/1000 [00:14<02:26,  6.62it/s]Ref scores:   3%|â–Ž         | 28/1000 [00:14<02:18,  7.02it/s]Ref scores:   3%|â–Ž         | 29/1000 [00:14<02:19,  6.97it/s]Ref scores:   3%|â–Ž         | 30/1000 [00:14<02:19,  6.94it/s]Ref scores:   3%|â–Ž         | 31/1000 [00:14<02:19,  6.93it/s]Ref scores:   3%|â–Ž         | 32/1000 [00:14<02:41,  5.98it/s]Ref scores:   3%|â–Ž         | 33/1000 [00:14<02:35,  6.21it/s]Ref scores:   3%|â–Ž         | 34/1000 [00:15<02:31,  6.38it/s]Ref scores:   4%|â–Ž         | 35/1000 [00:15<02:27,  6.54it/s]Ref scores:   4%|â–Ž         | 36/1000 [00:15<02:25,  6.63it/s]Ref scores:   4%|â–Ž         | 37/1000 [00:15<02:42,  5.93it/s]Ref scores:   4%|â–         | 38/1000 [00:15<02:35,  6.19it/s]Ref scores:   4%|â–         | 39/1000 [00:15<02:49,  5.66it/s]Ref scores:   4%|â–         | 40/1000 [00:16<02:40,  5.99it/s]Ref scores:   4%|â–         | 41/1000 [00:16<03:16,  4.88it/s]Ref scores:   4%|â–         | 42/1000 [00:16<02:59,  5.33it/s]Ref scores:   4%|â–         | 43/1000 [00:16<03:04,  5.20it/s]Ref scores:   4%|â–         | 44/1000 [00:16<03:10,  5.01it/s]Ref scores:   4%|â–         | 45/1000 [00:17<02:55,  5.46it/s]Ref scores:   5%|â–         | 46/1000 [00:17<02:45,  5.77it/s]Ref scores:   5%|â–         | 47/1000 [00:17<02:37,  6.04it/s]Ref scores:   5%|â–         | 48/1000 [00:17<02:21,  6.72it/s]Ref scores:   5%|â–         | 49/1000 [00:17<02:21,  6.71it/s]Ref scores:   5%|â–Œ         | 50/1000 [00:17<02:38,  6.01it/s]Ref scores:   5%|â–Œ         | 51/1000 [00:17<02:32,  6.24it/s]Ref scores:   5%|â–Œ         | 52/1000 [00:18<02:28,  6.40it/s]Ref scores:   5%|â–Œ         | 53/1000 [00:18<02:55,  5.38it/s]Ref scores:   5%|â–Œ         | 54/1000 [00:18<02:43,  5.78it/s]Ref scores:   6%|â–Œ         | 55/1000 [00:18<02:58,  5.29it/s]Ref scores:   6%|â–Œ         | 56/1000 [00:18<03:04,  5.11it/s]Ref scores:   6%|â–Œ         | 57/1000 [00:19<03:08,  5.01it/s]Ref scores:   6%|â–Œ         | 58/1000 [00:19<03:11,  4.92it/s]Ref scores:   6%|â–Œ         | 59/1000 [00:19<02:56,  5.34it/s]Ref scores:   6%|â–Œ         | 60/1000 [00:19<02:36,  5.99it/s]Ref scores:   6%|â–Œ         | 61/1000 [00:19<02:48,  5.59it/s]Ref scores:   6%|â–Œ         | 62/1000 [00:19<02:31,  6.18it/s]Ref scores:   6%|â–‹         | 63/1000 [00:20<03:08,  4.98it/s]Ref scores:   6%|â–‹         | 64/1000 [00:20<02:53,  5.41it/s]Ref scores:   6%|â–‹         | 65/1000 [00:20<02:41,  5.80it/s]Ref scores:   7%|â–‹         | 66/1000 [00:20<02:34,  6.06it/s]Ref scores:   7%|â–‹         | 67/1000 [00:20<02:29,  6.24it/s]Ref scores:   7%|â–‹         | 68/1000 [00:21<03:05,  5.02it/s]Ref scores:   7%|â–‹         | 69/1000 [00:21<02:50,  5.46it/s]Ref scores:   7%|â–‹         | 71/1000 [00:21<02:17,  6.77it/s]Ref scores:   7%|â–‹         | 72/1000 [00:21<02:17,  6.77it/s]Ref scores:   7%|â–‹         | 73/1000 [00:21<02:31,  6.13it/s]Ref scores:   7%|â–‹         | 74/1000 [00:22<02:31,  6.11it/s]Ref scores:   8%|â–Š         | 76/1000 [00:22<02:35,  5.94it/s]Ref scores:   8%|â–Š         | 77/1000 [00:22<02:34,  5.98it/s]Ref scores:   8%|â–Š         | 78/1000 [00:22<02:29,  6.18it/s]Ref scores:   8%|â–Š         | 79/1000 [00:22<02:40,  5.75it/s]Ref scores:   8%|â–Š         | 80/1000 [00:23<02:52,  5.33it/s]Ref scores:   8%|â–Š         | 81/1000 [00:23<02:41,  5.67it/s]Ref scores:   8%|â–Š         | 82/1000 [00:23<02:51,  5.36it/s]Ref scores:   8%|â–Š         | 83/1000 [00:23<02:58,  5.15it/s]Ref scores:   8%|â–Š         | 84/1000 [00:23<02:45,  5.52it/s]Ref scores:   8%|â–Š         | 85/1000 [00:24<02:35,  5.90it/s]Ref scores:   9%|â–Š         | 86/1000 [00:24<02:46,  5.49it/s]Ref scores:   9%|â–Š         | 87/1000 [00:24<02:36,  5.82it/s]Ref scores:   9%|â–‰         | 88/1000 [00:24<02:46,  5.47it/s]Ref scores:   9%|â–‰         | 89/1000 [00:24<02:57,  5.12it/s]Ref scores:   9%|â–‰         | 90/1000 [00:24<02:44,  5.54it/s]Ref scores:   9%|â–‰         | 91/1000 [00:25<02:35,  5.84it/s]Ref scores:   9%|â–‰         | 92/1000 [00:25<02:28,  6.10it/s]Ref scores:   9%|â–‰         | 94/1000 [00:25<02:03,  7.33it/s]Ref scores:  10%|â–‰         | 95/1000 [00:25<02:31,  5.99it/s]Ref scores:  10%|â–‰         | 96/1000 [00:25<02:44,  5.50it/s]Ref scores:  10%|â–‰         | 97/1000 [00:26<02:54,  5.17it/s]Ref scores:  10%|â–‰         | 98/1000 [00:26<02:43,  5.53it/s]Ref scores:  10%|â–‰         | 99/1000 [00:26<02:34,  5.83it/s]Ref scores:  10%|â–ˆ         | 100/1000 [00:26<02:20,  6.40it/s]Ref scores:  10%|â–ˆ         | 101/1000 [00:26<02:11,  6.86it/s]Ref scores:  10%|â–ˆ         | 102/1000 [00:26<02:11,  6.81it/s]Ref scores:  10%|â–ˆ         | 103/1000 [00:27<02:28,  6.04it/s]Ref scores:  10%|â–ˆ         | 104/1000 [00:27<02:40,  5.58it/s]Ref scores:  10%|â–ˆ         | 105/1000 [00:27<02:30,  5.96it/s]Ref scores:  11%|â–ˆ         | 106/1000 [00:27<02:14,  6.66it/s]Ref scores:  11%|â–ˆ         | 108/1000 [00:27<02:17,  6.51it/s]Ref scores:  11%|â–ˆ         | 109/1000 [00:27<02:18,  6.41it/s]Ref scores:  11%|â–ˆ         | 110/1000 [00:28<02:20,  6.33it/s]Ref scores:  11%|â–ˆ         | 112/1000 [00:28<02:04,  7.11it/s]Ref scores:  11%|â–ˆâ–        | 113/1000 [00:28<02:28,  5.97it/s]Ref scores:  11%|â–ˆâ–        | 114/1000 [00:28<02:24,  6.14it/s]Ref scores:  12%|â–ˆâ–        | 115/1000 [00:29<02:46,  5.32it/s]Ref scores:  12%|â–ˆâ–        | 116/1000 [00:29<02:35,  5.67it/s]Ref scores:  12%|â–ˆâ–        | 117/1000 [00:29<02:44,  5.35it/s]Ref scores:  12%|â–ˆâ–        | 118/1000 [00:29<02:35,  5.68it/s]Ref scores:  12%|â–ˆâ–        | 119/1000 [00:29<02:18,  6.34it/s]Ref scores:  12%|â–ˆâ–        | 120/1000 [00:29<02:09,  6.80it/s]Ref scores:  12%|â–ˆâ–        | 121/1000 [00:30<02:28,  5.90it/s]Ref scores:  12%|â–ˆâ–        | 122/1000 [00:30<02:22,  6.14it/s]Ref scores:  12%|â–ˆâ–        | 123/1000 [00:30<02:34,  5.67it/s]Ref scores:  12%|â–ˆâ–        | 124/1000 [00:30<02:27,  5.95it/s]Ref scores:  12%|â–ˆâ–Ž        | 125/1000 [00:30<02:13,  6.57it/s]Ref scores:  13%|â–ˆâ–Ž        | 127/1000 [00:30<02:10,  6.71it/s]Ref scores:  13%|â–ˆâ–Ž        | 128/1000 [00:31<02:34,  5.65it/s]Ref scores:  13%|â–ˆâ–Ž        | 129/1000 [00:31<02:27,  5.91it/s]Ref scores:  13%|â–ˆâ–Ž        | 130/1000 [00:31<02:36,  5.57it/s]Ref scores:  13%|â–ˆâ–Ž        | 131/1000 [00:31<02:32,  5.71it/s]Ref scores:  13%|â–ˆâ–Ž        | 132/1000 [00:31<02:25,  5.97it/s]Ref scores:  13%|â–ˆâ–Ž        | 133/1000 [00:32<02:47,  5.16it/s]Ref scores:  13%|â–ˆâ–Ž        | 134/1000 [00:32<02:35,  5.57it/s]Ref scores:  14%|â–ˆâ–Ž        | 135/1000 [00:32<02:53,  4.98it/s]Ref scores:  14%|â–ˆâ–Ž        | 136/1000 [00:32<02:39,  5.40it/s]Ref scores:  14%|â–ˆâ–Ž        | 137/1000 [00:32<02:29,  5.77it/s]Ref scores:  14%|â–ˆâ–        | 138/1000 [00:32<02:26,  5.86it/s]Ref scores:  14%|â–ˆâ–        | 139/1000 [00:33<02:38,  5.42it/s]Ref scores:  14%|â–ˆâ–        | 140/1000 [00:33<02:28,  5.78it/s]Ref scores:  14%|â–ˆâ–        | 141/1000 [00:33<02:21,  6.09it/s]Ref scores:  14%|â–ˆâ–        | 142/1000 [00:33<02:16,  6.28it/s]Ref scores:  14%|â–ˆâ–        | 143/1000 [00:33<02:12,  6.45it/s]Ref scores:  14%|â–ˆâ–        | 144/1000 [00:33<02:27,  5.79it/s]Ref scores:  14%|â–ˆâ–        | 145/1000 [00:34<02:40,  5.34it/s]Ref scores:  15%|â–ˆâ–        | 146/1000 [00:34<02:29,  5.71it/s]Ref scores:  15%|â–ˆâ–        | 147/1000 [00:34<02:14,  6.33it/s]Ref scores:  15%|â–ˆâ–        | 148/1000 [00:34<02:39,  5.33it/s]Ref scores:  15%|â–ˆâ–        | 149/1000 [00:34<02:45,  5.15it/s]Ref scores:  15%|â–ˆâ–Œ        | 150/1000 [00:35<02:33,  5.56it/s]Ref scores:  15%|â–ˆâ–Œ        | 151/1000 [00:35<02:52,  4.92it/s]Ref scores:  15%|â–ˆâ–Œ        | 152/1000 [00:35<02:53,  4.87it/s]Ref scores:  15%|â–ˆâ–Œ        | 153/1000 [00:35<02:57,  4.76it/s]Ref scores:  15%|â–ˆâ–Œ        | 154/1000 [00:35<02:56,  4.79it/s]Ref scores:  16%|â–ˆâ–Œ        | 155/1000 [00:36<02:57,  4.76it/s]Ref scores:  16%|â–ˆâ–Œ        | 156/1000 [00:36<03:19,  4.23it/s]Ref scores:  16%|â–ˆâ–Œ        | 157/1000 [00:36<02:55,  4.81it/s]Ref scores:  16%|â–ˆâ–Œ        | 158/1000 [00:36<03:09,  4.45it/s]Ref scores:  16%|â–ˆâ–Œ        | 159/1000 [00:37<03:09,  4.44it/s]Ref scores:  16%|â–ˆâ–Œ        | 160/1000 [00:37<03:09,  4.44it/s]Ref scores:  16%|â–ˆâ–Œ        | 161/1000 [00:37<03:17,  4.25it/s]Ref scores:  16%|â–ˆâ–Œ        | 162/1000 [00:37<02:54,  4.81it/s]Ref scores:  16%|â–ˆâ–‹        | 163/1000 [00:37<02:30,  5.57it/s]Ref scores:  16%|â–ˆâ–‹        | 164/1000 [00:38<02:39,  5.26it/s]Ref scores:  16%|â–ˆâ–‹        | 165/1000 [00:38<02:27,  5.65it/s]Ref scores:  17%|â–ˆâ–‹        | 166/1000 [00:38<02:19,  5.97it/s]Ref scores:  17%|â–ˆâ–‹        | 167/1000 [00:38<02:18,  6.03it/s]Ref scores:  17%|â–ˆâ–‹        | 168/1000 [00:38<02:41,  5.14it/s]Ref scores:  17%|â–ˆâ–‹        | 169/1000 [00:38<02:29,  5.55it/s]Ref scores:  17%|â–ˆâ–‹        | 170/1000 [00:39<02:21,  5.87it/s]Ref scores:  17%|â–ˆâ–‹        | 171/1000 [00:39<02:14,  6.16it/s]Ref scores:  17%|â–ˆâ–‹        | 172/1000 [00:39<02:10,  6.37it/s]Ref scores:  17%|â–ˆâ–‹        | 173/1000 [00:39<02:27,  5.62it/s]Ref scores:  17%|â–ˆâ–‹        | 174/1000 [00:39<02:18,  5.95it/s]Ref scores:  18%|â–ˆâ–Š        | 175/1000 [00:39<02:13,  6.19it/s]Ref scores:  18%|â–ˆâ–Š        | 176/1000 [00:40<02:38,  5.20it/s]Ref scores:  18%|â–ˆâ–Š        | 178/1000 [00:40<02:25,  5.67it/s]Ref scores:  18%|â–ˆâ–Š        | 180/1000 [00:40<02:16,  6.02it/s]Ref scores:  18%|â–ˆâ–Š        | 181/1000 [00:40<02:11,  6.21it/s]Ref scores:  18%|â–ˆâ–Š        | 182/1000 [00:41<02:09,  6.33it/s]Ref scores:  18%|â–ˆâ–Š        | 183/1000 [00:41<02:20,  5.82it/s]Ref scores:  18%|â–ˆâ–Š        | 184/1000 [00:41<02:33,  5.33it/s]Ref scores:  18%|â–ˆâ–Š        | 185/1000 [00:41<02:40,  5.09it/s]Ref scores:  19%|â–ˆâ–Š        | 186/1000 [00:41<02:27,  5.53it/s]Ref scores:  19%|â–ˆâ–Š        | 187/1000 [00:42<02:36,  5.20it/s]Ref scores:  19%|â–ˆâ–‰        | 188/1000 [00:42<02:25,  5.58it/s]Ref scores:  19%|â–ˆâ–‰        | 189/1000 [00:42<02:21,  5.74it/s]Ref scores:  19%|â–ˆâ–‰        | 191/1000 [00:42<02:00,  6.71it/s]Ref scores:  19%|â–ˆâ–‰        | 192/1000 [00:42<01:59,  6.74it/s]Ref scores:  19%|â–ˆâ–‰        | 193/1000 [00:42<01:52,  7.19it/s]Ref scores:  19%|â–ˆâ–‰        | 194/1000 [00:43<01:54,  7.04it/s]Ref scores:  20%|â–ˆâ–‰        | 195/1000 [00:43<02:20,  5.74it/s]Ref scores:  20%|â–ˆâ–‰        | 196/1000 [00:43<02:05,  6.41it/s]Ref scores:  20%|â–ˆâ–‰        | 197/1000 [00:43<02:02,  6.53it/s]Ref scores:  20%|â–ˆâ–‰        | 198/1000 [00:43<02:05,  6.41it/s]Ref scores:  20%|â–ˆâ–‰        | 199/1000 [00:43<01:55,  6.96it/s]Ref scores:  20%|â–ˆâ–ˆ        | 200/1000 [00:43<01:56,  6.90it/s]Ref scores:  20%|â–ˆâ–ˆ        | 201/1000 [00:44<01:48,  7.39it/s]Ref scores:  20%|â–ˆâ–ˆ        | 202/1000 [00:44<02:06,  6.30it/s]Ref scores:  20%|â–ˆâ–ˆ        | 203/1000 [00:44<02:07,  6.26it/s]Ref scores:  20%|â–ˆâ–ˆ        | 204/1000 [00:44<02:04,  6.40it/s]Ref scores:  20%|â–ˆâ–ˆ        | 205/1000 [00:44<02:01,  6.52it/s]Ref scores:  21%|â–ˆâ–ˆ        | 206/1000 [00:44<02:15,  5.87it/s]Ref scores:  21%|â–ˆâ–ˆ        | 207/1000 [00:45<02:08,  6.19it/s]Ref scores:  21%|â–ˆâ–ˆ        | 208/1000 [00:45<02:04,  6.37it/s]Ref scores:  21%|â–ˆâ–ˆ        | 209/1000 [00:45<02:01,  6.53it/s]Ref scores:  21%|â–ˆâ–ˆ        | 210/1000 [00:45<01:58,  6.65it/s]Ref scores:  21%|â–ˆâ–ˆ        | 211/1000 [00:45<01:57,  6.69it/s]Ref scores:  21%|â–ˆâ–ˆ        | 212/1000 [00:45<02:15,  5.84it/s]Ref scores:  21%|â–ˆâ–ˆâ–       | 213/1000 [00:46<02:07,  6.15it/s]Ref scores:  21%|â–ˆâ–ˆâ–       | 214/1000 [00:46<02:38,  4.94it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 216/1000 [00:46<02:22,  5.52it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 217/1000 [00:46<02:42,  4.81it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 218/1000 [00:47<03:00,  4.32it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 219/1000 [00:47<02:42,  4.81it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 220/1000 [00:47<02:27,  5.27it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 221/1000 [00:47<02:17,  5.65it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 222/1000 [00:47<02:04,  6.27it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 223/1000 [00:48<02:18,  5.59it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 224/1000 [00:48<02:37,  4.94it/s]Ref scores:  22%|â–ˆâ–ˆâ–Ž       | 225/1000 [00:48<02:50,  4.54it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 226/1000 [00:48<02:33,  5.03it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 227/1000 [00:48<02:39,  4.85it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 228/1000 [00:49<02:41,  4.79it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 229/1000 [00:49<02:26,  5.25it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 230/1000 [00:49<02:15,  5.69it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 231/1000 [00:49<02:08,  5.99it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 232/1000 [00:49<02:30,  5.12it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 233/1000 [00:49<02:22,  5.40it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 234/1000 [00:50<02:39,  4.81it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 235/1000 [00:50<02:41,  4.72it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 236/1000 [00:50<02:41,  4.74it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 237/1000 [00:50<02:41,  4.72it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 238/1000 [00:51<02:26,  5.20it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 239/1000 [00:51<02:16,  5.59it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 240/1000 [00:51<02:08,  5.92it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 241/1000 [00:51<02:06,  5.99it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 242/1000 [00:51<02:01,  6.21it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 243/1000 [00:51<01:58,  6.40it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 245/1000 [00:52<01:43,  7.27it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 247/1000 [00:52<01:37,  7.71it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 248/1000 [00:52<01:35,  7.85it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 249/1000 [00:52<01:51,  6.76it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 250/1000 [00:52<01:49,  6.84it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 251/1000 [00:52<01:48,  6.90it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 252/1000 [00:53<02:12,  5.66it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 253/1000 [00:53<02:21,  5.29it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 255/1000 [00:53<01:56,  6.37it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 256/1000 [00:53<01:54,  6.48it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 257/1000 [00:53<02:05,  5.91it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 258/1000 [00:54<02:16,  5.43it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 259/1000 [00:54<02:23,  5.16it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 260/1000 [00:54<02:13,  5.55it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 261/1000 [00:54<02:20,  5.26it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 262/1000 [00:54<02:26,  5.03it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 263/1000 [00:55<02:14,  5.46it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 264/1000 [00:55<02:09,  5.66it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 265/1000 [00:55<02:02,  5.99it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 266/1000 [00:55<01:50,  6.64it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 267/1000 [00:55<02:03,  5.94it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 268/1000 [00:56<02:22,  5.15it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 269/1000 [00:56<02:11,  5.57it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 270/1000 [00:56<02:03,  5.91it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 271/1000 [00:56<01:57,  6.18it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 272/1000 [00:56<02:18,  5.27it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 273/1000 [00:56<02:02,  5.96it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 274/1000 [00:57<02:13,  5.42it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 275/1000 [00:57<02:19,  5.20it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 276/1000 [00:57<02:32,  4.73it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 278/1000 [00:57<02:12,  5.45it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 279/1000 [00:57<02:00,  5.99it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 280/1000 [00:58<01:59,  6.04it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 281/1000 [00:58<02:08,  5.60it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 282/1000 [00:58<02:01,  5.92it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 283/1000 [00:58<01:59,  6.00it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 284/1000 [00:58<02:18,  5.16it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 285/1000 [00:59<02:08,  5.55it/s]Ref scores:  29%|â–ˆâ–ˆâ–Š       | 286/1000 [00:59<02:14,  5.29it/s]Ref scores:  29%|â–ˆâ–ˆâ–Š       | 287/1000 [00:59<02:19,  5.12it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 288/1000 [00:59<02:21,  5.03it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 289/1000 [00:59<02:02,  5.80it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 290/1000 [00:59<01:55,  6.13it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 291/1000 [01:00<02:06,  5.60it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 292/1000 [01:00<01:59,  5.94it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 293/1000 [01:00<02:09,  5.46it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 294/1000 [01:00<02:25,  4.86it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 295/1000 [01:00<02:36,  4.51it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 296/1000 [01:01<02:20,  5.02it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 298/1000 [01:01<01:52,  6.23it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 299/1000 [01:01<01:49,  6.41it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 300/1000 [01:01<02:01,  5.76it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 301/1000 [01:01<02:11,  5.34it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 302/1000 [01:02<02:02,  5.71it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 303/1000 [01:02<01:56,  5.99it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 304/1000 [01:02<02:05,  5.54it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 305/1000 [01:02<02:13,  5.20it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 306/1000 [01:02<02:03,  5.61it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 307/1000 [01:02<02:00,  5.76it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 308/1000 [01:03<01:54,  6.04it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 309/1000 [01:03<01:53,  6.07it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 310/1000 [01:03<01:50,  6.26it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 311/1000 [01:03<02:09,  5.31it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 312/1000 [01:03<01:59,  5.75it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆâ–      | 313/1000 [01:03<01:54,  6.02it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆâ–      | 314/1000 [01:04<02:03,  5.56it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 315/1000 [01:04<01:49,  6.23it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 316/1000 [01:04<01:46,  6.40it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 317/1000 [01:04<01:44,  6.55it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 318/1000 [01:04<01:58,  5.78it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 319/1000 [01:04<01:51,  6.08it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 320/1000 [01:05<01:51,  6.10it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 321/1000 [01:05<01:48,  6.28it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 322/1000 [01:05<02:01,  5.60it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 323/1000 [01:05<02:16,  4.95it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 324/1000 [01:05<02:20,  4.82it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 325/1000 [01:06<02:37,  4.28it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 326/1000 [01:06<02:41,  4.17it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 327/1000 [01:06<02:22,  4.73it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 328/1000 [01:06<02:38,  4.24it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 329/1000 [01:07<02:33,  4.38it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 330/1000 [01:07<02:38,  4.22it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 331/1000 [01:07<02:19,  4.81it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 332/1000 [01:07<02:06,  5.28it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 333/1000 [01:07<01:56,  5.74it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 334/1000 [01:08<02:06,  5.26it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 335/1000 [01:08<01:56,  5.70it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 336/1000 [01:08<01:49,  6.06it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 337/1000 [01:08<01:40,  6.62it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 338/1000 [01:08<01:37,  6.76it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 339/1000 [01:08<01:36,  6.87it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 340/1000 [01:09<02:00,  5.50it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 342/1000 [01:09<01:49,  6.01it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 343/1000 [01:09<01:39,  6.58it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 344/1000 [01:09<01:38,  6.66it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 345/1000 [01:09<01:52,  5.84it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 346/1000 [01:09<01:46,  6.14it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 347/1000 [01:10<01:42,  6.34it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 348/1000 [01:10<01:40,  6.49it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 349/1000 [01:10<01:59,  5.44it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 350/1000 [01:10<01:51,  5.82it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 351/1000 [01:10<01:46,  6.09it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 352/1000 [01:11<01:57,  5.53it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 353/1000 [01:11<01:43,  6.24it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 354/1000 [01:11<01:35,  6.78it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 355/1000 [01:11<01:34,  6.82it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 356/1000 [01:11<01:33,  6.87it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 357/1000 [01:11<01:34,  6.84it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 358/1000 [01:11<01:33,  6.86it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 359/1000 [01:11<01:33,  6.87it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 360/1000 [01:12<01:45,  6.09it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 361/1000 [01:12<01:56,  5.49it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 362/1000 [01:12<01:48,  5.86it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 363/1000 [01:12<01:56,  5.47it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 364/1000 [01:12<01:52,  5.66it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 365/1000 [01:13<01:39,  6.40it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 366/1000 [01:13<01:46,  5.94it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 367/1000 [01:13<01:35,  6.61it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 368/1000 [01:13<01:34,  6.66it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 369/1000 [01:13<01:55,  5.46it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 370/1000 [01:13<01:48,  5.79it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 371/1000 [01:13<01:36,  6.50it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 372/1000 [01:14<01:55,  5.42it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 373/1000 [01:14<02:00,  5.22it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 374/1000 [01:14<01:46,  5.90it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 375/1000 [01:14<01:41,  6.16it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 376/1000 [01:14<01:38,  6.36it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 378/1000 [01:15<01:37,  6.38it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 379/1000 [01:15<01:57,  5.26it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 380/1000 [01:15<02:00,  5.13it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 381/1000 [01:15<01:47,  5.74it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 382/1000 [01:15<01:43,  5.99it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 383/1000 [01:16<01:58,  5.20it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 384/1000 [01:16<01:50,  5.59it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 385/1000 [01:16<01:44,  5.91it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 386/1000 [01:16<01:39,  6.15it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 387/1000 [01:16<02:03,  4.96it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 388/1000 [01:17<01:56,  5.26it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 390/1000 [01:17<01:46,  5.75it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 391/1000 [01:17<01:41,  6.01it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 392/1000 [01:17<01:55,  5.25it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 393/1000 [01:18<01:58,  5.12it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 394/1000 [01:18<01:45,  5.74it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 395/1000 [01:18<01:40,  6.03it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 396/1000 [01:18<01:37,  6.23it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 397/1000 [01:18<01:54,  5.26it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 398/1000 [01:18<01:46,  5.66it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 399/1000 [01:19<02:00,  4.99it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 400/1000 [01:19<01:50,  5.45it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 401/1000 [01:19<01:43,  5.78it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 402/1000 [01:19<01:38,  6.08it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 403/1000 [01:19<01:34,  6.32it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 404/1000 [01:19<01:32,  6.44it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 405/1000 [01:19<01:33,  6.34it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 406/1000 [01:20<01:31,  6.47it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 407/1000 [01:20<01:29,  6.60it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 408/1000 [01:20<01:24,  7.04it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 409/1000 [01:20<01:49,  5.38it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 410/1000 [01:20<01:42,  5.77it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 411/1000 [01:21<01:57,  5.01it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 412/1000 [01:21<02:01,  4.82it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 413/1000 [01:21<02:01,  4.81it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 414/1000 [01:21<01:51,  5.27it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 415/1000 [01:21<01:46,  5.51it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 416/1000 [01:21<01:42,  5.69it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 417/1000 [01:22<01:32,  6.28it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 418/1000 [01:22<01:29,  6.50it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 420/1000 [01:22<01:19,  7.29it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 421/1000 [01:22<01:20,  7.17it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 422/1000 [01:22<01:21,  7.09it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 423/1000 [01:23<01:39,  5.78it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 424/1000 [01:23<01:46,  5.43it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 425/1000 [01:23<01:50,  5.21it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 426/1000 [01:23<01:52,  5.10it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 427/1000 [01:23<01:51,  5.12it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 428/1000 [01:24<01:54,  4.99it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 429/1000 [01:24<01:45,  5.42it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 430/1000 [01:24<01:38,  5.76it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 431/1000 [01:24<01:33,  6.06it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 432/1000 [01:24<01:30,  6.30it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 433/1000 [01:24<01:27,  6.50it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 434/1000 [01:25<01:45,  5.38it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 435/1000 [01:25<01:48,  5.19it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 436/1000 [01:25<01:51,  5.08it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 437/1000 [01:25<01:52,  4.99it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 438/1000 [01:25<01:37,  5.75it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 439/1000 [01:26<01:43,  5.40it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 440/1000 [01:26<01:36,  5.80it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 441/1000 [01:26<01:31,  6.08it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 442/1000 [01:26<01:28,  6.32it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 443/1000 [01:26<01:38,  5.63it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 444/1000 [01:26<01:35,  5.79it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 445/1000 [01:26<01:34,  5.90it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 446/1000 [01:27<01:29,  6.17it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 447/1000 [01:27<01:26,  6.42it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 448/1000 [01:27<01:24,  6.52it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 449/1000 [01:27<01:22,  6.64it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 450/1000 [01:27<01:22,  6.71it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 451/1000 [01:27<01:20,  6.81it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 452/1000 [01:27<01:20,  6.84it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 453/1000 [01:28<01:13,  7.40it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 454/1000 [01:28<01:33,  5.84it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 455/1000 [01:28<01:39,  5.47it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 456/1000 [01:28<01:33,  5.84it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 457/1000 [01:28<01:41,  5.35it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 458/1000 [01:29<01:34,  5.73it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 459/1000 [01:29<01:29,  6.03it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 460/1000 [01:29<01:36,  5.62it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 461/1000 [01:29<01:30,  5.93it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 462/1000 [01:29<01:27,  6.18it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 463/1000 [01:29<01:32,  5.82it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 464/1000 [01:30<01:45,  5.08it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 465/1000 [01:30<01:37,  5.52it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 466/1000 [01:30<01:31,  5.86it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 467/1000 [01:30<01:26,  6.14it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 468/1000 [01:30<01:19,  6.71it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 469/1000 [01:31<01:42,  5.17it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 470/1000 [01:31<01:47,  4.94it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 472/1000 [01:31<01:25,  6.14it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 473/1000 [01:31<01:23,  6.29it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 474/1000 [01:31<01:30,  5.78it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 475/1000 [01:31<01:26,  6.05it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 476/1000 [01:32<01:25,  6.09it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 477/1000 [01:32<01:33,  5.57it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 478/1000 [01:32<01:30,  5.74it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 479/1000 [01:32<01:43,  5.04it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 480/1000 [01:33<01:46,  4.89it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 481/1000 [01:33<01:36,  5.36it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 482/1000 [01:33<01:32,  5.57it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 483/1000 [01:33<01:27,  5.90it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 484/1000 [01:33<01:23,  6.15it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 485/1000 [01:33<01:30,  5.66it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 486/1000 [01:34<01:36,  5.34it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 487/1000 [01:34<01:47,  4.79it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 488/1000 [01:34<01:48,  4.71it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 489/1000 [01:34<01:38,  5.21it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 490/1000 [01:34<01:26,  5.90it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 491/1000 [01:34<01:23,  6.13it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 492/1000 [01:35<01:20,  6.31it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 494/1000 [01:35<01:19,  6.33it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 495/1000 [01:35<01:18,  6.46it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 496/1000 [01:35<01:32,  5.48it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 497/1000 [01:36<01:42,  4.89it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 498/1000 [01:36<01:34,  5.33it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 499/1000 [01:36<01:37,  5.16it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 500/1000 [01:36<01:40,  4.95it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 501/1000 [01:36<01:32,  5.41it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 502/1000 [01:36<01:26,  5.75it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 503/1000 [01:37<01:32,  5.36it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 504/1000 [01:37<01:28,  5.57it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 505/1000 [01:37<01:33,  5.31it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 506/1000 [01:37<01:26,  5.68it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 507/1000 [01:37<01:33,  5.27it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 508/1000 [01:38<01:26,  5.66it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 509/1000 [01:38<01:21,  5.99it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 510/1000 [01:38<01:35,  5.12it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 511/1000 [01:38<01:28,  5.52it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 512/1000 [01:38<01:35,  5.13it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 513/1000 [01:39<01:39,  4.90it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 514/1000 [01:39<01:30,  5.39it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 515/1000 [01:39<01:24,  5.77it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 516/1000 [01:39<01:28,  5.45it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 517/1000 [01:39<01:25,  5.65it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 518/1000 [01:39<01:17,  6.25it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 519/1000 [01:40<01:31,  5.28it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 520/1000 [01:40<01:24,  5.69it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 521/1000 [01:40<01:19,  6.00it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 522/1000 [01:40<01:32,  5.15it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 523/1000 [01:40<01:36,  4.95it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 524/1000 [01:40<01:27,  5.45it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 525/1000 [01:41<01:21,  5.85it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 526/1000 [01:41<01:17,  6.14it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 527/1000 [01:41<01:14,  6.37it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 528/1000 [01:41<01:12,  6.54it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 529/1000 [01:41<01:11,  6.62it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 530/1000 [01:41<01:06,  7.11it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 531/1000 [01:41<01:01,  7.61it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 532/1000 [01:42<01:03,  7.40it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 533/1000 [01:42<01:04,  7.23it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 534/1000 [01:42<01:05,  7.10it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 535/1000 [01:42<01:06,  7.00it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 536/1000 [01:42<01:02,  7.40it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 537/1000 [01:42<01:12,  6.36it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 538/1000 [01:43<01:31,  5.05it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 539/1000 [01:43<01:23,  5.49it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 540/1000 [01:43<01:20,  5.69it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 541/1000 [01:43<01:16,  5.97it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 542/1000 [01:43<01:33,  4.90it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 543/1000 [01:44<01:27,  5.23it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 544/1000 [01:44<01:23,  5.49it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 545/1000 [01:44<01:26,  5.24it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 546/1000 [01:44<01:23,  5.42it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 547/1000 [01:44<01:14,  6.07it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 548/1000 [01:44<01:11,  6.31it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 549/1000 [01:44<01:11,  6.27it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 550/1000 [01:45<01:12,  6.25it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 551/1000 [01:45<01:11,  6.24it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 552/1000 [01:45<01:09,  6.42it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 553/1000 [01:45<01:08,  6.54it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 554/1000 [01:45<01:09,  6.42it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 555/1000 [01:45<01:08,  6.53it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 556/1000 [01:46<01:03,  7.02it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 557/1000 [01:46<01:12,  6.15it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 558/1000 [01:46<01:09,  6.40it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 559/1000 [01:46<01:15,  5.83it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 560/1000 [01:46<01:20,  5.49it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 561/1000 [01:46<01:15,  5.84it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 562/1000 [01:47<01:13,  5.94it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 563/1000 [01:47<01:10,  6.21it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 564/1000 [01:47<01:17,  5.63it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 565/1000 [01:47<01:20,  5.39it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 566/1000 [01:47<01:23,  5.18it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 567/1000 [01:48<01:32,  4.69it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 568/1000 [01:48<01:22,  5.21it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 569/1000 [01:48<01:16,  5.62it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 570/1000 [01:48<01:19,  5.42it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 571/1000 [01:48<01:32,  4.62it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 572/1000 [01:49<01:20,  5.34it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 573/1000 [01:49<01:14,  5.76it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 574/1000 [01:49<01:10,  6.05it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 575/1000 [01:49<01:07,  6.28it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 576/1000 [01:49<01:20,  5.29it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 577/1000 [01:49<01:15,  5.62it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 578/1000 [01:50<01:20,  5.27it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 579/1000 [01:50<01:14,  5.68it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 580/1000 [01:50<01:19,  5.30it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 581/1000 [01:50<01:15,  5.53it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 582/1000 [01:50<01:11,  5.87it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 583/1000 [01:50<01:08,  6.13it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 584/1000 [01:51<01:14,  5.61it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 585/1000 [01:51<01:09,  5.98it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 586/1000 [01:51<01:05,  6.27it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 587/1000 [01:51<01:18,  5.28it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 588/1000 [01:51<01:12,  5.68it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 589/1000 [01:51<01:05,  6.32it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 590/1000 [01:52<01:17,  5.31it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 591/1000 [01:52<01:11,  5.71it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 592/1000 [01:52<01:17,  5.25it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 593/1000 [01:52<01:11,  5.69it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 594/1000 [01:52<01:07,  6.00it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 595/1000 [01:53<01:19,  5.11it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 596/1000 [01:53<01:12,  5.54it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 597/1000 [01:53<01:15,  5.31it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 598/1000 [01:53<01:10,  5.72it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 599/1000 [01:53<01:15,  5.31it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 600/1000 [01:53<01:10,  5.70it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 601/1000 [01:54<01:06,  5.97it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 602/1000 [01:54<01:12,  5.46it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 603/1000 [01:54<01:07,  5.85it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 604/1000 [01:54<01:12,  5.48it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 605/1000 [01:54<01:21,  4.87it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 606/1000 [01:55<01:15,  5.21it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 607/1000 [01:55<01:09,  5.64it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 608/1000 [01:55<01:13,  5.37it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 609/1000 [01:55<01:08,  5.74it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 610/1000 [01:55<01:04,  6.04it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 611/1000 [01:55<01:01,  6.33it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 612/1000 [01:56<00:59,  6.53it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 613/1000 [01:56<00:58,  6.61it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 614/1000 [01:56<00:58,  6.65it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 615/1000 [01:56<00:57,  6.70it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 616/1000 [01:56<01:05,  5.85it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 617/1000 [01:56<01:09,  5.49it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 619/1000 [01:57<00:59,  6.42it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 620/1000 [01:57<01:05,  5.79it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 621/1000 [01:57<01:04,  5.89it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 622/1000 [01:57<01:07,  5.56it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 623/1000 [01:57<01:04,  5.88it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 624/1000 [01:58<01:00,  6.19it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 625/1000 [01:58<01:00,  6.19it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 626/1000 [01:58<00:58,  6.40it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 627/1000 [01:58<00:53,  6.92it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 628/1000 [01:58<01:06,  5.60it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 629/1000 [01:59<01:19,  4.69it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 630/1000 [01:59<01:10,  5.22it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 631/1000 [01:59<01:07,  5.47it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 632/1000 [01:59<01:03,  5.81it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 633/1000 [01:59<01:16,  4.78it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 634/1000 [01:59<01:09,  5.27it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 635/1000 [02:00<01:04,  5.68it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 636/1000 [02:00<01:09,  5.25it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 637/1000 [02:00<01:05,  5.51it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 638/1000 [02:00<01:09,  5.20it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 639/1000 [02:00<01:04,  5.60it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 640/1000 [02:01<01:12,  4.93it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 641/1000 [02:01<01:06,  5.41it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 642/1000 [02:01<01:13,  4.85it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 643/1000 [02:01<01:14,  4.80it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 644/1000 [02:01<01:07,  5.29it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 645/1000 [02:02<01:17,  4.56it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 646/1000 [02:02<01:09,  5.09it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 647/1000 [02:02<01:04,  5.50it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 648/1000 [02:02<01:11,  4.92it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 649/1000 [02:02<01:11,  4.89it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 650/1000 [02:02<01:05,  5.38it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 651/1000 [02:03<01:16,  4.55it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 652/1000 [02:03<01:10,  4.94it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 653/1000 [02:03<01:16,  4.56it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 654/1000 [02:03<01:07,  5.09it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 655/1000 [02:04<01:08,  5.02it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 656/1000 [02:04<01:10,  4.90it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 657/1000 [02:04<01:11,  4.80it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 658/1000 [02:04<01:13,  4.68it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 659/1000 [02:04<01:13,  4.64it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 660/1000 [02:05<01:20,  4.22it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 661/1000 [02:05<01:22,  4.13it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 662/1000 [02:05<01:18,  4.30it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 663/1000 [02:05<01:09,  4.87it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 664/1000 [02:05<01:03,  5.31it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 665/1000 [02:06<00:58,  5.69it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 666/1000 [02:06<01:03,  5.29it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 667/1000 [02:06<00:58,  5.70it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 668/1000 [02:06<01:01,  5.41it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 669/1000 [02:06<01:08,  4.85it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 670/1000 [02:07<01:08,  4.85it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 671/1000 [02:07<00:58,  5.67it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 672/1000 [02:07<01:06,  4.96it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 673/1000 [02:07<01:00,  5.42it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 674/1000 [02:07<00:55,  5.83it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 675/1000 [02:07<00:53,  6.09it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 676/1000 [02:08<00:50,  6.36it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 677/1000 [02:08<00:49,  6.56it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 678/1000 [02:08<00:58,  5.47it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 679/1000 [02:08<00:55,  5.84it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 680/1000 [02:08<00:58,  5.47it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 681/1000 [02:09<01:09,  4.61it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 682/1000 [02:09<01:07,  4.69it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 683/1000 [02:09<01:01,  5.17it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 684/1000 [02:09<00:58,  5.44it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 685/1000 [02:09<01:00,  5.23it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 686/1000 [02:10<01:12,  4.31it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 687/1000 [02:10<01:04,  4.86it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 688/1000 [02:10<00:58,  5.32it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 689/1000 [02:10<00:52,  5.91it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 690/1000 [02:10<01:00,  5.12it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 691/1000 [02:11<01:06,  4.65it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 692/1000 [02:11<00:59,  5.17it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 693/1000 [02:11<00:54,  5.60it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 694/1000 [02:11<00:51,  5.99it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 695/1000 [02:11<00:50,  6.04it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 696/1000 [02:11<00:55,  5.46it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 697/1000 [02:12<00:53,  5.66it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 698/1000 [02:12<00:56,  5.38it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 699/1000 [02:12<00:58,  5.11it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 700/1000 [02:12<01:00,  4.98it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 701/1000 [02:12<00:56,  5.30it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 702/1000 [02:13<00:58,  5.10it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 703/1000 [02:13<00:51,  5.78it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 704/1000 [02:13<00:54,  5.41it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 705/1000 [02:13<01:01,  4.82it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 706/1000 [02:13<00:55,  5.32it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 707/1000 [02:14<00:57,  5.12it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 708/1000 [02:14<01:02,  4.65it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 709/1000 [02:14<00:57,  5.03it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 710/1000 [02:14<00:54,  5.33it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 711/1000 [02:14<00:50,  5.70it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 712/1000 [02:14<00:47,  6.02it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 713/1000 [02:15<00:55,  5.15it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 714/1000 [02:15<00:51,  5.56it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 715/1000 [02:15<00:45,  6.26it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 716/1000 [02:15<00:44,  6.40it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 717/1000 [02:15<00:42,  6.59it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 718/1000 [02:16<00:51,  5.46it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 719/1000 [02:16<00:53,  5.21it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 720/1000 [02:16<00:47,  5.92it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 721/1000 [02:16<00:44,  6.22it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 722/1000 [02:16<00:55,  5.02it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 723/1000 [02:16<00:51,  5.33it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 724/1000 [02:17<00:48,  5.71it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 725/1000 [02:17<00:54,  5.01it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 726/1000 [02:17<00:50,  5.45it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 727/1000 [02:17<00:52,  5.24it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 728/1000 [02:17<00:47,  5.67it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 729/1000 [02:18<00:50,  5.38it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 730/1000 [02:18<00:44,  6.03it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 731/1000 [02:18<00:42,  6.32it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 732/1000 [02:18<00:47,  5.66it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 733/1000 [02:18<00:49,  5.35it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 734/1000 [02:18<00:46,  5.74it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 735/1000 [02:18<00:41,  6.37it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 736/1000 [02:19<00:41,  6.41it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 737/1000 [02:19<00:39,  6.58it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 738/1000 [02:19<00:44,  5.95it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 739/1000 [02:19<00:50,  5.13it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 740/1000 [02:19<00:46,  5.54it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 741/1000 [02:20<00:49,  5.19it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 742/1000 [02:20<00:54,  4.75it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 744/1000 [02:20<00:42,  5.98it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 745/1000 [02:20<00:50,  5.03it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 746/1000 [02:21<00:47,  5.30it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 747/1000 [02:21<00:44,  5.66it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 748/1000 [02:21<00:42,  5.95it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 749/1000 [02:21<00:44,  5.69it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 750/1000 [02:21<00:41,  5.98it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 751/1000 [02:21<00:41,  6.03it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 752/1000 [02:22<00:50,  4.89it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 753/1000 [02:22<00:54,  4.55it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 754/1000 [02:22<00:48,  5.08it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 755/1000 [02:22<00:42,  5.78it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 756/1000 [02:22<00:38,  6.37it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 757/1000 [02:22<00:34,  6.98it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 758/1000 [02:23<00:39,  6.13it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 759/1000 [02:23<00:37,  6.38it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 760/1000 [02:23<00:36,  6.52it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 761/1000 [02:23<00:36,  6.61it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 762/1000 [02:23<00:35,  6.66it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 763/1000 [02:23<00:42,  5.54it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 764/1000 [02:24<00:44,  5.30it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 765/1000 [02:24<00:41,  5.67it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 766/1000 [02:24<00:43,  5.36it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 767/1000 [02:24<00:50,  4.59it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 768/1000 [02:24<00:49,  4.64it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 769/1000 [02:25<00:44,  5.18it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 770/1000 [02:25<00:41,  5.61it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 771/1000 [02:25<00:38,  5.95it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 772/1000 [02:25<00:36,  6.17it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 773/1000 [02:25<00:36,  6.16it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 774/1000 [02:25<00:35,  6.33it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 775/1000 [02:26<00:42,  5.33it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 776/1000 [02:26<00:39,  5.72it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 777/1000 [02:26<00:37,  6.02it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 778/1000 [02:26<00:45,  4.89it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 779/1000 [02:26<00:41,  5.34it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 780/1000 [02:27<00:42,  5.18it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 781/1000 [02:27<00:37,  5.85it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 782/1000 [02:27<00:35,  6.13it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 783/1000 [02:27<00:34,  6.30it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 784/1000 [02:27<00:33,  6.50it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 786/1000 [02:27<00:32,  6.49it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 787/1000 [02:28<00:32,  6.56it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 788/1000 [02:28<00:34,  6.12it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 789/1000 [02:28<00:36,  5.74it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 790/1000 [02:28<00:35,  5.86it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 791/1000 [02:28<00:34,  6.11it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 792/1000 [02:28<00:30,  6.76it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 793/1000 [02:29<00:34,  6.05it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 794/1000 [02:29<00:32,  6.33it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 795/1000 [02:29<00:32,  6.27it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 796/1000 [02:29<00:40,  5.07it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 798/1000 [02:29<00:32,  6.27it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 799/1000 [02:30<00:36,  5.46it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 800/1000 [02:30<00:34,  5.78it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 801/1000 [02:30<00:33,  6.02it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 802/1000 [02:30<00:32,  6.15it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 803/1000 [02:30<00:34,  5.67it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 804/1000 [02:30<00:32,  5.99it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 806/1000 [02:31<00:30,  6.29it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 807/1000 [02:31<00:29,  6.45it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 808/1000 [02:31<00:29,  6.56it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 809/1000 [02:31<00:28,  6.64it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 810/1000 [02:31<00:28,  6.74it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 811/1000 [02:32<00:27,  6.83it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 812/1000 [02:32<00:30,  6.08it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 813/1000 [02:32<00:29,  6.29it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 814/1000 [02:32<00:32,  5.65it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 815/1000 [02:32<00:31,  5.96it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 816/1000 [02:32<00:33,  5.43it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 817/1000 [02:33<00:31,  5.81it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 818/1000 [02:33<00:29,  6.10it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 819/1000 [02:33<00:34,  5.22it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 820/1000 [02:33<00:30,  6.00it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 821/1000 [02:33<00:27,  6.54it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 822/1000 [02:33<00:26,  6.60it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 823/1000 [02:34<00:32,  5.44it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 824/1000 [02:34<00:30,  5.81it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 825/1000 [02:34<00:38,  4.57it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 826/1000 [02:34<00:35,  4.97it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 827/1000 [02:34<00:31,  5.41it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 828/1000 [02:35<00:33,  5.21it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 829/1000 [02:35<00:33,  5.06it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 830/1000 [02:35<00:30,  5.55it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 832/1000 [02:35<00:25,  6.69it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 833/1000 [02:35<00:24,  6.73it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 834/1000 [02:35<00:24,  6.82it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 835/1000 [02:36<00:29,  5.62it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 837/1000 [02:36<00:26,  6.08it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 838/1000 [02:36<00:30,  5.40it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 839/1000 [02:37<00:31,  5.11it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 840/1000 [02:37<00:29,  5.51it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 841/1000 [02:37<00:27,  5.82it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 842/1000 [02:37<00:31,  5.09it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 843/1000 [02:37<00:27,  5.77it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 844/1000 [02:37<00:29,  5.32it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 845/1000 [02:38<00:27,  5.72it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 846/1000 [02:38<00:24,  6.39it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 847/1000 [02:38<00:26,  5.78it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 848/1000 [02:38<00:32,  4.73it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 849/1000 [02:38<00:28,  5.23it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 850/1000 [02:38<00:27,  5.48it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 851/1000 [02:39<00:25,  5.81it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 852/1000 [02:39<00:27,  5.32it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 853/1000 [02:39<00:28,  5.18it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 854/1000 [02:39<00:29,  4.96it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 855/1000 [02:39<00:27,  5.27it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 856/1000 [02:40<00:25,  5.66it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 857/1000 [02:40<00:28,  4.98it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 858/1000 [02:40<00:29,  4.88it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 859/1000 [02:40<00:27,  5.20it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 860/1000 [02:40<00:24,  5.65it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 861/1000 [02:41<00:23,  5.97it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 862/1000 [02:41<00:24,  5.55it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 863/1000 [02:41<00:27,  4.90it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 864/1000 [02:41<00:29,  4.54it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 865/1000 [02:41<00:31,  4.33it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 866/1000 [02:42<00:30,  4.40it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 867/1000 [02:42<00:26,  4.93it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 868/1000 [02:42<00:26,  4.91it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 869/1000 [02:42<00:27,  4.85it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 870/1000 [02:42<00:24,  5.35it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 871/1000 [02:43<00:22,  5.73it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 872/1000 [02:43<00:19,  6.44it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 873/1000 [02:43<00:22,  5.73it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 874/1000 [02:43<00:20,  6.01it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 875/1000 [02:43<00:22,  5.60it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 876/1000 [02:43<00:23,  5.36it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 877/1000 [02:44<00:25,  4.82it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 878/1000 [02:44<00:28,  4.30it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 879/1000 [02:44<00:25,  4.83it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 880/1000 [02:44<00:22,  5.32it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 881/1000 [02:44<00:20,  5.70it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 882/1000 [02:45<00:19,  6.01it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 883/1000 [02:45<00:18,  6.29it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 884/1000 [02:45<00:20,  5.72it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 885/1000 [02:45<00:19,  6.05it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 887/1000 [02:45<00:15,  7.09it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 888/1000 [02:46<00:18,  5.91it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 889/1000 [02:46<00:18,  6.17it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 890/1000 [02:46<00:17,  6.32it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 891/1000 [02:46<00:18,  5.74it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 892/1000 [02:46<00:19,  5.47it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 893/1000 [02:46<00:17,  6.09it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 894/1000 [02:47<00:19,  5.56it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 895/1000 [02:47<00:23,  4.47it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 896/1000 [02:47<00:20,  5.02it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 897/1000 [02:47<00:20,  4.95it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 898/1000 [02:48<00:22,  4.55it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 899/1000 [02:48<00:20,  5.05it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 900/1000 [02:48<00:18,  5.49it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 901/1000 [02:48<00:17,  5.79it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 902/1000 [02:48<00:19,  5.02it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 903/1000 [02:48<00:19,  4.88it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 904/1000 [02:49<00:17,  5.37it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 905/1000 [02:49<00:15,  6.14it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 906/1000 [02:49<00:13,  6.77it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 907/1000 [02:49<00:12,  7.31it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 908/1000 [02:49<00:14,  6.22it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 909/1000 [02:49<00:16,  5.59it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 910/1000 [02:49<00:14,  6.36it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 911/1000 [02:50<00:15,  5.77it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 912/1000 [02:50<00:14,  6.11it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 913/1000 [02:50<00:13,  6.31it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 914/1000 [02:50<00:15,  5.61it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 915/1000 [02:50<00:14,  5.96it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 916/1000 [02:50<00:13,  6.25it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 917/1000 [02:51<00:12,  6.43it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 918/1000 [02:51<00:12,  6.56it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 919/1000 [02:51<00:15,  5.17it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 920/1000 [02:51<00:17,  4.48it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 921/1000 [02:52<00:15,  5.02it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 922/1000 [02:52<00:14,  5.45it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 923/1000 [02:52<00:14,  5.26it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 924/1000 [02:52<00:12,  5.92it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 925/1000 [02:52<00:12,  5.99it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 927/1000 [02:52<00:10,  6.95it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 928/1000 [02:53<00:11,  6.24it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 929/1000 [02:53<00:11,  6.38it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 930/1000 [02:53<00:10,  6.83it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 931/1000 [02:53<00:10,  6.82it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 932/1000 [02:53<00:11,  5.97it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 933/1000 [02:53<00:10,  6.18it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 934/1000 [02:54<00:12,  5.28it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 935/1000 [02:54<00:12,  5.13it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 936/1000 [02:54<00:12,  5.01it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 937/1000 [02:54<00:11,  5.39it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 938/1000 [02:54<00:12,  5.15it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 939/1000 [02:55<00:10,  5.63it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 940/1000 [02:55<00:10,  5.95it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 941/1000 [02:55<00:11,  5.14it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 942/1000 [02:55<00:10,  5.60it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 943/1000 [02:55<00:09,  5.95it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 944/1000 [02:55<00:10,  5.42it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 945/1000 [02:56<00:10,  5.20it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 946/1000 [02:56<00:09,  5.65it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 947/1000 [02:56<00:08,  5.97it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 948/1000 [02:56<00:08,  6.25it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 949/1000 [02:56<00:07,  6.48it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 950/1000 [02:56<00:08,  5.82it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 951/1000 [02:57<00:07,  6.42it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 952/1000 [02:57<00:07,  6.60it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 953/1000 [02:57<00:07,  6.69it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 954/1000 [02:57<00:08,  5.51it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 956/1000 [02:57<00:06,  6.60it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 957/1000 [02:57<00:06,  6.73it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 958/1000 [02:58<00:06,  6.83it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 959/1000 [02:58<00:06,  6.83it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 961/1000 [02:58<00:06,  6.28it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 962/1000 [02:58<00:06,  5.45it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 963/1000 [02:58<00:06,  6.08it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 964/1000 [02:59<00:06,  5.56it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 965/1000 [02:59<00:06,  5.32it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 966/1000 [02:59<00:06,  5.07it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 967/1000 [02:59<00:06,  4.91it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 968/1000 [03:00<00:05,  5.38it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 969/1000 [03:00<00:05,  5.76it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 970/1000 [03:00<00:04,  6.36it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 971/1000 [03:00<00:04,  7.03it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 972/1000 [03:00<00:04,  6.94it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 973/1000 [03:00<00:03,  6.99it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 974/1000 [03:00<00:03,  7.05it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 975/1000 [03:00<00:03,  7.10it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 976/1000 [03:01<00:03,  6.86it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 977/1000 [03:01<00:03,  6.93it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 978/1000 [03:01<00:02,  7.50it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 979/1000 [03:01<00:02,  7.99it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 980/1000 [03:01<00:02,  7.73it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 981/1000 [03:01<00:02,  8.12it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 982/1000 [03:01<00:02,  7.76it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 983/1000 [03:01<00:02,  7.55it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 985/1000 [03:02<00:01,  8.11it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 986/1000 [03:02<00:01,  7.86it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 987/1000 [03:02<00:01,  7.64it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 988/1000 [03:02<00:01,  7.50it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 989/1000 [03:02<00:01,  7.37it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 990/1000 [03:02<00:01,  7.34it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 991/1000 [03:03<00:01,  7.25it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 992/1000 [03:03<00:01,  6.17it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 993/1000 [03:03<00:01,  6.74it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 994/1000 [03:03<00:00,  6.63it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 995/1000 [03:03<00:00,  5.80it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 996/1000 [03:03<00:00,  6.13it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 997/1000 [03:04<00:00,  6.45it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 998/1000 [03:04<00:00,  6.65it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 999/1000 [03:04<00:00,  5.16it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [03:04<00:00,  5.66it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [03:04<00:00,  5.42it/s]
DONE (9.36s)
DONE (8.35s)
Computing criterion:   0%|          | 0/20 [00:00<?, ?it/s]Computing criterion:   5%|â–Œ         | 1/20 [00:18<05:44, 18.14s/it]Computing criterion:  10%|â–ˆ         | 2/20 [00:35<05:19, 17.75s/it]Computing criterion:  15%|â–ˆâ–Œ        | 3/20 [00:53<05:05, 17.95s/it]Computing criterion:  20%|â–ˆâ–ˆ        | 4/20 [01:11<04:42, 17.66s/it]Computing criterion:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [01:29<04:26, 17.78s/it]Computing criterion:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [01:47<04:11, 17.93s/it]Computing criterion:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [02:04<03:51, 17.82s/it]Computing criterion:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [02:22<03:34, 17.89s/it]Computing criterion:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [02:40<03:17, 17.95s/it]Computing criterion:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [02:59<03:00, 18.10s/it]Computing criterion:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [03:16<02:40, 17.86s/it]Computing criterion:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [03:33<02:21, 17.64s/it]Computing criterion:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [03:51<02:03, 17.58s/it]Computing criterion:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [04:09<01:46, 17.82s/it]Computing criterion:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [04:28<01:29, 18.00s/it]Computing criterion:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [04:46<01:12, 18.03s/it]Computing criterion:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [05:03<00:53, 17.94s/it]Computing criterion:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [05:21<00:35, 17.84s/it]Computing criterion:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [05:38<00:17, 17.62s/it]Computing criterion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [05:56<00:00, 17.84s/it]Computing criterion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [05:56<00:00, 17.85s/it]
Ref scores:   0%|          | 0/1000 [00:00<?, ?it/s]Ref scores:   0%|          | 1/1000 [00:03<50:50,  3.05s/it]Ref scores:   0%|          | 2/1000 [00:03<22:14,  1.34s/it]Ref scores:   0%|          | 3/1000 [00:03<14:20,  1.16it/s]Ref scores:   0%|          | 4/1000 [00:03<09:32,  1.74it/s]Ref scores:   0%|          | 5/1000 [00:03<06:43,  2.47it/s]Ref scores:   1%|          | 7/1000 [00:03<04:13,  3.91it/s]Ref scores:   1%|          | 8/1000 [00:04<03:47,  4.35it/s]Ref scores:   1%|          | 9/1000 [00:04<03:56,  4.19it/s]Ref scores:   1%|          | 10/1000 [00:04<03:33,  4.64it/s]Ref scores:   1%|          | 11/1000 [00:04<03:10,  5.20it/s]Ref scores:   1%|          | 12/1000 [00:04<02:53,  5.70it/s]Ref scores:   1%|â–         | 13/1000 [00:04<02:32,  6.47it/s]Ref scores:   1%|â–         | 14/1000 [00:05<02:18,  7.13it/s]Ref scores:   2%|â–         | 15/1000 [00:05<02:53,  5.69it/s]Ref scores:   2%|â–         | 16/1000 [00:05<02:46,  5.92it/s]Ref scores:   2%|â–         | 17/1000 [00:05<02:36,  6.29it/s]Ref scores:   2%|â–         | 18/1000 [00:05<02:34,  6.37it/s]Ref scores:   2%|â–         | 19/1000 [00:05<02:27,  6.64it/s]Ref scores:   2%|â–         | 20/1000 [00:05<02:23,  6.85it/s]Ref scores:   2%|â–         | 21/1000 [00:06<02:24,  6.79it/s]Ref scores:   2%|â–         | 22/1000 [00:06<02:25,  6.74it/s]Ref scores:   2%|â–         | 23/1000 [00:06<02:20,  6.94it/s]Ref scores:   2%|â–         | 24/1000 [00:06<02:55,  5.58it/s]Ref scores:   2%|â–Ž         | 25/1000 [00:06<02:41,  6.03it/s]Ref scores:   3%|â–Ž         | 26/1000 [00:06<02:37,  6.17it/s]Ref scores:   3%|â–Ž         | 27/1000 [00:07<02:34,  6.30it/s]Ref scores:   3%|â–Ž         | 28/1000 [00:07<02:34,  6.28it/s]Ref scores:   3%|â–Ž         | 29/1000 [00:07<02:27,  6.60it/s]Ref scores:   3%|â–Ž         | 30/1000 [00:07<02:21,  6.83it/s]Ref scores:   3%|â–Ž         | 31/1000 [00:07<02:18,  7.01it/s]Ref scores:   3%|â–Ž         | 32/1000 [00:07<02:20,  6.88it/s]Ref scores:   3%|â–Ž         | 33/1000 [00:07<02:17,  7.03it/s]Ref scores:   3%|â–Ž         | 34/1000 [00:08<02:52,  5.61it/s]Ref scores:   4%|â–Ž         | 35/1000 [00:08<02:44,  5.88it/s]Ref scores:   4%|â–Ž         | 36/1000 [00:08<02:38,  6.10it/s]Ref scores:   4%|â–Ž         | 37/1000 [00:08<02:29,  6.43it/s]Ref scores:   4%|â–         | 38/1000 [00:08<02:23,  6.69it/s]Ref scores:   4%|â–         | 39/1000 [00:08<02:23,  6.68it/s]Ref scores:   4%|â–         | 40/1000 [00:09<02:55,  5.48it/s]Ref scores:   4%|â–         | 41/1000 [00:09<03:16,  4.87it/s]Ref scores:   4%|â–         | 42/1000 [00:09<03:12,  4.99it/s]Ref scores:   4%|â–         | 43/1000 [00:09<02:53,  5.53it/s]Ref scores:   4%|â–         | 44/1000 [00:10<03:12,  4.98it/s]Ref scores:   5%|â–         | 46/1000 [00:10<02:31,  6.30it/s]Ref scores:   5%|â–         | 48/1000 [00:10<02:12,  7.21it/s]Ref scores:   5%|â–         | 49/1000 [00:10<02:11,  7.25it/s]Ref scores:   5%|â–Œ         | 50/1000 [00:10<02:13,  7.10it/s]Ref scores:   5%|â–Œ         | 51/1000 [00:10<02:12,  7.18it/s]Ref scores:   5%|â–Œ         | 52/1000 [00:10<02:02,  7.75it/s]Ref scores:   5%|â–Œ         | 53/1000 [00:11<02:46,  5.68it/s]Ref scores:   5%|â–Œ         | 54/1000 [00:11<02:35,  6.08it/s]Ref scores:   6%|â–Œ         | 55/1000 [00:11<02:27,  6.40it/s]Ref scores:   6%|â–Œ         | 56/1000 [00:11<02:21,  6.68it/s]Ref scores:   6%|â–Œ         | 57/1000 [00:11<02:17,  6.86it/s]Ref scores:   6%|â–Œ         | 58/1000 [00:11<02:14,  7.00it/s]Ref scores:   6%|â–Œ         | 59/1000 [00:12<02:18,  6.80it/s]Ref scores:   6%|â–Œ         | 60/1000 [00:12<02:19,  6.72it/s]Ref scores:   6%|â–Œ         | 61/1000 [00:12<02:15,  6.91it/s]Ref scores:   6%|â–Œ         | 62/1000 [00:12<02:12,  7.06it/s]Ref scores:   6%|â–‹         | 63/1000 [00:12<02:18,  6.78it/s]Ref scores:   6%|â–‹         | 64/1000 [00:12<02:07,  7.36it/s]Ref scores:   6%|â–‹         | 65/1000 [00:12<01:59,  7.83it/s]Ref scores:   7%|â–‹         | 66/1000 [00:13<02:06,  7.40it/s]Ref scores:   7%|â–‹         | 67/1000 [00:13<02:06,  7.40it/s]Ref scores:   7%|â–‹         | 68/1000 [00:13<02:41,  5.77it/s]Ref scores:   7%|â–‹         | 69/1000 [00:13<02:29,  6.23it/s]Ref scores:   7%|â–‹         | 70/1000 [00:13<02:21,  6.55it/s]Ref scores:   7%|â–‹         | 71/1000 [00:14<02:52,  5.39it/s]Ref scores:   7%|â–‹         | 72/1000 [00:14<02:53,  5.36it/s]Ref scores:   7%|â–‹         | 73/1000 [00:14<02:39,  5.83it/s]Ref scores:   7%|â–‹         | 74/1000 [00:14<03:02,  5.07it/s]Ref scores:   8%|â–Š         | 75/1000 [00:14<02:44,  5.61it/s]Ref scores:   8%|â–Š         | 76/1000 [00:14<02:32,  6.06it/s]Ref scores:   8%|â–Š         | 77/1000 [00:15<02:28,  6.21it/s]Ref scores:   8%|â–Š         | 78/1000 [00:15<02:21,  6.53it/s]Ref scores:   8%|â–Š         | 79/1000 [00:15<02:14,  6.83it/s]Ref scores:   8%|â–Š         | 80/1000 [00:15<02:12,  6.96it/s]Ref scores:   8%|â–Š         | 81/1000 [00:15<02:09,  7.07it/s]Ref scores:   8%|â–Š         | 82/1000 [00:15<02:43,  5.63it/s]Ref scores:   8%|â–Š         | 83/1000 [00:15<02:31,  6.07it/s]Ref scores:   8%|â–Š         | 84/1000 [00:16<02:23,  6.41it/s]Ref scores:   8%|â–Š         | 85/1000 [00:16<02:16,  6.68it/s]Ref scores:   9%|â–Š         | 86/1000 [00:16<02:12,  6.90it/s]Ref scores:   9%|â–Š         | 87/1000 [00:16<02:09,  7.03it/s]Ref scores:   9%|â–‰         | 88/1000 [00:16<02:00,  7.56it/s]Ref scores:   9%|â–‰         | 89/1000 [00:16<02:05,  7.25it/s]Ref scores:   9%|â–‰         | 90/1000 [00:16<02:04,  7.29it/s]Ref scores:   9%|â–‰         | 91/1000 [00:17<02:03,  7.34it/s]Ref scores:   9%|â–‰         | 93/1000 [00:17<01:51,  8.11it/s]Ref scores:   9%|â–‰         | 94/1000 [00:17<01:54,  7.91it/s]Ref scores:  10%|â–‰         | 95/1000 [00:17<02:00,  7.52it/s]Ref scores:  10%|â–‰         | 96/1000 [00:17<02:00,  7.48it/s]Ref scores:  10%|â–‰         | 97/1000 [00:17<02:33,  5.90it/s]Ref scores:  10%|â–‰         | 98/1000 [00:18<02:23,  6.27it/s]Ref scores:  10%|â–‰         | 99/1000 [00:18<02:11,  6.87it/s]Ref scores:  10%|â–ˆ         | 100/1000 [00:18<02:08,  7.03it/s]Ref scores:  10%|â–ˆ         | 102/1000 [00:18<01:59,  7.52it/s]Ref scores:  10%|â–ˆ         | 103/1000 [00:18<02:04,  7.19it/s]Ref scores:  10%|â–ˆ         | 104/1000 [00:18<02:03,  7.25it/s]Ref scores:  10%|â–ˆ         | 105/1000 [00:18<02:08,  6.96it/s]Ref scores:  11%|â–ˆ         | 106/1000 [00:19<02:06,  7.04it/s]Ref scores:  11%|â–ˆ         | 107/1000 [00:19<02:09,  6.88it/s]Ref scores:  11%|â–ˆ         | 108/1000 [00:19<02:07,  7.02it/s]Ref scores:  11%|â–ˆ         | 109/1000 [00:19<02:46,  5.34it/s]Ref scores:  11%|â–ˆ         | 110/1000 [00:19<02:39,  5.58it/s]Ref scores:  11%|â–ˆ         | 111/1000 [00:20<02:32,  5.84it/s]Ref scores:  11%|â–ˆ         | 112/1000 [00:20<02:22,  6.24it/s]Ref scores:  11%|â–ˆâ–        | 113/1000 [00:20<02:15,  6.53it/s]Ref scores:  11%|â–ˆâ–        | 114/1000 [00:20<02:11,  6.76it/s]Ref scores:  12%|â–ˆâ–        | 115/1000 [00:20<02:14,  6.56it/s]Ref scores:  12%|â–ˆâ–        | 116/1000 [00:20<02:15,  6.54it/s]Ref scores:  12%|â–ˆâ–        | 117/1000 [00:20<02:09,  6.82it/s]Ref scores:  12%|â–ˆâ–        | 118/1000 [00:21<02:06,  7.00it/s]Ref scores:  12%|â–ˆâ–        | 119/1000 [00:21<02:03,  7.14it/s]Ref scores:  12%|â–ˆâ–        | 120/1000 [00:21<02:00,  7.28it/s]Ref scores:  12%|â–ˆâ–        | 121/1000 [00:21<02:04,  7.08it/s]Ref scores:  12%|â–ˆâ–        | 122/1000 [00:21<02:01,  7.25it/s]Ref scores:  12%|â–ˆâ–        | 123/1000 [00:21<01:51,  7.84it/s]Ref scores:  12%|â–ˆâ–        | 124/1000 [00:21<01:53,  7.73it/s]Ref scores:  12%|â–ˆâ–Ž        | 125/1000 [00:21<02:00,  7.25it/s]Ref scores:  13%|â–ˆâ–Ž        | 126/1000 [00:22<01:58,  7.37it/s]Ref scores:  13%|â–ˆâ–Ž        | 127/1000 [00:22<02:02,  7.12it/s]Ref scores:  13%|â–ˆâ–Ž        | 128/1000 [00:22<01:52,  7.77it/s]Ref scores:  13%|â–ˆâ–Ž        | 129/1000 [00:22<02:33,  5.66it/s]Ref scores:  13%|â–ˆâ–Ž        | 131/1000 [00:22<02:05,  6.93it/s]Ref scores:  13%|â–ˆâ–Ž        | 132/1000 [00:22<02:02,  7.07it/s]Ref scores:  13%|â–ˆâ–Ž        | 133/1000 [00:23<02:06,  6.87it/s]Ref scores:  13%|â–ˆâ–Ž        | 134/1000 [00:23<02:30,  5.74it/s]Ref scores:  14%|â–ˆâ–Ž        | 135/1000 [00:23<02:20,  6.17it/s]Ref scores:  14%|â–ˆâ–Ž        | 136/1000 [00:23<02:17,  6.29it/s]Ref scores:  14%|â–ˆâ–Ž        | 137/1000 [00:23<02:14,  6.41it/s]Ref scores:  14%|â–ˆâ–        | 138/1000 [00:23<02:08,  6.70it/s]Ref scores:  14%|â–ˆâ–        | 139/1000 [00:24<02:04,  6.90it/s]Ref scores:  14%|â–ˆâ–        | 140/1000 [00:24<02:06,  6.81it/s]Ref scores:  14%|â–ˆâ–        | 141/1000 [00:24<02:09,  6.65it/s]Ref scores:  14%|â–ˆâ–        | 142/1000 [00:24<02:04,  6.89it/s]Ref scores:  14%|â–ˆâ–        | 143/1000 [00:24<02:00,  7.10it/s]Ref scores:  14%|â–ˆâ–        | 144/1000 [00:24<01:57,  7.26it/s]Ref scores:  14%|â–ˆâ–        | 145/1000 [00:24<01:56,  7.35it/s]Ref scores:  15%|â–ˆâ–        | 146/1000 [00:25<01:58,  7.19it/s]Ref scores:  15%|â–ˆâ–        | 147/1000 [00:25<02:29,  5.69it/s]Ref scores:  15%|â–ˆâ–        | 148/1000 [00:25<02:18,  6.17it/s]Ref scores:  15%|â–ˆâ–        | 149/1000 [00:25<02:09,  6.56it/s]Ref scores:  15%|â–ˆâ–Œ        | 150/1000 [00:25<02:45,  5.15it/s]Ref scores:  15%|â–ˆâ–Œ        | 152/1000 [00:26<02:10,  6.49it/s]Ref scores:  15%|â–ˆâ–Œ        | 153/1000 [00:26<02:05,  6.73it/s]Ref scores:  15%|â–ˆâ–Œ        | 154/1000 [00:26<02:29,  5.66it/s]Ref scores:  16%|â–ˆâ–Œ        | 155/1000 [00:26<02:23,  5.90it/s]Ref scores:  16%|â–ˆâ–Œ        | 156/1000 [00:26<02:08,  6.56it/s]Ref scores:  16%|â–ˆâ–Œ        | 157/1000 [00:26<02:03,  6.81it/s]Ref scores:  16%|â–ˆâ–Œ        | 158/1000 [00:27<02:04,  6.78it/s]Ref scores:  16%|â–ˆâ–Œ        | 160/1000 [00:27<01:51,  7.53it/s]Ref scores:  16%|â–ˆâ–Œ        | 161/1000 [00:27<01:54,  7.31it/s]Ref scores:  16%|â–ˆâ–Œ        | 162/1000 [00:27<01:58,  7.09it/s]Ref scores:  16%|â–ˆâ–‹        | 163/1000 [00:27<02:24,  5.78it/s]Ref scores:  16%|â–ˆâ–‹        | 164/1000 [00:27<02:15,  6.19it/s]Ref scores:  16%|â–ˆâ–‹        | 165/1000 [00:28<02:07,  6.55it/s]Ref scores:  17%|â–ˆâ–‹        | 166/1000 [00:28<02:03,  6.77it/s]Ref scores:  17%|â–ˆâ–‹        | 167/1000 [00:28<01:59,  6.99it/s]Ref scores:  17%|â–ˆâ–‹        | 168/1000 [00:28<01:56,  7.15it/s]Ref scores:  17%|â–ˆâ–‹        | 169/1000 [00:28<01:58,  7.02it/s]Ref scores:  17%|â–ˆâ–‹        | 171/1000 [00:28<01:44,  7.91it/s]Ref scores:  17%|â–ˆâ–‹        | 172/1000 [00:28<01:39,  8.31it/s]Ref scores:  17%|â–ˆâ–‹        | 173/1000 [00:29<01:41,  8.11it/s]Ref scores:  18%|â–ˆâ–Š        | 175/1000 [00:29<01:58,  6.98it/s]Ref scores:  18%|â–ˆâ–Š        | 176/1000 [00:29<01:55,  7.12it/s]Ref scores:  18%|â–ˆâ–Š        | 177/1000 [00:29<01:53,  7.23it/s]Ref scores:  18%|â–ˆâ–Š        | 178/1000 [00:29<01:55,  7.10it/s]Ref scores:  18%|â–ˆâ–Š        | 179/1000 [00:30<02:22,  5.75it/s]Ref scores:  18%|â–ˆâ–Š        | 180/1000 [00:30<02:18,  5.92it/s]Ref scores:  18%|â–ˆâ–Š        | 182/1000 [00:30<01:54,  7.13it/s]Ref scores:  18%|â–ˆâ–Š        | 183/1000 [00:30<01:53,  7.22it/s]Ref scores:  18%|â–ˆâ–Š        | 184/1000 [00:30<01:55,  7.06it/s]Ref scores:  18%|â–ˆâ–Š        | 185/1000 [00:30<01:53,  7.17it/s]Ref scores:  19%|â–ˆâ–Š        | 186/1000 [00:30<01:52,  7.27it/s]Ref scores:  19%|â–ˆâ–Š        | 187/1000 [00:31<01:50,  7.33it/s]Ref scores:  19%|â–ˆâ–‰        | 188/1000 [00:31<01:53,  7.14it/s]Ref scores:  19%|â–ˆâ–‰        | 189/1000 [00:31<01:51,  7.26it/s]Ref scores:  19%|â–ˆâ–‰        | 190/1000 [00:31<01:56,  6.96it/s]Ref scores:  19%|â–ˆâ–‰        | 191/1000 [00:31<01:54,  7.09it/s]Ref scores:  19%|â–ˆâ–‰        | 192/1000 [00:31<01:52,  7.21it/s]Ref scores:  19%|â–ˆâ–‰        | 194/1000 [00:32<01:46,  7.56it/s]Ref scores:  20%|â–ˆâ–‰        | 195/1000 [00:32<01:50,  7.31it/s]Ref scores:  20%|â–ˆâ–‰        | 196/1000 [00:32<01:49,  7.37it/s]Ref scores:  20%|â–ˆâ–‰        | 197/1000 [00:32<01:51,  7.20it/s]Ref scores:  20%|â–ˆâ–‰        | 198/1000 [00:32<01:50,  7.26it/s]Ref scores:  20%|â–ˆâ–‰        | 199/1000 [00:32<01:49,  7.34it/s]Ref scores:  20%|â–ˆâ–ˆ        | 200/1000 [00:32<01:48,  7.40it/s]Ref scores:  20%|â–ˆâ–ˆ        | 201/1000 [00:33<01:59,  6.67it/s]Ref scores:  20%|â–ˆâ–ˆ        | 202/1000 [00:33<01:56,  6.87it/s]Ref scores:  20%|â–ˆâ–ˆ        | 203/1000 [00:33<01:58,  6.72it/s]Ref scores:  20%|â–ˆâ–ˆ        | 204/1000 [00:33<01:54,  6.97it/s]Ref scores:  20%|â–ˆâ–ˆ        | 205/1000 [00:33<01:51,  7.15it/s]Ref scores:  21%|â–ˆâ–ˆ        | 206/1000 [00:33<01:49,  7.27it/s]Ref scores:  21%|â–ˆâ–ˆ        | 207/1000 [00:33<01:47,  7.35it/s]Ref scores:  21%|â–ˆâ–ˆ        | 208/1000 [00:34<01:50,  7.17it/s]Ref scores:  21%|â–ˆâ–ˆ        | 209/1000 [00:34<01:52,  7.00it/s]Ref scores:  21%|â–ˆâ–ˆ        | 210/1000 [00:34<01:50,  7.12it/s]Ref scores:  21%|â–ˆâ–ˆ        | 211/1000 [00:34<01:48,  7.25it/s]Ref scores:  21%|â–ˆâ–ˆ        | 212/1000 [00:34<01:46,  7.37it/s]Ref scores:  21%|â–ˆâ–ˆâ–       | 213/1000 [00:34<01:49,  7.16it/s]Ref scores:  21%|â–ˆâ–ˆâ–       | 214/1000 [00:34<01:52,  6.99it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 215/1000 [00:35<01:53,  6.91it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 216/1000 [00:35<01:56,  6.72it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 218/1000 [00:35<01:41,  7.73it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 219/1000 [00:35<01:41,  7.70it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 220/1000 [00:35<01:47,  7.28it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 221/1000 [00:35<02:12,  5.87it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 222/1000 [00:36<02:03,  6.28it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 223/1000 [00:36<01:58,  6.56it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 224/1000 [00:36<01:54,  6.79it/s]Ref scores:  22%|â–ˆâ–ˆâ–Ž       | 225/1000 [00:36<02:26,  5.29it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 226/1000 [00:36<02:13,  5.81it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 227/1000 [00:36<02:04,  6.22it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 228/1000 [00:37<02:25,  5.29it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 229/1000 [00:37<02:16,  5.65it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 230/1000 [00:37<02:00,  6.39it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 231/1000 [00:37<01:58,  6.47it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 232/1000 [00:37<01:57,  6.53it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 233/1000 [00:38<02:21,  5.40it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 234/1000 [00:38<02:09,  5.92it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 235/1000 [00:38<02:01,  6.31it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 236/1000 [00:38<01:59,  6.41it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 237/1000 [00:38<01:57,  6.49it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 238/1000 [00:38<01:52,  6.75it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 239/1000 [00:38<01:48,  6.99it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 240/1000 [00:38<01:46,  7.12it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 242/1000 [00:39<01:33,  8.09it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 243/1000 [00:39<01:35,  7.95it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 244/1000 [00:39<01:36,  7.85it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 245/1000 [00:39<01:41,  7.43it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 246/1000 [00:39<01:40,  7.48it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 247/1000 [00:39<01:35,  7.92it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 248/1000 [00:39<01:39,  7.55it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 249/1000 [00:40<01:45,  7.13it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 250/1000 [00:40<01:43,  7.24it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 251/1000 [00:40<01:42,  7.33it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 252/1000 [00:40<01:40,  7.41it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 253/1000 [00:40<01:44,  7.18it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 254/1000 [00:40<01:42,  7.31it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 255/1000 [00:40<01:41,  7.37it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 257/1000 [00:41<01:34,  7.90it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 258/1000 [00:41<01:34,  7.82it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 259/1000 [00:41<01:35,  7.76it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 260/1000 [00:41<01:39,  7.46it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 261/1000 [00:41<01:39,  7.46it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 262/1000 [00:42<02:13,  5.51it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 263/1000 [00:42<02:06,  5.80it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 264/1000 [00:42<01:58,  6.22it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 265/1000 [00:42<01:51,  6.58it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 266/1000 [00:42<01:54,  6.40it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 267/1000 [00:42<02:16,  5.35it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 268/1000 [00:43<02:08,  5.71it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 269/1000 [00:43<02:04,  5.88it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 270/1000 [00:43<02:01,  6.02it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 271/1000 [00:43<02:22,  5.13it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 272/1000 [00:43<02:09,  5.64it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 273/1000 [00:43<01:59,  6.06it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 274/1000 [00:44<02:05,  5.77it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 275/1000 [00:44<02:03,  5.89it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 276/1000 [00:44<01:59,  6.07it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 277/1000 [00:44<01:52,  6.43it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 278/1000 [00:44<01:48,  6.67it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 279/1000 [00:44<01:49,  6.60it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 280/1000 [00:44<01:40,  7.18it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 281/1000 [00:45<01:45,  6.83it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 282/1000 [00:45<02:09,  5.54it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 283/1000 [00:45<02:00,  5.96it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 284/1000 [00:45<01:59,  6.00it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 285/1000 [00:45<01:52,  6.37it/s]Ref scores:  29%|â–ˆâ–ˆâ–Š       | 286/1000 [00:45<01:47,  6.65it/s]Ref scores:  29%|â–ˆâ–ˆâ–Š       | 287/1000 [00:46<01:38,  7.21it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 288/1000 [00:46<01:38,  7.20it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 289/1000 [00:46<01:38,  7.23it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 290/1000 [00:46<01:41,  7.00it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 291/1000 [00:46<01:43,  6.82it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 292/1000 [00:46<01:45,  6.74it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 293/1000 [00:47<02:09,  5.45it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 295/1000 [00:47<01:40,  7.01it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 296/1000 [00:47<01:39,  7.08it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 297/1000 [00:47<02:08,  5.46it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 298/1000 [00:47<02:02,  5.71it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 299/1000 [00:47<02:00,  5.83it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 300/1000 [00:48<01:53,  6.19it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 301/1000 [00:48<01:51,  6.27it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 302/1000 [00:48<01:40,  6.95it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 303/1000 [00:48<01:39,  7.01it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 304/1000 [00:48<01:38,  7.07it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 305/1000 [00:48<01:37,  7.14it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 306/1000 [00:48<01:36,  7.16it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 307/1000 [00:49<01:36,  7.21it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 308/1000 [00:49<01:35,  7.25it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 309/1000 [00:49<01:35,  7.24it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 310/1000 [00:49<02:02,  5.65it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 311/1000 [00:49<01:57,  5.86it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 312/1000 [00:50<02:23,  4.78it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆâ–      | 313/1000 [00:50<02:11,  5.21it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆâ–      | 314/1000 [00:50<02:03,  5.55it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 315/1000 [00:50<02:00,  5.66it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 317/1000 [00:50<01:41,  6.72it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 318/1000 [00:50<01:42,  6.66it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 319/1000 [00:51<01:40,  6.79it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 320/1000 [00:51<01:41,  6.69it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 321/1000 [00:51<01:42,  6.60it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 322/1000 [00:51<01:44,  6.47it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 323/1000 [00:51<01:35,  7.05it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 324/1000 [00:51<01:35,  7.10it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 325/1000 [00:52<01:59,  5.67it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 326/1000 [00:52<01:51,  6.07it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 327/1000 [00:52<01:44,  6.44it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 328/1000 [00:52<01:35,  7.01it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 329/1000 [00:52<01:34,  7.08it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 330/1000 [00:52<01:36,  6.91it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 331/1000 [00:52<01:35,  7.03it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 332/1000 [00:53<01:37,  6.86it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 333/1000 [00:53<01:35,  6.98it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 334/1000 [00:53<01:56,  5.74it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 335/1000 [00:53<01:42,  6.51it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 336/1000 [00:53<01:43,  6.41it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 337/1000 [00:53<01:32,  7.13it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 338/1000 [00:53<01:32,  7.12it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 339/1000 [00:54<01:35,  6.93it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 340/1000 [00:54<01:33,  7.04it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 341/1000 [00:54<01:36,  6.84it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 343/1000 [00:54<01:25,  7.67it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 344/1000 [00:54<01:26,  7.56it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 345/1000 [00:54<01:22,  7.96it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 346/1000 [00:55<01:54,  5.73it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 347/1000 [00:55<01:50,  5.92it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 348/1000 [00:55<02:07,  5.12it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 349/1000 [00:55<01:55,  5.61it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 350/1000 [00:55<01:47,  6.02it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 351/1000 [00:55<01:45,  6.16it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 352/1000 [00:56<01:40,  6.45it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 353/1000 [00:56<01:40,  6.46it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 354/1000 [00:56<01:32,  6.96it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 355/1000 [00:56<01:31,  7.09it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 356/1000 [00:56<01:33,  6.87it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 357/1000 [00:56<01:35,  6.74it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 358/1000 [00:56<01:33,  6.89it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 359/1000 [00:57<01:31,  6.99it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 360/1000 [00:57<01:30,  7.08it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 361/1000 [00:57<01:23,  7.64it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 362/1000 [00:57<01:25,  7.49it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 363/1000 [00:57<01:49,  5.80it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 364/1000 [00:57<01:36,  6.59it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 365/1000 [00:57<01:37,  6.51it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 366/1000 [00:58<01:34,  6.69it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 367/1000 [00:58<01:35,  6.60it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 368/1000 [00:58<01:33,  6.76it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 369/1000 [00:58<01:31,  6.90it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 370/1000 [00:58<01:34,  6.65it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 371/1000 [00:58<01:35,  6.59it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 372/1000 [00:59<01:32,  6.76it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 373/1000 [00:59<01:53,  5.52it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 374/1000 [00:59<01:45,  5.95it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 375/1000 [00:59<01:42,  6.08it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 376/1000 [00:59<02:01,  5.13it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 377/1000 [01:00<01:54,  5.46it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 378/1000 [01:00<01:48,  5.74it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 379/1000 [01:00<01:41,  6.12it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 380/1000 [01:00<01:39,  6.22it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 381/1000 [01:00<01:35,  6.48it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 382/1000 [01:00<01:32,  6.67it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 383/1000 [01:00<01:29,  6.88it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 384/1000 [01:01<01:28,  6.99it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 385/1000 [01:01<01:26,  7.13it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 386/1000 [01:01<01:19,  7.68it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 387/1000 [01:01<01:24,  7.28it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 388/1000 [01:01<01:24,  7.28it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 389/1000 [01:01<01:24,  7.24it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 390/1000 [01:01<01:46,  5.72it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 391/1000 [01:02<02:09,  4.71it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 392/1000 [01:02<01:54,  5.29it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 393/1000 [01:02<01:49,  5.53it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 394/1000 [01:02<01:45,  5.76it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 395/1000 [01:02<01:38,  6.12it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 396/1000 [01:02<01:36,  6.23it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 397/1000 [01:03<01:37,  6.20it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 398/1000 [01:03<01:37,  6.18it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 399/1000 [01:03<01:36,  6.23it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 401/1000 [01:03<01:25,  7.04it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 402/1000 [01:03<01:19,  7.51it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 403/1000 [01:03<01:20,  7.43it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 404/1000 [01:04<01:40,  5.91it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 405/1000 [01:04<01:35,  6.26it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 406/1000 [01:04<01:26,  6.90it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 407/1000 [01:04<01:20,  7.40it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 408/1000 [01:04<01:20,  7.31it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 409/1000 [01:04<01:24,  7.03it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 410/1000 [01:04<01:18,  7.49it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 411/1000 [01:05<01:23,  7.04it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 412/1000 [01:05<01:27,  6.75it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 413/1000 [01:05<01:24,  6.91it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 414/1000 [01:05<01:49,  5.37it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 415/1000 [01:05<01:40,  5.85it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 416/1000 [01:05<01:33,  6.22it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 417/1000 [01:06<01:29,  6.51it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 418/1000 [01:06<01:27,  6.69it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 419/1000 [01:06<01:18,  7.37it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 420/1000 [01:06<01:21,  7.09it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 421/1000 [01:06<01:20,  7.15it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 422/1000 [01:06<01:20,  7.18it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 423/1000 [01:06<01:14,  7.70it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 424/1000 [01:07<01:16,  7.51it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 425/1000 [01:07<01:17,  7.45it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 426/1000 [01:07<01:17,  7.38it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 427/1000 [01:07<01:38,  5.82it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 428/1000 [01:07<01:36,  5.92it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 429/1000 [01:07<01:26,  6.59it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 430/1000 [01:08<01:33,  6.08it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 431/1000 [01:08<01:49,  5.22it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 432/1000 [01:08<01:43,  5.51it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 433/1000 [01:08<01:38,  5.74it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 434/1000 [01:08<01:32,  6.13it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 435/1000 [01:08<01:31,  6.20it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 436/1000 [01:09<01:36,  5.85it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 437/1000 [01:09<01:30,  6.21it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 438/1000 [01:09<01:26,  6.52it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 439/1000 [01:09<01:23,  6.75it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 440/1000 [01:09<01:21,  6.89it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 441/1000 [01:09<01:51,  5.02it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 442/1000 [01:10<01:43,  5.39it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 443/1000 [01:10<01:55,  4.84it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 444/1000 [01:10<01:37,  5.69it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 445/1000 [01:10<01:51,  4.98it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 446/1000 [01:10<01:40,  5.53it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 447/1000 [01:11<01:58,  4.68it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 448/1000 [01:11<01:45,  5.25it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 449/1000 [01:11<01:38,  5.58it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 450/1000 [01:11<01:32,  5.97it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 451/1000 [01:11<01:29,  6.10it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 452/1000 [01:11<01:29,  6.10it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 453/1000 [01:12<01:25,  6.41it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 454/1000 [01:12<01:24,  6.43it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 455/1000 [01:12<01:21,  6.65it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 456/1000 [01:12<01:22,  6.60it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 457/1000 [01:12<01:39,  5.47it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 458/1000 [01:12<01:31,  5.91it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 459/1000 [01:13<01:26,  6.24it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 460/1000 [01:13<01:27,  6.20it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 461/1000 [01:13<01:42,  5.27it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 462/1000 [01:13<01:36,  5.58it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 463/1000 [01:13<01:29,  5.98it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 464/1000 [01:13<01:24,  6.31it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 466/1000 [01:14<01:16,  7.01it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 467/1000 [01:14<01:15,  7.05it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 468/1000 [01:14<01:18,  6.81it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 469/1000 [01:14<01:16,  6.93it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 470/1000 [01:14<01:33,  5.70it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 471/1000 [01:14<01:27,  6.06it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 472/1000 [01:15<01:25,  6.18it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 473/1000 [01:15<01:25,  6.17it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 474/1000 [01:15<01:24,  6.22it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 475/1000 [01:15<01:39,  5.30it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 476/1000 [01:15<01:33,  5.60it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 477/1000 [01:16<01:50,  4.73it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 478/1000 [01:16<01:38,  5.28it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 479/1000 [01:16<01:26,  6.01it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 480/1000 [01:16<01:24,  6.14it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 481/1000 [01:16<01:20,  6.42it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 482/1000 [01:16<01:18,  6.62it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 483/1000 [01:16<01:16,  6.79it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 484/1000 [01:17<01:14,  6.90it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 485/1000 [01:17<01:16,  6.75it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 486/1000 [01:17<01:14,  6.89it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 487/1000 [01:17<01:13,  6.98it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 488/1000 [01:17<01:31,  5.62it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 489/1000 [01:17<01:27,  5.83it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 490/1000 [01:18<01:22,  6.15it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 492/1000 [01:18<01:12,  6.98it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 493/1000 [01:18<01:15,  6.74it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 494/1000 [01:18<01:13,  6.85it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 495/1000 [01:18<01:28,  5.69it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 496/1000 [01:19<01:23,  6.05it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 497/1000 [01:19<01:36,  5.23it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 498/1000 [01:19<01:36,  5.21it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 499/1000 [01:19<01:23,  5.99it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 500/1000 [01:19<01:22,  6.09it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 501/1000 [01:19<01:12,  6.85it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 502/1000 [01:19<01:11,  6.92it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 503/1000 [01:20<01:14,  6.68it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 504/1000 [01:20<01:15,  6.61it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 505/1000 [01:20<01:12,  6.82it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 506/1000 [01:20<01:28,  5.57it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 507/1000 [01:20<01:17,  6.35it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 508/1000 [01:20<01:18,  6.25it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 509/1000 [01:21<01:11,  6.87it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 510/1000 [01:21<01:10,  7.00it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 511/1000 [01:21<01:13,  6.67it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 512/1000 [01:21<01:13,  6.61it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 513/1000 [01:21<01:07,  7.23it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 514/1000 [01:21<01:07,  7.22it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 515/1000 [01:21<01:03,  7.63it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 516/1000 [01:22<01:04,  7.49it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 517/1000 [01:22<01:05,  7.38it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 518/1000 [01:22<01:08,  7.00it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 519/1000 [01:22<01:11,  6.70it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 520/1000 [01:22<01:13,  6.50it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 521/1000 [01:22<01:14,  6.47it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 522/1000 [01:22<01:11,  6.65it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 523/1000 [01:23<01:10,  6.77it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 524/1000 [01:23<01:09,  6.86it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 525/1000 [01:23<01:08,  6.93it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 526/1000 [01:23<01:09,  6.78it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 527/1000 [01:23<01:25,  5.53it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 528/1000 [01:23<01:14,  6.29it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 529/1000 [01:24<01:11,  6.54it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 530/1000 [01:24<01:09,  6.73it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 531/1000 [01:24<01:08,  6.86it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 532/1000 [01:24<01:06,  7.01it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 533/1000 [01:24<01:09,  6.71it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 534/1000 [01:24<01:11,  6.52it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 535/1000 [01:24<01:12,  6.39it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 536/1000 [01:25<01:26,  5.38it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 538/1000 [01:25<01:09,  6.66it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 539/1000 [01:25<01:09,  6.65it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 541/1000 [01:25<01:04,  7.07it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 542/1000 [01:25<01:04,  7.11it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 543/1000 [01:26<00:59,  7.64it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 544/1000 [01:26<01:00,  7.59it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 545/1000 [01:26<01:02,  7.33it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 546/1000 [01:26<01:03,  7.11it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 548/1000 [01:26<00:54,  8.30it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 549/1000 [01:27<01:13,  6.11it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 550/1000 [01:27<01:09,  6.44it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 551/1000 [01:27<01:08,  6.51it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 552/1000 [01:27<01:06,  6.76it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 553/1000 [01:27<01:04,  6.94it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 554/1000 [01:27<01:05,  6.85it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 555/1000 [01:27<00:59,  7.42it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 557/1000 [01:27<00:51,  8.58it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 558/1000 [01:28<00:53,  8.31it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 559/1000 [01:28<00:57,  7.67it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 560/1000 [01:28<01:12,  6.05it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 561/1000 [01:28<01:10,  6.23it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 562/1000 [01:28<01:08,  6.37it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 563/1000 [01:28<01:01,  7.11it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 564/1000 [01:29<01:02,  6.99it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 565/1000 [01:29<01:00,  7.17it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 566/1000 [01:29<01:06,  6.52it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 567/1000 [01:29<01:05,  6.56it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 568/1000 [01:29<01:03,  6.83it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 569/1000 [01:29<01:01,  7.04it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 570/1000 [01:29<00:59,  7.18it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 572/1000 [01:30<00:56,  7.54it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 573/1000 [01:30<00:56,  7.54it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 574/1000 [01:30<00:59,  7.21it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 575/1000 [01:30<00:57,  7.35it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 576/1000 [01:30<00:57,  7.37it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 577/1000 [01:30<00:58,  7.18it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 578/1000 [01:31<01:00,  6.93it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 579/1000 [01:31<00:59,  7.08it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 580/1000 [01:31<00:55,  7.62it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 581/1000 [01:31<01:10,  5.91it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 582/1000 [01:31<01:22,  5.08it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 583/1000 [01:31<01:13,  5.65it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 584/1000 [01:32<01:04,  6.45it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 585/1000 [01:32<01:03,  6.51it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 586/1000 [01:32<00:57,  7.14it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 587/1000 [01:32<00:54,  7.64it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 588/1000 [01:32<01:09,  5.92it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 589/1000 [01:32<01:07,  6.13it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 590/1000 [01:32<01:03,  6.47it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 591/1000 [01:33<01:19,  5.13it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 592/1000 [01:33<01:14,  5.45it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 593/1000 [01:33<01:23,  4.87it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 594/1000 [01:33<01:16,  5.31it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 595/1000 [01:33<01:09,  5.83it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 597/1000 [01:34<00:59,  6.78it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 598/1000 [01:34<00:59,  6.76it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 599/1000 [01:34<00:57,  6.92it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 600/1000 [01:34<00:53,  7.45it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 601/1000 [01:34<00:53,  7.53it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 602/1000 [01:34<00:52,  7.55it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 603/1000 [01:34<00:52,  7.51it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 604/1000 [01:35<00:52,  7.50it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 605/1000 [01:35<00:52,  7.49it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 606/1000 [01:35<00:54,  7.25it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 607/1000 [01:35<00:55,  7.04it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 608/1000 [01:35<00:56,  6.91it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 609/1000 [01:35<00:55,  7.08it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 610/1000 [01:35<00:53,  7.22it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 611/1000 [01:36<00:55,  7.03it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 612/1000 [01:36<00:54,  7.13it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 613/1000 [01:36<00:56,  6.89it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 614/1000 [01:36<00:54,  7.06it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 615/1000 [01:36<00:53,  7.19it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 616/1000 [01:36<00:52,  7.28it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 617/1000 [01:36<00:54,  7.07it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 618/1000 [01:37<00:55,  6.93it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 619/1000 [01:37<00:55,  6.85it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 620/1000 [01:37<00:51,  7.38it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 621/1000 [01:37<00:51,  7.42it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 622/1000 [01:37<00:50,  7.46it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 623/1000 [01:37<00:53,  7.08it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 625/1000 [01:38<00:48,  7.69it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 626/1000 [01:38<00:49,  7.62it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 627/1000 [01:38<00:49,  7.59it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 628/1000 [01:38<00:49,  7.58it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 629/1000 [01:38<00:48,  7.57it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 630/1000 [01:38<00:48,  7.58it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 631/1000 [01:38<00:48,  7.55it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 632/1000 [01:38<00:50,  7.28it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 633/1000 [01:39<00:52,  7.05it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 634/1000 [01:39<01:04,  5.65it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 635/1000 [01:39<01:01,  5.92it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 636/1000 [01:39<00:57,  6.31it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 637/1000 [01:39<00:54,  6.63it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 638/1000 [01:39<00:54,  6.68it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 639/1000 [01:40<00:52,  6.90it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 640/1000 [01:40<00:52,  6.82it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 641/1000 [01:40<00:53,  6.70it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 642/1000 [01:40<00:51,  6.95it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 643/1000 [01:40<01:03,  5.61it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 644/1000 [01:40<00:58,  6.08it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 645/1000 [01:41<00:54,  6.47it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 646/1000 [01:41<00:52,  6.74it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 647/1000 [01:41<01:04,  5.49it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 648/1000 [01:41<00:58,  5.97it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 649/1000 [01:41<00:51,  6.75it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 650/1000 [01:41<00:50,  6.94it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 651/1000 [01:41<00:50,  6.86it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 652/1000 [01:42<00:49,  7.04it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 653/1000 [01:42<00:50,  6.82it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 654/1000 [01:42<00:51,  6.69it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 655/1000 [01:42<00:49,  6.92it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 656/1000 [01:42<00:48,  7.09it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 657/1000 [01:42<00:49,  6.96it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 658/1000 [01:42<00:48,  7.08it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 659/1000 [01:43<00:47,  7.18it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 660/1000 [01:43<00:48,  7.00it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 661/1000 [01:43<00:50,  6.78it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 662/1000 [01:43<00:48,  6.97it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 663/1000 [01:43<00:47,  7.13it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 664/1000 [01:43<00:43,  7.75it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 665/1000 [01:43<00:45,  7.36it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 666/1000 [01:44<00:41,  7.96it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 667/1000 [01:44<00:42,  7.81it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 668/1000 [01:44<00:59,  5.61it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 669/1000 [01:44<00:54,  6.06it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 670/1000 [01:44<00:53,  6.21it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 671/1000 [01:45<01:02,  5.23it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 672/1000 [01:45<00:58,  5.58it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 673/1000 [01:45<00:54,  6.04it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 674/1000 [01:45<00:50,  6.45it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 675/1000 [01:45<00:48,  6.73it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 676/1000 [01:45<00:48,  6.71it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 677/1000 [01:45<00:46,  6.94it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 678/1000 [01:46<00:57,  5.62it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 679/1000 [01:46<00:54,  5.86it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 680/1000 [01:46<00:51,  6.25it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 681/1000 [01:46<00:50,  6.37it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 682/1000 [01:46<00:47,  6.67it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 683/1000 [01:46<00:47,  6.68it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 684/1000 [01:46<00:45,  6.88it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 685/1000 [01:47<00:44,  7.03it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 686/1000 [01:47<00:55,  5.66it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 687/1000 [01:47<00:48,  6.42it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 688/1000 [01:47<00:43,  7.13it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 689/1000 [01:47<00:54,  5.67it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 690/1000 [01:47<00:52,  5.88it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 691/1000 [01:48<01:00,  5.08it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 692/1000 [01:48<00:56,  5.47it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 693/1000 [01:48<01:03,  4.87it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 694/1000 [01:48<00:53,  5.69it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 695/1000 [01:48<00:51,  5.97it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 696/1000 [01:49<00:47,  6.40it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 697/1000 [01:49<00:45,  6.69it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 698/1000 [01:49<00:58,  5.18it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 699/1000 [01:49<00:52,  5.76it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 700/1000 [01:49<00:50,  5.98it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 701/1000 [01:49<00:48,  6.17it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 702/1000 [01:50<00:47,  6.28it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 703/1000 [01:50<00:45,  6.60it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 704/1000 [01:50<00:44,  6.62it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 706/1000 [01:50<00:47,  6.23it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 707/1000 [01:50<00:45,  6.50it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 708/1000 [01:50<00:45,  6.46it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 709/1000 [01:51<00:45,  6.42it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 710/1000 [01:51<00:40,  7.09it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 711/1000 [01:51<00:41,  6.95it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 712/1000 [01:51<00:41,  6.87it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 713/1000 [01:51<00:51,  5.58it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 714/1000 [01:51<00:47,  6.05it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 715/1000 [01:52<00:44,  6.42it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 716/1000 [01:52<00:42,  6.70it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 717/1000 [01:52<00:42,  6.69it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 718/1000 [01:52<00:40,  6.89it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 719/1000 [01:52<00:39,  7.04it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 721/1000 [01:52<00:42,  6.49it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 722/1000 [01:53<00:42,  6.52it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 723/1000 [01:53<00:42,  6.54it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 724/1000 [01:53<00:40,  6.80it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 725/1000 [01:53<00:41,  6.65it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 726/1000 [01:53<00:49,  5.55it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 727/1000 [01:54<00:58,  4.67it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 728/1000 [01:54<00:51,  5.25it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 729/1000 [01:54<00:48,  5.60it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 730/1000 [01:54<00:45,  5.90it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 731/1000 [01:54<00:44,  6.10it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 732/1000 [01:54<00:41,  6.44it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 733/1000 [01:54<00:39,  6.71it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 734/1000 [01:55<00:38,  6.90it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 735/1000 [01:55<00:37,  7.07it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 736/1000 [01:55<00:36,  7.18it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 737/1000 [01:55<00:37,  7.03it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 738/1000 [01:55<00:36,  7.17it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 739/1000 [01:55<00:36,  7.24it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 740/1000 [01:55<00:35,  7.32it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 741/1000 [01:56<00:35,  7.37it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 742/1000 [01:56<00:44,  5.82it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 743/1000 [01:56<00:45,  5.67it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 744/1000 [01:56<00:51,  4.94it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 745/1000 [01:56<00:43,  5.81it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 746/1000 [01:56<00:42,  6.03it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 747/1000 [01:57<00:49,  5.14it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 748/1000 [01:57<00:44,  5.69it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 749/1000 [01:57<00:56,  4.46it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 750/1000 [01:57<00:48,  5.10it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 751/1000 [01:57<00:44,  5.64it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 752/1000 [01:58<00:40,  6.07it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 753/1000 [01:58<00:39,  6.24it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 754/1000 [01:58<00:39,  6.28it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 755/1000 [01:58<00:34,  7.03it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 756/1000 [01:58<00:35,  6.80it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 757/1000 [01:58<00:35,  6.76it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 758/1000 [01:58<00:34,  6.97it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 759/1000 [01:59<00:43,  5.59it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 760/1000 [01:59<00:41,  5.80it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 761/1000 [01:59<00:39,  6.01it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 762/1000 [01:59<00:37,  6.42it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 763/1000 [01:59<00:35,  6.72it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 764/1000 [02:00<00:43,  5.45it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 765/1000 [02:00<00:39,  5.95it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 766/1000 [02:00<00:45,  5.12it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 767/1000 [02:00<00:41,  5.67it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 768/1000 [02:00<00:37,  6.12it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 769/1000 [02:00<00:35,  6.46it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 770/1000 [02:01<00:42,  5.37it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 771/1000 [02:01<00:39,  5.87it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 772/1000 [02:01<00:36,  6.27it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 773/1000 [02:01<00:34,  6.62it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 774/1000 [02:01<00:32,  6.87it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 775/1000 [02:01<00:40,  5.59it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 776/1000 [02:02<00:37,  6.04it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 777/1000 [02:02<00:33,  6.75it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 778/1000 [02:02<00:31,  6.95it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 779/1000 [02:02<00:31,  7.12it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 780/1000 [02:02<00:30,  7.23it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 781/1000 [02:02<00:29,  7.30it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 782/1000 [02:02<00:30,  7.09it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 783/1000 [02:02<00:31,  6.86it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 784/1000 [02:03<00:32,  6.68it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 785/1000 [02:03<00:39,  5.49it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 786/1000 [02:03<00:34,  6.27it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 787/1000 [02:03<00:33,  6.36it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 788/1000 [02:03<00:39,  5.37it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 789/1000 [02:04<00:35,  5.89it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 790/1000 [02:04<00:34,  6.11it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 791/1000 [02:04<00:30,  6.88it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 792/1000 [02:04<00:37,  5.56it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 793/1000 [02:04<00:37,  5.51it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 794/1000 [02:04<00:35,  5.82it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 795/1000 [02:05<00:33,  6.04it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 796/1000 [02:05<00:39,  5.17it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 797/1000 [02:05<00:42,  4.74it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 799/1000 [02:05<00:32,  6.22it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 800/1000 [02:05<00:31,  6.30it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 801/1000 [02:06<00:31,  6.30it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 802/1000 [02:06<00:30,  6.58it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 803/1000 [02:06<00:29,  6.61it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 805/1000 [02:06<00:26,  7.29it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 806/1000 [02:06<00:27,  7.05it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 807/1000 [02:06<00:27,  7.15it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 808/1000 [02:07<00:26,  7.24it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 809/1000 [02:07<00:27,  7.07it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 810/1000 [02:07<00:26,  7.18it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 811/1000 [02:07<00:26,  7.03it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 812/1000 [02:07<00:27,  6.91it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 814/1000 [02:07<00:21,  8.52it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 815/1000 [02:08<00:28,  6.56it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 816/1000 [02:08<00:27,  6.58it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 817/1000 [02:08<00:26,  6.81it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 818/1000 [02:08<00:27,  6.70it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 819/1000 [02:08<00:26,  6.73it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 820/1000 [02:08<00:26,  6.70it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 821/1000 [02:09<00:32,  5.50it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 822/1000 [02:09<00:29,  6.01it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 823/1000 [02:09<00:27,  6.37it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 824/1000 [02:09<00:27,  6.37it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 825/1000 [02:09<00:32,  5.34it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 826/1000 [02:09<00:30,  5.62it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 827/1000 [02:10<00:34,  4.97it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 828/1000 [02:10<00:37,  4.61it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 829/1000 [02:10<00:32,  5.22it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 831/1000 [02:10<00:25,  6.57it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 832/1000 [02:10<00:24,  6.79it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 833/1000 [02:10<00:24,  6.80it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 834/1000 [02:11<00:24,  6.78it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 835/1000 [02:11<00:23,  6.99it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 836/1000 [02:11<00:22,  7.16it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 837/1000 [02:11<00:22,  7.27it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 838/1000 [02:11<00:22,  7.33it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 839/1000 [02:11<00:21,  7.36it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 840/1000 [02:11<00:21,  7.43it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 841/1000 [02:12<00:22,  7.19it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 842/1000 [02:12<00:29,  5.42it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 843/1000 [02:12<00:27,  5.74it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 844/1000 [02:12<00:26,  5.98it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 845/1000 [02:12<00:23,  6.69it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 846/1000 [02:12<00:23,  6.69it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 847/1000 [02:13<00:23,  6.57it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 848/1000 [02:13<00:22,  6.85it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 849/1000 [02:13<00:21,  7.04it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 850/1000 [02:13<00:21,  6.83it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 851/1000 [02:13<00:21,  7.00it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 852/1000 [02:13<00:19,  7.66it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 853/1000 [02:13<00:20,  7.21it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 854/1000 [02:14<00:20,  7.26it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 855/1000 [02:14<00:19,  7.34it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 856/1000 [02:14<00:19,  7.42it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 857/1000 [02:14<00:19,  7.45it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 858/1000 [02:14<00:18,  7.50it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 859/1000 [02:14<00:19,  7.23it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 860/1000 [02:14<00:19,  7.29it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 861/1000 [02:15<00:19,  7.10it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 862/1000 [02:15<00:19,  7.21it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 863/1000 [02:15<00:17,  7.80it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 864/1000 [02:15<00:18,  7.38it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 865/1000 [02:15<00:18,  7.43it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 867/1000 [02:15<00:16,  7.93it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 868/1000 [02:15<00:17,  7.47it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 869/1000 [02:16<00:18,  7.22it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 870/1000 [02:16<00:18,  6.98it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 871/1000 [02:16<00:18,  6.87it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 872/1000 [02:16<00:18,  7.02it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 873/1000 [02:16<00:17,  7.15it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 874/1000 [02:16<00:17,  7.02it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 875/1000 [02:16<00:17,  7.12it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 876/1000 [02:17<00:21,  5.70it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 877/1000 [02:17<00:19,  6.18it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 878/1000 [02:17<00:24,  4.98it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 879/1000 [02:17<00:23,  5.10it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 880/1000 [02:17<00:21,  5.65it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 881/1000 [02:18<00:19,  6.11it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 882/1000 [02:18<00:18,  6.27it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 883/1000 [02:18<00:18,  6.41it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 884/1000 [02:18<00:17,  6.58it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 885/1000 [02:18<00:17,  6.63it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 886/1000 [02:18<00:20,  5.49it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 887/1000 [02:19<00:18,  5.99it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 888/1000 [02:19<00:17,  6.41it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 889/1000 [02:19<00:16,  6.71it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 890/1000 [02:19<00:15,  6.92it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 891/1000 [02:19<00:15,  6.82it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 892/1000 [02:19<00:15,  6.99it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 893/1000 [02:19<00:15,  7.12it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 894/1000 [02:20<00:15,  6.89it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 895/1000 [02:20<00:14,  7.05it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 896/1000 [02:20<00:14,  7.18it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 897/1000 [02:20<00:14,  7.28it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 898/1000 [02:20<00:14,  7.08it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 899/1000 [02:20<00:14,  7.09it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 900/1000 [02:20<00:14,  6.85it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 901/1000 [02:20<00:14,  7.06it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 902/1000 [02:21<00:17,  5.70it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 903/1000 [02:21<00:15,  6.43it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 904/1000 [02:21<00:14,  6.70it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 905/1000 [02:21<00:14,  6.69it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 907/1000 [02:21<00:12,  7.45it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 908/1000 [02:22<00:12,  7.22it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 909/1000 [02:22<00:12,  7.07it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 910/1000 [02:22<00:12,  7.16it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 911/1000 [02:22<00:12,  7.23it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 912/1000 [02:22<00:12,  7.29it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 913/1000 [02:22<00:11,  7.35it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 914/1000 [02:22<00:11,  7.40it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 915/1000 [02:22<00:11,  7.48it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 916/1000 [02:23<00:11,  7.45it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 918/1000 [02:23<00:10,  7.92it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 919/1000 [02:23<00:10,  7.80it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 920/1000 [02:23<00:13,  5.78it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 921/1000 [02:23<00:12,  6.18it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 922/1000 [02:24<00:14,  5.28it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 923/1000 [02:24<00:12,  6.08it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 924/1000 [02:24<00:14,  5.20it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 925/1000 [02:24<00:13,  5.71it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 926/1000 [02:24<00:12,  5.88it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 927/1000 [02:24<00:11,  6.09it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 928/1000 [02:25<00:11,  6.44it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 929/1000 [02:25<00:10,  6.71it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 930/1000 [02:25<00:10,  6.93it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 931/1000 [02:25<00:10,  6.87it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 932/1000 [02:25<00:12,  5.57it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 933/1000 [02:25<00:11,  5.87it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 934/1000 [02:26<00:10,  6.06it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 936/1000 [02:26<00:09,  7.05it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 937/1000 [02:26<00:09,  6.91it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 938/1000 [02:26<00:09,  6.76it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 939/1000 [02:26<00:10,  5.60it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 940/1000 [02:27<00:09,  6.01it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 941/1000 [02:27<00:08,  6.64it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 942/1000 [02:27<00:07,  7.35it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 943/1000 [02:27<00:07,  7.35it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 944/1000 [02:27<00:07,  7.34it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 945/1000 [02:27<00:07,  7.38it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 946/1000 [02:27<00:07,  7.45it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 947/1000 [02:27<00:07,  7.41it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 948/1000 [02:28<00:07,  7.36it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 949/1000 [02:28<00:06,  7.32it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 950/1000 [02:28<00:06,  7.32it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 951/1000 [02:28<00:07,  6.95it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 952/1000 [02:28<00:06,  7.07it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 953/1000 [02:28<00:06,  7.70it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 954/1000 [02:28<00:07,  5.85it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 955/1000 [02:29<00:07,  6.25it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 956/1000 [02:29<00:06,  6.52it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 957/1000 [02:29<00:06,  7.09it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 958/1000 [02:29<00:06,  6.81it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 959/1000 [02:29<00:07,  5.52it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 960/1000 [02:29<00:06,  6.00it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 961/1000 [02:30<00:06,  6.38it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 962/1000 [02:30<00:05,  6.68it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 963/1000 [02:30<00:05,  6.88it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 964/1000 [02:30<00:05,  7.02it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 965/1000 [02:30<00:05,  6.89it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 966/1000 [02:30<00:04,  7.03it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 967/1000 [02:30<00:04,  7.13it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 968/1000 [02:31<00:04,  6.96it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 969/1000 [02:31<00:04,  6.82it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 970/1000 [02:31<00:05,  5.22it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 971/1000 [02:31<00:05,  5.48it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 972/1000 [02:31<00:04,  5.93it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 973/1000 [02:31<00:04,  6.27it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 974/1000 [02:32<00:04,  6.27it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 975/1000 [02:32<00:04,  5.25it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 976/1000 [02:32<00:04,  5.74it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 977/1000 [02:32<00:03,  6.16it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 978/1000 [02:32<00:03,  6.47it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 979/1000 [02:32<00:02,  7.07it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 980/1000 [02:33<00:02,  6.92it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 982/1000 [02:33<00:02,  7.54it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 983/1000 [02:33<00:02,  7.87it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 984/1000 [02:33<00:02,  7.33it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 985/1000 [02:33<00:02,  7.06it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 986/1000 [02:33<00:01,  7.13it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 988/1000 [02:34<00:01,  7.99it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 989/1000 [02:34<00:01,  7.47it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 990/1000 [02:34<00:01,  7.23it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 991/1000 [02:34<00:01,  7.26it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 992/1000 [02:34<00:01,  7.27it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 993/1000 [02:34<00:00,  7.32it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 994/1000 [02:34<00:00,  7.31it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 995/1000 [02:35<00:00,  7.06it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 996/1000 [02:35<00:00,  7.11it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 997/1000 [02:35<00:00,  7.19it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 998/1000 [02:35<00:00,  6.99it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 999/1000 [02:35<00:00,  7.09it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [02:35<00:00,  6.91it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [02:35<00:00,  6.42it/s]
DONE (2.77s)
DONE (8.19s)
loss_threshold ROC AUC: 0.496996, PR AUC: 0.4956215115253504, tpr_at_low_fpr: {0.001: 0.0, 0.01: 0.009}
min_k_threshold ROC AUC: 0.500677, PR AUC: 0.5032345089504372, tpr_at_low_fpr: {0.001: 0.0, 0.01: 0.008}
zlib_threshold ROC AUC: 0.499314, PR AUC: 0.4988112563762992, tpr_at_low_fpr: {0.001: 0.0, 0.01: 0.012}
ref-stablelm-base-alpha-3b-v2_threshold ROC AUC: 0.5026470000000001, PR AUC: 0.5038578231789758, tpr_at_low_fpr: {0.001: 0.0, 0.01: 0.007}
loss_threshold roc_auc: 0.497
min_k_threshold roc_auc: 0.501
zlib_threshold roc_auc: 0.499
ref-stablelm-base-alpha-3b-v2_threshold roc_auc: 0.503
Finished.
