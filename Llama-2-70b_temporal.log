nohup: ignoring input
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
results_new/neo125_github_experiment/meta-llama_Llama-2-70b-hf/temporal_arxiv_2023_06
Saving results to absolute path: /home/chawins/mimir/tmp_results/neo125_github_experiment/meta-llama_Llama-2-70b-hf/temporal_arxiv_2023_06
LOG: cache_dir is /home/chawins/.cache
Using cache dir /home/chawins/.cache
Loading BASE model meta-llama/Llama-2-70b-hf...
Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.embed_tokens.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:   7%|â–‹         | 1/15 [00:00<00:02,  5.73it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  13%|â–ˆâ–Ž        | 2/15 [00:00<00:02,  5.91it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 3/15 [00:00<00:02,  5.91it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:00<00:01,  5.87it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:00<00:01,  5.83it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:01<00:01,  5.86it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:01<00:01,  5.86it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:01<00:01,  5.87it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:01<00:01,  5.88it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:01<00:00,  5.97it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:01<00:00,  6.03it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:02<00:00,  6.10it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:02<00:00,  6.00it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:02<00:00,  5.96it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for lm_head.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:02<00:00,  6.17it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:02<00:00,  5.98it/s]
  0%|          | 0/57 [00:00<?, ?w/s]model.embed_tokens.weight:   0%|          | 0/57 [00:00<?, ?w/s, dev=0]model.embed_tokens.weight:   2%|â–         | 1/57 [00:00<00:10,  5.47w/s, dev=0]model.layers.0.input_layernorm.weight:   2%|â–         | 1/57 [00:00<00:10,  5.46w/s, dev=0]model.layers.0.mlp.down_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:05, 10.90w/s, dev=0]  model.layers.0.mlp.down_proj.weight:   5%|â–Œ         | 3/57 [00:00<00:06,  8.78w/s, dev=0]model.layers.0.mlp.gate_proj.weight:   5%|â–Œ         | 3/57 [00:00<00:06,  8.77w/s, dev=0]model.layers.0.mlp.up_proj.weight:   7%|â–‹         | 4/57 [00:00<00:06,  7.94w/s, dev=0]  model.layers.0.mlp.up_proj.weight:   9%|â–‰         | 5/57 [00:00<00:07,  7.12w/s, dev=0]model.layers.0.post_attention_layernorm.weight:   9%|â–‰         | 5/57 [00:00<00:07,  7.12w/s, dev=0]model.layers.0.self_attn.k_proj.weight:  11%|â–ˆ         | 6/57 [00:00<00:05,  8.54w/s, dev=0]        model.layers.0.self_attn.o_proj.weight:  12%|â–ˆâ–        | 7/57 [00:00<00:05,  9.82w/s, dev=0]model.layers.0.self_attn.q_proj.weight:  14%|â–ˆâ–        | 8/57 [00:00<00:04, 10.38w/s, dev=0]model.layers.0.self_attn.q_proj.weight:  16%|â–ˆâ–Œ        | 9/57 [00:00<00:04, 10.96w/s, dev=0]model.layers.0.self_attn.rotary_emb.inv_freq:  16%|â–ˆâ–Œ        | 9/57 [00:00<00:04, 10.96w/s, dev=0]model.layers.0.self_attn.v_proj.weight:  18%|â–ˆâ–Š        | 10/57 [00:00<00:03, 12.17w/s, dev=0]     model.layers.1.input_layernorm.weight:  19%|â–ˆâ–‰        | 11/57 [00:00<00:03, 13.25w/s, dev=0] model.layers.1.mlp.down_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:00<00:03, 14.45w/s, dev=0]  model.layers.1.mlp.down_proj.weight:  23%|â–ˆâ–ˆâ–Ž       | 13/57 [00:01<00:03, 12.92w/s, dev=0]model.layers.1.mlp.gate_proj.weight:  23%|â–ˆâ–ˆâ–Ž       | 13/57 [00:01<00:03, 12.91w/s, dev=0]model.layers.1.mlp.up_proj.weight:  25%|â–ˆâ–ˆâ–       | 14/57 [00:01<00:04, 10.30w/s, dev=0]  model.layers.1.post_attention_layernorm.weight:  26%|â–ˆâ–ˆâ–‹       | 15/57 [00:01<00:04,  9.79w/s, dev=0]model.layers.1.self_attn.k_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:01<00:03, 10.44w/s, dev=0]        model.layers.1.self_attn.k_proj.weight:  30%|â–ˆâ–ˆâ–‰       | 17/57 [00:01<00:03, 11.03w/s, dev=0]model.layers.1.self_attn.o_proj.weight:  30%|â–ˆâ–ˆâ–‰       | 17/57 [00:01<00:03, 11.03w/s, dev=0]model.layers.1.self_attn.q_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 18/57 [00:01<00:03, 11.34w/s, dev=0]model.layers.1.self_attn.rotary_emb.inv_freq:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 19/57 [00:01<00:03, 10.10w/s, dev=0]model.layers.1.self_attn.v_proj.weight:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 20/57 [00:01<00:03, 10.63w/s, dev=0]      model.layers.1.self_attn.v_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 21/57 [00:01<00:03, 11.08w/s, dev=0]model.layers.2.input_layernorm.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 21/57 [00:01<00:03, 11.08w/s, dev=0] model.layers.2.mlp.down_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:01<00:03, 11.61w/s, dev=0]  model.layers.2.mlp.gate_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 23/57 [00:02<00:03, 11.20w/s, dev=0]model.layers.2.mlp.up_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/57 [00:02<00:03, 10.84w/s, dev=0]  model.layers.2.mlp.up_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/57 [00:02<00:03,  9.50w/s, dev=0]model.layers.2.post_attention_layernorm.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/57 [00:02<00:03,  9.50w/s, dev=0]model.layers.2.self_attn.k_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 26/57 [00:02<00:03,  9.88w/s, dev=0]        model.layers.2.self_attn.o_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 27/57 [00:02<00:02, 10.23w/s, dev=0]model.layers.2.self_attn.q_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:02<00:02, 10.42w/s, dev=0]model.layers.2.self_attn.q_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 29/57 [00:02<00:02, 10.60w/s, dev=0]model.layers.2.self_attn.rotary_emb.inv_freq:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 29/57 [00:02<00:02, 10.60w/s, dev=0]model.layers.2.self_attn.v_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 30/57 [00:02<00:02, 10.96w/s, dev=0]      model.layers.3.input_layernorm.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 31/57 [00:02<00:02, 11.30w/s, dev=0] model.layers.3.mlp.down_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 32/57 [00:02<00:02, 11.66w/s, dev=0]  model.layers.3.mlp.down_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 33/57 [00:02<00:02, 11.36w/s, dev=0]model.layers.3.mlp.gate_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 33/57 [00:02<00:02, 11.36w/s, dev=0]model.layers.3.mlp.up_proj.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:03<00:02, 10.82w/s, dev=0]  model.layers.3.post_attention_layernorm.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/57 [00:03<00:02, 10.60w/s, dev=0]model.layers.3.self_attn.k_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 36/57 [00:03<00:01, 10.90w/s, dev=0]        model.layers.3.self_attn.k_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 37/57 [00:03<00:01, 10.67w/s, dev=0]model.layers.3.self_attn.o_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 37/57 [00:03<00:01, 10.67w/s, dev=0]model.layers.3.self_attn.q_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 38/57 [00:03<00:01, 10.80w/s, dev=0]model.layers.3.self_attn.rotary_emb.inv_freq:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 39/57 [00:03<00:01, 10.93w/s, dev=0]model.layers.3.self_attn.v_proj.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:03<00:01, 11.21w/s, dev=0]      model.layers.3.self_attn.v_proj.weight:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/57 [00:03<00:01, 11.47w/s, dev=0]model.layers.4.input_layernorm.weight:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/57 [00:03<00:01, 11.47w/s, dev=0] model.layers.4.mlp.down_proj.weight:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 42/57 [00:03<00:01, 11.74w/s, dev=0]  model.layers.4.mlp.gate_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 43/57 [00:03<00:01, 11.46w/s, dev=0]model.layers.4.mlp.up_proj.weight:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 44/57 [00:03<00:01, 11.22w/s, dev=0]  model.layers.4.mlp.up_proj.weight:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 45/57 [00:04<00:01, 10.88w/s, dev=0]model.layers.4.post_attention_layernorm.weight:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 45/57 [00:04<00:01, 10.88w/s, dev=0]model.layers.4.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:04<00:00, 11.12w/s, dev=0]        model.layers.4.self_attn.o_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 47/57 [00:04<00:00, 11.33w/s, dev=0]model.layers.4.self_attn.q_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/57 [00:04<00:00, 11.44w/s, dev=0]model.layers.4.self_attn.q_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 49/57 [00:04<00:00, 11.53w/s, dev=0]model.layers.4.self_attn.rotary_emb.inv_freq:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 49/57 [00:04<00:00, 11.53w/s, dev=0]model.layers.4.self_attn.v_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 50/57 [00:04<00:00, 11.77w/s, dev=0]      model.layers.5.mlp.gate_proj.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 51/57 [00:04<00:00, 11.98w/s, dev=0]   model.layers.5.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:04<00:00, 11.75w/s, dev=0]model.layers.5.self_attn.k_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 53/57 [00:04<00:00, 11.76w/s, dev=0]model.layers.5.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 53/57 [00:04<00:00, 11.76w/s, dev=0]model.layers.5.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/57 [00:04<00:00, 11.85w/s, dev=0]model.layers.5.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 55/57 [00:04<00:00, 11.92w/s, dev=0]model.layers.5.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 56/57 [00:04<00:00, 12.14w/s, dev=0]      model.layers.5.self_attn.v_proj.weight: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:04<00:00, 12.33w/s, dev=0]                                                                                               0%|          | 0/59 [00:00<?, ?w/s]model.layers.5.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=0]model.layers.5.mlp.down_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 1626.96w/s, dev=0]model.layers.5.mlp.down_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:05, 11.14w/s, dev=0]  model.layers.5.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:05, 11.13w/s, dev=0]  model.layers.5.post_attention_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:06,  8.44w/s, dev=0]model.layers.5.post_attention_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:04, 11.24w/s, dev=0]model.layers.6.input_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:04, 11.24w/s, dev=0]         model.layers.6.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 14.04w/s, dev=0]  model.layers.6.mlp.down_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:04, 11.45w/s, dev=0]model.layers.6.mlp.gate_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:04, 11.44w/s, dev=0]model.layers.6.mlp.up_proj.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:05, 10.16w/s, dev=0]  model.layers.6.mlp.up_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:05,  9.21w/s, dev=0]model.layers.6.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:05,  9.21w/s, dev=0]model.layers.6.self_attn.k_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:04, 10.36w/s, dev=0]        model.layers.6.self_attn.o_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:04, 11.39w/s, dev=0]model.layers.6.self_attn.q_proj.weight:  19%|â–ˆâ–Š        | 11/59 [00:00<00:04, 11.81w/s, dev=0]model.layers.6.self_attn.q_proj.weight:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:03, 12.19w/s, dev=0]model.layers.6.self_attn.rotary_emb.inv_freq:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:03, 12.19w/s, dev=0]model.layers.6.self_attn.v_proj.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:03, 13.20w/s, dev=0]      model.layers.7.input_layernorm.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:03, 14.10w/s, dev=0] model.layers.7.mlp.down_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:02, 15.11w/s, dev=0]  model.layers.7.mlp.down_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:03, 13.98w/s, dev=0]model.layers.7.mlp.gate_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:03, 13.98w/s, dev=0]model.layers.7.mlp.up_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:01<00:03, 13.12w/s, dev=0]  model.layers.7.post_attention_layernorm.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:03, 12.58w/s, dev=0]model.layers.7.self_attn.k_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:03, 13.27w/s, dev=0]        model.layers.7.self_attn.k_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:02, 13.92w/s, dev=0]model.layers.7.self_attn.o_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:02, 13.92w/s, dev=0]model.layers.7.self_attn.q_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:01<00:02, 14.14w/s, dev=0]model.layers.7.self_attn.rotary_emb.inv_freq:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:02, 14.36w/s, dev=0]model.layers.7.self_attn.v_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:02, 15.01w/s, dev=0]      model.layers.7.self_attn.v_proj.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:02, 15.59w/s, dev=0]model.layers.8.input_layernorm.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:02, 15.59w/s, dev=0] model.layers.8.mlp.down_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:02, 16.24w/s, dev=0]  model.layers.8.mlp.gate_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:02, 15.27w/s, dev=0]model.layers.8.mlp.up_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:01<00:02, 14.68w/s, dev=0]  model.layers.8.mlp.up_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:02, 14.21w/s, dev=0]model.layers.8.post_attention_layernorm.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:02, 14.20w/s, dev=0]model.layers.8.self_attn.k_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:02, 14.71w/s, dev=0]        model.layers.8.self_attn.o_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:01<00:01, 15.17w/s, dev=0]model.layers.8.self_attn.q_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:02<00:01, 15.38w/s, dev=0]model.layers.8.self_attn.rotary_emb.inv_freq:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:02<00:01, 15.58w/s, dev=0]model.layers.8.self_attn.v_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:02<00:01, 16.06w/s, dev=0]      model.layers.9.input_layernorm.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:02<00:01, 16.50w/s, dev=0] model.layers.9.mlp.down_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:02<00:01, 16.99w/s, dev=0]  model.layers.9.mlp.down_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:02<00:01, 16.33w/s, dev=0]model.layers.9.mlp.gate_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:02<00:01, 16.33w/s, dev=0]model.layers.9.mlp.up_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:02<00:01, 15.61w/s, dev=0]  model.layers.9.post_attention_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:02<00:01, 15.20w/s, dev=0]model.layers.9.self_attn.k_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:02<00:01, 15.60w/s, dev=0]        model.layers.9.self_attn.o_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:02<00:01, 15.96w/s, dev=0]model.layers.9.self_attn.q_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:02<00:01, 16.07w/s, dev=0]model.layers.9.self_attn.rotary_emb.inv_freq:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:02<00:01, 16.16w/s, dev=0]model.layers.9.self_attn.v_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:02<00:00, 16.54w/s, dev=0]      model.layers.9.self_attn.v_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:02<00:00, 16.88w/s, dev=0]model.layers.10.input_layernorm.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:02<00:00, 16.88w/s, dev=0]model.layers.10.mlp.down_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:02<00:00, 17.26w/s, dev=0]  model.layers.10.mlp.gate_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:02<00:00, 16.61w/s, dev=0]model.layers.10.mlp.up_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:02<00:00, 16.10w/s, dev=0]  model.layers.10.post_attention_layernorm.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:03<00:00, 15.52w/s, dev=0]model.layers.10.self_attn.k_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:03<00:00, 15.84w/s, dev=0]        model.layers.10.self_attn.o_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:03<00:00, 16.12w/s, dev=0]model.layers.10.self_attn.q_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:03<00:00, 16.17w/s, dev=0]model.layers.10.self_attn.q_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:03<00:00, 16.21w/s, dev=0]model.layers.10.self_attn.rotary_emb.inv_freq:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:03<00:00, 16.21w/s, dev=0]model.layers.10.self_attn.v_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:03<00:00, 16.52w/s, dev=0]      model.layers.11.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:03<00:00, 16.79w/s, dev=0]model.layers.11.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:03<00:00, 17.06w/s, dev=0]model.layers.11.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:03<00:00, 17.15w/s, dev=0]model.layers.11.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:03<00:00, 17.23w/s, dev=0]model.layers.11.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:03<00:00, 17.53w/s, dev=0]                                                                                                      0%|          | 0/55 [00:00<?, ?w/s]model.layers.11.input_layernorm.weight:   0%|          | 0/55 [00:00<?, ?w/s, dev=0]model.layers.11.mlp.down_proj.weight:   2%|â–         | 1/55 [00:00<00:00, 1639.68w/s, dev=0]model.layers.11.mlp.down_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:03, 13.33w/s, dev=0]  model.layers.11.mlp.gate_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:03, 13.30w/s, dev=0]model.layers.11.mlp.up_proj.weight:   5%|â–Œ         | 3/55 [00:00<00:05,  9.75w/s, dev=0]  model.layers.11.mlp.up_proj.weight:   7%|â–‹         | 4/55 [00:00<00:05,  8.79w/s, dev=0]model.layers.11.post_attention_layernorm.weight:   7%|â–‹         | 4/55 [00:00<00:05,  8.78w/s, dev=0]model.layers.12.input_layernorm.weight:   9%|â–‰         | 5/55 [00:00<00:04, 10.97w/s, dev=0]         model.layers.12.mlp.down_proj.weight:  11%|â–ˆ         | 6/55 [00:00<00:03, 13.15w/s, dev=0]  model.layers.12.mlp.down_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:04, 11.72w/s, dev=0]model.layers.12.mlp.gate_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:04, 11.72w/s, dev=0]model.layers.12.mlp.up_proj.weight:  15%|â–ˆâ–        | 8/55 [00:00<00:04, 10.52w/s, dev=0]  model.layers.12.post_attention_layernorm.weight:  16%|â–ˆâ–‹        | 9/55 [00:00<00:04,  9.65w/s, dev=0]model.layers.12.post_attention_layernorm.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:04, 10.72w/s, dev=0]model.layers.12.self_attn.k_proj.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:04, 10.72w/s, dev=0]        model.layers.12.self_attn.o_proj.weight:  20%|â–ˆâ–ˆ        | 11/55 [00:00<00:03, 11.71w/s, dev=0]model.layers.12.self_attn.q_proj.weight:  22%|â–ˆâ–ˆâ–       | 12/55 [00:00<00:03, 12.17w/s, dev=0]model.layers.12.self_attn.q_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:01<00:03, 12.58w/s, dev=0]model.layers.12.self_attn.rotary_emb.inv_freq:  24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:01<00:03, 12.58w/s, dev=0]model.layers.12.self_attn.v_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 14/55 [00:01<00:03, 13.54w/s, dev=0]      model.layers.13.input_layernorm.weight:  27%|â–ˆâ–ˆâ–‹       | 15/55 [00:01<00:02, 14.41w/s, dev=0] model.layers.13.mlp.down_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 16/55 [00:01<00:02, 15.37w/s, dev=0]  model.layers.13.mlp.down_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:01<00:02, 14.13w/s, dev=0]model.layers.13.mlp.gate_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:01<00:02, 14.13w/s, dev=0]model.layers.13.mlp.up_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/55 [00:01<00:02, 13.14w/s, dev=0]  model.layers.13.post_attention_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–      | 19/55 [00:01<00:02, 12.50w/s, dev=0]model.layers.13.self_attn.k_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:01<00:02, 13.16w/s, dev=0]        model.layers.13.self_attn.k_proj.weight:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/55 [00:01<00:02, 13.76w/s, dev=0]model.layers.13.self_attn.o_proj.weight:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/55 [00:01<00:02, 13.76w/s, dev=0]model.layers.13.self_attn.q_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/55 [00:01<00:02, 13.99w/s, dev=0]model.layers.13.self_attn.rotary_emb.inv_freq:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 23/55 [00:01<00:02, 14.22w/s, dev=0]model.layers.13.self_attn.v_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 24/55 [00:01<00:02, 14.83w/s, dev=0]      model.layers.14.input_layernorm.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:01<00:01, 15.37w/s, dev=0] model.layers.14.input_layernorm.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 26/55 [00:01<00:01, 15.99w/s, dev=0]model.layers.14.mlp.down_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 26/55 [00:01<00:01, 15.99w/s, dev=0]  model.layers.14.mlp.gate_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 27/55 [00:01<00:01, 15.06w/s, dev=0]model.layers.14.mlp.up_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 28/55 [00:01<00:01, 14.27w/s, dev=0]  model.layers.14.post_attention_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/55 [00:02<00:01, 13.72w/s, dev=0]model.layers.14.self_attn.k_proj.weight:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 30/55 [00:02<00:01, 14.19w/s, dev=0]        model.layers.14.self_attn.k_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/55 [00:02<00:01, 14.60w/s, dev=0]model.layers.14.self_attn.o_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/55 [00:02<00:01, 14.60w/s, dev=0]model.layers.14.self_attn.q_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 32/55 [00:02<00:01, 14.75w/s, dev=0]model.layers.14.self_attn.rotary_emb.inv_freq:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/55 [00:02<00:01, 14.90w/s, dev=0]model.layers.14.self_attn.v_proj.weight:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/55 [00:02<00:01, 15.35w/s, dev=0]      model.layers.15.input_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:02<00:01, 15.73w/s, dev=0] model.layers.15.input_layernorm.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 36/55 [00:02<00:01, 16.18w/s, dev=0]model.layers.15.mlp.down_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 36/55 [00:02<00:01, 16.18w/s, dev=0]  model.layers.15.mlp.gate_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 37/55 [00:02<00:01, 15.62w/s, dev=0]model.layers.15.mlp.up_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 38/55 [00:02<00:01, 15.20w/s, dev=0]  model.layers.15.post_attention_layernorm.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 39/55 [00:02<00:01, 14.73w/s, dev=0]model.layers.15.self_attn.k_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 40/55 [00:02<00:00, 15.11w/s, dev=0]        model.layers.15.self_attn.k_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/55 [00:02<00:00, 15.44w/s, dev=0]model.layers.15.self_attn.o_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/55 [00:02<00:00, 15.43w/s, dev=0]model.layers.15.self_attn.q_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 42/55 [00:02<00:00, 15.56w/s, dev=0]model.layers.15.self_attn.rotary_emb.inv_freq:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 43/55 [00:02<00:00, 15.67w/s, dev=0]model.layers.15.self_attn.v_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 44/55 [00:02<00:00, 16.04w/s, dev=0]      model.layers.16.input_layernorm.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:02<00:00, 16.36w/s, dev=0] model.layers.16.mlp.down_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 46/55 [00:02<00:00, 16.72w/s, dev=0]  model.layers.16.mlp.down_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 47/55 [00:02<00:00, 16.23w/s, dev=0]model.layers.16.mlp.gate_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 47/55 [00:02<00:00, 16.23w/s, dev=0]model.layers.16.mlp.up_proj.weight:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 48/55 [00:03<00:00, 15.76w/s, dev=0]  model.layers.16.post_attention_layernorm.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 49/55 [00:03<00:00, 15.30w/s, dev=0]model.layers.16.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 50/55 [00:03<00:00, 15.61w/s, dev=0]        model.layers.16.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 51/55 [00:03<00:00, 15.89w/s, dev=0]model.layers.16.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 52/55 [00:03<00:00, 15.98w/s, dev=0]model.layers.16.self_attn.q_proj.weight:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 53/55 [00:03<00:00, 16.07w/s, dev=0]model.layers.16.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 53/55 [00:03<00:00, 16.07w/s, dev=0]model.layers.16.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 54/55 [00:03<00:00, 16.37w/s, dev=0]                                                                                                      0%|          | 0/57 [00:00<?, ?w/s]model.layers.17.input_layernorm.weight:   0%|          | 0/57 [00:00<?, ?w/s, dev=0]model.layers.17.mlp.down_proj.weight:   2%|â–         | 1/57 [00:00<00:00, 1728.18w/s, dev=0]model.layers.17.mlp.down_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:04, 13.42w/s, dev=0]  model.layers.17.mlp.gate_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:04, 13.40w/s, dev=0]model.layers.17.mlp.up_proj.weight:   5%|â–Œ         | 3/57 [00:00<00:05, 10.05w/s, dev=0]  model.layers.17.mlp.up_proj.weight:   7%|â–‹         | 4/57 [00:00<00:05,  8.94w/s, dev=0]model.layers.17.post_attention_layernorm.weight:   7%|â–‹         | 4/57 [00:00<00:05,  8.93w/s, dev=0]model.layers.17.self_attn.k_proj.weight:   9%|â–‰         | 5/57 [00:00<00:04, 11.16w/s, dev=0]        model.layers.17.self_attn.o_proj.weight:  11%|â–ˆ         | 6/57 [00:00<00:03, 13.20w/s, dev=0]model.layers.17.self_attn.q_proj.weight:  12%|â–ˆâ–        | 7/57 [00:00<00:03, 13.82w/s, dev=0]model.layers.17.self_attn.q_proj.weight:  14%|â–ˆâ–        | 8/57 [00:00<00:03, 14.30w/s, dev=0]model.layers.17.self_attn.rotary_emb.inv_freq:  14%|â–ˆâ–        | 8/57 [00:00<00:03, 14.30w/s, dev=0]model.layers.17.self_attn.v_proj.weight:  16%|â–ˆâ–Œ        | 9/57 [00:00<00:02, 16.07w/s, dev=0]      model.layers.18.input_layernorm.weight:  18%|â–ˆâ–Š        | 10/57 [00:00<00:02, 17.57w/s, dev=0]model.layers.18.mlp.down_proj.weight:  19%|â–ˆâ–‰        | 11/57 [00:00<00:02, 19.32w/s, dev=0]  model.layers.18.mlp.down_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:00<00:02, 16.73w/s, dev=0]model.layers.18.mlp.gate_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:00<00:02, 16.73w/s, dev=0]model.layers.18.mlp.up_proj.weight:  23%|â–ˆâ–ˆâ–Ž       | 13/57 [00:00<00:02, 15.01w/s, dev=0]  model.layers.18.post_attention_layernorm.weight:  25%|â–ˆâ–ˆâ–       | 14/57 [00:01<00:03, 13.75w/s, dev=0]model.layers.18.self_attn.k_proj.weight:  26%|â–ˆâ–ˆâ–‹       | 15/57 [00:01<00:02, 14.72w/s, dev=0]        model.layers.18.self_attn.k_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:01<00:02, 15.60w/s, dev=0]model.layers.18.self_attn.o_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:01<00:02, 15.60w/s, dev=0]model.layers.18.self_attn.q_proj.weight:  30%|â–ˆâ–ˆâ–‰       | 17/57 [00:01<00:02, 15.96w/s, dev=0]model.layers.18.self_attn.rotary_emb.inv_freq:  32%|â–ˆâ–ˆâ–ˆâ–      | 18/57 [00:01<00:02, 16.13w/s, dev=0]model.layers.18.self_attn.v_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 19/57 [00:01<00:02, 17.02w/s, dev=0]      model.layers.19.input_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 20/57 [00:01<00:02, 17.80w/s, dev=0] model.layers.19.mlp.down_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 21/57 [00:01<00:01, 18.69w/s, dev=0]  model.layers.19.mlp.down_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:01<00:02, 16.92w/s, dev=0]model.layers.19.mlp.gate_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:01<00:02, 16.92w/s, dev=0]model.layers.19.mlp.up_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 23/57 [00:01<00:02, 15.65w/s, dev=0]  model.layers.19.post_attention_layernorm.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/57 [00:01<00:02, 13.47w/s, dev=0]model.layers.19.self_attn.k_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/57 [00:01<00:02, 14.03w/s, dev=0]        model.layers.19.self_attn.o_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 26/57 [00:01<00:02, 14.54w/s, dev=0]model.layers.19.self_attn.q_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 27/57 [00:01<00:02, 14.61w/s, dev=0]model.layers.19.self_attn.q_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:01<00:01, 14.78w/s, dev=0]model.layers.19.self_attn.rotary_emb.inv_freq:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:01<00:01, 14.78w/s, dev=0]model.layers.19.self_attn.v_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 29/57 [00:01<00:01, 15.31w/s, dev=0]      model.layers.20.input_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 30/57 [00:02<00:02, 12.90w/s, dev=1] model.layers.20.mlp.down_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 31/57 [00:02<00:01, 13.30w/s, dev=1]  model.layers.20.mlp.gate_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 32/57 [00:02<00:01, 12.77w/s, dev=1]model.layers.20.mlp.up_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 33/57 [00:02<00:02, 11.33w/s, dev=1]  model.layers.20.mlp.up_proj.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:03<00:02, 10.99w/s, dev=1]model.layers.20.post_attention_layernorm.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:03<00:02, 10.99w/s, dev=1]model.layers.20.self_attn.k_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/57 [00:03<00:01, 11.31w/s, dev=1]        model.layers.20.self_attn.o_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 36/57 [00:03<00:01, 11.61w/s, dev=1]model.layers.20.self_attn.q_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 37/57 [00:03<00:01, 11.74w/s, dev=1]model.layers.20.self_attn.rotary_emb.inv_freq:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 38/57 [00:03<00:01, 11.08w/s, dev=1]model.layers.20.self_attn.v_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 39/57 [00:03<00:01, 11.37w/s, dev=1]      model.layers.20.self_attn.v_proj.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:03<00:01, 11.63w/s, dev=1]model.layers.21.input_layernorm.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:03<00:01, 11.63w/s, dev=1] model.layers.21.mlp.down_proj.weight:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/57 [00:03<00:01, 11.92w/s, dev=1]  model.layers.21.mlp.gate_proj.weight:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 42/57 [00:03<00:01, 11.62w/s, dev=1]model.layers.21.mlp.up_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 43/57 [00:03<00:01, 11.32w/s, dev=1]  model.layers.21.post_attention_layernorm.weight:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 44/57 [00:04<00:01, 10.84w/s, dev=1]model.layers.21.self_attn.k_proj.weight:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 45/57 [00:04<00:01, 11.08w/s, dev=1]        model.layers.21.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:04<00:00, 11.31w/s, dev=1]model.layers.21.self_attn.o_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:04<00:00, 11.31w/s, dev=1]model.layers.21.self_attn.q_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 47/57 [00:04<00:00, 11.40w/s, dev=1]model.layers.21.self_attn.rotary_emb.inv_freq:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/57 [00:04<00:00, 11.49w/s, dev=1]model.layers.21.self_attn.v_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 49/57 [00:04<00:00, 11.73w/s, dev=1]      model.layers.22.mlp.down_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 50/57 [00:04<00:00, 11.95w/s, dev=1]   model.layers.22.mlp.gate_proj.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 51/57 [00:04<00:00, 11.68w/s, dev=1]model.layers.22.mlp.gate_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:04<00:00, 11.31w/s, dev=1]model.layers.22.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:04<00:00, 11.31w/s, dev=1]model.layers.22.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 53/57 [00:04<00:00, 11.50w/s, dev=1]model.layers.22.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/57 [00:04<00:00, 11.58w/s, dev=1]model.layers.22.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 55/57 [00:04<00:00, 11.67w/s, dev=1]model.layers.22.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 56/57 [00:04<00:00, 11.88w/s, dev=1]                                                                                                      0%|          | 0/59 [00:00<?, ?w/s]model.layers.22.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=1]model.layers.22.mlp.up_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 1327.31w/s, dev=1]model.layers.22.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 11.98w/s, dev=1]  model.layers.22.post_attention_layernorm.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 11.96w/s, dev=1]model.layers.23.input_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:03, 17.90w/s, dev=1]         model.layers.23.mlp.down_proj.weight:   7%|â–‹         | 4/59 [00:00<00:02, 23.83w/s, dev=1]  model.layers.23.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:04, 12.39w/s, dev=1]model.layers.23.mlp.gate_proj.weight:   8%|â–Š         | 5/59 [00:00<00:04, 12.38w/s, dev=1]model.layers.23.mlp.up_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:05, 10.59w/s, dev=1]  model.layers.23.post_attention_layernorm.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:05,  9.64w/s, dev=1]model.layers.23.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:04, 11.01w/s, dev=1]model.layers.23.self_attn.k_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:04, 11.00w/s, dev=1]        model.layers.23.self_attn.o_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:04, 12.26w/s, dev=1]model.layers.23.self_attn.q_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:03, 12.81w/s, dev=1]model.layers.23.self_attn.q_proj.weight:  19%|â–ˆâ–Š        | 11/59 [00:00<00:03, 13.28w/s, dev=1]model.layers.23.self_attn.rotary_emb.inv_freq:  19%|â–ˆâ–Š        | 11/59 [00:00<00:03, 13.28w/s, dev=1]model.layers.23.self_attn.v_proj.weight:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:03, 14.48w/s, dev=1]      model.layers.24.input_layernorm.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:02, 15.57w/s, dev=1] model.layers.24.mlp.down_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:02, 16.76w/s, dev=1]  model.layers.24.mlp.down_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:02, 15.15w/s, dev=1]model.layers.24.mlp.gate_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:02, 15.14w/s, dev=1]model.layers.24.mlp.up_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:03, 13.74w/s, dev=1]  model.layers.24.post_attention_layernorm.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:01<00:03, 12.84w/s, dev=1]model.layers.24.self_attn.k_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:03, 13.59w/s, dev=1]        model.layers.24.self_attn.k_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:02, 14.27w/s, dev=1]model.layers.24.self_attn.o_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:02, 14.27w/s, dev=1]model.layers.24.self_attn.q_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:02, 14.47w/s, dev=1]model.layers.24.self_attn.rotary_emb.inv_freq:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:01<00:02, 14.66w/s, dev=1]model.layers.24.self_attn.v_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:02, 15.35w/s, dev=1]      model.layers.24.self_attn.v_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:02, 15.97w/s, dev=1]model.layers.25.input_layernorm.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:02, 15.97w/s, dev=1] model.layers.25.mlp.down_proj.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:02, 16.66w/s, dev=1]  model.layers.25.mlp.gate_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:02, 15.58w/s, dev=1]model.layers.25.mlp.up_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:02, 14.72w/s, dev=1]  model.layers.25.mlp.up_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:01<00:02, 13.98w/s, dev=1]model.layers.25.post_attention_layernorm.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:01<00:02, 13.98w/s, dev=1]model.layers.25.self_attn.k_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:02, 14.49w/s, dev=1]        model.layers.25.self_attn.o_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:02, 14.96w/s, dev=1]model.layers.25.self_attn.q_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:01<00:01, 15.11w/s, dev=1]model.layers.25.self_attn.q_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:02<00:01, 15.23w/s, dev=1]model.layers.25.self_attn.rotary_emb.inv_freq:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:02<00:01, 15.23w/s, dev=1]model.layers.25.self_attn.v_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:02<00:01, 15.72w/s, dev=1]      model.layers.26.input_layernorm.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:02<00:01, 16.14w/s, dev=1] model.layers.26.mlp.down_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:02<00:01, 16.63w/s, dev=1]  model.layers.26.mlp.down_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:02<00:01, 15.77w/s, dev=1]model.layers.26.mlp.gate_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:02<00:01, 15.77w/s, dev=1]model.layers.26.mlp.up_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:02<00:01, 15.07w/s, dev=1]  model.layers.26.post_attention_layernorm.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:02<00:01, 14.49w/s, dev=1]model.layers.26.self_attn.k_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:02<00:01, 14.87w/s, dev=1]        model.layers.26.self_attn.k_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:02<00:01, 15.21w/s, dev=1]model.layers.26.self_attn.o_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:02<00:01, 15.21w/s, dev=1]model.layers.26.self_attn.q_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:02<00:01, 15.29w/s, dev=1]model.layers.26.self_attn.rotary_emb.inv_freq:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:02<00:01, 15.37w/s, dev=1]model.layers.26.self_attn.v_proj.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:02<00:01, 15.74w/s, dev=1]      model.layers.26.self_attn.v_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:02<00:00, 16.07w/s, dev=1]model.layers.27.input_layernorm.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:02<00:00, 16.07w/s, dev=1] model.layers.27.mlp.down_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:02<00:00, 16.44w/s, dev=1]  model.layers.27.mlp.gate_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:02<00:00, 15.81w/s, dev=1]model.layers.27.mlp.up_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:02<00:00, 15.34w/s, dev=1]  model.layers.27.mlp.up_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:03<00:00, 14.86w/s, dev=1]model.layers.27.post_attention_layernorm.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:03<00:00, 14.86w/s, dev=1]model.layers.27.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:03<00:00, 15.18w/s, dev=1]        model.layers.27.self_attn.o_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:03<00:00, 15.46w/s, dev=1]model.layers.27.self_attn.q_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:03<00:00, 15.55w/s, dev=1]model.layers.27.self_attn.q_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:03<00:00, 15.63w/s, dev=1]model.layers.27.self_attn.rotary_emb.inv_freq:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:03<00:00, 15.63w/s, dev=1]model.layers.27.self_attn.v_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:03<00:00, 15.93w/s, dev=1]      model.layers.28.mlp.gate_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:03<00:00, 16.20w/s, dev=1]   model.layers.28.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:03<00:00, 15.66w/s, dev=1]model.layers.28.self_attn.k_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:03<00:00, 15.92w/s, dev=1]model.layers.28.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:03<00:00, 15.92w/s, dev=1]model.layers.28.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:03<00:00, 15.97w/s, dev=1]model.layers.28.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:03<00:00, 16.01w/s, dev=1]model.layers.28.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:03<00:00, 16.29w/s, dev=1]      model.layers.28.self_attn.v_proj.weight: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 59/59 [00:03<00:00, 16.54w/s, dev=1]                                                                                                0%|          | 0/59 [00:00<?, ?w/s]model.layers.28.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=1]model.layers.28.mlp.down_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 1329.41w/s, dev=1]model.layers.28.mlp.down_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 11.53w/s, dev=1]  model.layers.28.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 11.51w/s, dev=1]  model.layers.28.post_attention_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:06,  8.79w/s, dev=1]model.layers.28.post_attention_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:04, 11.71w/s, dev=1]model.layers.29.input_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:04, 11.71w/s, dev=1]         model.layers.29.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 14.62w/s, dev=1]  model.layers.29.mlp.down_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:04, 12.17w/s, dev=1]model.layers.29.mlp.gate_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:04, 12.16w/s, dev=1]model.layers.29.mlp.up_proj.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:04, 10.73w/s, dev=1]  model.layers.29.mlp.up_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:05,  9.86w/s, dev=1]model.layers.29.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:05,  9.86w/s, dev=1]model.layers.29.self_attn.k_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:04, 11.09w/s, dev=1]        model.layers.29.self_attn.o_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:04, 12.22w/s, dev=1]model.layers.29.self_attn.q_proj.weight:  19%|â–ˆâ–Š        | 11/59 [00:00<00:03, 12.76w/s, dev=1]model.layers.29.self_attn.rotary_emb.inv_freq:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:03, 13.22w/s, dev=1]model.layers.29.self_attn.v_proj.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:03, 14.32w/s, dev=1]      model.layers.29.self_attn.v_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:02, 15.31w/s, dev=1]model.layers.30.input_layernorm.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:02, 15.31w/s, dev=1] model.layers.30.mlp.down_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:02, 16.40w/s, dev=1]  model.layers.30.mlp.gate_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:02, 14.99w/s, dev=1]model.layers.30.mlp.up_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:01<00:03, 13.85w/s, dev=1]  model.layers.30.post_attention_layernorm.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:03, 13.04w/s, dev=1]model.layers.30.self_attn.k_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:02, 13.76w/s, dev=1]        model.layers.30.self_attn.k_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:02, 14.42w/s, dev=1]model.layers.30.self_attn.o_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:02, 14.42w/s, dev=1]model.layers.30.self_attn.q_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:01<00:02, 14.67w/s, dev=1]model.layers.30.self_attn.rotary_emb.inv_freq:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:02, 14.91w/s, dev=1]model.layers.30.self_attn.v_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:02, 15.58w/s, dev=1]      model.layers.31.input_layernorm.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:02, 16.19w/s, dev=1] model.layers.31.mlp.down_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:02, 16.86w/s, dev=1]  model.layers.31.mlp.down_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:02, 15.77w/s, dev=1]model.layers.31.mlp.gate_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:02, 15.77w/s, dev=1]model.layers.31.mlp.up_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:01<00:02, 14.95w/s, dev=1]  model.layers.31.post_attention_layernorm.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:02, 14.16w/s, dev=1]model.layers.31.self_attn.k_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:02, 14.67w/s, dev=1]        model.layers.31.self_attn.o_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:01<00:01, 15.11w/s, dev=1]model.layers.31.self_attn.q_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:02<00:01, 15.25w/s, dev=1]model.layers.31.self_attn.q_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:02<00:01, 15.38w/s, dev=1]model.layers.31.self_attn.rotary_emb.inv_freq:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:02<00:01, 15.38w/s, dev=1]model.layers.31.self_attn.v_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:02<00:01, 15.86w/s, dev=1]      model.layers.32.input_layernorm.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:02<00:01, 16.28w/s, dev=1] model.layers.32.mlp.down_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:02<00:01, 16.75w/s, dev=1]  model.layers.32.mlp.gate_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:02<00:01, 15.89w/s, dev=1]model.layers.32.mlp.up_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:02<00:01, 15.29w/s, dev=1]  model.layers.32.mlp.up_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:02<00:01, 14.71w/s, dev=1]model.layers.32.post_attention_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:02<00:01, 14.70w/s, dev=1]model.layers.32.self_attn.k_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:02<00:01, 15.09w/s, dev=1]        model.layers.32.self_attn.o_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:02<00:01, 15.43w/s, dev=1]model.layers.32.self_attn.q_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:02<00:01, 15.52w/s, dev=1]model.layers.32.self_attn.rotary_emb.inv_freq:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:02<00:01, 15.63w/s, dev=1]model.layers.32.self_attn.v_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:02<00:01, 16.00w/s, dev=1]      model.layers.32.self_attn.v_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:02<00:00, 16.33w/s, dev=1]model.layers.33.input_layernorm.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:02<00:00, 16.33w/s, dev=1] model.layers.33.mlp.down_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:02<00:00, 16.70w/s, dev=1]  model.layers.33.mlp.gate_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:02<00:00, 16.11w/s, dev=1]model.layers.33.mlp.up_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:03<00:00, 15.59w/s, dev=1]  model.layers.33.post_attention_layernorm.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:03<00:00, 15.12w/s, dev=1]model.layers.33.self_attn.k_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:03<00:00, 15.43w/s, dev=1]        model.layers.33.self_attn.k_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:03<00:00, 15.70w/s, dev=1]model.layers.33.self_attn.o_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:03<00:00, 15.70w/s, dev=1]model.layers.33.self_attn.q_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:03<00:00, 15.79w/s, dev=1]model.layers.33.self_attn.rotary_emb.inv_freq:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:03<00:00, 15.87w/s, dev=1]model.layers.33.self_attn.v_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:03<00:00, 16.17w/s, dev=1]      model.layers.34.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:03<00:00, 16.45w/s, dev=1]model.layers.34.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:03<00:00, 16.72w/s, dev=1]model.layers.34.self_attn.o_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:03<00:00, 16.79w/s, dev=1]model.layers.34.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:03<00:00, 16.79w/s, dev=1]model.layers.34.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:03<00:00, 16.83w/s, dev=1]model.layers.34.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:03<00:00, 17.13w/s, dev=1]                                                                                                      0%|          | 0/55 [00:00<?, ?w/s]model.layers.34.input_layernorm.weight:   0%|          | 0/55 [00:00<?, ?w/s, dev=1]model.layers.34.mlp.down_proj.weight:   2%|â–         | 1/55 [00:00<00:00, 1346.05w/s, dev=1]model.layers.34.mlp.down_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:04, 12.66w/s, dev=1]  model.layers.34.mlp.gate_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:04, 12.64w/s, dev=1]model.layers.34.mlp.up_proj.weight:   5%|â–Œ         | 3/55 [00:00<00:05,  9.18w/s, dev=1]  model.layers.34.mlp.up_proj.weight:   7%|â–‹         | 4/55 [00:00<00:06,  8.19w/s, dev=1]model.layers.34.post_attention_layernorm.weight:   7%|â–‹         | 4/55 [00:00<00:06,  8.18w/s, dev=1]model.layers.35.input_layernorm.weight:   9%|â–‰         | 5/55 [00:00<00:04, 10.22w/s, dev=1]         model.layers.35.mlp.down_proj.weight:  11%|â–ˆ         | 6/55 [00:00<00:03, 12.26w/s, dev=1]  model.layers.35.mlp.down_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:04, 10.60w/s, dev=1]model.layers.35.mlp.gate_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:04, 10.60w/s, dev=1]model.layers.35.mlp.up_proj.weight:  15%|â–ˆâ–        | 8/55 [00:00<00:04,  9.87w/s, dev=1]  model.layers.35.post_attention_layernorm.weight:  16%|â–ˆâ–‹        | 9/55 [00:00<00:04,  9.26w/s, dev=1]model.layers.35.post_attention_layernorm.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:04, 10.29w/s, dev=1]model.layers.35.self_attn.k_proj.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:04, 10.29w/s, dev=1]        model.layers.35.self_attn.o_proj.weight:  20%|â–ˆâ–ˆ        | 11/55 [00:00<00:03, 11.24w/s, dev=1]model.layers.35.self_attn.q_proj.weight:  22%|â–ˆâ–ˆâ–       | 12/55 [00:01<00:03, 11.67w/s, dev=1]model.layers.35.self_attn.q_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:01<00:03, 12.04w/s, dev=1]model.layers.35.self_attn.rotary_emb.inv_freq:  24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:01<00:03, 12.04w/s, dev=1]model.layers.35.self_attn.v_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 14/55 [00:01<00:03, 12.96w/s, dev=1]      model.layers.36.input_layernorm.weight:  27%|â–ˆâ–ˆâ–‹       | 15/55 [00:01<00:02, 13.77w/s, dev=1] model.layers.36.mlp.down_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 16/55 [00:01<00:02, 14.69w/s, dev=1]  model.layers.36.mlp.down_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:01<00:02, 13.53w/s, dev=1]model.layers.36.mlp.gate_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:01<00:02, 13.53w/s, dev=1]model.layers.36.mlp.up_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/55 [00:01<00:02, 12.78w/s, dev=1]  model.layers.36.post_attention_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–      | 19/55 [00:01<00:02, 12.16w/s, dev=1]model.layers.36.self_attn.k_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:01<00:02, 12.80w/s, dev=1]        model.layers.36.self_attn.k_proj.weight:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/55 [00:01<00:02, 13.37w/s, dev=1]model.layers.36.self_attn.o_proj.weight:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/55 [00:01<00:02, 13.37w/s, dev=1]model.layers.36.self_attn.q_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/55 [00:01<00:02, 13.59w/s, dev=1]model.layers.36.self_attn.rotary_emb.inv_freq:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 23/55 [00:01<00:02, 13.80w/s, dev=1]model.layers.36.self_attn.v_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 24/55 [00:01<00:02, 14.39w/s, dev=1]      model.layers.36.self_attn.v_proj.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:01<00:02, 14.93w/s, dev=1]model.layers.37.input_layernorm.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:01<00:02, 14.93w/s, dev=1] model.layers.37.mlp.down_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 26/55 [00:01<00:01, 15.53w/s, dev=1]  model.layers.37.mlp.gate_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 27/55 [00:01<00:01, 14.74w/s, dev=1]model.layers.37.mlp.up_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 28/55 [00:01<00:01, 14.08w/s, dev=1]  model.layers.37.mlp.up_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/55 [00:02<00:01, 13.45w/s, dev=1]model.layers.37.post_attention_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/55 [00:02<00:01, 13.45w/s, dev=1]model.layers.37.self_attn.k_proj.weight:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 30/55 [00:02<00:01, 13.91w/s, dev=1]        model.layers.37.self_attn.o_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/55 [00:02<00:01, 14.33w/s, dev=1]model.layers.37.self_attn.q_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 32/55 [00:02<00:01, 14.50w/s, dev=1]model.layers.37.self_attn.rotary_emb.inv_freq:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/55 [00:02<00:01, 14.64w/s, dev=1]model.layers.37.self_attn.v_proj.weight:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/55 [00:02<00:01, 15.08w/s, dev=1]      model.layers.37.self_attn.v_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:02<00:01, 15.49w/s, dev=1]model.layers.38.input_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:02<00:01, 15.48w/s, dev=1] model.layers.38.mlp.down_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 36/55 [00:02<00:01, 15.92w/s, dev=1]  model.layers.38.mlp.gate_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 37/55 [00:02<00:01, 15.33w/s, dev=1]model.layers.38.mlp.up_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 38/55 [00:02<00:01, 14.72w/s, dev=1]  model.layers.38.post_attention_layernorm.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 39/55 [00:02<00:01, 14.23w/s, dev=1]model.layers.38.self_attn.k_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 40/55 [00:02<00:01, 14.59w/s, dev=1]        model.layers.38.self_attn.k_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/55 [00:02<00:00, 14.92w/s, dev=1]model.layers.38.self_attn.o_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/55 [00:02<00:00, 14.92w/s, dev=1]model.layers.38.self_attn.q_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 42/55 [00:03<00:00, 13.13w/s, dev=1]model.layers.38.self_attn.rotary_emb.inv_freq:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 43/55 [00:03<00:01, 11.62w/s, dev=1]model.layers.38.self_attn.v_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 44/55 [00:03<00:00, 11.88w/s, dev=1]      model.layers.39.input_layernorm.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:03<00:00, 11.71w/s, dev=1] model.layers.39.mlp.down_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 46/55 [00:03<00:00, 11.97w/s, dev=1]  model.layers.39.mlp.down_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 47/55 [00:04<00:00, 10.82w/s, dev=1]model.layers.39.mlp.gate_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 47/55 [00:04<00:00, 10.82w/s, dev=1]model.layers.39.mlp.up_proj.weight:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 48/55 [00:04<00:00, 10.60w/s, dev=1]  model.layers.39.post_attention_layernorm.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 49/55 [00:04<00:00, 10.47w/s, dev=1]model.layers.39.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 50/55 [00:04<00:00, 10.69w/s, dev=1]        model.layers.39.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 51/55 [00:04<00:00, 10.88w/s, dev=1]model.layers.39.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 52/55 [00:04<00:00, 10.99w/s, dev=1]model.layers.39.self_attn.q_proj.weight:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 53/55 [00:04<00:00, 11.09w/s, dev=1]model.layers.39.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 53/55 [00:04<00:00, 11.09w/s, dev=1]model.layers.39.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 54/55 [00:04<00:00, 11.30w/s, dev=1]                                                                                                      0%|          | 0/57 [00:00<?, ?w/s]model.layers.40.input_layernorm.weight:   0%|          | 0/57 [00:00<?, ?w/s, dev=1]model.layers.40.mlp.down_proj.weight:   2%|â–         | 1/57 [00:00<00:00, 1250.91w/s, dev=1]model.layers.40.mlp.down_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:04, 12.32w/s, dev=1]  model.layers.40.mlp.gate_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:04, 12.30w/s, dev=1]model.layers.40.mlp.up_proj.weight:   5%|â–Œ         | 3/57 [00:00<00:05,  9.61w/s, dev=1]  model.layers.40.mlp.up_proj.weight:   7%|â–‹         | 4/57 [00:00<00:06,  8.64w/s, dev=1]model.layers.40.post_attention_layernorm.weight:   7%|â–‹         | 4/57 [00:00<00:06,  8.64w/s, dev=1]model.layers.40.self_attn.k_proj.weight:   9%|â–‰         | 5/57 [00:00<00:04, 10.79w/s, dev=1]        model.layers.40.self_attn.o_proj.weight:  11%|â–ˆ         | 6/57 [00:00<00:03, 12.77w/s, dev=1]model.layers.40.self_attn.q_proj.weight:  12%|â–ˆâ–        | 7/57 [00:00<00:03, 13.65w/s, dev=1]model.layers.40.self_attn.rotary_emb.inv_freq:  14%|â–ˆâ–        | 8/57 [00:00<00:03, 14.37w/s, dev=1]model.layers.40.self_attn.v_proj.weight:  16%|â–ˆâ–Œ        | 9/57 [00:00<00:02, 16.16w/s, dev=1]      model.layers.40.self_attn.v_proj.weight:  18%|â–ˆâ–Š        | 10/57 [00:00<00:02, 17.75w/s, dev=1]model.layers.41.input_layernorm.weight:  18%|â–ˆâ–Š        | 10/57 [00:00<00:03, 15.46w/s, dev=2] model.layers.41.mlp.down_proj.weight:  19%|â–ˆâ–‰        | 11/57 [00:00<00:02, 16.98w/s, dev=2]  model.layers.41.mlp.gate_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:00<00:03, 14.59w/s, dev=2]model.layers.41.mlp.up_proj.weight:  23%|â–ˆâ–ˆâ–Ž       | 13/57 [00:00<00:03, 13.15w/s, dev=2]  model.layers.41.post_attention_layernorm.weight:  25%|â–ˆâ–ˆâ–       | 14/57 [00:01<00:03, 12.23w/s, dev=2]model.layers.41.self_attn.k_proj.weight:  26%|â–ˆâ–ˆâ–‹       | 15/57 [00:01<00:03, 13.10w/s, dev=2]        model.layers.41.self_attn.k_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:01<00:02, 13.89w/s, dev=2]model.layers.41.self_attn.o_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:01<00:02, 13.89w/s, dev=2]model.layers.41.self_attn.q_proj.weight:  30%|â–ˆâ–ˆâ–‰       | 17/57 [00:01<00:02, 14.19w/s, dev=2]model.layers.41.self_attn.rotary_emb.inv_freq:  32%|â–ˆâ–ˆâ–ˆâ–      | 18/57 [00:01<00:02, 14.44w/s, dev=2]model.layers.41.self_attn.v_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 19/57 [00:01<00:02, 15.23w/s, dev=2]      model.layers.42.input_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 20/57 [00:01<00:02, 15.93w/s, dev=2] model.layers.42.mlp.down_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 21/57 [00:01<00:02, 16.72w/s, dev=2]  model.layers.42.mlp.down_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:01<00:02, 15.46w/s, dev=2]model.layers.42.mlp.gate_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:01<00:02, 15.46w/s, dev=2]model.layers.42.mlp.up_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 23/57 [00:01<00:02, 14.44w/s, dev=2]  model.layers.42.post_attention_layernorm.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/57 [00:01<00:02, 13.67w/s, dev=2]model.layers.42.self_attn.k_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/57 [00:01<00:02, 14.24w/s, dev=2]        model.layers.42.self_attn.o_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 26/57 [00:01<00:02, 14.74w/s, dev=2]model.layers.42.self_attn.q_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 27/57 [00:01<00:02, 14.92w/s, dev=2]model.layers.42.self_attn.q_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:01<00:01, 15.07w/s, dev=2]model.layers.42.self_attn.rotary_emb.inv_freq:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:01<00:01, 15.07w/s, dev=2]model.layers.42.self_attn.v_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 29/57 [00:01<00:01, 15.61w/s, dev=2]      model.layers.43.input_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 30/57 [00:01<00:01, 16.09w/s, dev=2] model.layers.43.mlp.down_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 31/57 [00:01<00:01, 16.62w/s, dev=2]  model.layers.43.mlp.gate_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 32/57 [00:02<00:01, 15.81w/s, dev=2]model.layers.43.mlp.up_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 33/57 [00:02<00:01, 15.02w/s, dev=2]  model.layers.43.mlp.up_proj.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:02<00:01, 14.28w/s, dev=2]model.layers.43.post_attention_layernorm.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:02<00:01, 14.28w/s, dev=2]model.layers.43.self_attn.k_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/57 [00:02<00:01, 14.70w/s, dev=2]        model.layers.43.self_attn.o_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 36/57 [00:02<00:01, 15.07w/s, dev=2]model.layers.43.self_attn.q_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 37/57 [00:02<00:01, 15.16w/s, dev=2]model.layers.43.self_attn.rotary_emb.inv_freq:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 38/57 [00:02<00:01, 15.24w/s, dev=2]model.layers.43.self_attn.v_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 39/57 [00:02<00:01, 15.64w/s, dev=2]      model.layers.43.self_attn.v_proj.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:02<00:01, 15.99w/s, dev=2]model.layers.44.input_layernorm.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:02<00:01, 15.99w/s, dev=2] model.layers.44.mlp.down_proj.weight:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/57 [00:02<00:00, 16.39w/s, dev=2]  model.layers.44.mlp.gate_proj.weight:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 42/57 [00:02<00:00, 15.67w/s, dev=2]model.layers.44.mlp.up_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 43/57 [00:02<00:00, 15.13w/s, dev=2]  model.layers.44.post_attention_layernorm.weight:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 44/57 [00:03<00:00, 14.67w/s, dev=2]model.layers.44.self_attn.k_proj.weight:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 45/57 [00:03<00:00, 15.00w/s, dev=2]        model.layers.44.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:03<00:00, 15.29w/s, dev=2]model.layers.44.self_attn.o_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:03<00:00, 15.29w/s, dev=2]model.layers.44.self_attn.q_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 47/57 [00:03<00:00, 15.38w/s, dev=2]model.layers.44.self_attn.rotary_emb.inv_freq:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/57 [00:03<00:00, 15.47w/s, dev=2]model.layers.44.self_attn.v_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 49/57 [00:03<00:00, 15.79w/s, dev=2]      model.layers.45.mlp.down_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 50/57 [00:03<00:00, 16.08w/s, dev=2]   model.layers.45.mlp.gate_proj.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 51/57 [00:03<00:00, 15.65w/s, dev=2]model.layers.45.mlp.gate_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:03<00:00, 15.22w/s, dev=2]model.layers.45.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:03<00:00, 15.22w/s, dev=2]model.layers.45.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 53/57 [00:03<00:00, 15.47w/s, dev=2]model.layers.45.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/57 [00:03<00:00, 15.51w/s, dev=2]model.layers.45.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 55/57 [00:03<00:00, 15.57w/s, dev=2]model.layers.45.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 56/57 [00:03<00:00, 15.85w/s, dev=2]                                                                                                      0%|          | 0/59 [00:00<?, ?w/s]model.layers.45.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=2]model.layers.45.mlp.up_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 1165.08w/s, dev=2]model.layers.45.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:13,  4.12w/s, dev=2]  model.layers.45.post_attention_layernorm.weight:   3%|â–Ž         | 2/59 [00:00<00:13,  4.12w/s, dev=2]model.layers.46.input_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:09,  6.17w/s, dev=2]         model.layers.46.mlp.down_proj.weight:   7%|â–‹         | 4/59 [00:00<00:06,  8.22w/s, dev=2]  model.layers.46.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:10,  5.02w/s, dev=2]model.layers.46.mlp.gate_proj.weight:   8%|â–Š         | 5/59 [00:00<00:10,  5.02w/s, dev=2]model.layers.46.mlp.up_proj.weight:  10%|â–ˆ         | 6/59 [00:01<00:10,  5.14w/s, dev=2]  model.layers.46.post_attention_layernorm.weight:  12%|â–ˆâ–        | 7/59 [00:01<00:11,  4.63w/s, dev=2]model.layers.46.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:01<00:09,  5.30w/s, dev=2]model.layers.46.self_attn.k_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:01<00:09,  5.29w/s, dev=2]        model.layers.46.self_attn.o_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:01<00:08,  5.88w/s, dev=2]model.layers.46.self_attn.q_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:01<00:07,  6.33w/s, dev=2]model.layers.46.self_attn.q_proj.weight:  19%|â–ˆâ–Š        | 11/59 [00:01<00:07,  6.76w/s, dev=2]model.layers.46.self_attn.rotary_emb.inv_freq:  19%|â–ˆâ–Š        | 11/59 [00:01<00:07,  6.75w/s, dev=2]model.layers.46.self_attn.v_proj.weight:  20%|â–ˆâ–ˆ        | 12/59 [00:01<00:06,  7.37w/s, dev=2]      model.layers.47.input_layernorm.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:01<00:05,  7.95w/s, dev=2] model.layers.47.mlp.down_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:01<00:05,  8.56w/s, dev=2]  model.layers.47.mlp.down_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:01<00:05,  8.34w/s, dev=2]model.layers.47.mlp.gate_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:01<00:05,  8.34w/s, dev=2]model.layers.47.mlp.up_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:05,  8.17w/s, dev=2]  model.layers.47.post_attention_layernorm.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:02<00:05,  8.03w/s, dev=2]model.layers.47.self_attn.k_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:02<00:04,  8.50w/s, dev=2]        model.layers.47.self_attn.k_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:02<00:04,  8.95w/s, dev=2]model.layers.47.self_attn.o_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:02<00:04,  8.95w/s, dev=2]model.layers.47.self_attn.q_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:02<00:04,  9.22w/s, dev=2]model.layers.47.self_attn.rotary_emb.inv_freq:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:02<00:04,  9.48w/s, dev=2]model.layers.47.self_attn.v_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:02<00:03,  9.92w/s, dev=2]      model.layers.48.input_layernorm.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:02<00:03, 10.35w/s, dev=2] model.layers.48.mlp.down_proj.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:02<00:03, 10.79w/s, dev=2]  model.layers.48.mlp.down_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:02<00:03, 10.50w/s, dev=2]model.layers.48.mlp.gate_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:02<00:03, 10.50w/s, dev=2]model.layers.48.mlp.up_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:02<00:03, 10.26w/s, dev=2]  model.layers.48.post_attention_layernorm.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:02<00:03, 10.01w/s, dev=2]model.layers.48.self_attn.k_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:02<00:02, 10.38w/s, dev=2]        model.layers.48.self_attn.o_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:02<00:02, 10.73w/s, dev=2]model.layers.48.self_attn.q_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:02<00:02, 10.91w/s, dev=2]model.layers.48.self_attn.q_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:02<00:02, 11.09w/s, dev=2]model.layers.48.self_attn.rotary_emb.inv_freq:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:02<00:02, 11.09w/s, dev=2]model.layers.48.self_attn.v_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:02<00:02, 11.44w/s, dev=2]      model.layers.49.input_layernorm.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:02<00:02, 11.77w/s, dev=2] model.layers.49.mlp.down_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:02<00:02, 12.13w/s, dev=2]  model.layers.49.mlp.gate_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:02<00:02, 11.80w/s, dev=2]model.layers.49.mlp.up_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:03<00:01, 11.53w/s, dev=2]  model.layers.49.mlp.up_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:03<00:01, 11.31w/s, dev=2]model.layers.49.post_attention_layernorm.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:03<00:01, 11.31w/s, dev=2]model.layers.49.self_attn.k_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:03<00:01, 11.61w/s, dev=2]        model.layers.49.self_attn.o_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:03<00:01, 11.90w/s, dev=2]model.layers.49.self_attn.q_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:03<00:01, 12.03w/s, dev=2]model.layers.49.self_attn.rotary_emb.inv_freq:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:03<00:01, 12.16w/s, dev=2]model.layers.49.self_attn.v_proj.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:03<00:01, 12.45w/s, dev=2]      model.layers.49.self_attn.v_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:03<00:01, 12.72w/s, dev=2]model.layers.50.input_layernorm.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:03<00:01, 12.72w/s, dev=2] model.layers.50.mlp.down_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:03<00:01, 13.01w/s, dev=2]  model.layers.50.mlp.gate_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:03<00:01, 12.71w/s, dev=2]model.layers.50.mlp.up_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:03<00:01, 12.46w/s, dev=2]  model.layers.50.post_attention_layernorm.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:03<00:00, 12.22w/s, dev=2]model.layers.50.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:03<00:00, 12.48w/s, dev=2]        model.layers.50.self_attn.k_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:03<00:00, 12.71w/s, dev=2]model.layers.50.self_attn.o_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:03<00:00, 12.71w/s, dev=2]model.layers.50.self_attn.q_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:03<00:00, 12.82w/s, dev=2]model.layers.50.self_attn.rotary_emb.inv_freq:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:03<00:00, 12.92w/s, dev=2]model.layers.50.self_attn.v_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:03<00:00, 13.17w/s, dev=2]      model.layers.51.mlp.gate_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:03<00:00, 13.40w/s, dev=2]   model.layers.51.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:04<00:00, 13.17w/s, dev=2]model.layers.51.self_attn.k_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:04<00:00, 13.39w/s, dev=2]model.layers.51.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:04<00:00, 13.39w/s, dev=2]model.layers.51.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:04<00:00, 13.49w/s, dev=2]model.layers.51.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:04<00:00, 13.58w/s, dev=2]model.layers.51.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:04<00:00, 13.81w/s, dev=2]                                                                                                      0%|          | 0/59 [00:00<?, ?w/s]model.layers.51.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=2]model.layers.51.mlp.down_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 1165.41w/s, dev=2]model.layers.51.mlp.down_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 12.01w/s, dev=2]  model.layers.51.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 12.00w/s, dev=2]  model.layers.51.post_attention_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:05,  9.41w/s, dev=2]model.layers.51.post_attention_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:04, 12.54w/s, dev=2]model.layers.52.input_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:04, 12.53w/s, dev=2]         model.layers.52.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 15.65w/s, dev=2]  model.layers.52.mlp.down_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:04, 12.93w/s, dev=2]model.layers.52.mlp.gate_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:04, 12.93w/s, dev=2]model.layers.52.mlp.up_proj.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:04, 11.42w/s, dev=2]  model.layers.52.mlp.up_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:04, 10.44w/s, dev=2]model.layers.52.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:04, 10.44w/s, dev=2]model.layers.52.self_attn.k_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:04, 11.73w/s, dev=2]        model.layers.52.self_attn.o_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:03, 12.92w/s, dev=2]model.layers.52.self_attn.q_proj.weight:  19%|â–ˆâ–Š        | 11/59 [00:00<00:03, 13.40w/s, dev=2]model.layers.52.self_attn.rotary_emb.inv_freq:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:03, 13.85w/s, dev=2]model.layers.52.self_attn.rotary_emb.inv_freq:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:03, 15.00w/s, dev=2]model.layers.52.self_attn.v_proj.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:03, 15.00w/s, dev=2]      model.layers.53.input_layernorm.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:02, 16.04w/s, dev=2] model.layers.53.mlp.down_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:02, 17.18w/s, dev=2]  model.layers.53.mlp.gate_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:02, 15.50w/s, dev=2]model.layers.53.mlp.up_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:01<00:02, 14.27w/s, dev=2]  model.layers.53.mlp.up_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:03, 13.28w/s, dev=2]model.layers.53.post_attention_layernorm.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:03, 13.28w/s, dev=2]model.layers.53.self_attn.k_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:02, 14.01w/s, dev=2]        model.layers.53.self_attn.o_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:02, 14.68w/s, dev=2]model.layers.53.self_attn.q_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:01<00:02, 14.91w/s, dev=2]model.layers.53.self_attn.rotary_emb.inv_freq:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:02, 15.09w/s, dev=2]model.layers.53.self_attn.rotary_emb.inv_freq:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:02, 15.77w/s, dev=2]model.layers.53.self_attn.v_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:02, 15.77w/s, dev=2]      model.layers.54.input_layernorm.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:02, 16.36w/s, dev=2] model.layers.54.mlp.down_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:01, 17.04w/s, dev=2]  model.layers.54.mlp.gate_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:02, 15.95w/s, dev=2]model.layers.54.mlp.up_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:01<00:02, 15.10w/s, dev=2]  model.layers.54.mlp.up_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:02, 14.40w/s, dev=2]model.layers.54.post_attention_layernorm.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:02, 14.40w/s, dev=2]model.layers.54.self_attn.k_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:02, 14.91w/s, dev=2]        model.layers.54.self_attn.o_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:01<00:01, 15.36w/s, dev=2]model.layers.54.self_attn.q_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:01<00:01, 15.52w/s, dev=2]model.layers.54.self_attn.rotary_emb.inv_freq:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:02<00:01, 15.68w/s, dev=2]model.layers.54.self_attn.v_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:02<00:01, 16.16w/s, dev=2]      model.layers.54.self_attn.v_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:02<00:01, 16.60w/s, dev=2]model.layers.55.input_layernorm.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:02<00:01, 16.60w/s, dev=2] model.layers.55.mlp.down_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:02<00:01, 17.09w/s, dev=2]  model.layers.55.mlp.gate_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:02<00:01, 16.42w/s, dev=2]model.layers.55.mlp.up_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:02<00:01, 15.74w/s, dev=2]  model.layers.55.post_attention_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:02<00:01, 15.11w/s, dev=2]model.layers.55.self_attn.k_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:02<00:01, 15.51w/s, dev=2]        model.layers.55.self_attn.k_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:02<00:01, 15.86w/s, dev=2]model.layers.55.self_attn.o_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:02<00:01, 15.86w/s, dev=2]model.layers.55.self_attn.q_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:02<00:01, 15.97w/s, dev=2]model.layers.55.self_attn.rotary_emb.inv_freq:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:02<00:01, 16.08w/s, dev=2]model.layers.55.self_attn.v_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:02<00:00, 16.46w/s, dev=2]      model.layers.56.input_layernorm.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:02<00:00, 16.80w/s, dev=2] model.layers.56.mlp.down_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:02<00:00, 17.18w/s, dev=2]  model.layers.56.mlp.down_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:02<00:00, 16.57w/s, dev=2]model.layers.56.mlp.gate_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:02<00:00, 16.57w/s, dev=2]model.layers.56.mlp.up_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:02<00:00, 16.05w/s, dev=2]  model.layers.56.post_attention_layernorm.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:03<00:00, 15.57w/s, dev=2]model.layers.56.self_attn.k_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:03<00:00, 15.89w/s, dev=2]        model.layers.56.self_attn.o_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:03<00:00, 16.18w/s, dev=2]model.layers.56.self_attn.q_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:03<00:00, 16.27w/s, dev=2]model.layers.56.self_attn.q_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:03<00:00, 16.34w/s, dev=2]model.layers.56.self_attn.rotary_emb.inv_freq:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:03<00:00, 16.34w/s, dev=2]model.layers.56.self_attn.v_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:03<00:00, 16.65w/s, dev=2]      model.layers.57.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:03<00:00, 16.93w/s, dev=2]model.layers.57.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:03<00:00, 17.21w/s, dev=2]model.layers.57.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:03<00:00, 17.27w/s, dev=2]model.layers.57.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:03<00:00, 17.33w/s, dev=2]model.layers.57.self_attn.rotary_emb.inv_freq:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:03<00:00, 17.63w/s, dev=2]model.layers.57.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:03<00:00, 17.63w/s, dev=2]                                                                                                      0%|          | 0/55 [00:00<?, ?w/s]model.layers.57.input_layernorm.weight:   0%|          | 0/55 [00:00<?, ?w/s, dev=2]model.layers.57.mlp.down_proj.weight:   2%|â–         | 1/55 [00:00<00:00, 1166.06w/s, dev=2]model.layers.57.mlp.down_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:04, 12.54w/s, dev=2]  model.layers.57.mlp.gate_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:04, 12.53w/s, dev=2]model.layers.57.mlp.up_proj.weight:   5%|â–Œ         | 3/55 [00:00<00:05,  9.63w/s, dev=2]  model.layers.57.mlp.up_proj.weight:   7%|â–‹         | 4/55 [00:00<00:06,  8.45w/s, dev=2]model.layers.57.post_attention_layernorm.weight:   7%|â–‹         | 4/55 [00:00<00:06,  8.45w/s, dev=2]model.layers.58.input_layernorm.weight:   9%|â–‰         | 5/55 [00:00<00:04, 10.55w/s, dev=2]         model.layers.58.mlp.down_proj.weight:  11%|â–ˆ         | 6/55 [00:00<00:03, 12.65w/s, dev=2]  model.layers.58.mlp.down_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:04, 11.07w/s, dev=2]model.layers.58.mlp.gate_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:04, 11.07w/s, dev=2]model.layers.58.mlp.up_proj.weight:  15%|â–ˆâ–        | 8/55 [00:00<00:04, 10.16w/s, dev=2]  model.layers.58.post_attention_layernorm.weight:  16%|â–ˆâ–‹        | 9/55 [00:00<00:04,  9.48w/s, dev=2]model.layers.58.post_attention_layernorm.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:04, 10.53w/s, dev=2]model.layers.58.self_attn.k_proj.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:04, 10.53w/s, dev=2]        model.layers.58.self_attn.o_proj.weight:  20%|â–ˆâ–ˆ        | 11/55 [00:00<00:03, 11.50w/s, dev=2]model.layers.58.self_attn.q_proj.weight:  22%|â–ˆâ–ˆâ–       | 12/55 [00:01<00:03, 11.97w/s, dev=2]model.layers.58.self_attn.rotary_emb.inv_freq:  24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:01<00:03, 12.41w/s, dev=2]model.layers.58.self_attn.v_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 14/55 [00:01<00:03, 13.36w/s, dev=2]      model.layers.58.self_attn.v_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 15/55 [00:01<00:02, 14.23w/s, dev=2]model.layers.59.input_layernorm.weight:  27%|â–ˆâ–ˆâ–‹       | 15/55 [00:01<00:02, 14.23w/s, dev=2] model.layers.59.mlp.down_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 16/55 [00:01<00:02, 15.17w/s, dev=2]  model.layers.59.mlp.gate_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:01<00:02, 14.09w/s, dev=2]model.layers.59.mlp.up_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/55 [00:01<00:02, 13.28w/s, dev=2]  model.layers.59.post_attention_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–      | 19/55 [00:01<00:02, 12.59w/s, dev=2]model.layers.59.post_attention_layernorm.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:01<00:02, 13.25w/s, dev=2]model.layers.59.self_attn.k_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:01<00:02, 13.25w/s, dev=2]        model.layers.59.self_attn.o_proj.weight:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/55 [00:01<00:02, 13.86w/s, dev=2]model.layers.59.self_attn.q_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/55 [00:01<00:02, 14.11w/s, dev=2]model.layers.59.self_attn.rotary_emb.inv_freq:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 23/55 [00:01<00:02, 14.34w/s, dev=2]model.layers.59.self_attn.v_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 24/55 [00:01<00:02, 14.96w/s, dev=2]      model.layers.59.self_attn.v_proj.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:01<00:01, 15.52w/s, dev=2]model.layers.60.input_layernorm.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:01<00:01, 15.52w/s, dev=2] model.layers.60.mlp.down_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 26/55 [00:01<00:01, 16.14w/s, dev=2]  model.layers.60.mlp.gate_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 27/55 [00:01<00:01, 15.31w/s, dev=2]model.layers.60.mlp.up_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 28/55 [00:01<00:01, 14.60w/s, dev=2]  model.layers.60.post_attention_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/55 [00:02<00:01, 14.00w/s, dev=2]model.layers.60.post_attention_layernorm.weight:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 30/55 [00:02<00:01, 14.48w/s, dev=2]model.layers.60.self_attn.k_proj.weight:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 30/55 [00:02<00:01, 14.48w/s, dev=2]        model.layers.60.self_attn.o_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/55 [00:02<00:01, 14.91w/s, dev=2]model.layers.60.self_attn.q_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 32/55 [00:02<00:01, 15.07w/s, dev=2]model.layers.60.self_attn.rotary_emb.inv_freq:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/55 [00:02<00:01, 15.23w/s, dev=2]model.layers.60.self_attn.v_proj.weight:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/55 [00:02<00:01, 15.68w/s, dev=2]      model.layers.60.self_attn.v_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:02<00:01, 16.09w/s, dev=2]model.layers.61.input_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:02<00:01, 16.09w/s, dev=2] model.layers.61.mlp.down_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 36/55 [00:02<00:01, 16.54w/s, dev=2]  model.layers.61.mlp.gate_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 37/55 [00:02<00:01, 15.85w/s, dev=2]model.layers.61.mlp.up_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 38/55 [00:02<00:01, 15.25w/s, dev=2]  model.layers.61.post_attention_layernorm.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 39/55 [00:02<00:01, 14.73w/s, dev=2]model.layers.61.post_attention_layernorm.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 40/55 [00:02<00:00, 15.10w/s, dev=2]model.layers.61.self_attn.k_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 40/55 [00:02<00:00, 15.10w/s, dev=2]        model.layers.61.self_attn.o_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/55 [00:02<00:00, 15.44w/s, dev=2]model.layers.61.self_attn.q_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 42/55 [00:02<00:00, 15.54w/s, dev=2]model.layers.61.self_attn.rotary_emb.inv_freq:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 43/55 [00:02<00:00, 15.63w/s, dev=2]model.layers.61.self_attn.v_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 44/55 [00:02<00:00, 15.99w/s, dev=2]      model.layers.61.self_attn.v_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:02<00:00, 16.31w/s, dev=2]model.layers.62.input_layernorm.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:03<00:00, 14.56w/s, dev=3] model.layers.62.mlp.down_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 46/55 [00:03<00:00, 14.87w/s, dev=3]  model.layers.62.mlp.gate_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 47/55 [00:03<00:00, 14.45w/s, dev=3]model.layers.62.mlp.up_proj.weight:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 48/55 [00:03<00:00, 14.07w/s, dev=3]  model.layers.62.post_attention_layernorm.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 49/55 [00:03<00:00, 13.73w/s, dev=3]model.layers.62.post_attention_layernorm.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 50/55 [00:03<00:00, 14.01w/s, dev=3]model.layers.62.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 50/55 [00:03<00:00, 14.01w/s, dev=3]        model.layers.62.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 51/55 [00:03<00:00, 14.26w/s, dev=3]model.layers.62.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 52/55 [00:03<00:00, 14.35w/s, dev=3]model.layers.62.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 53/55 [00:03<00:00, 14.43w/s, dev=3]model.layers.62.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 54/55 [00:03<00:00, 14.70w/s, dev=3]      model.layers.62.self_attn.v_proj.weight: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55/55 [00:03<00:00, 14.95w/s, dev=3]                                                                                                0%|          | 0/57 [00:00<?, ?w/s]model.layers.63.input_layernorm.weight:   0%|          | 0/57 [00:00<?, ?w/s, dev=3]model.layers.63.mlp.down_proj.weight:   2%|â–         | 1/57 [00:00<00:00, 1045.70w/s, dev=3]model.layers.63.mlp.down_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:04, 12.57w/s, dev=3]  model.layers.63.mlp.gate_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:04, 12.55w/s, dev=3]model.layers.63.mlp.up_proj.weight:   5%|â–Œ         | 3/57 [00:00<00:05,  9.22w/s, dev=3]  model.layers.63.mlp.up_proj.weight:   7%|â–‹         | 4/57 [00:00<00:06,  8.13w/s, dev=3]model.layers.63.post_attention_layernorm.weight:   7%|â–‹         | 4/57 [00:00<00:06,  8.13w/s, dev=3]model.layers.63.self_attn.k_proj.weight:   9%|â–‰         | 5/57 [00:00<00:05, 10.15w/s, dev=3]        model.layers.63.self_attn.k_proj.weight:  11%|â–ˆ         | 6/57 [00:00<00:05,  8.93w/s, dev=3]model.layers.63.self_attn.o_proj.weight:  11%|â–ˆ         | 6/57 [00:00<00:05,  8.93w/s, dev=3]model.layers.63.self_attn.q_proj.weight:  12%|â–ˆâ–        | 7/57 [00:00<00:05,  9.58w/s, dev=3]model.layers.63.self_attn.q_proj.weight:  14%|â–ˆâ–        | 8/57 [00:00<00:04, 10.14w/s, dev=3]model.layers.63.self_attn.rotary_emb.inv_freq:  14%|â–ˆâ–        | 8/57 [00:00<00:04, 10.13w/s, dev=3]model.layers.63.self_attn.v_proj.weight:  16%|â–ˆâ–Œ        | 9/57 [00:00<00:04, 11.40w/s, dev=3]      model.layers.63.self_attn.v_proj.weight:  18%|â–ˆâ–Š        | 10/57 [00:01<00:04,  9.91w/s, dev=3]model.layers.64.input_layernorm.weight:  18%|â–ˆâ–Š        | 10/57 [00:01<00:04,  9.90w/s, dev=3] model.layers.64.mlp.down_proj.weight:  19%|â–ˆâ–‰        | 11/57 [00:01<00:04, 10.89w/s, dev=3]  model.layers.64.mlp.down_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:01<00:04, 10.24w/s, dev=3]model.layers.64.mlp.gate_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:01<00:04, 10.24w/s, dev=3]model.layers.64.mlp.up_proj.weight:  23%|â–ˆâ–ˆâ–Ž       | 13/57 [00:01<00:05,  8.62w/s, dev=3]  model.layers.64.mlp.up_proj.weight:  25%|â–ˆâ–ˆâ–       | 14/57 [00:01<00:05,  7.65w/s, dev=3]model.layers.64.post_attention_layernorm.weight:  25%|â–ˆâ–ˆâ–       | 14/57 [00:01<00:05,  7.65w/s, dev=3]model.layers.64.self_attn.k_proj.weight:  26%|â–ˆâ–ˆâ–‹       | 15/57 [00:01<00:05,  8.20w/s, dev=3]        model.layers.64.self_attn.o_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:01<00:04,  8.37w/s, dev=3]model.layers.64.self_attn.o_proj.weight:  30%|â–ˆâ–ˆâ–‰       | 17/57 [00:01<00:04,  8.59w/s, dev=3]model.layers.64.self_attn.q_proj.weight:  30%|â–ˆâ–ˆâ–‰       | 17/57 [00:01<00:04,  8.59w/s, dev=3]model.layers.64.self_attn.rotary_emb.inv_freq:  32%|â–ˆâ–ˆâ–ˆâ–      | 18/57 [00:02<00:04,  8.29w/s, dev=3]model.layers.64.self_attn.v_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 19/57 [00:02<00:04,  8.75w/s, dev=3]      model.layers.64.self_attn.v_proj.weight:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 20/57 [00:02<00:04,  9.17w/s, dev=3]model.layers.65.input_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 20/57 [00:02<00:04,  9.17w/s, dev=3] model.layers.65.mlp.down_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 21/57 [00:02<00:03,  9.63w/s, dev=3]  model.layers.65.mlp.gate_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:02<00:03,  9.36w/s, dev=3]model.layers.65.mlp.gate_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 23/57 [00:02<00:03,  9.11w/s, dev=3]model.layers.65.mlp.up_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 23/57 [00:02<00:03,  9.11w/s, dev=3]  model.layers.65.post_attention_layernorm.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/57 [00:02<00:03,  8.72w/s, dev=3]model.layers.65.self_attn.k_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/57 [00:02<00:03,  9.08w/s, dev=3]        model.layers.65.self_attn.k_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 26/57 [00:02<00:03,  9.42w/s, dev=3]model.layers.65.self_attn.o_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 26/57 [00:02<00:03,  9.42w/s, dev=3]model.layers.65.self_attn.q_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 27/57 [00:02<00:03,  9.62w/s, dev=3]model.layers.65.self_attn.rotary_emb.inv_freq:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:02<00:02,  9.82w/s, dev=3]model.layers.65.self_attn.v_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 29/57 [00:02<00:02, 10.17w/s, dev=3]      model.layers.66.input_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 30/57 [00:02<00:02, 10.49w/s, dev=3] model.layers.66.mlp.down_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 31/57 [00:02<00:02, 10.84w/s, dev=3]  model.layers.66.mlp.down_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 32/57 [00:03<00:02, 10.59w/s, dev=3]model.layers.66.mlp.gate_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 32/57 [00:03<00:02, 10.59w/s, dev=3]model.layers.66.mlp.up_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 33/57 [00:03<00:02,  9.98w/s, dev=3]  model.layers.66.post_attention_layernorm.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:03<00:02,  9.79w/s, dev=3]model.layers.66.self_attn.k_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/57 [00:03<00:02, 10.08w/s, dev=3]        model.layers.66.self_attn.o_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 36/57 [00:03<00:02,  9.72w/s, dev=3]model.layers.66.self_attn.q_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 37/57 [00:03<00:02,  9.86w/s, dev=3]model.layers.66.self_attn.q_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 38/57 [00:03<00:01,  9.99w/s, dev=3]model.layers.66.self_attn.rotary_emb.inv_freq:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 38/57 [00:03<00:01,  9.99w/s, dev=3]model.layers.66.self_attn.v_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 39/57 [00:03<00:01, 10.25w/s, dev=3]      model.layers.67.input_layernorm.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:03<00:01, 10.50w/s, dev=3] model.layers.67.mlp.down_proj.weight:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/57 [00:03<00:01, 10.76w/s, dev=3]  model.layers.67.mlp.gate_proj.weight:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 42/57 [00:03<00:01, 10.57w/s, dev=3]model.layers.67.mlp.up_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 43/57 [00:04<00:01,  9.89w/s, dev=3]  model.layers.67.mlp.up_proj.weight:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 44/57 [00:04<00:01,  9.76w/s, dev=3]model.layers.67.post_attention_layernorm.weight:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 44/57 [00:04<00:01,  9.76w/s, dev=3]model.layers.67.self_attn.k_proj.weight:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 45/57 [00:04<00:01,  9.98w/s, dev=3]        model.layers.67.self_attn.o_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:04<00:01,  9.78w/s, dev=3]model.layers.67.self_attn.q_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 47/57 [00:04<00:01,  9.89w/s, dev=3]model.layers.67.self_attn.rotary_emb.inv_freq:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/57 [00:04<00:00,  9.99w/s, dev=3]model.layers.67.self_attn.v_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 49/57 [00:04<00:00, 10.20w/s, dev=3]      model.layers.67.self_attn.v_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 50/57 [00:04<00:00, 10.39w/s, dev=3]model.layers.68.mlp.down_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 50/57 [00:04<00:00, 10.39w/s, dev=3]   model.layers.68.mlp.gate_proj.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 51/57 [00:04<00:00, 10.26w/s, dev=3]model.layers.68.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:05<00:00,  9.86w/s, dev=3]model.layers.68.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 53/57 [00:05<00:00, 10.04w/s, dev=3]model.layers.68.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/57 [00:05<00:00, 10.14w/s, dev=3]model.layers.68.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 55/57 [00:05<00:00, 10.24w/s, dev=3]model.layers.68.self_attn.rotary_emb.inv_freq:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 56/57 [00:05<00:00, 10.42w/s, dev=3]model.layers.68.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 56/57 [00:05<00:00, 10.42w/s, dev=3]                                                                                                      0%|          | 0/59 [00:00<?, ?w/s]model.layers.68.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=3]model.layers.68.mlp.up_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 987.36w/s, dev=3]model.layers.68.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 13.63w/s, dev=3] model.layers.68.post_attention_layernorm.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 13.60w/s, dev=3]model.layers.69.input_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:02, 20.35w/s, dev=3]         model.layers.69.mlp.down_proj.weight:   7%|â–‹         | 4/59 [00:00<00:02, 27.10w/s, dev=3]  model.layers.69.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 14.00w/s, dev=3]model.layers.69.mlp.gate_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 13.99w/s, dev=3]model.layers.69.mlp.up_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:04, 11.67w/s, dev=3]  model.layers.69.post_attention_layernorm.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:04, 10.48w/s, dev=3]model.layers.69.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:04, 11.97w/s, dev=3]model.layers.69.self_attn.k_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:04, 11.97w/s, dev=3]        model.layers.69.self_attn.o_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:03, 13.34w/s, dev=3]model.layers.69.self_attn.q_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:03, 13.91w/s, dev=3]model.layers.69.self_attn.rotary_emb.inv_freq:  19%|â–ˆâ–Š        | 11/59 [00:00<00:03, 14.41w/s, dev=3]model.layers.69.self_attn.v_proj.weight:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:02, 15.72w/s, dev=3]      model.layers.69.self_attn.v_proj.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:02, 16.89w/s, dev=3]model.layers.70.input_layernorm.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:02, 16.89w/s, dev=3] model.layers.70.mlp.down_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:02, 18.18w/s, dev=3]  model.layers.70.mlp.gate_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:02, 16.26w/s, dev=3]model.layers.70.mlp.up_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:02, 14.99w/s, dev=3]  model.layers.70.post_attention_layernorm.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:01<00:02, 14.02w/s, dev=3]model.layers.70.post_attention_layernorm.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:02, 14.84w/s, dev=3]model.layers.70.self_attn.k_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:02, 14.84w/s, dev=3]        model.layers.70.self_attn.o_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:02, 15.59w/s, dev=3]model.layers.70.self_attn.q_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:02, 15.87w/s, dev=3]model.layers.70.self_attn.rotary_emb.inv_freq:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:01<00:02, 16.09w/s, dev=3]model.layers.70.self_attn.v_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:02, 16.85w/s, dev=3]      model.layers.71.input_layernorm.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:02, 17.53w/s, dev=3] model.layers.71.mlp.down_proj.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:01, 18.29w/s, dev=3]  model.layers.71.mlp.down_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:01, 17.18w/s, dev=3]model.layers.71.mlp.gate_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:01, 17.18w/s, dev=3]model.layers.71.mlp.up_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:02, 16.23w/s, dev=3]  model.layers.71.post_attention_layernorm.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:01<00:02, 15.47w/s, dev=3]model.layers.71.self_attn.k_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:01, 16.04w/s, dev=3]        model.layers.71.self_attn.o_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:01, 16.56w/s, dev=3]model.layers.71.self_attn.q_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:01<00:01, 16.73w/s, dev=3]model.layers.71.self_attn.rotary_emb.inv_freq:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:01<00:01, 16.89w/s, dev=3]model.layers.71.self_attn.rotary_emb.inv_freq:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:01<00:01, 17.43w/s, dev=3]model.layers.71.self_attn.v_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:01<00:01, 17.43w/s, dev=3]      model.layers.72.input_layernorm.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:01<00:01, 17.89w/s, dev=3] model.layers.72.mlp.down_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:01<00:01, 18.43w/s, dev=3]  model.layers.72.mlp.gate_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:01<00:01, 17.57w/s, dev=3]model.layers.72.mlp.up_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:02<00:01, 16.85w/s, dev=3]  model.layers.72.post_attention_layernorm.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:02<00:01, 16.20w/s, dev=3]model.layers.72.self_attn.k_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:02<00:01, 16.63w/s, dev=3]        model.layers.72.self_attn.k_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:02<00:01, 17.02w/s, dev=3]model.layers.72.self_attn.o_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:02<00:01, 17.02w/s, dev=3]model.layers.72.self_attn.q_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:02<00:01, 17.14w/s, dev=3]model.layers.72.self_attn.rotary_emb.inv_freq:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:02<00:01, 17.24w/s, dev=3]model.layers.72.self_attn.v_proj.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:02<00:00, 17.66w/s, dev=3]      model.layers.73.input_layernorm.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:02<00:00, 18.01w/s, dev=3] model.layers.73.mlp.down_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:02<00:00, 18.43w/s, dev=3]  model.layers.73.mlp.gate_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:02<00:00, 17.78w/s, dev=3]model.layers.73.mlp.gate_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:02<00:00, 17.20w/s, dev=3]model.layers.73.mlp.up_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:02<00:00, 17.20w/s, dev=3]  model.layers.73.post_attention_layernorm.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:02<00:00, 16.68w/s, dev=3]model.layers.73.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:02<00:00, 17.04w/s, dev=3]        model.layers.73.self_attn.o_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:02<00:00, 17.34w/s, dev=3]model.layers.73.self_attn.q_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:02<00:00, 17.43w/s, dev=3]model.layers.73.self_attn.rotary_emb.inv_freq:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:02<00:00, 17.51w/s, dev=3]model.layers.73.self_attn.v_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:02<00:00, 17.85w/s, dev=3]      model.layers.73.self_attn.v_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:02<00:00, 18.15w/s, dev=3]model.layers.74.mlp.gate_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:02<00:00, 18.15w/s, dev=3]   model.layers.74.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:03<00:00, 17.62w/s, dev=3]model.layers.74.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:03<00:00, 17.91w/s, dev=3]model.layers.74.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:03<00:00, 17.98w/s, dev=3]model.layers.74.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:03<00:00, 18.05w/s, dev=3]model.layers.74.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:03<00:00, 18.37w/s, dev=3]                                                                                                      0%|          | 0/55 [00:00<?, ?w/s]model.layers.74.input_layernorm.weight:   0%|          | 0/55 [00:00<?, ?w/s, dev=3]model.layers.74.mlp.down_proj.weight:   2%|â–         | 1/55 [00:00<00:00, 967.99w/s, dev=3]model.layers.74.mlp.down_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:04, 11.91w/s, dev=3] model.layers.74.mlp.up_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:04, 11.89w/s, dev=3]  model.layers.74.post_attention_layernorm.weight:   5%|â–Œ         | 3/55 [00:00<00:05,  9.11w/s, dev=3]model.layers.74.post_attention_layernorm.weight:   7%|â–‹         | 4/55 [00:00<00:04, 12.13w/s, dev=3]model.layers.75.input_layernorm.weight:   7%|â–‹         | 4/55 [00:00<00:04, 12.12w/s, dev=3]         model.layers.75.mlp.down_proj.weight:   9%|â–‰         | 5/55 [00:00<00:03, 15.14w/s, dev=3]  model.layers.75.mlp.down_proj.weight:  11%|â–ˆ         | 6/55 [00:00<00:03, 12.31w/s, dev=3]model.layers.75.mlp.gate_proj.weight:  11%|â–ˆ         | 6/55 [00:00<00:03, 12.31w/s, dev=3]model.layers.75.mlp.up_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:04, 10.83w/s, dev=3]  model.layers.75.mlp.up_proj.weight:  15%|â–ˆâ–        | 8/55 [00:00<00:04, 10.06w/s, dev=3]model.layers.75.post_attention_layernorm.weight:  15%|â–ˆâ–        | 8/55 [00:00<00:04, 10.06w/s, dev=3]model.layers.75.self_attn.k_proj.weight:  16%|â–ˆâ–‹        | 9/55 [00:00<00:04, 11.31w/s, dev=3]        model.layers.75.self_attn.o_proj.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:03, 12.45w/s, dev=3]model.layers.75.self_attn.q_proj.weight:  20%|â–ˆâ–ˆ        | 11/55 [00:00<00:03, 12.98w/s, dev=3]model.layers.75.self_attn.rotary_emb.inv_freq:  22%|â–ˆâ–ˆâ–       | 12/55 [00:00<00:03, 13.47w/s, dev=3]model.layers.75.self_attn.v_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:00<00:02, 14.58w/s, dev=3]      model.layers.75.self_attn.v_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 14/55 [00:00<00:02, 15.58w/s, dev=3]model.layers.76.input_layernorm.weight:  25%|â–ˆâ–ˆâ–Œ       | 14/55 [00:00<00:02, 15.58w/s, dev=3] model.layers.76.mlp.down_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 15/55 [00:00<00:02, 16.69w/s, dev=3]  model.layers.76.mlp.gate_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 16/55 [00:01<00:02, 15.22w/s, dev=3]model.layers.76.mlp.up_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:01<00:02, 14.07w/s, dev=3]  model.layers.76.post_attention_layernorm.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/55 [00:01<00:02, 13.27w/s, dev=3]model.layers.76.self_attn.k_proj.weight:  35%|â–ˆâ–ˆâ–ˆâ–      | 19/55 [00:01<00:02, 14.00w/s, dev=3]        model.layers.76.self_attn.k_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:01<00:02, 14.67w/s, dev=3]model.layers.76.self_attn.o_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:01<00:02, 14.67w/s, dev=3]model.layers.76.self_attn.q_proj.weight:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/55 [00:01<00:02, 14.94w/s, dev=3]model.layers.76.self_attn.rotary_emb.inv_freq:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/55 [00:01<00:02, 15.18w/s, dev=3]model.layers.76.self_attn.v_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 23/55 [00:01<00:02, 15.87w/s, dev=3]      model.layers.77.input_layernorm.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 24/55 [00:01<00:01, 16.47w/s, dev=3] model.layers.77.mlp.down_proj.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:01<00:01, 17.15w/s, dev=3]  model.layers.77.mlp.down_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 26/55 [00:01<00:01, 16.17w/s, dev=3]model.layers.77.mlp.gate_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 26/55 [00:01<00:01, 16.16w/s, dev=3]model.layers.77.mlp.up_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 27/55 [00:01<00:01, 15.30w/s, dev=3]  model.layers.77.post_attention_layernorm.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 28/55 [00:01<00:01, 14.55w/s, dev=3]model.layers.77.self_attn.k_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/55 [00:01<00:01, 15.07w/s, dev=3]        model.layers.77.self_attn.o_proj.weight:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 30/55 [00:01<00:01, 15.52w/s, dev=3]model.layers.77.self_attn.q_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/55 [00:01<00:01, 15.65w/s, dev=3]model.layers.77.self_attn.q_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 32/55 [00:02<00:01, 15.77w/s, dev=3]model.layers.77.self_attn.rotary_emb.inv_freq:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 32/55 [00:02<00:01, 15.77w/s, dev=3]model.layers.77.self_attn.v_proj.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/55 [00:02<00:01, 16.26w/s, dev=3]      model.layers.78.input_layernorm.weight:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/55 [00:02<00:01, 16.69w/s, dev=3] model.layers.78.mlp.down_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:02<00:01, 17.18w/s, dev=3]  model.layers.78.mlp.gate_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 36/55 [00:02<00:01, 16.41w/s, dev=3]model.layers.78.mlp.up_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 37/55 [00:02<00:01, 15.78w/s, dev=3]  model.layers.78.mlp.up_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 38/55 [00:02<00:01, 15.22w/s, dev=3]model.layers.78.post_attention_layernorm.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 38/55 [00:02<00:01, 15.22w/s, dev=3]model.layers.78.self_attn.k_proj.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 39/55 [00:02<00:01, 15.62w/s, dev=3]        model.layers.78.self_attn.o_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 40/55 [00:02<00:00, 15.97w/s, dev=3]model.layers.78.self_attn.q_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/55 [00:02<00:00, 16.06w/s, dev=3]model.layers.78.self_attn.rotary_emb.inv_freq:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 42/55 [00:02<00:00, 16.16w/s, dev=3]model.layers.78.self_attn.v_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 43/55 [00:02<00:00, 16.54w/s, dev=3]      model.layers.78.self_attn.v_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 44/55 [00:02<00:00, 16.88w/s, dev=3]model.layers.79.input_layernorm.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 44/55 [00:02<00:00, 16.88w/s, dev=3] model.layers.79.mlp.down_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:02<00:00, 17.26w/s, dev=3]  model.layers.79.mlp.gate_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 46/55 [00:02<00:00, 16.63w/s, dev=3]model.layers.79.mlp.up_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 47/55 [00:02<00:00, 16.11w/s, dev=3]  model.layers.79.post_attention_layernorm.weight:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 48/55 [00:03<00:00, 15.67w/s, dev=3]model.layers.79.self_attn.k_proj.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 49/55 [00:03<00:00, 15.99w/s, dev=3]        model.layers.79.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 50/55 [00:03<00:00, 16.29w/s, dev=3]model.layers.79.self_attn.o_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 50/55 [00:03<00:00, 16.28w/s, dev=3]model.layers.79.self_attn.q_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 51/55 [00:03<00:00, 16.38w/s, dev=3]model.layers.79.self_attn.rotary_emb.inv_freq:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 52/55 [00:03<00:00, 16.49w/s, dev=3]model.layers.79.self_attn.v_proj.weight:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 53/55 [00:03<00:00, 16.80w/s, dev=3]      model.norm.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 54/55 [00:03<00:00, 17.07w/s, dev=3]                                                                                                0%|          | 0/1 [00:00<?, ?w/s]lm_head.weight:   0%|          | 0/1 [00:00<?, ?w/s, dev=3]lm_head.weight: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  6.33w/s, dev=3]                                                                   self.device: cuda:0
Loading BASE model stabilityai/stablelm-base-alpha-3b-v2...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:01<00:01,  1.46s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.27it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.13it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[2] MOVING BASE MODEL TO GPU.../home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for iamgroot42/mimir contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/iamgroot42/mimir
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
Loading dataset the_pile...
Loading from HuggingFace!
Loading dataset the_pile...
Loading from HuggingFace!
Generating samples:   0%|          | 0/21 [00:00<?, ?it/s]Generating samples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:00<00:00, 800.58it/s]
NEW N_SAMPLES IS  1000
Writing raw data to tmp_results/neo125_github_experiment/meta-llama_Llama-2-70b-hf/temporal_arxiv_2023_06/raw_data.json
Writing raw data to tmp_results/neo125_github_experiment/meta-llama_Llama-2-70b-hf/temporal_arxiv_2023_06/raw_data_lens.json
Computing criterion:   0%|          | 0/20 [00:00<?, ?it/s]Computing criterion:   5%|â–Œ         | 1/20 [00:25<07:59, 25.26s/it]Computing criterion:  10%|â–ˆ         | 2/20 [00:44<06:32, 21.83s/it]Computing criterion:  15%|â–ˆâ–Œ        | 3/20 [01:04<05:52, 20.73s/it]Computing criterion:  20%|â–ˆâ–ˆ        | 4/20 [01:24<05:28, 20.52s/it]Computing criterion:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [01:44<05:07, 20.48s/it]Computing criterion:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [02:05<04:45, 20.42s/it]Computing criterion:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [02:25<04:26, 20.53s/it]Computing criterion:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [02:46<04:05, 20.48s/it]Computing criterion:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [03:06<03:44, 20.44s/it]Computing criterion:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [03:26<03:23, 20.39s/it]Computing criterion:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [03:45<02:59, 19.96s/it]Computing criterion:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [04:04<02:36, 19.60s/it]Computing criterion:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [04:23<02:15, 19.35s/it]Computing criterion:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [04:42<01:55, 19.23s/it]Computing criterion:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [05:00<01:35, 19.01s/it]Computing criterion:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [05:19<01:15, 18.84s/it]Computing criterion:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [05:37<00:55, 18.67s/it]Computing criterion:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [05:55<00:36, 18.49s/it]Computing criterion:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [06:14<00:18, 18.52s/it]Computing criterion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [06:32<00:00, 18.60s/it]Computing criterion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [06:32<00:00, 19.65s/it]
Ref scores:   0%|          | 0/1000 [00:00<?, ?it/s]Ref scores:   0%|          | 1/1000 [00:07<2:03:40,  7.43s/it]Ref scores:   0%|          | 2/1000 [00:07<52:55,  3.18s/it]  Ref scores:   0%|          | 3/1000 [00:07<29:56,  1.80s/it]Ref scores:   0%|          | 4/1000 [00:08<19:29,  1.17s/it]Ref scores:   0%|          | 5/1000 [00:08<13:24,  1.24it/s]Ref scores:   1%|          | 6/1000 [00:08<10:02,  1.65it/s]Ref scores:   1%|          | 7/1000 [00:08<07:34,  2.18it/s]Ref scores:   1%|          | 8/1000 [00:08<05:56,  2.79it/s]Ref scores:   1%|          | 9/1000 [00:08<05:27,  3.03it/s]Ref scores:   1%|          | 10/1000 [00:09<04:30,  3.66it/s]Ref scores:   1%|          | 11/1000 [00:09<03:53,  4.24it/s]Ref scores:   1%|          | 12/1000 [00:09<03:27,  4.76it/s]Ref scores:   1%|â–         | 13/1000 [00:09<03:09,  5.20it/s]Ref scores:   1%|â–         | 14/1000 [00:09<03:15,  5.05it/s]Ref scores:   2%|â–         | 15/1000 [00:09<03:01,  5.43it/s]Ref scores:   2%|â–         | 16/1000 [00:10<02:51,  5.73it/s]Ref scores:   2%|â–         | 17/1000 [00:10<03:06,  5.27it/s]Ref scores:   2%|â–         | 18/1000 [00:10<03:13,  5.07it/s]Ref scores:   2%|â–         | 19/1000 [00:10<03:30,  4.66it/s]Ref scores:   2%|â–         | 20/1000 [00:10<03:10,  5.14it/s]Ref scores:   2%|â–         | 21/1000 [00:11<03:18,  4.94it/s]Ref scores:   2%|â–         | 22/1000 [00:11<03:02,  5.37it/s]Ref scores:   2%|â–         | 23/1000 [00:11<02:50,  5.74it/s]Ref scores:   2%|â–         | 24/1000 [00:11<02:43,  5.98it/s]Ref scores:   2%|â–Ž         | 25/1000 [00:11<02:55,  5.56it/s]Ref scores:   3%|â–Ž         | 26/1000 [00:11<02:38,  6.13it/s]Ref scores:   3%|â–Ž         | 27/1000 [00:12<02:35,  6.27it/s]Ref scores:   3%|â–Ž         | 28/1000 [00:12<02:32,  6.38it/s]Ref scores:   3%|â–Ž         | 29/1000 [00:12<02:51,  5.66it/s]Ref scores:   3%|â–Ž         | 30/1000 [00:12<02:43,  5.93it/s]Ref scores:   3%|â–Ž         | 31/1000 [00:12<02:59,  5.40it/s]Ref scores:   3%|â–Ž         | 32/1000 [00:12<02:54,  5.55it/s]Ref scores:   3%|â–Ž         | 33/1000 [00:13<02:46,  5.82it/s]Ref scores:   3%|â–Ž         | 34/1000 [00:13<02:59,  5.38it/s]Ref scores:   4%|â–Ž         | 35/1000 [00:13<02:49,  5.68it/s]Ref scores:   4%|â–Ž         | 36/1000 [00:13<02:42,  5.95it/s]Ref scores:   4%|â–Ž         | 37/1000 [00:13<03:09,  5.07it/s]Ref scores:   4%|â–         | 38/1000 [00:14<02:55,  5.47it/s]Ref scores:   4%|â–         | 39/1000 [00:14<03:04,  5.21it/s]Ref scores:   4%|â–         | 40/1000 [00:14<02:51,  5.58it/s]Ref scores:   4%|â–         | 41/1000 [00:14<02:48,  5.70it/s]Ref scores:   4%|â–         | 42/1000 [00:14<02:58,  5.36it/s]Ref scores:   4%|â–         | 43/1000 [00:14<02:47,  5.71it/s]Ref scores:   4%|â–         | 44/1000 [00:15<02:40,  5.97it/s]Ref scores:   4%|â–         | 45/1000 [00:15<03:06,  5.11it/s]Ref scores:   5%|â–         | 46/1000 [00:15<02:53,  5.49it/s]Ref scores:   5%|â–         | 47/1000 [00:15<03:02,  5.24it/s]Ref scores:   5%|â–         | 48/1000 [00:15<03:08,  5.05it/s]Ref scores:   5%|â–         | 49/1000 [00:16<03:34,  4.43it/s]Ref scores:   5%|â–Œ         | 50/1000 [00:16<03:12,  4.93it/s]Ref scores:   5%|â–Œ         | 51/1000 [00:16<02:57,  5.35it/s]Ref scores:   5%|â–Œ         | 52/1000 [00:16<03:18,  4.77it/s]Ref scores:   5%|â–Œ         | 53/1000 [00:16<03:00,  5.24it/s]Ref scores:   5%|â–Œ         | 54/1000 [00:17<02:49,  5.58it/s]Ref scores:   6%|â–Œ         | 55/1000 [00:17<02:32,  6.18it/s]Ref scores:   6%|â–Œ         | 56/1000 [00:17<02:59,  5.25it/s]Ref scores:   6%|â–Œ         | 57/1000 [00:17<03:08,  5.00it/s]Ref scores:   6%|â–Œ         | 58/1000 [00:17<02:59,  5.24it/s]Ref scores:   6%|â–Œ         | 59/1000 [00:18<02:47,  5.63it/s]Ref scores:   6%|â–Œ         | 60/1000 [00:18<02:44,  5.72it/s]Ref scores:   6%|â–Œ         | 61/1000 [00:18<02:37,  5.95it/s]Ref scores:   6%|â–Œ         | 62/1000 [00:18<02:33,  6.10it/s]Ref scores:   6%|â–‹         | 63/1000 [00:18<02:30,  6.23it/s]Ref scores:   6%|â–‹         | 64/1000 [00:18<02:32,  6.12it/s]Ref scores:   6%|â–‹         | 65/1000 [00:18<02:34,  6.05it/s]Ref scores:   7%|â–‹         | 66/1000 [00:19<02:30,  6.20it/s]Ref scores:   7%|â–‹         | 67/1000 [00:19<02:27,  6.33it/s]Ref scores:   7%|â–‹         | 68/1000 [00:19<02:57,  5.26it/s]Ref scores:   7%|â–‹         | 69/1000 [00:19<02:46,  5.58it/s]Ref scores:   7%|â–‹         | 70/1000 [00:19<02:37,  5.91it/s]Ref scores:   7%|â–‹         | 71/1000 [00:19<02:30,  6.17it/s]Ref scores:   7%|â–‹         | 72/1000 [00:20<02:46,  5.57it/s]Ref scores:   7%|â–‹         | 73/1000 [00:20<02:43,  5.66it/s]Ref scores:   7%|â–‹         | 74/1000 [00:20<02:35,  5.94it/s]Ref scores:   8%|â–Š         | 75/1000 [00:20<02:49,  5.45it/s]Ref scores:   8%|â–Š         | 76/1000 [00:20<02:40,  5.76it/s]Ref scores:   8%|â–Š         | 77/1000 [00:21<02:34,  5.96it/s]Ref scores:   8%|â–Š         | 78/1000 [00:21<02:30,  6.14it/s]Ref scores:   8%|â–Š         | 79/1000 [00:21<02:26,  6.27it/s]Ref scores:   8%|â–Š         | 80/1000 [00:21<02:16,  6.76it/s]Ref scores:   8%|â–Š         | 81/1000 [00:21<02:09,  7.10it/s]Ref scores:   8%|â–Š         | 82/1000 [00:21<02:28,  6.18it/s]Ref scores:   8%|â–Š         | 83/1000 [00:22<02:43,  5.59it/s]Ref scores:   8%|â–Š         | 84/1000 [00:22<02:53,  5.29it/s]Ref scores:   8%|â–Š         | 85/1000 [00:22<02:42,  5.64it/s]Ref scores:   9%|â–Š         | 86/1000 [00:22<02:53,  5.28it/s]Ref scores:   9%|â–Š         | 87/1000 [00:22<03:01,  5.03it/s]Ref scores:   9%|â–‰         | 88/1000 [00:22<02:53,  5.27it/s]Ref scores:   9%|â–‰         | 89/1000 [00:23<02:43,  5.59it/s]Ref scores:   9%|â–‰         | 90/1000 [00:23<03:06,  4.89it/s]Ref scores:   9%|â–‰         | 91/1000 [00:23<02:49,  5.36it/s]Ref scores:   9%|â–‰         | 92/1000 [00:23<02:39,  5.68it/s]Ref scores:   9%|â–‰         | 93/1000 [00:23<02:32,  5.96it/s]Ref scores:   9%|â–‰         | 94/1000 [00:24<02:27,  6.15it/s]Ref scores:  10%|â–‰         | 95/1000 [00:24<02:24,  6.26it/s]Ref scores:  10%|â–‰         | 96/1000 [00:24<02:22,  6.36it/s]Ref scores:  10%|â–‰         | 97/1000 [00:24<02:20,  6.44it/s]Ref scores:  10%|â–‰         | 98/1000 [00:24<02:18,  6.50it/s]Ref scores:  10%|â–‰         | 99/1000 [00:24<02:38,  5.69it/s]Ref scores:  10%|â–ˆ         | 100/1000 [00:24<02:29,  6.01it/s]Ref scores:  10%|â–ˆ         | 101/1000 [00:25<02:42,  5.53it/s]Ref scores:  10%|â–ˆ         | 102/1000 [00:25<02:34,  5.80it/s]Ref scores:  10%|â–ˆ         | 103/1000 [00:25<02:28,  6.04it/s]Ref scores:  10%|â–ˆ         | 104/1000 [00:25<02:24,  6.21it/s]Ref scores:  10%|â–ˆ         | 105/1000 [00:25<02:20,  6.38it/s]Ref scores:  11%|â–ˆ         | 106/1000 [00:26<02:35,  5.76it/s]Ref scores:  11%|â–ˆ         | 107/1000 [00:26<02:27,  6.06it/s]Ref scores:  11%|â–ˆ         | 108/1000 [00:26<02:23,  6.20it/s]Ref scores:  11%|â–ˆ         | 109/1000 [00:26<02:49,  5.27it/s]Ref scores:  11%|â–ˆ         | 110/1000 [00:26<02:43,  5.44it/s]Ref scores:  11%|â–ˆ         | 111/1000 [00:26<02:34,  5.75it/s]Ref scores:  11%|â–ˆ         | 112/1000 [00:27<02:28,  5.97it/s]Ref scores:  11%|â–ˆâ–        | 113/1000 [00:27<02:24,  6.15it/s]Ref scores:  11%|â–ˆâ–        | 114/1000 [00:27<02:21,  6.27it/s]Ref scores:  12%|â–ˆâ–        | 115/1000 [00:27<02:17,  6.44it/s]Ref scores:  12%|â–ˆâ–        | 116/1000 [00:27<02:16,  6.49it/s]Ref scores:  12%|â–ˆâ–        | 117/1000 [00:27<02:42,  5.42it/s]Ref scores:  12%|â–ˆâ–        | 118/1000 [00:28<02:48,  5.23it/s]Ref scores:  12%|â–ˆâ–        | 119/1000 [00:28<02:37,  5.59it/s]Ref scores:  12%|â–ˆâ–        | 120/1000 [00:28<02:22,  6.19it/s]Ref scores:  12%|â–ˆâ–        | 121/1000 [00:28<02:19,  6.30it/s]Ref scores:  12%|â–ˆâ–        | 122/1000 [00:28<02:16,  6.45it/s]Ref scores:  12%|â–ˆâ–        | 123/1000 [00:28<02:12,  6.61it/s]Ref scores:  12%|â–ˆâ–        | 124/1000 [00:29<02:39,  5.50it/s]Ref scores:  12%|â–ˆâ–Ž        | 125/1000 [00:29<02:47,  5.22it/s]Ref scores:  13%|â–ˆâ–Ž        | 126/1000 [00:29<02:36,  5.57it/s]Ref scores:  13%|â–ˆâ–Ž        | 127/1000 [00:29<02:30,  5.81it/s]Ref scores:  13%|â–ˆâ–Ž        | 128/1000 [00:29<02:28,  5.88it/s]Ref scores:  13%|â–ˆâ–Ž        | 129/1000 [00:29<02:16,  6.39it/s]Ref scores:  13%|â–ˆâ–Ž        | 130/1000 [00:30<02:15,  6.40it/s]Ref scores:  13%|â–ˆâ–Ž        | 131/1000 [00:30<02:15,  6.43it/s]Ref scores:  13%|â–ˆâ–Ž        | 132/1000 [00:30<02:14,  6.46it/s]Ref scores:  13%|â–ˆâ–Ž        | 133/1000 [00:30<02:30,  5.77it/s]Ref scores:  13%|â–ˆâ–Ž        | 134/1000 [00:30<02:24,  6.01it/s]Ref scores:  14%|â–ˆâ–Ž        | 135/1000 [00:30<02:18,  6.23it/s]Ref scores:  14%|â–ˆâ–Ž        | 136/1000 [00:31<02:44,  5.25it/s]Ref scores:  14%|â–ˆâ–Ž        | 137/1000 [00:31<02:50,  5.06it/s]Ref scores:  14%|â–ˆâ–        | 138/1000 [00:31<02:39,  5.42it/s]Ref scores:  14%|â–ˆâ–        | 139/1000 [00:31<02:23,  6.00it/s]Ref scores:  14%|â–ˆâ–        | 140/1000 [00:31<02:19,  6.15it/s]Ref scores:  14%|â–ˆâ–        | 141/1000 [00:31<02:17,  6.26it/s]Ref scores:  14%|â–ˆâ–        | 142/1000 [00:32<02:15,  6.35it/s]Ref scores:  14%|â–ˆâ–        | 143/1000 [00:32<02:13,  6.41it/s]Ref scores:  14%|â–ˆâ–        | 144/1000 [00:32<02:40,  5.34it/s]Ref scores:  14%|â–ˆâ–        | 145/1000 [00:32<02:45,  5.15it/s]Ref scores:  15%|â–ˆâ–        | 146/1000 [00:32<02:52,  4.96it/s]Ref scores:  15%|â–ˆâ–        | 147/1000 [00:33<02:32,  5.60it/s]Ref scores:  15%|â–ˆâ–        | 148/1000 [00:33<02:24,  5.89it/s]Ref scores:  15%|â–ˆâ–        | 149/1000 [00:33<02:35,  5.47it/s]Ref scores:  15%|â–ˆâ–Œ        | 150/1000 [00:33<02:26,  5.82it/s]Ref scores:  15%|â–ˆâ–Œ        | 151/1000 [00:33<02:20,  6.03it/s]Ref scores:  15%|â–ˆâ–Œ        | 152/1000 [00:33<02:16,  6.20it/s]Ref scores:  15%|â–ˆâ–Œ        | 153/1000 [00:34<02:41,  5.25it/s]Ref scores:  15%|â–ˆâ–Œ        | 154/1000 [00:34<02:48,  5.01it/s]Ref scores:  16%|â–ˆâ–Œ        | 155/1000 [00:34<02:35,  5.42it/s]Ref scores:  16%|â–ˆâ–Œ        | 156/1000 [00:34<02:45,  5.11it/s]Ref scores:  16%|â–ˆâ–Œ        | 157/1000 [00:34<02:53,  4.85it/s]Ref scores:  16%|â–ˆâ–Œ        | 158/1000 [00:35<02:40,  5.25it/s]Ref scores:  16%|â–ˆâ–Œ        | 159/1000 [00:35<02:35,  5.42it/s]Ref scores:  16%|â–ˆâ–Œ        | 160/1000 [00:35<02:26,  5.73it/s]Ref scores:  16%|â–ˆâ–Œ        | 161/1000 [00:35<02:20,  5.97it/s]Ref scores:  16%|â–ˆâ–Œ        | 162/1000 [00:35<02:16,  6.16it/s]Ref scores:  16%|â–ˆâ–‹        | 163/1000 [00:35<02:12,  6.33it/s]Ref scores:  16%|â–ˆâ–‹        | 164/1000 [00:36<02:10,  6.42it/s]Ref scores:  16%|â–ˆâ–‹        | 165/1000 [00:36<02:12,  6.29it/s]Ref scores:  17%|â–ˆâ–‹        | 166/1000 [00:36<02:27,  5.67it/s]Ref scores:  17%|â–ˆâ–‹        | 167/1000 [00:36<02:19,  5.96it/s]Ref scores:  17%|â–ˆâ–‹        | 168/1000 [00:36<02:41,  5.16it/s]Ref scores:  17%|â–ˆâ–‹        | 169/1000 [00:37<02:48,  4.94it/s]Ref scores:  17%|â–ˆâ–‹        | 170/1000 [00:37<02:34,  5.38it/s]Ref scores:  17%|â–ˆâ–‹        | 171/1000 [00:37<02:39,  5.20it/s]Ref scores:  17%|â–ˆâ–‹        | 172/1000 [00:37<02:43,  5.07it/s]Ref scores:  17%|â–ˆâ–‹        | 173/1000 [00:37<02:45,  5.00it/s]Ref scores:  17%|â–ˆâ–‹        | 174/1000 [00:37<02:46,  4.95it/s]Ref scores:  18%|â–ˆâ–Š        | 175/1000 [00:38<02:33,  5.39it/s]Ref scores:  18%|â–ˆâ–Š        | 176/1000 [00:38<02:28,  5.56it/s]Ref scores:  18%|â–ˆâ–Š        | 177/1000 [00:38<02:24,  5.70it/s]Ref scores:  18%|â–ˆâ–Š        | 178/1000 [00:38<02:16,  6.01it/s]Ref scores:  18%|â–ˆâ–Š        | 179/1000 [00:38<02:12,  6.18it/s]Ref scores:  18%|â–ˆâ–Š        | 180/1000 [00:38<02:09,  6.31it/s]Ref scores:  18%|â–ˆâ–Š        | 181/1000 [00:39<02:34,  5.30it/s]Ref scores:  18%|â–ˆâ–Š        | 182/1000 [00:39<02:24,  5.65it/s]Ref scores:  18%|â–ˆâ–Š        | 183/1000 [00:39<02:18,  5.90it/s]Ref scores:  18%|â–ˆâ–Š        | 184/1000 [00:39<02:12,  6.17it/s]Ref scores:  18%|â–ˆâ–Š        | 185/1000 [00:39<02:08,  6.34it/s]Ref scores:  19%|â–ˆâ–Š        | 186/1000 [00:40<02:33,  5.30it/s]Ref scores:  19%|â–ˆâ–Š        | 187/1000 [00:40<02:23,  5.66it/s]Ref scores:  19%|â–ˆâ–‰        | 188/1000 [00:40<02:32,  5.32it/s]Ref scores:  19%|â–ˆâ–‰        | 189/1000 [00:40<02:22,  5.70it/s]Ref scores:  19%|â–ˆâ–‰        | 190/1000 [00:40<02:32,  5.32it/s]Ref scores:  19%|â–ˆâ–‰        | 191/1000 [00:40<02:22,  5.67it/s]Ref scores:  19%|â–ˆâ–‰        | 192/1000 [00:41<02:30,  5.36it/s]Ref scores:  19%|â–ˆâ–‰        | 193/1000 [00:41<02:37,  5.12it/s]Ref scores:  19%|â–ˆâ–‰        | 194/1000 [00:41<02:26,  5.50it/s]Ref scores:  20%|â–ˆâ–‰        | 195/1000 [00:41<02:36,  5.14it/s]Ref scores:  20%|â–ˆâ–‰        | 196/1000 [00:41<02:37,  5.12it/s]Ref scores:  20%|â–ˆâ–‰        | 197/1000 [00:42<02:24,  5.56it/s]Ref scores:  20%|â–ˆâ–‰        | 198/1000 [00:42<02:16,  5.90it/s]Ref scores:  20%|â–ˆâ–‰        | 199/1000 [00:42<02:11,  6.09it/s]Ref scores:  20%|â–ˆâ–ˆ        | 200/1000 [00:42<02:19,  5.75it/s]Ref scores:  20%|â–ˆâ–ˆ        | 201/1000 [00:42<02:11,  6.06it/s]Ref scores:  20%|â–ˆâ–ˆ        | 202/1000 [00:42<02:08,  6.23it/s]Ref scores:  20%|â–ˆâ–ˆ        | 203/1000 [00:42<02:04,  6.39it/s]Ref scores:  20%|â–ˆâ–ˆ        | 204/1000 [00:43<02:19,  5.70it/s]Ref scores:  20%|â–ˆâ–ˆ        | 205/1000 [00:43<02:30,  5.28it/s]Ref scores:  21%|â–ˆâ–ˆ        | 206/1000 [00:43<02:25,  5.46it/s]Ref scores:  21%|â–ˆâ–ˆ        | 207/1000 [00:43<02:32,  5.21it/s]Ref scores:  21%|â–ˆâ–ˆ        | 208/1000 [00:44<02:33,  5.15it/s]Ref scores:  21%|â–ˆâ–ˆ        | 209/1000 [00:44<02:40,  4.93it/s]Ref scores:  21%|â–ˆâ–ˆ        | 210/1000 [00:44<02:42,  4.87it/s]Ref scores:  21%|â–ˆâ–ˆ        | 211/1000 [00:44<02:55,  4.50it/s]Ref scores:  21%|â–ˆâ–ˆ        | 212/1000 [00:44<02:52,  4.56it/s]Ref scores:  21%|â–ˆâ–ˆâ–       | 213/1000 [00:45<02:55,  4.49it/s]Ref scores:  21%|â–ˆâ–ˆâ–       | 214/1000 [00:45<02:52,  4.55it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 215/1000 [00:45<02:36,  5.01it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 216/1000 [00:45<02:26,  5.37it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 217/1000 [00:45<02:32,  5.15it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 218/1000 [00:46<02:20,  5.57it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 219/1000 [00:46<02:07,  6.13it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 220/1000 [00:46<02:05,  6.23it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 221/1000 [00:46<02:02,  6.34it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 222/1000 [00:46<02:00,  6.46it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 223/1000 [00:46<01:59,  6.48it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 224/1000 [00:46<02:16,  5.68it/s]Ref scores:  22%|â–ˆâ–ˆâ–Ž       | 225/1000 [00:47<02:25,  5.33it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 226/1000 [00:47<02:41,  4.78it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 227/1000 [00:47<02:28,  5.22it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 228/1000 [00:47<02:18,  5.58it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 229/1000 [00:47<02:11,  5.85it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 230/1000 [00:48<02:21,  5.44it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 231/1000 [00:48<02:28,  5.18it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 232/1000 [00:48<02:32,  5.04it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 233/1000 [00:48<02:21,  5.43it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 234/1000 [00:48<02:28,  5.16it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 235/1000 [00:49<02:18,  5.52it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 236/1000 [00:49<02:11,  5.83it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 237/1000 [00:49<02:00,  6.35it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 238/1000 [00:49<01:57,  6.47it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 239/1000 [00:49<02:22,  5.33it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 240/1000 [00:50<02:40,  4.74it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 241/1000 [00:50<02:43,  4.64it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 242/1000 [00:50<02:28,  5.10it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 243/1000 [00:50<02:11,  5.77it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 244/1000 [00:50<02:20,  5.38it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 245/1000 [00:50<02:36,  4.82it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 246/1000 [00:51<02:28,  5.09it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 247/1000 [00:51<02:41,  4.65it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 248/1000 [00:51<02:41,  4.66it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 249/1000 [00:51<02:27,  5.09it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 250/1000 [00:51<02:17,  5.45it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 251/1000 [00:52<02:10,  5.73it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 252/1000 [00:52<02:04,  6.02it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 253/1000 [00:52<02:01,  6.15it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 254/1000 [00:52<02:23,  5.20it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 255/1000 [00:52<02:27,  5.04it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 256/1000 [00:53<02:32,  4.89it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 257/1000 [00:53<02:20,  5.28it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 258/1000 [00:53<02:27,  5.04it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 259/1000 [00:53<02:15,  5.47it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 260/1000 [00:53<02:32,  4.84it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 261/1000 [00:54<02:21,  5.24it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 262/1000 [00:54<02:25,  5.06it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 263/1000 [00:54<02:15,  5.45it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 264/1000 [00:54<02:08,  5.74it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 265/1000 [00:54<02:17,  5.36it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 266/1000 [00:55<02:32,  4.81it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 267/1000 [00:55<02:24,  5.08it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 268/1000 [00:55<02:07,  5.72it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 269/1000 [00:55<02:16,  5.34it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 270/1000 [00:55<02:32,  4.80it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 271/1000 [00:56<02:43,  4.45it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 272/1000 [00:56<02:27,  4.94it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 273/1000 [00:56<02:10,  5.56it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 274/1000 [00:56<02:17,  5.26it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 275/1000 [00:56<02:09,  5.60it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 276/1000 [00:56<02:18,  5.21it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 277/1000 [00:57<02:25,  4.98it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 278/1000 [00:57<02:13,  5.39it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 279/1000 [00:57<02:05,  5.73it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 280/1000 [00:57<02:04,  5.77it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 281/1000 [00:57<01:59,  6.02it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 282/1000 [00:57<02:10,  5.52it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 283/1000 [00:58<02:03,  5.82it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 284/1000 [00:58<01:52,  6.34it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 285/1000 [00:58<01:55,  6.20it/s]Ref scores:  29%|â–ˆâ–ˆâ–Š       | 286/1000 [00:58<01:56,  6.11it/s]Ref scores:  29%|â–ˆâ–ˆâ–Š       | 287/1000 [00:58<01:54,  6.23it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 288/1000 [00:58<01:52,  6.33it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 289/1000 [00:59<01:55,  6.18it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 290/1000 [00:59<02:16,  5.18it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 291/1000 [00:59<02:07,  5.54it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 292/1000 [00:59<02:02,  5.80it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 293/1000 [00:59<01:57,  5.99it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 294/1000 [00:59<02:07,  5.53it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 295/1000 [01:00<02:01,  5.79it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 296/1000 [01:00<02:09,  5.43it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 297/1000 [01:00<02:02,  5.75it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 298/1000 [01:00<02:13,  5.25it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 299/1000 [01:00<02:05,  5.59it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 300/1000 [01:01<02:13,  5.24it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 301/1000 [01:01<02:19,  5.00it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 302/1000 [01:01<02:25,  4.79it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 303/1000 [01:01<02:26,  4.77it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 304/1000 [01:01<02:12,  5.23it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 305/1000 [01:02<02:05,  5.54it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 306/1000 [01:02<02:12,  5.24it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 307/1000 [01:02<02:07,  5.43it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 308/1000 [01:02<02:15,  5.09it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 309/1000 [01:02<02:06,  5.45it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 310/1000 [01:02<01:58,  5.81it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 311/1000 [01:03<02:07,  5.42it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 312/1000 [01:03<02:12,  5.19it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆâ–      | 313/1000 [01:03<02:03,  5.55it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆâ–      | 314/1000 [01:03<02:21,  4.86it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 315/1000 [01:04<02:24,  4.75it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 316/1000 [01:04<02:12,  5.18it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 317/1000 [01:04<02:26,  4.67it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 318/1000 [01:04<02:12,  5.16it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 319/1000 [01:04<02:03,  5.53it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 320/1000 [01:04<01:57,  5.80it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 321/1000 [01:05<02:06,  5.36it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 322/1000 [01:05<02:16,  4.97it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 323/1000 [01:05<02:19,  4.84it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 324/1000 [01:05<02:08,  5.28it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 325/1000 [01:05<01:59,  5.64it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 326/1000 [01:06<01:54,  5.90it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 327/1000 [01:06<01:49,  6.15it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 328/1000 [01:06<02:00,  5.58it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 329/1000 [01:06<01:55,  5.82it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 330/1000 [01:06<02:20,  4.77it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 331/1000 [01:07<02:20,  4.75it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 332/1000 [01:07<02:03,  5.42it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 333/1000 [01:07<01:56,  5.75it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 334/1000 [01:07<02:03,  5.39it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 335/1000 [01:07<02:17,  4.83it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 336/1000 [01:07<02:06,  5.26it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 337/1000 [01:08<02:10,  5.08it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 338/1000 [01:08<02:14,  4.91it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 339/1000 [01:08<01:58,  5.57it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 340/1000 [01:08<01:52,  5.86it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 341/1000 [01:08<02:02,  5.38it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 342/1000 [01:09<01:58,  5.53it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 343/1000 [01:09<01:56,  5.63it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 344/1000 [01:09<02:12,  4.94it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 345/1000 [01:09<02:02,  5.35it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 346/1000 [01:09<02:15,  4.81it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 347/1000 [01:10<02:08,  5.10it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 348/1000 [01:10<02:20,  4.65it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 349/1000 [01:10<02:05,  5.17it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 350/1000 [01:10<01:55,  5.62it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 351/1000 [01:10<01:48,  5.96it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 352/1000 [01:10<01:59,  5.43it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 353/1000 [01:11<02:05,  5.16it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 354/1000 [01:11<02:08,  5.03it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 355/1000 [01:11<02:11,  4.89it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 356/1000 [01:11<02:14,  4.81it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 357/1000 [01:11<02:00,  5.32it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 358/1000 [01:12<01:55,  5.56it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 359/1000 [01:12<01:51,  5.72it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 360/1000 [01:12<01:49,  5.85it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 361/1000 [01:12<01:44,  6.13it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 362/1000 [01:12<01:40,  6.36it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 363/1000 [01:12<01:51,  5.71it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 364/1000 [01:13<01:45,  6.01it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 365/1000 [01:13<01:55,  5.47it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 366/1000 [01:13<01:48,  5.86it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 367/1000 [01:13<01:42,  6.15it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 368/1000 [01:13<01:52,  5.60it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 369/1000 [01:13<01:45,  5.95it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 370/1000 [01:14<01:53,  5.54it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 371/1000 [01:14<02:00,  5.20it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 372/1000 [01:14<01:54,  5.47it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 373/1000 [01:14<01:42,  6.13it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 374/1000 [01:14<01:51,  5.64it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 375/1000 [01:15<01:44,  5.98it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 376/1000 [01:15<01:43,  6.04it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 377/1000 [01:15<01:53,  5.49it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 378/1000 [01:15<02:00,  5.14it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 379/1000 [01:15<02:12,  4.67it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 380/1000 [01:16<02:11,  4.70it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 381/1000 [01:16<01:58,  5.21it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 382/1000 [01:16<01:49,  5.62it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 383/1000 [01:16<01:43,  5.97it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 384/1000 [01:16<01:51,  5.53it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 385/1000 [01:16<01:44,  5.89it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 386/1000 [01:17<01:39,  6.16it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 387/1000 [01:17<01:36,  6.35it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 388/1000 [01:17<01:34,  6.51it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 389/1000 [01:17<01:44,  5.83it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 390/1000 [01:17<01:39,  6.13it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 391/1000 [01:17<01:56,  5.24it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 392/1000 [01:18<02:01,  4.99it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 393/1000 [01:18<02:06,  4.79it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 394/1000 [01:18<02:09,  4.69it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 395/1000 [01:18<01:56,  5.19it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 396/1000 [01:18<01:47,  5.61it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 397/1000 [01:19<01:55,  5.20it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 398/1000 [01:19<01:59,  5.03it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 399/1000 [01:19<01:45,  5.71it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 400/1000 [01:19<01:51,  5.37it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 401/1000 [01:19<01:57,  5.09it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 402/1000 [01:20<02:00,  4.97it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 403/1000 [01:20<01:49,  5.45it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 404/1000 [01:20<02:02,  4.85it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 405/1000 [01:20<01:54,  5.19it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 406/1000 [01:20<01:46,  5.60it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 407/1000 [01:21<01:52,  5.26it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 408/1000 [01:21<02:04,  4.74it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 409/1000 [01:21<01:53,  5.23it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 410/1000 [01:21<01:44,  5.64it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 411/1000 [01:21<01:38,  5.97it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 412/1000 [01:21<01:55,  5.11it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 413/1000 [01:22<01:41,  5.79it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 414/1000 [01:22<01:49,  5.34it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 415/1000 [01:22<01:55,  5.08it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 416/1000 [01:22<01:59,  4.90it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 417/1000 [01:22<01:48,  5.35it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 418/1000 [01:23<01:45,  5.53it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 419/1000 [01:23<01:39,  5.84it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 420/1000 [01:23<01:35,  6.07it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 421/1000 [01:23<01:31,  6.30it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 422/1000 [01:23<01:49,  5.28it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 423/1000 [01:24<01:54,  5.02it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 424/1000 [01:24<01:41,  5.70it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 425/1000 [01:24<01:47,  5.33it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 426/1000 [01:24<01:52,  5.10it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 427/1000 [01:24<01:39,  5.78it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 428/1000 [01:24<01:33,  6.12it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 429/1000 [01:24<01:30,  6.32it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 430/1000 [01:25<01:41,  5.61it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 431/1000 [01:25<01:47,  5.27it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 432/1000 [01:25<01:39,  5.71it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 433/1000 [01:25<01:34,  5.98it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 434/1000 [01:25<01:51,  5.06it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 435/1000 [01:26<01:46,  5.33it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 436/1000 [01:26<01:51,  5.04it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 437/1000 [01:26<01:42,  5.47it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 438/1000 [01:26<01:39,  5.63it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 439/1000 [01:26<01:34,  5.91it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 440/1000 [01:26<01:34,  5.93it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 441/1000 [01:27<01:44,  5.37it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 442/1000 [01:27<01:37,  5.73it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 443/1000 [01:27<01:52,  4.94it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 444/1000 [01:27<01:57,  4.73it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 445/1000 [01:28<01:58,  4.69it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 446/1000 [01:28<01:58,  4.66it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 447/1000 [01:28<02:01,  4.54it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 448/1000 [01:28<01:58,  4.64it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 449/1000 [01:28<01:47,  5.14it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 450/1000 [01:29<01:58,  4.65it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 451/1000 [01:29<01:46,  5.16it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 452/1000 [01:29<02:03,  4.44it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 453/1000 [01:29<02:03,  4.44it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 454/1000 [01:30<02:09,  4.21it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 455/1000 [01:30<02:07,  4.26it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 456/1000 [01:30<01:55,  4.70it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 457/1000 [01:30<01:44,  5.21it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 458/1000 [01:30<01:32,  5.88it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 459/1000 [01:30<01:41,  5.35it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 460/1000 [01:31<01:33,  5.74it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 461/1000 [01:31<01:40,  5.34it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 462/1000 [01:31<01:46,  5.07it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 463/1000 [01:31<01:40,  5.36it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 464/1000 [01:31<01:32,  5.76it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 465/1000 [01:32<01:40,  5.30it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 466/1000 [01:32<01:46,  4.99it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 467/1000 [01:32<01:33,  5.69it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 468/1000 [01:32<01:47,  4.95it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 469/1000 [01:32<01:38,  5.42it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 470/1000 [01:33<01:44,  5.07it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 471/1000 [01:33<01:44,  5.04it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 472/1000 [01:33<01:35,  5.51it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 473/1000 [01:33<01:29,  5.90it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 474/1000 [01:33<01:37,  5.38it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 475/1000 [01:33<01:33,  5.59it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 476/1000 [01:34<01:28,  5.95it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 477/1000 [01:34<01:24,  6.22it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 478/1000 [01:34<01:21,  6.42it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 479/1000 [01:34<01:19,  6.54it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 480/1000 [01:34<01:18,  6.60it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 481/1000 [01:34<01:13,  7.06it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 482/1000 [01:34<01:09,  7.45it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 483/1000 [01:35<01:28,  5.82it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 484/1000 [01:35<01:34,  5.43it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 485/1000 [01:35<01:31,  5.63it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 486/1000 [01:35<01:22,  6.24it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 487/1000 [01:35<01:19,  6.44it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 488/1000 [01:35<01:18,  6.52it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 489/1000 [01:36<01:29,  5.69it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 490/1000 [01:36<01:37,  5.22it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 491/1000 [01:36<01:48,  4.71it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 492/1000 [01:36<01:48,  4.69it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 493/1000 [01:37<01:47,  4.70it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 494/1000 [01:37<01:39,  5.06it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 495/1000 [01:37<01:31,  5.52it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 496/1000 [01:37<01:25,  5.88it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 497/1000 [01:37<01:39,  5.06it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 498/1000 [01:38<01:48,  4.63it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 499/1000 [01:38<01:47,  4.67it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 500/1000 [01:38<01:36,  5.18it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 501/1000 [01:38<01:28,  5.61it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 502/1000 [01:38<01:26,  5.78it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 503/1000 [01:38<01:34,  5.29it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 505/1000 [01:39<01:15,  6.51it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 506/1000 [01:39<01:29,  5.54it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 507/1000 [01:39<01:24,  5.84it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 508/1000 [01:39<01:21,  6.07it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 509/1000 [01:39<01:28,  5.56it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 510/1000 [01:40<01:23,  5.84it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 511/1000 [01:40<01:20,  6.08it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 512/1000 [01:40<01:27,  5.59it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 513/1000 [01:40<01:33,  5.20it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 514/1000 [01:40<01:23,  5.85it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 515/1000 [01:40<01:29,  5.43it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 516/1000 [01:41<01:35,  5.08it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 517/1000 [01:41<01:38,  4.93it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 518/1000 [01:41<01:29,  5.36it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 519/1000 [01:41<01:24,  5.71it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 520/1000 [01:41<01:22,  5.80it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 521/1000 [01:42<01:30,  5.28it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 522/1000 [01:42<01:34,  5.07it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 523/1000 [01:42<01:27,  5.48it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 524/1000 [01:42<01:22,  5.80it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 525/1000 [01:42<01:14,  6.34it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 526/1000 [01:43<01:29,  5.28it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 527/1000 [01:43<01:40,  4.71it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 528/1000 [01:43<01:47,  4.40it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 529/1000 [01:43<01:32,  5.11it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 530/1000 [01:43<01:41,  4.62it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 531/1000 [01:44<01:34,  4.97it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 532/1000 [01:44<01:25,  5.44it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 533/1000 [01:44<01:19,  5.86it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 534/1000 [01:44<01:18,  5.95it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 535/1000 [01:44<01:17,  6.02it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 536/1000 [01:44<01:30,  5.15it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 537/1000 [01:45<01:22,  5.59it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 538/1000 [01:45<01:18,  5.90it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 539/1000 [01:45<01:24,  5.47it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 540/1000 [01:45<01:15,  6.10it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 541/1000 [01:45<01:23,  5.49it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 542/1000 [01:46<01:30,  5.09it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 543/1000 [01:46<01:38,  4.63it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 544/1000 [01:46<01:29,  5.09it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 545/1000 [01:46<01:22,  5.54it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 546/1000 [01:46<01:27,  5.17it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 547/1000 [01:47<01:37,  4.65it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 548/1000 [01:47<01:27,  5.19it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 549/1000 [01:47<01:30,  5.00it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 550/1000 [01:47<01:33,  4.83it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 551/1000 [01:47<01:25,  5.27it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 552/1000 [01:48<01:35,  4.69it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 553/1000 [01:48<01:36,  4.63it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 554/1000 [01:48<01:27,  5.12it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 555/1000 [01:48<01:20,  5.51it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 556/1000 [01:48<01:27,  5.09it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 557/1000 [01:49<01:36,  4.59it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 558/1000 [01:49<01:27,  5.07it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 559/1000 [01:49<01:36,  4.57it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 560/1000 [01:49<01:43,  4.27it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 561/1000 [01:50<01:42,  4.30it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 562/1000 [01:50<01:32,  4.74it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 563/1000 [01:50<01:23,  5.24it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 564/1000 [01:50<01:27,  4.99it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 565/1000 [01:50<01:29,  4.85it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 566/1000 [01:50<01:21,  5.32it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 567/1000 [01:51<01:24,  5.10it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 568/1000 [01:51<01:17,  5.57it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 569/1000 [01:51<01:21,  5.30it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 570/1000 [01:51<01:24,  5.08it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 571/1000 [01:51<01:26,  4.95it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 572/1000 [01:52<01:19,  5.41it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 573/1000 [01:52<01:22,  5.19it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 574/1000 [01:52<01:15,  5.63it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 575/1000 [01:52<01:07,  6.26it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 576/1000 [01:52<01:06,  6.40it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 577/1000 [01:52<01:04,  6.55it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 578/1000 [01:52<01:03,  6.62it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 579/1000 [01:53<01:02,  6.69it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 580/1000 [01:53<01:02,  6.73it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 581/1000 [01:53<00:58,  7.20it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 582/1000 [01:53<01:13,  5.65it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 583/1000 [01:53<01:19,  5.27it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 584/1000 [01:53<01:10,  5.90it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 585/1000 [01:54<01:14,  5.55it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 586/1000 [01:54<01:10,  5.89it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 587/1000 [01:54<01:07,  6.14it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 588/1000 [01:54<01:14,  5.50it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 589/1000 [01:54<01:20,  5.14it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 590/1000 [01:55<01:20,  5.10it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 591/1000 [01:55<01:14,  5.53it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 592/1000 [01:55<01:09,  5.91it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 593/1000 [01:55<01:05,  6.21it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 594/1000 [01:55<01:11,  5.68it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 595/1000 [01:55<01:04,  6.29it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 596/1000 [01:56<01:12,  5.59it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 597/1000 [01:56<01:04,  6.22it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 598/1000 [01:56<01:10,  5.70it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 599/1000 [01:56<01:06,  6.02it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 600/1000 [01:56<01:11,  5.57it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 601/1000 [01:57<01:21,  4.92it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 602/1000 [01:57<01:14,  5.37it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 603/1000 [01:57<01:09,  5.75it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 604/1000 [01:57<01:05,  6.06it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 605/1000 [01:57<01:02,  6.30it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 606/1000 [01:57<01:14,  5.29it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 607/1000 [01:58<01:08,  5.71it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 608/1000 [01:58<01:12,  5.39it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 609/1000 [01:58<01:16,  5.09it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 610/1000 [01:58<01:20,  4.84it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 611/1000 [01:58<01:15,  5.18it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 612/1000 [01:59<01:18,  4.93it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 613/1000 [01:59<01:11,  5.41it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 614/1000 [01:59<01:03,  6.07it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 615/1000 [01:59<01:10,  5.46it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 616/1000 [01:59<01:05,  5.82it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 617/1000 [01:59<01:03,  6.05it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 618/1000 [02:00<00:58,  6.57it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 619/1000 [02:00<01:07,  5.68it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 620/1000 [02:00<01:03,  5.97it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 621/1000 [02:00<00:58,  6.49it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 622/1000 [02:00<00:57,  6.59it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 623/1000 [02:00<00:56,  6.69it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 624/1000 [02:01<01:09,  5.44it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 625/1000 [02:01<01:04,  5.79it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 626/1000 [02:01<01:01,  6.06it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 627/1000 [02:01<01:08,  5.46it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 628/1000 [02:01<01:13,  5.05it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 629/1000 [02:01<01:08,  5.45it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 630/1000 [02:02<01:17,  4.79it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 631/1000 [02:02<01:12,  5.11it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 632/1000 [02:02<01:15,  4.90it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 633/1000 [02:02<01:08,  5.38it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 634/1000 [02:02<01:04,  5.72it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 635/1000 [02:03<01:08,  5.35it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 636/1000 [02:03<01:03,  5.71it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 637/1000 [02:03<01:09,  5.25it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 638/1000 [02:03<01:12,  4.99it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 639/1000 [02:03<01:06,  5.44it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 640/1000 [02:04<01:02,  5.78it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 641/1000 [02:04<00:59,  6.05it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 642/1000 [02:04<00:54,  6.58it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 643/1000 [02:04<00:53,  6.62it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 644/1000 [02:04<00:50,  7.01it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 645/1000 [02:04<00:51,  6.93it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 646/1000 [02:04<00:57,  6.18it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 647/1000 [02:05<00:55,  6.34it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 648/1000 [02:05<01:06,  5.31it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 649/1000 [02:05<01:01,  5.68it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 650/1000 [02:05<01:06,  5.29it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 651/1000 [02:05<01:01,  5.66it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 652/1000 [02:06<01:07,  5.16it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 653/1000 [02:06<01:02,  5.57it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 654/1000 [02:06<00:58,  5.90it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 655/1000 [02:06<00:56,  6.13it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 656/1000 [02:06<00:54,  6.29it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 657/1000 [02:06<01:05,  5.25it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 658/1000 [02:07<01:07,  5.04it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 659/1000 [02:07<01:02,  5.44it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 660/1000 [02:07<01:10,  4.80it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 661/1000 [02:07<01:12,  4.69it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 662/1000 [02:07<01:05,  5.16it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 663/1000 [02:08<01:07,  4.97it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 664/1000 [02:08<01:02,  5.38it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 665/1000 [02:08<00:58,  5.77it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 666/1000 [02:08<01:02,  5.36it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 667/1000 [02:08<01:05,  5.11it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 668/1000 [02:09<01:00,  5.52it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 669/1000 [02:09<00:56,  5.83it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 670/1000 [02:09<00:54,  6.10it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 671/1000 [02:09<01:03,  5.15it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 672/1000 [02:09<01:06,  4.96it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 673/1000 [02:10<01:07,  4.84it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 674/1000 [02:10<01:08,  4.75it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 675/1000 [02:10<01:02,  5.22it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 676/1000 [02:10<00:59,  5.46it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 677/1000 [02:10<00:55,  5.83it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 678/1000 [02:10<00:53,  6.05it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 679/1000 [02:10<00:51,  6.26it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 680/1000 [02:11<00:51,  6.19it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 681/1000 [02:11<00:51,  6.13it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 682/1000 [02:11<00:50,  6.34it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 683/1000 [02:11<00:49,  6.45it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 684/1000 [02:11<00:54,  5.78it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 685/1000 [02:12<01:06,  4.75it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 686/1000 [02:12<01:06,  4.72it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 687/1000 [02:12<01:00,  5.18it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 688/1000 [02:12<00:56,  5.54it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 689/1000 [02:12<00:52,  5.89it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 690/1000 [02:12<00:52,  5.94it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 691/1000 [02:13<00:50,  6.15it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 692/1000 [02:13<00:55,  5.59it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 693/1000 [02:13<00:59,  5.19it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 694/1000 [02:13<00:54,  5.60it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 695/1000 [02:13<00:59,  5.14it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 696/1000 [02:14<00:56,  5.38it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 697/1000 [02:14<00:52,  5.72it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 698/1000 [02:14<00:50,  6.00it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 699/1000 [02:14<00:48,  6.18it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 700/1000 [02:14<00:54,  5.53it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 701/1000 [02:14<00:55,  5.35it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 702/1000 [02:15<00:58,  5.11it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 703/1000 [02:15<00:53,  5.53it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 704/1000 [02:15<00:50,  5.88it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 705/1000 [02:15<00:58,  5.06it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 706/1000 [02:15<01:03,  4.62it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 707/1000 [02:16<00:55,  5.32it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 708/1000 [02:16<01:01,  4.73it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 709/1000 [02:16<01:02,  4.64it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 710/1000 [02:16<01:03,  4.57it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 711/1000 [02:16<00:57,  5.07it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 712/1000 [02:17<00:52,  5.52it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 713/1000 [02:17<00:55,  5.20it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 714/1000 [02:17<00:51,  5.59it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 715/1000 [02:17<00:49,  5.70it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 716/1000 [02:17<00:47,  6.00it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 717/1000 [02:17<00:45,  6.21it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 718/1000 [02:18<00:51,  5.50it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 719/1000 [02:18<00:53,  5.23it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 720/1000 [02:18<00:56,  4.97it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 721/1000 [02:18<00:57,  4.89it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 722/1000 [02:19<00:57,  4.81it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 723/1000 [02:19<00:52,  5.27it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 724/1000 [02:19<00:48,  5.64it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 725/1000 [02:19<00:51,  5.30it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 726/1000 [02:19<00:48,  5.65it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 727/1000 [02:19<00:45,  5.94it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 728/1000 [02:20<00:44,  6.17it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 729/1000 [02:20<00:42,  6.32it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 730/1000 [02:20<00:48,  5.57it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 731/1000 [02:20<00:45,  5.89it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 732/1000 [02:20<00:43,  6.15it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 733/1000 [02:20<00:48,  5.47it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 734/1000 [02:21<00:54,  4.85it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 735/1000 [02:21<00:56,  4.68it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 736/1000 [02:21<00:50,  5.20it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 737/1000 [02:21<00:47,  5.57it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 738/1000 [02:21<00:44,  5.86it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 739/1000 [02:22<00:48,  5.33it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 740/1000 [02:22<00:45,  5.73it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 741/1000 [02:22<00:44,  5.82it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 742/1000 [02:22<00:40,  6.37it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 743/1000 [02:22<00:45,  5.70it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 744/1000 [02:22<00:48,  5.32it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 745/1000 [02:23<00:44,  5.69it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 746/1000 [02:23<00:42,  5.97it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 747/1000 [02:23<00:40,  6.21it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 748/1000 [02:23<00:44,  5.61it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 749/1000 [02:23<00:42,  5.92it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 750/1000 [02:23<00:46,  5.38it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 751/1000 [02:24<00:41,  6.02it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 752/1000 [02:24<00:37,  6.54it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 753/1000 [02:24<00:37,  6.60it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 754/1000 [02:24<00:35,  7.00it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 755/1000 [02:24<00:35,  6.89it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 756/1000 [02:24<00:33,  7.34it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 757/1000 [02:24<00:35,  6.88it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 758/1000 [02:25<00:41,  5.87it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 759/1000 [02:25<00:50,  4.79it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 760/1000 [02:25<00:50,  4.73it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 761/1000 [02:25<00:45,  5.21it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 762/1000 [02:26<00:47,  5.02it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 763/1000 [02:26<00:43,  5.42it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 764/1000 [02:26<00:49,  4.79it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 765/1000 [02:26<00:49,  4.74it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 766/1000 [02:26<00:50,  4.63it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 767/1000 [02:27<00:50,  4.58it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 768/1000 [02:27<00:45,  5.06it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 769/1000 [02:27<00:47,  4.86it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 770/1000 [02:27<00:48,  4.72it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 771/1000 [02:27<00:43,  5.21it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 772/1000 [02:27<00:40,  5.63it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 773/1000 [02:28<00:46,  4.89it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 774/1000 [02:28<00:47,  4.80it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 775/1000 [02:28<00:42,  5.30it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 776/1000 [02:28<00:40,  5.50it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 777/1000 [02:29<00:42,  5.21it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 778/1000 [02:29<00:47,  4.67it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 779/1000 [02:29<00:48,  4.60it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 780/1000 [02:29<00:43,  5.10it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 781/1000 [02:29<00:37,  5.80it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 782/1000 [02:29<00:37,  5.87it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 783/1000 [02:30<00:40,  5.35it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 784/1000 [02:30<00:37,  5.70it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 785/1000 [02:30<00:34,  6.29it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 786/1000 [02:30<00:37,  5.69it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 787/1000 [02:30<00:35,  5.97it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 788/1000 [02:30<00:35,  6.00it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 789/1000 [02:31<00:34,  6.19it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 790/1000 [02:31<00:31,  6.68it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 791/1000 [02:31<00:38,  5.44it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 792/1000 [02:31<00:36,  5.78it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 793/1000 [02:31<00:34,  6.05it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 794/1000 [02:31<00:32,  6.26it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 795/1000 [02:32<00:32,  6.38it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 796/1000 [02:32<00:31,  6.50it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 797/1000 [02:32<00:31,  6.55it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 798/1000 [02:32<00:30,  6.59it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 799/1000 [02:32<00:35,  5.71it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 800/1000 [02:32<00:33,  6.02it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 801/1000 [02:33<00:36,  5.44it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 802/1000 [02:33<00:32,  6.09it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 803/1000 [02:33<00:35,  5.56it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 804/1000 [02:33<00:36,  5.37it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 805/1000 [02:33<00:33,  5.74it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 806/1000 [02:33<00:32,  6.02it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 807/1000 [02:34<00:31,  6.19it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 808/1000 [02:34<00:34,  5.63it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 809/1000 [02:34<00:30,  6.23it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 810/1000 [02:34<00:29,  6.37it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 811/1000 [02:34<00:35,  5.29it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 812/1000 [02:34<00:33,  5.68it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 813/1000 [02:35<00:32,  5.78it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 814/1000 [02:35<00:30,  6.08it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 815/1000 [02:35<00:33,  5.46it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 816/1000 [02:35<00:35,  5.17it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 817/1000 [02:35<00:32,  5.56it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 818/1000 [02:36<00:37,  4.88it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 819/1000 [02:36<00:38,  4.73it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 820/1000 [02:36<00:34,  5.18it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 821/1000 [02:36<00:38,  4.63it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 822/1000 [02:36<00:34,  5.13it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 823/1000 [02:37<00:34,  5.07it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 824/1000 [02:37<00:30,  5.71it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 825/1000 [02:37<00:29,  6.00it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 826/1000 [02:37<00:26,  6.52it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 827/1000 [02:37<00:34,  5.06it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 828/1000 [02:37<00:31,  5.50it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 829/1000 [02:38<00:29,  5.81it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 830/1000 [02:38<00:28,  6.04it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 831/1000 [02:38<00:26,  6.29it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 832/1000 [02:38<00:30,  5.56it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 833/1000 [02:38<00:28,  5.87it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 834/1000 [02:39<00:33,  4.99it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 835/1000 [02:39<00:30,  5.44it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 836/1000 [02:39<00:31,  5.15it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 837/1000 [02:39<00:35,  4.62it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 838/1000 [02:39<00:32,  4.97it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 839/1000 [02:40<00:29,  5.41it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 840/1000 [02:40<00:31,  5.11it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 841/1000 [02:40<00:29,  5.36it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 842/1000 [02:40<00:30,  5.12it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 843/1000 [02:40<00:28,  5.52it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 844/1000 [02:40<00:26,  5.88it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 845/1000 [02:41<00:28,  5.36it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 846/1000 [02:41<00:26,  5.71it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 847/1000 [02:41<00:25,  5.97it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 848/1000 [02:41<00:25,  5.98it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 849/1000 [02:41<00:23,  6.49it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 850/1000 [02:41<00:27,  5.38it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 851/1000 [02:42<00:25,  5.75it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 852/1000 [02:42<00:23,  6.33it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 853/1000 [02:42<00:25,  5.68it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 854/1000 [02:42<00:27,  5.26it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 855/1000 [02:42<00:25,  5.64it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 856/1000 [02:42<00:23,  6.23it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 857/1000 [02:43<00:22,  6.39it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 858/1000 [02:43<00:27,  5.24it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 859/1000 [02:43<00:24,  5.66it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 860/1000 [02:43<00:23,  5.96it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 861/1000 [02:43<00:22,  6.23it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 862/1000 [02:43<00:21,  6.38it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 863/1000 [02:44<00:21,  6.49it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 864/1000 [02:44<00:19,  6.92it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 865/1000 [02:44<00:18,  7.25it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 866/1000 [02:44<00:22,  6.04it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 867/1000 [02:44<00:21,  6.25it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 868/1000 [02:44<00:20,  6.39it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 869/1000 [02:45<00:20,  6.50it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 870/1000 [02:45<00:24,  5.36it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 871/1000 [02:45<00:29,  4.32it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 872/1000 [02:45<00:26,  4.87it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 873/1000 [02:45<00:23,  5.33it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 874/1000 [02:46<00:25,  5.04it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 875/1000 [02:46<00:22,  5.46it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 876/1000 [02:46<00:23,  5.17it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 877/1000 [02:46<00:25,  4.91it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 878/1000 [02:46<00:23,  5.20it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 879/1000 [02:47<00:21,  5.57it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 880/1000 [02:47<00:20,  5.90it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 881/1000 [02:47<00:19,  6.11it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 882/1000 [02:47<00:17,  6.63it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 883/1000 [02:47<00:18,  6.41it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 884/1000 [02:47<00:18,  6.27it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 885/1000 [02:47<00:18,  6.19it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 886/1000 [02:48<00:17,  6.35it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 887/1000 [02:48<00:17,  6.48it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 888/1000 [02:48<00:17,  6.56it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 889/1000 [02:48<00:17,  6.37it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 890/1000 [02:48<00:17,  6.27it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 891/1000 [02:48<00:17,  6.19it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 892/1000 [02:49<00:17,  6.15it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 893/1000 [02:49<00:16,  6.37it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 894/1000 [02:49<00:16,  6.48it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 895/1000 [02:49<00:18,  5.76it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 896/1000 [02:49<00:16,  6.36it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 897/1000 [02:49<00:16,  6.25it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 898/1000 [02:50<00:15,  6.40it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 899/1000 [02:50<00:15,  6.49it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 900/1000 [02:50<00:15,  6.58it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 901/1000 [02:50<00:14,  6.61it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 902/1000 [02:50<00:14,  6.62it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 903/1000 [02:50<00:17,  5.40it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 904/1000 [02:51<00:16,  5.75it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 905/1000 [02:51<00:17,  5.37it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 906/1000 [02:51<00:16,  5.55it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 907/1000 [02:51<00:15,  5.86it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 908/1000 [02:51<00:14,  6.13it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 909/1000 [02:51<00:14,  6.30it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 910/1000 [02:52<00:15,  5.66it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 911/1000 [02:52<00:16,  5.31it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 912/1000 [02:52<00:15,  5.53it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 913/1000 [02:52<00:17,  4.88it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 914/1000 [02:52<00:16,  5.31it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 915/1000 [02:53<00:14,  5.68it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 916/1000 [02:53<00:16,  5.24it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 917/1000 [02:53<00:14,  5.60it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 918/1000 [02:53<00:14,  5.72it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 919/1000 [02:53<00:13,  5.98it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 920/1000 [02:53<00:12,  6.19it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 921/1000 [02:54<00:12,  6.36it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 922/1000 [02:54<00:12,  6.48it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 923/1000 [02:54<00:11,  6.61it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 924/1000 [02:54<00:13,  5.78it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 925/1000 [02:54<00:12,  6.05it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 926/1000 [02:54<00:11,  6.25it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 927/1000 [02:54<00:11,  6.37it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 928/1000 [02:55<00:11,  6.48it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 929/1000 [02:55<00:12,  5.71it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 930/1000 [02:55<00:11,  6.00it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 931/1000 [02:55<00:11,  6.22it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 932/1000 [02:55<00:10,  6.34it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 933/1000 [02:56<00:11,  5.60it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 934/1000 [02:56<00:13,  4.87it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 935/1000 [02:56<00:11,  5.58it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 936/1000 [02:56<00:10,  5.87it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 937/1000 [02:56<00:11,  5.33it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 938/1000 [02:56<00:12,  5.10it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 939/1000 [02:57<00:11,  5.51it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 940/1000 [02:57<00:10,  5.85it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 941/1000 [02:57<00:09,  6.08it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 942/1000 [02:57<00:09,  6.26it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 943/1000 [02:57<00:10,  5.26it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 944/1000 [02:57<00:09,  5.63it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 945/1000 [02:58<00:09,  5.93it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 946/1000 [02:58<00:10,  5.36it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 947/1000 [02:58<00:11,  4.77it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 948/1000 [02:58<00:09,  5.25it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 949/1000 [02:59<00:10,  4.95it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 950/1000 [02:59<00:10,  4.88it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 951/1000 [02:59<00:10,  4.54it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 952/1000 [02:59<00:10,  4.51it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 953/1000 [02:59<00:09,  5.03it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 954/1000 [02:59<00:08,  5.46it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 955/1000 [03:00<00:07,  5.82it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 956/1000 [03:00<00:07,  6.05it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 957/1000 [03:00<00:07,  5.46it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 958/1000 [03:00<00:07,  5.80it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 959/1000 [03:00<00:06,  6.05it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 960/1000 [03:00<00:06,  6.25it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 961/1000 [03:01<00:07,  4.98it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 962/1000 [03:01<00:07,  4.86it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 963/1000 [03:01<00:07,  4.78it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 964/1000 [03:01<00:06,  5.26it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 965/1000 [03:01<00:05,  5.91it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 966/1000 [03:02<00:06,  5.46it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 967/1000 [03:02<00:05,  5.78it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 968/1000 [03:02<00:05,  5.85it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 969/1000 [03:02<00:05,  6.10it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 970/1000 [03:02<00:04,  6.28it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 971/1000 [03:02<00:04,  6.41it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 972/1000 [03:03<00:04,  5.64it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 973/1000 [03:03<00:04,  5.76it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 974/1000 [03:03<00:04,  6.02it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 975/1000 [03:03<00:04,  6.21it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 976/1000 [03:03<00:03,  6.37it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 977/1000 [03:03<00:03,  6.51it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 978/1000 [03:04<00:04,  5.35it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 979/1000 [03:04<00:03,  5.71it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 980/1000 [03:04<00:04,  4.96it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 981/1000 [03:04<00:03,  5.41it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 982/1000 [03:04<00:03,  5.74it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 983/1000 [03:05<00:03,  4.71it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 984/1000 [03:05<00:03,  4.67it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 985/1000 [03:05<00:02,  5.15it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 986/1000 [03:05<00:02,  5.54it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 987/1000 [03:05<00:02,  5.86it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 988/1000 [03:06<00:02,  5.90it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 989/1000 [03:06<00:01,  6.18it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 990/1000 [03:06<00:01,  6.38it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 991/1000 [03:06<00:01,  6.46it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 992/1000 [03:06<00:01,  6.55it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 993/1000 [03:06<00:01,  6.63it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 994/1000 [03:06<00:00,  6.63it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 995/1000 [03:07<00:00,  5.84it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 996/1000 [03:07<00:00,  5.90it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 997/1000 [03:07<00:00,  6.09it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 998/1000 [03:07<00:00,  6.33it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 999/1000 [03:07<00:00,  5.56it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [03:08<00:00,  4.66it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [03:08<00:00,  5.32it/s]
DONE (7.15s)
DONE (7.88s)
Computing criterion:   0%|          | 0/20 [00:00<?, ?it/s]Computing criterion:   5%|â–Œ         | 1/20 [00:19<06:07, 19.33s/it]Computing criterion:  10%|â–ˆ         | 2/20 [00:38<05:49, 19.40s/it]Computing criterion:  15%|â–ˆâ–Œ        | 3/20 [00:58<05:35, 19.72s/it]Computing criterion:  20%|â–ˆâ–ˆ        | 4/20 [01:17<05:10, 19.43s/it]Computing criterion:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [01:36<04:46, 19.12s/it]Computing criterion:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [01:55<04:26, 19.05s/it]Computing criterion:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [02:13<04:03, 18.76s/it]Computing criterion:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [02:31<03:42, 18.58s/it]Computing criterion:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [02:50<03:24, 18.61s/it]Computing criterion:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [03:07<03:02, 18.28s/it]Computing criterion:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [03:26<02:44, 18.32s/it]Computing criterion:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [03:44<02:26, 18.25s/it]Computing criterion:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [04:02<02:07, 18.17s/it]Computing criterion:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [04:20<01:48, 18.10s/it]Computing criterion:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [04:39<01:32, 18.45s/it]Computing criterion:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [04:58<01:14, 18.64s/it]Computing criterion:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [05:17<00:55, 18.67s/it]Computing criterion:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [05:36<00:37, 18.82s/it]Computing criterion:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [05:55<00:18, 18.87s/it]Computing criterion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [06:14<00:00, 18.86s/it]Computing criterion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [06:14<00:00, 18.72s/it]
Ref scores:   0%|          | 0/1000 [00:00<?, ?it/s]Ref scores:   0%|          | 1/1000 [00:02<40:17,  2.42s/it]Ref scores:   0%|          | 2/1000 [00:02<18:12,  1.09s/it]Ref scores:   0%|          | 3/1000 [00:02<11:25,  1.45it/s]Ref scores:   0%|          | 4/1000 [00:02<07:52,  2.11it/s]Ref scores:   0%|          | 5/1000 [00:03<06:19,  2.62it/s]Ref scores:   1%|          | 6/1000 [00:03<04:59,  3.32it/s]Ref scores:   1%|          | 7/1000 [00:03<04:42,  3.52it/s]Ref scores:   1%|          | 8/1000 [00:03<03:46,  4.38it/s]Ref scores:   1%|          | 9/1000 [00:03<03:21,  4.91it/s]Ref scores:   1%|          | 10/1000 [00:03<02:59,  5.52it/s]Ref scores:   1%|          | 11/1000 [00:04<02:49,  5.84it/s]Ref scores:   1%|          | 12/1000 [00:04<02:42,  6.06it/s]Ref scores:   1%|â–         | 13/1000 [00:04<02:37,  6.26it/s]Ref scores:   1%|â–         | 14/1000 [00:04<02:22,  6.90it/s]Ref scores:   2%|â–         | 15/1000 [00:04<02:12,  7.43it/s]Ref scores:   2%|â–         | 16/1000 [00:04<02:33,  6.42it/s]Ref scores:   2%|â–         | 17/1000 [00:04<02:30,  6.52it/s]Ref scores:   2%|â–         | 18/1000 [00:05<02:19,  7.02it/s]Ref scores:   2%|â–         | 19/1000 [00:05<02:21,  6.94it/s]Ref scores:   2%|â–         | 20/1000 [00:05<02:21,  6.91it/s]Ref scores:   2%|â–         | 21/1000 [00:05<02:11,  7.43it/s]Ref scores:   2%|â–         | 22/1000 [00:05<02:15,  7.22it/s]Ref scores:   2%|â–         | 23/1000 [00:05<02:07,  7.69it/s]Ref scores:   2%|â–         | 24/1000 [00:05<02:30,  6.50it/s]Ref scores:   2%|â–Ž         | 25/1000 [00:06<02:33,  6.35it/s]Ref scores:   3%|â–Ž         | 26/1000 [00:06<02:31,  6.42it/s]Ref scores:   3%|â–Ž         | 27/1000 [00:06<03:01,  5.37it/s]Ref scores:   3%|â–Ž         | 28/1000 [00:06<03:11,  5.08it/s]Ref scores:   3%|â–Ž         | 29/1000 [00:06<02:56,  5.49it/s]Ref scores:   3%|â–Ž         | 30/1000 [00:07<02:46,  5.83it/s]Ref scores:   3%|â–Ž         | 31/1000 [00:07<02:40,  6.05it/s]Ref scores:   3%|â–Ž         | 32/1000 [00:07<02:24,  6.71it/s]Ref scores:   3%|â–Ž         | 33/1000 [00:07<02:28,  6.51it/s]Ref scores:   3%|â–Ž         | 34/1000 [00:07<02:27,  6.57it/s]Ref scores:   4%|â–Ž         | 35/1000 [00:07<02:14,  7.15it/s]Ref scores:   4%|â–Ž         | 36/1000 [00:07<02:17,  7.00it/s]Ref scores:   4%|â–Ž         | 37/1000 [00:08<02:18,  6.97it/s]Ref scores:   4%|â–         | 38/1000 [00:08<02:08,  7.50it/s]Ref scores:   4%|â–         | 39/1000 [00:08<02:11,  7.28it/s]Ref scores:   4%|â–         | 40/1000 [00:08<02:13,  7.17it/s]Ref scores:   4%|â–         | 41/1000 [00:08<02:33,  6.26it/s]Ref scores:   4%|â–         | 42/1000 [00:08<02:29,  6.39it/s]Ref scores:   4%|â–         | 43/1000 [00:08<02:27,  6.51it/s]Ref scores:   4%|â–         | 44/1000 [00:09<02:25,  6.58it/s]Ref scores:   4%|â–         | 45/1000 [00:09<02:23,  6.63it/s]Ref scores:   5%|â–         | 46/1000 [00:09<02:43,  5.83it/s]Ref scores:   5%|â–         | 47/1000 [00:09<03:06,  5.12it/s]Ref scores:   5%|â–         | 48/1000 [00:09<02:48,  5.63it/s]Ref scores:   5%|â–         | 49/1000 [00:09<02:40,  5.92it/s]Ref scores:   5%|â–Œ         | 50/1000 [00:10<02:34,  6.16it/s]Ref scores:   5%|â–Œ         | 51/1000 [00:10<02:29,  6.34it/s]Ref scores:   5%|â–Œ         | 52/1000 [00:10<02:44,  5.77it/s]Ref scores:   5%|â–Œ         | 53/1000 [00:10<02:57,  5.33it/s]Ref scores:   5%|â–Œ         | 54/1000 [00:10<02:46,  5.67it/s]Ref scores:   6%|â–Œ         | 55/1000 [00:10<02:28,  6.38it/s]Ref scores:   6%|â–Œ         | 56/1000 [00:11<02:25,  6.47it/s]Ref scores:   6%|â–Œ         | 57/1000 [00:11<02:23,  6.59it/s]Ref scores:   6%|â–Œ         | 58/1000 [00:11<02:17,  6.86it/s]Ref scores:   6%|â–Œ         | 59/1000 [00:11<02:17,  6.84it/s]Ref scores:   6%|â–Œ         | 60/1000 [00:11<02:49,  5.55it/s]Ref scores:   6%|â–Œ         | 61/1000 [00:11<02:40,  5.84it/s]Ref scores:   6%|â–Œ         | 62/1000 [00:12<03:03,  5.13it/s]Ref scores:   6%|â–‹         | 63/1000 [00:12<02:49,  5.54it/s]Ref scores:   6%|â–‹         | 64/1000 [00:12<02:38,  5.89it/s]Ref scores:   6%|â–‹         | 65/1000 [00:12<02:33,  6.10it/s]Ref scores:   7%|â–‹         | 66/1000 [00:12<02:29,  6.26it/s]Ref scores:   7%|â–‹         | 67/1000 [00:12<02:26,  6.39it/s]Ref scores:   7%|â–‹         | 68/1000 [00:13<02:40,  5.82it/s]Ref scores:   7%|â–‹         | 69/1000 [00:13<02:32,  6.11it/s]Ref scores:   7%|â–‹         | 70/1000 [00:13<02:48,  5.53it/s]Ref scores:   7%|â–‹         | 71/1000 [00:13<02:57,  5.24it/s]Ref scores:   7%|â–‹         | 72/1000 [00:13<02:44,  5.65it/s]Ref scores:   7%|â–‹         | 73/1000 [00:13<02:25,  6.37it/s]Ref scores:   7%|â–‹         | 74/1000 [00:14<02:38,  5.83it/s]Ref scores:   8%|â–Š         | 75/1000 [00:14<02:31,  6.09it/s]Ref scores:   8%|â–Š         | 76/1000 [00:14<02:45,  5.57it/s]Ref scores:   8%|â–Š         | 77/1000 [00:14<02:36,  5.90it/s]Ref scores:   8%|â–Š         | 78/1000 [00:14<02:30,  6.11it/s]Ref scores:   8%|â–Š         | 79/1000 [00:15<02:45,  5.56it/s]Ref scores:   8%|â–Š         | 80/1000 [00:15<02:37,  5.85it/s]Ref scores:   8%|â–Š         | 81/1000 [00:15<02:35,  5.90it/s]Ref scores:   8%|â–Š         | 82/1000 [00:15<02:28,  6.16it/s]Ref scores:   8%|â–Š         | 83/1000 [00:15<02:24,  6.34it/s]Ref scores:   8%|â–Š         | 84/1000 [00:15<02:21,  6.45it/s]Ref scores:   8%|â–Š         | 85/1000 [00:15<02:19,  6.54it/s]Ref scores:   9%|â–Š         | 86/1000 [00:16<02:19,  6.56it/s]Ref scores:   9%|â–Š         | 87/1000 [00:16<02:38,  5.77it/s]Ref scores:   9%|â–‰         | 88/1000 [00:16<02:30,  6.05it/s]Ref scores:   9%|â–‰         | 89/1000 [00:16<02:44,  5.53it/s]Ref scores:   9%|â–‰         | 90/1000 [00:16<02:35,  5.85it/s]Ref scores:   9%|â–‰         | 91/1000 [00:16<02:29,  6.09it/s]Ref scores:   9%|â–‰         | 92/1000 [00:17<02:44,  5.53it/s]Ref scores:   9%|â–‰         | 93/1000 [00:17<02:42,  5.56it/s]Ref scores:   9%|â–‰         | 94/1000 [00:17<02:34,  5.87it/s]Ref scores:  10%|â–‰         | 95/1000 [00:17<02:26,  6.16it/s]Ref scores:  10%|â–‰         | 96/1000 [00:17<02:23,  6.29it/s]Ref scores:  10%|â–‰         | 97/1000 [00:17<02:20,  6.42it/s]Ref scores:  10%|â–‰         | 98/1000 [00:18<02:17,  6.54it/s]Ref scores:  10%|â–‰         | 99/1000 [00:18<02:33,  5.88it/s]Ref scores:  10%|â–ˆ         | 100/1000 [00:18<02:27,  6.09it/s]Ref scores:  10%|â–ˆ         | 101/1000 [00:18<02:41,  5.56it/s]Ref scores:  10%|â–ˆ         | 102/1000 [00:18<02:32,  5.89it/s]Ref scores:  10%|â–ˆ         | 103/1000 [00:19<02:45,  5.43it/s]Ref scores:  10%|â–ˆ         | 104/1000 [00:19<02:35,  5.77it/s]Ref scores:  10%|â–ˆ         | 105/1000 [00:19<02:25,  6.16it/s]Ref scores:  11%|â–ˆ         | 106/1000 [00:19<02:22,  6.28it/s]Ref scores:  11%|â–ˆ         | 107/1000 [00:19<02:16,  6.56it/s]Ref scores:  11%|â–ˆ         | 108/1000 [00:19<02:14,  6.64it/s]Ref scores:  11%|â–ˆ         | 109/1000 [00:20<02:41,  5.52it/s]Ref scores:  11%|â–ˆ         | 110/1000 [00:20<02:32,  5.85it/s]Ref scores:  11%|â–ˆ         | 111/1000 [00:20<02:25,  6.13it/s]Ref scores:  11%|â–ˆ         | 112/1000 [00:20<02:20,  6.31it/s]Ref scores:  11%|â–ˆâ–        | 113/1000 [00:20<02:16,  6.48it/s]Ref scores:  11%|â–ˆâ–        | 114/1000 [00:20<02:34,  5.74it/s]Ref scores:  12%|â–ˆâ–        | 115/1000 [00:21<02:55,  5.03it/s]Ref scores:  12%|â–ˆâ–        | 116/1000 [00:21<03:10,  4.64it/s]Ref scores:  12%|â–ˆâ–        | 117/1000 [00:21<02:52,  5.12it/s]Ref scores:  12%|â–ˆâ–        | 118/1000 [00:21<02:57,  4.97it/s]Ref scores:  12%|â–ˆâ–        | 119/1000 [00:21<03:11,  4.60it/s]Ref scores:  12%|â–ˆâ–        | 120/1000 [00:22<02:57,  4.96it/s]Ref scores:  12%|â–ˆâ–        | 121/1000 [00:22<02:43,  5.37it/s]Ref scores:  12%|â–ˆâ–        | 122/1000 [00:22<02:34,  5.69it/s]Ref scores:  12%|â–ˆâ–        | 123/1000 [00:22<02:46,  5.25it/s]Ref scores:  12%|â–ˆâ–        | 124/1000 [00:22<03:04,  4.74it/s]Ref scores:  12%|â–ˆâ–Ž        | 125/1000 [00:23<02:47,  5.21it/s]Ref scores:  13%|â–ˆâ–Ž        | 126/1000 [00:23<02:52,  5.06it/s]Ref scores:  13%|â–ˆâ–Ž        | 127/1000 [00:23<02:40,  5.44it/s]Ref scores:  13%|â–ˆâ–Ž        | 128/1000 [00:23<02:48,  5.16it/s]Ref scores:  13%|â–ˆâ–Ž        | 129/1000 [00:23<02:36,  5.56it/s]Ref scores:  13%|â–ˆâ–Ž        | 130/1000 [00:23<02:23,  6.06it/s]Ref scores:  13%|â–ˆâ–Ž        | 131/1000 [00:24<02:23,  6.04it/s]Ref scores:  13%|â–ˆâ–Ž        | 132/1000 [00:24<02:46,  5.22it/s]Ref scores:  13%|â–ˆâ–Ž        | 133/1000 [00:24<02:34,  5.60it/s]Ref scores:  13%|â–ˆâ–Ž        | 134/1000 [00:24<02:26,  5.90it/s]Ref scores:  14%|â–ˆâ–Ž        | 135/1000 [00:24<02:49,  5.11it/s]Ref scores:  14%|â–ˆâ–Ž        | 136/1000 [00:25<02:37,  5.50it/s]Ref scores:  14%|â–ˆâ–Ž        | 137/1000 [00:25<02:46,  5.19it/s]Ref scores:  14%|â–ˆâ–        | 138/1000 [00:25<02:50,  5.06it/s]Ref scores:  14%|â–ˆâ–        | 139/1000 [00:25<02:37,  5.45it/s]Ref scores:  14%|â–ˆâ–        | 140/1000 [00:25<02:28,  5.78it/s]Ref scores:  14%|â–ˆâ–        | 141/1000 [00:25<02:22,  6.04it/s]Ref scores:  14%|â–ˆâ–        | 142/1000 [00:26<02:07,  6.71it/s]Ref scores:  14%|â–ˆâ–        | 143/1000 [00:26<02:04,  6.87it/s]Ref scores:  14%|â–ˆâ–        | 144/1000 [00:26<02:20,  6.09it/s]Ref scores:  14%|â–ˆâ–        | 145/1000 [00:26<02:16,  6.27it/s]Ref scores:  15%|â–ˆâ–        | 146/1000 [00:26<02:12,  6.44it/s]Ref scores:  15%|â–ˆâ–        | 147/1000 [00:26<02:15,  6.30it/s]Ref scores:  15%|â–ˆâ–        | 148/1000 [00:27<02:17,  6.21it/s]Ref scores:  15%|â–ˆâ–        | 149/1000 [00:27<02:29,  5.69it/s]Ref scores:  15%|â–ˆâ–Œ        | 150/1000 [00:27<02:49,  5.03it/s]Ref scores:  15%|â–ˆâ–Œ        | 151/1000 [00:27<02:55,  4.85it/s]Ref scores:  15%|â–ˆâ–Œ        | 152/1000 [00:27<03:16,  4.32it/s]Ref scores:  15%|â–ˆâ–Œ        | 153/1000 [00:28<03:13,  4.38it/s]Ref scores:  15%|â–ˆâ–Œ        | 154/1000 [00:28<02:51,  4.93it/s]Ref scores:  16%|â–ˆâ–Œ        | 155/1000 [00:28<02:36,  5.39it/s]Ref scores:  16%|â–ˆâ–Œ        | 156/1000 [00:28<02:24,  5.84it/s]Ref scores:  16%|â–ˆâ–Œ        | 157/1000 [00:28<02:16,  6.16it/s]Ref scores:  16%|â–ˆâ–Œ        | 158/1000 [00:28<02:02,  6.87it/s]Ref scores:  16%|â–ˆâ–Œ        | 159/1000 [00:29<02:20,  5.96it/s]Ref scores:  16%|â–ˆâ–Œ        | 160/1000 [00:29<02:06,  6.66it/s]Ref scores:  16%|â–ˆâ–Œ        | 161/1000 [00:29<02:04,  6.75it/s]Ref scores:  16%|â–ˆâ–Œ        | 162/1000 [00:29<02:07,  6.56it/s]Ref scores:  16%|â–ˆâ–‹        | 163/1000 [00:29<01:56,  7.16it/s]Ref scores:  16%|â–ˆâ–‹        | 164/1000 [00:29<02:14,  6.20it/s]Ref scores:  16%|â–ˆâ–‹        | 165/1000 [00:29<02:10,  6.41it/s]Ref scores:  17%|â–ˆâ–‹        | 166/1000 [00:30<02:11,  6.36it/s]Ref scores:  17%|â–ˆâ–‹        | 167/1000 [00:30<02:07,  6.54it/s]Ref scores:  17%|â–ˆâ–‹        | 168/1000 [00:30<02:04,  6.70it/s]Ref scores:  17%|â–ˆâ–‹        | 169/1000 [00:30<02:07,  6.54it/s]Ref scores:  17%|â–ˆâ–‹        | 170/1000 [00:30<02:04,  6.64it/s]Ref scores:  17%|â–ˆâ–‹        | 171/1000 [00:30<02:21,  5.88it/s]Ref scores:  17%|â–ˆâ–‹        | 172/1000 [00:31<02:42,  5.09it/s]Ref scores:  17%|â–ˆâ–‹        | 173/1000 [00:31<02:29,  5.54it/s]Ref scores:  17%|â–ˆâ–‹        | 174/1000 [00:31<02:20,  5.90it/s]Ref scores:  18%|â–ˆâ–Š        | 175/1000 [00:31<02:32,  5.41it/s]Ref scores:  18%|â–ˆâ–Š        | 176/1000 [00:31<02:21,  5.83it/s]Ref scores:  18%|â–ˆâ–Š        | 177/1000 [00:31<02:12,  6.19it/s]Ref scores:  18%|â–ˆâ–Š        | 178/1000 [00:32<02:11,  6.24it/s]Ref scores:  18%|â–ˆâ–Š        | 179/1000 [00:32<02:07,  6.45it/s]Ref scores:  18%|â–ˆâ–Š        | 180/1000 [00:32<02:03,  6.67it/s]Ref scores:  18%|â–ˆâ–Š        | 181/1000 [00:32<01:52,  7.26it/s]Ref scores:  18%|â–ˆâ–Š        | 182/1000 [00:32<01:45,  7.76it/s]Ref scores:  18%|â–ˆâ–Š        | 183/1000 [00:32<01:47,  7.60it/s]Ref scores:  18%|â–ˆâ–Š        | 184/1000 [00:32<01:49,  7.42it/s]Ref scores:  18%|â–ˆâ–Š        | 185/1000 [00:33<01:51,  7.31it/s]Ref scores:  19%|â–ˆâ–Š        | 186/1000 [00:33<01:56,  6.99it/s]Ref scores:  19%|â–ˆâ–Š        | 187/1000 [00:33<01:55,  7.05it/s]Ref scores:  19%|â–ˆâ–‰        | 188/1000 [00:33<01:55,  7.03it/s]Ref scores:  19%|â–ˆâ–‰        | 189/1000 [00:33<01:54,  7.06it/s]Ref scores:  19%|â–ˆâ–‰        | 190/1000 [00:33<01:54,  7.06it/s]Ref scores:  19%|â–ˆâ–‰        | 191/1000 [00:33<01:54,  7.06it/s]Ref scores:  19%|â–ˆâ–‰        | 192/1000 [00:34<01:54,  7.05it/s]Ref scores:  19%|â–ˆâ–‰        | 193/1000 [00:34<02:09,  6.23it/s]Ref scores:  19%|â–ˆâ–‰        | 194/1000 [00:34<02:05,  6.42it/s]Ref scores:  20%|â–ˆâ–‰        | 195/1000 [00:34<02:07,  6.33it/s]Ref scores:  20%|â–ˆâ–‰        | 196/1000 [00:34<01:55,  6.96it/s]Ref scores:  20%|â–ˆâ–‰        | 197/1000 [00:34<02:29,  5.36it/s]Ref scores:  20%|â–ˆâ–‰        | 198/1000 [00:35<02:19,  5.75it/s]Ref scores:  20%|â–ˆâ–‰        | 199/1000 [00:35<02:11,  6.08it/s]Ref scores:  20%|â–ˆâ–ˆ        | 200/1000 [00:35<02:07,  6.28it/s]Ref scores:  20%|â–ˆâ–ˆ        | 201/1000 [00:35<02:20,  5.67it/s]Ref scores:  20%|â–ˆâ–ˆ        | 202/1000 [00:35<02:13,  5.97it/s]Ref scores:  20%|â–ˆâ–ˆ        | 203/1000 [00:35<02:23,  5.55it/s]Ref scores:  20%|â–ˆâ–ˆ        | 204/1000 [00:36<02:41,  4.91it/s]Ref scores:  20%|â–ˆâ–ˆ        | 205/1000 [00:36<02:19,  5.69it/s]Ref scores:  21%|â–ˆâ–ˆ        | 206/1000 [00:36<02:13,  5.96it/s]Ref scores:  21%|â–ˆâ–ˆ        | 207/1000 [00:36<02:08,  6.15it/s]Ref scores:  21%|â–ˆâ–ˆ        | 208/1000 [00:36<02:31,  5.22it/s]Ref scores:  21%|â–ˆâ–ˆ        | 209/1000 [00:37<02:21,  5.61it/s]Ref scores:  21%|â–ˆâ–ˆ        | 210/1000 [00:37<02:11,  5.99it/s]Ref scores:  21%|â–ˆâ–ˆ        | 211/1000 [00:37<02:03,  6.37it/s]Ref scores:  21%|â–ˆâ–ˆ        | 212/1000 [00:37<02:00,  6.52it/s]Ref scores:  21%|â–ˆâ–ˆâ–       | 213/1000 [00:37<02:15,  5.82it/s]Ref scores:  21%|â–ˆâ–ˆâ–       | 214/1000 [00:37<02:08,  6.10it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 215/1000 [00:38<02:20,  5.59it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 216/1000 [00:38<02:12,  5.94it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 217/1000 [00:38<02:05,  6.23it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 218/1000 [00:38<02:16,  5.72it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 219/1000 [00:38<02:25,  5.39it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 220/1000 [00:38<02:14,  5.78it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 221/1000 [00:39<02:12,  5.90it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 222/1000 [00:39<02:06,  6.15it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 223/1000 [00:39<02:06,  6.15it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 224/1000 [00:39<02:02,  6.32it/s]Ref scores:  22%|â–ˆâ–ˆâ–Ž       | 225/1000 [00:39<02:32,  5.07it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 226/1000 [00:40<02:38,  4.89it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 227/1000 [00:40<02:24,  5.37it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 228/1000 [00:40<02:30,  5.13it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 229/1000 [00:40<02:18,  5.56it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 230/1000 [00:40<02:24,  5.33it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 231/1000 [00:40<02:29,  5.15it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 232/1000 [00:41<02:17,  5.59it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 233/1000 [00:41<02:23,  5.35it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 234/1000 [00:41<02:13,  5.73it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 235/1000 [00:41<02:06,  6.05it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 236/1000 [00:41<02:01,  6.28it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 237/1000 [00:41<01:50,  6.90it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 238/1000 [00:41<01:50,  6.90it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 239/1000 [00:42<02:07,  5.98it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 240/1000 [00:42<02:01,  6.27it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 241/1000 [00:42<01:57,  6.49it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 242/1000 [00:42<02:12,  5.72it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 243/1000 [00:42<02:05,  6.04it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 244/1000 [00:43<01:59,  6.32it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 245/1000 [00:43<01:57,  6.45it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 246/1000 [00:43<01:55,  6.53it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 247/1000 [00:43<01:53,  6.66it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 248/1000 [00:43<02:17,  5.46it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 249/1000 [00:43<02:09,  5.81it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 250/1000 [00:44<02:26,  5.10it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 251/1000 [00:44<02:33,  4.89it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 252/1000 [00:44<02:34,  4.84it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 253/1000 [00:44<02:24,  5.16it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 254/1000 [00:44<02:28,  5.02it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 255/1000 [00:45<02:16,  5.44it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 256/1000 [00:45<02:08,  5.78it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 257/1000 [00:45<02:18,  5.38it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 258/1000 [00:45<02:12,  5.58it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 259/1000 [00:45<02:05,  5.89it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 260/1000 [00:45<01:52,  6.58it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 261/1000 [00:45<01:50,  6.67it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 262/1000 [00:46<02:13,  5.55it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 263/1000 [00:46<02:21,  5.22it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 264/1000 [00:46<02:10,  5.65it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 265/1000 [00:46<02:02,  5.99it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 266/1000 [00:46<01:57,  6.26it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 267/1000 [00:47<01:46,  6.89it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 268/1000 [00:47<01:46,  6.86it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 269/1000 [00:47<01:45,  6.92it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 270/1000 [00:47<01:38,  7.43it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 271/1000 [00:47<01:39,  7.30it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 272/1000 [00:47<01:41,  7.17it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 273/1000 [00:47<01:35,  7.65it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 274/1000 [00:48<02:00,  6.01it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 275/1000 [00:48<01:55,  6.25it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 276/1000 [00:48<01:45,  6.89it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 277/1000 [00:48<01:42,  7.05it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 278/1000 [00:48<01:43,  6.99it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 279/1000 [00:48<01:57,  6.12it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 280/1000 [00:49<02:09,  5.55it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 281/1000 [00:49<02:02,  5.87it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 282/1000 [00:49<01:55,  6.20it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 283/1000 [00:49<01:55,  6.19it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 284/1000 [00:49<02:15,  5.30it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 285/1000 [00:49<02:05,  5.68it/s]Ref scores:  29%|â–ˆâ–ˆâ–Š       | 286/1000 [00:50<01:58,  6.00it/s]Ref scores:  29%|â–ˆâ–ˆâ–Š       | 287/1000 [00:50<02:10,  5.46it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 288/1000 [00:50<02:25,  4.91it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 289/1000 [00:50<02:11,  5.42it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 290/1000 [00:50<02:02,  5.79it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 291/1000 [00:50<01:56,  6.07it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 292/1000 [00:51<02:05,  5.64it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 293/1000 [00:51<01:50,  6.38it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 294/1000 [00:51<01:47,  6.58it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 295/1000 [00:51<01:44,  6.72it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 296/1000 [00:51<01:43,  6.78it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 297/1000 [00:51<01:42,  6.83it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 298/1000 [00:52<01:55,  6.09it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 299/1000 [00:52<01:51,  6.26it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 300/1000 [00:52<02:01,  5.74it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 301/1000 [00:52<01:54,  6.08it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 302/1000 [00:52<01:50,  6.34it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 303/1000 [00:52<01:46,  6.52it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 304/1000 [00:52<01:38,  7.05it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 305/1000 [00:53<01:52,  6.20it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 306/1000 [00:53<01:48,  6.39it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 307/1000 [00:53<02:01,  5.72it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 308/1000 [00:53<01:53,  6.07it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 309/1000 [00:53<01:53,  6.09it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 310/1000 [00:53<01:54,  6.03it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 311/1000 [00:54<01:42,  6.69it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 312/1000 [00:54<01:45,  6.53it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆâ–      | 313/1000 [00:54<01:43,  6.64it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆâ–      | 314/1000 [00:54<02:03,  5.54it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 315/1000 [00:54<01:56,  5.89it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 316/1000 [00:54<01:54,  5.98it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 317/1000 [00:55<01:46,  6.42it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 318/1000 [00:55<01:42,  6.63it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 319/1000 [00:55<01:40,  6.76it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 320/1000 [00:55<01:39,  6.86it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 321/1000 [00:55<01:52,  6.04it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 322/1000 [00:55<01:48,  6.26it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 323/1000 [00:55<01:44,  6.46it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 324/1000 [00:56<01:41,  6.64it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 325/1000 [00:56<01:42,  6.61it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 326/1000 [00:56<01:39,  6.75it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 327/1000 [00:56<01:38,  6.83it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 328/1000 [00:56<01:36,  6.96it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 329/1000 [00:56<01:51,  6.02it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 330/1000 [00:57<01:43,  6.44it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 331/1000 [00:57<02:03,  5.41it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 332/1000 [00:57<02:09,  5.15it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 333/1000 [00:57<02:13,  5.01it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 334/1000 [00:57<02:16,  4.86it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 335/1000 [00:58<02:03,  5.38it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 336/1000 [00:58<01:54,  5.82it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 337/1000 [00:58<01:48,  6.14it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 338/1000 [00:58<01:43,  6.40it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 339/1000 [00:58<01:56,  5.69it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 340/1000 [00:58<02:01,  5.42it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 341/1000 [00:59<01:52,  5.85it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 342/1000 [00:59<01:39,  6.59it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 343/1000 [00:59<01:37,  6.76it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 344/1000 [00:59<01:59,  5.49it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 345/1000 [00:59<02:04,  5.27it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 346/1000 [00:59<01:58,  5.54it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 347/1000 [01:00<01:49,  5.96it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 348/1000 [01:00<01:44,  6.26it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 349/1000 [01:00<01:44,  6.21it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 350/1000 [01:00<01:40,  6.46it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 351/1000 [01:00<01:39,  6.52it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 352/1000 [01:00<01:37,  6.68it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 353/1000 [01:00<01:35,  6.76it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 354/1000 [01:01<01:34,  6.87it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 355/1000 [01:01<01:32,  6.94it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 356/1000 [01:01<01:53,  5.68it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 357/1000 [01:01<01:40,  6.40it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 358/1000 [01:01<01:31,  7.03it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 359/1000 [01:01<01:30,  7.05it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 360/1000 [01:02<01:45,  6.06it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 361/1000 [01:02<02:03,  5.19it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 362/1000 [01:02<01:52,  5.67it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 363/1000 [01:02<02:01,  5.26it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 364/1000 [01:02<01:51,  5.72it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 365/1000 [01:02<01:44,  6.07it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 366/1000 [01:03<01:54,  5.54it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 367/1000 [01:03<01:46,  5.95it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 368/1000 [01:03<01:40,  6.28it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 369/1000 [01:03<01:51,  5.66it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 370/1000 [01:03<01:47,  5.83it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 371/1000 [01:04<01:54,  5.48it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 372/1000 [01:04<01:46,  5.90it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 373/1000 [01:04<01:38,  6.34it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 374/1000 [01:04<01:37,  6.45it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 375/1000 [01:04<01:34,  6.61it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 376/1000 [01:04<01:46,  5.84it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 377/1000 [01:05<01:54,  5.43it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 378/1000 [01:05<01:49,  5.67it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 379/1000 [01:05<02:03,  5.04it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 380/1000 [01:05<01:52,  5.50it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 381/1000 [01:05<01:57,  5.25it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 382/1000 [01:05<01:51,  5.54it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 383/1000 [01:06<01:44,  5.91it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 384/1000 [01:06<01:39,  6.19it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 385/1000 [01:06<01:55,  5.31it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 386/1000 [01:06<02:07,  4.82it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 387/1000 [01:06<01:58,  5.19it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 388/1000 [01:07<01:48,  5.65it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 389/1000 [01:07<01:41,  6.04it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 390/1000 [01:07<01:30,  6.71it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 391/1000 [01:07<01:40,  6.06it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 392/1000 [01:07<01:36,  6.32it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 393/1000 [01:07<01:36,  6.31it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 394/1000 [01:07<01:32,  6.55it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 395/1000 [01:08<01:33,  6.46it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 396/1000 [01:08<01:52,  5.39it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 397/1000 [01:08<01:41,  5.93it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 398/1000 [01:08<01:36,  6.23it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 399/1000 [01:08<01:32,  6.47it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 400/1000 [01:08<01:30,  6.64it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 401/1000 [01:09<01:28,  6.74it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 402/1000 [01:09<01:27,  6.80it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 403/1000 [01:09<01:27,  6.83it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 404/1000 [01:09<01:26,  6.85it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 405/1000 [01:09<01:37,  6.07it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 406/1000 [01:09<01:54,  5.20it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 407/1000 [01:10<02:05,  4.74it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 408/1000 [01:10<01:50,  5.36it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 409/1000 [01:10<01:42,  5.78it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 410/1000 [01:10<01:30,  6.50it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 411/1000 [01:10<01:22,  7.11it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 412/1000 [01:10<01:34,  6.25it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 413/1000 [01:11<01:30,  6.45it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 414/1000 [01:11<01:28,  6.65it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 415/1000 [01:11<01:26,  6.74it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 416/1000 [01:11<01:36,  6.04it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 417/1000 [01:11<01:31,  6.37it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 418/1000 [01:11<01:42,  5.68it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 419/1000 [01:12<01:36,  6.02it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 420/1000 [01:12<01:31,  6.34it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 421/1000 [01:12<01:39,  5.80it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 422/1000 [01:12<01:48,  5.34it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 423/1000 [01:12<01:51,  5.16it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 424/1000 [01:13<02:01,  4.75it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 425/1000 [01:13<01:49,  5.27it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 426/1000 [01:13<01:39,  5.75it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 427/1000 [01:13<01:37,  5.90it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 428/1000 [01:13<01:32,  6.17it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 429/1000 [01:13<01:23,  6.83it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 430/1000 [01:13<01:22,  6.90it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 431/1000 [01:14<01:21,  7.00it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 432/1000 [01:14<01:19,  7.14it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 433/1000 [01:14<01:22,  6.88it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 434/1000 [01:14<01:21,  6.96it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 435/1000 [01:14<01:31,  6.14it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 436/1000 [01:14<01:28,  6.38it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 437/1000 [01:14<01:28,  6.35it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 438/1000 [01:15<01:28,  6.34it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 439/1000 [01:15<01:44,  5.38it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 440/1000 [01:15<01:48,  5.14it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 441/1000 [01:15<01:53,  4.94it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 442/1000 [01:16<01:55,  4.85it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 443/1000 [01:16<01:44,  5.35it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 444/1000 [01:16<01:35,  5.80it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 445/1000 [01:16<01:30,  6.12it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 446/1000 [01:16<01:45,  5.27it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 447/1000 [01:16<01:37,  5.69it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 448/1000 [01:17<01:43,  5.34it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 449/1000 [01:17<01:53,  4.85it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 450/1000 [01:17<01:45,  5.21it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 451/1000 [01:17<01:54,  4.78it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 452/1000 [01:17<01:38,  5.59it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 453/1000 [01:17<01:26,  6.32it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 454/1000 [01:18<01:24,  6.46it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 455/1000 [01:18<01:22,  6.60it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 456/1000 [01:18<01:20,  6.72it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 457/1000 [01:18<01:14,  7.31it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 458/1000 [01:18<01:09,  7.78it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 459/1000 [01:18<01:11,  7.52it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 460/1000 [01:18<01:11,  7.57it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 461/1000 [01:19<01:12,  7.40it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 462/1000 [01:19<01:23,  6.43it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 463/1000 [01:19<01:21,  6.62it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 464/1000 [01:19<01:19,  6.74it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 465/1000 [01:19<01:18,  6.82it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 466/1000 [01:19<01:17,  6.91it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 467/1000 [01:19<01:11,  7.46it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 468/1000 [01:20<01:12,  7.37it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 469/1000 [01:20<01:13,  7.27it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 470/1000 [01:20<01:11,  7.40it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 471/1000 [01:20<01:24,  6.27it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 472/1000 [01:20<01:19,  6.65it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 473/1000 [01:20<01:28,  5.94it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 474/1000 [01:20<01:22,  6.38it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 475/1000 [01:21<01:20,  6.54it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 476/1000 [01:21<01:21,  6.44it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 477/1000 [01:21<01:27,  5.99it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 478/1000 [01:21<01:22,  6.31it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 479/1000 [01:21<01:19,  6.53it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 480/1000 [01:21<01:18,  6.64it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 481/1000 [01:22<01:11,  7.26it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 482/1000 [01:22<01:12,  7.16it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 483/1000 [01:22<01:23,  6.17it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 484/1000 [01:22<01:18,  6.57it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 485/1000 [01:22<01:16,  6.71it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 486/1000 [01:22<01:15,  6.84it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 487/1000 [01:22<01:14,  6.90it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 488/1000 [01:23<01:23,  6.10it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 489/1000 [01:23<01:20,  6.36it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 490/1000 [01:23<01:17,  6.55it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 491/1000 [01:23<01:15,  6.71it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 492/1000 [01:23<01:14,  6.83it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 493/1000 [01:23<01:13,  6.89it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 494/1000 [01:23<01:08,  7.44it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 495/1000 [01:24<01:08,  7.37it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 496/1000 [01:24<01:19,  6.36it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 497/1000 [01:24<01:16,  6.57it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 498/1000 [01:24<01:10,  7.16it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 499/1000 [01:24<01:20,  6.23it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 500/1000 [01:24<01:27,  5.73it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 501/1000 [01:25<01:39,  5.03it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 502/1000 [01:25<01:30,  5.51it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 503/1000 [01:25<01:24,  5.89it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 504/1000 [01:25<01:36,  5.15it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 505/1000 [01:25<01:28,  5.60it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 506/1000 [01:26<01:34,  5.25it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 507/1000 [01:26<01:26,  5.69it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 508/1000 [01:26<01:22,  5.98it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 509/1000 [01:26<01:35,  5.17it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 510/1000 [01:26<01:27,  5.61it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 511/1000 [01:26<01:17,  6.34it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 512/1000 [01:27<01:17,  6.30it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 513/1000 [01:27<01:15,  6.48it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 514/1000 [01:27<01:22,  5.87it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 515/1000 [01:27<01:13,  6.58it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 516/1000 [01:27<01:27,  5.50it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 517/1000 [01:27<01:22,  5.86it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 518/1000 [01:28<01:16,  6.29it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 519/1000 [01:28<01:13,  6.52it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 520/1000 [01:28<01:22,  5.82it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 521/1000 [01:28<01:18,  6.11it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 522/1000 [01:28<01:15,  6.36it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 523/1000 [01:28<01:08,  6.96it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 524/1000 [01:28<01:09,  6.89it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 525/1000 [01:29<01:19,  5.96it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 526/1000 [01:29<01:16,  6.17it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 527/1000 [01:29<01:14,  6.35it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 528/1000 [01:29<01:13,  6.46it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 529/1000 [01:29<01:11,  6.56it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 530/1000 [01:29<01:20,  5.82it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 531/1000 [01:30<01:17,  6.09it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 532/1000 [01:30<01:14,  6.29it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 533/1000 [01:30<01:22,  5.65it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 534/1000 [01:30<01:18,  5.93it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 535/1000 [01:30<01:15,  6.18it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 536/1000 [01:30<01:12,  6.42it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 537/1000 [01:31<01:26,  5.37it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 538/1000 [01:31<01:20,  5.75it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 539/1000 [01:31<01:25,  5.38it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 540/1000 [01:31<01:20,  5.73it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 541/1000 [01:31<01:18,  5.83it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 542/1000 [01:32<01:14,  6.13it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 543/1000 [01:32<01:22,  5.53it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 544/1000 [01:32<01:26,  5.26it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 545/1000 [01:32<01:22,  5.49it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 546/1000 [01:32<01:18,  5.82it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 547/1000 [01:32<01:09,  6.51it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 548/1000 [01:33<01:18,  5.75it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 549/1000 [01:33<01:23,  5.42it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 550/1000 [01:33<01:17,  5.80it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 551/1000 [01:33<01:22,  5.42it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 552/1000 [01:33<01:17,  5.76it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 553/1000 [01:34<01:23,  5.38it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 554/1000 [01:34<01:27,  5.12it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 555/1000 [01:34<01:35,  4.67it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 556/1000 [01:34<01:25,  5.16it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 557/1000 [01:34<01:18,  5.61it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 558/1000 [01:34<01:14,  5.94it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 559/1000 [01:35<01:11,  6.20it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 560/1000 [01:35<01:08,  6.39it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 561/1000 [01:35<01:15,  5.78it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 562/1000 [01:35<01:12,  6.04it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 563/1000 [01:35<01:05,  6.69it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 564/1000 [01:35<01:13,  5.97it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 565/1000 [01:36<01:09,  6.25it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 566/1000 [01:36<01:15,  5.75it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 567/1000 [01:36<01:20,  5.41it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 568/1000 [01:36<01:14,  5.78it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 569/1000 [01:36<01:11,  6.03it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 570/1000 [01:36<01:17,  5.52it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 571/1000 [01:37<01:08,  6.25it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 572/1000 [01:37<01:04,  6.59it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 573/1000 [01:37<01:17,  5.52it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 574/1000 [01:37<01:12,  5.91it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 575/1000 [01:37<01:09,  6.15it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 576/1000 [01:37<01:14,  5.67it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 577/1000 [01:38<01:10,  5.98it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 578/1000 [01:38<01:20,  5.21it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 579/1000 [01:38<01:14,  5.63it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 580/1000 [01:38<01:10,  6.00it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 581/1000 [01:38<01:02,  6.66it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 582/1000 [01:38<01:02,  6.74it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 583/1000 [01:39<01:09,  6.02it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 584/1000 [01:39<01:15,  5.50it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 585/1000 [01:39<01:10,  5.86it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 586/1000 [01:39<01:15,  5.47it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 587/1000 [01:39<01:10,  5.84it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 588/1000 [01:40<01:20,  5.09it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 589/1000 [01:40<01:14,  5.54it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 590/1000 [01:40<01:05,  6.25it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 591/1000 [01:40<01:12,  5.61it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 592/1000 [01:40<01:10,  5.76it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 593/1000 [01:40<01:16,  5.34it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 594/1000 [01:41<01:10,  5.75it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 595/1000 [01:41<01:06,  6.08it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 596/1000 [01:41<01:12,  5.55it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 597/1000 [01:41<01:04,  6.27it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 598/1000 [01:41<00:58,  6.91it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 599/1000 [01:41<01:14,  5.36it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 600/1000 [01:42<01:05,  6.11it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 601/1000 [01:42<01:03,  6.31it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 602/1000 [01:42<01:01,  6.52it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 603/1000 [01:42<00:59,  6.63it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 604/1000 [01:42<00:57,  6.88it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 605/1000 [01:42<00:57,  6.92it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 606/1000 [01:42<00:56,  6.94it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 607/1000 [01:43<00:52,  7.44it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 608/1000 [01:43<01:01,  6.39it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 609/1000 [01:43<00:59,  6.58it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 610/1000 [01:43<01:00,  6.44it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 611/1000 [01:43<00:59,  6.57it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 612/1000 [01:43<01:11,  5.46it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 613/1000 [01:44<01:02,  6.19it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 614/1000 [01:44<01:00,  6.39it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 615/1000 [01:44<00:58,  6.58it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 616/1000 [01:44<00:57,  6.66it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 617/1000 [01:44<01:05,  5.87it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 618/1000 [01:44<01:04,  5.89it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 619/1000 [01:45<01:03,  5.97it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 620/1000 [01:45<01:13,  5.14it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 621/1000 [01:45<01:07,  5.61it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 622/1000 [01:45<01:11,  5.27it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 623/1000 [01:45<01:05,  5.76it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 624/1000 [01:45<01:09,  5.42it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 625/1000 [01:46<01:04,  5.84it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 626/1000 [01:46<01:00,  6.16it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 627/1000 [01:46<00:58,  6.42it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 628/1000 [01:46<00:56,  6.62it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 629/1000 [01:46<01:02,  5.95it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 630/1000 [01:46<00:58,  6.30it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 631/1000 [01:47<01:08,  5.37it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 632/1000 [01:47<01:03,  5.82it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 633/1000 [01:47<01:09,  5.31it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 634/1000 [01:47<01:03,  5.80it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 635/1000 [01:47<01:08,  5.36it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 636/1000 [01:47<01:02,  5.79it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 637/1000 [01:48<01:15,  4.83it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 638/1000 [01:48<01:07,  5.36it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 639/1000 [01:48<01:02,  5.76it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 640/1000 [01:48<01:15,  4.80it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 641/1000 [01:48<01:04,  5.58it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 642/1000 [01:49<00:59,  5.98it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 643/1000 [01:49<00:56,  6.27it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 644/1000 [01:49<01:02,  5.70it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 645/1000 [01:49<00:55,  6.39it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 646/1000 [01:49<00:53,  6.58it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 647/1000 [01:49<00:49,  7.13it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 648/1000 [01:49<00:49,  7.18it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 649/1000 [01:50<00:55,  6.31it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 650/1000 [01:50<00:53,  6.53it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 651/1000 [01:50<00:59,  5.86it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 652/1000 [01:50<00:56,  6.21it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 653/1000 [01:50<00:53,  6.50it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 654/1000 [01:51<00:58,  5.87it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 655/1000 [01:51<01:02,  5.50it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 656/1000 [01:51<00:55,  6.20it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 657/1000 [01:51<00:59,  5.74it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 658/1000 [01:51<00:55,  6.12it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 659/1000 [01:51<01:00,  5.65it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 660/1000 [01:52<00:56,  6.05it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 661/1000 [01:52<01:00,  5.64it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 662/1000 [01:52<01:04,  5.27it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 663/1000 [01:52<01:05,  5.13it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 664/1000 [01:52<00:59,  5.61it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 665/1000 [01:52<00:55,  6.03it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 666/1000 [01:53<01:00,  5.50it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 667/1000 [01:53<00:55,  5.95it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 668/1000 [01:53<00:53,  6.22it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 669/1000 [01:53<00:52,  6.25it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 670/1000 [01:53<00:50,  6.48it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 671/1000 [01:53<00:49,  6.65it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 672/1000 [01:54<00:48,  6.79it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 673/1000 [01:54<00:58,  5.58it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 674/1000 [01:54<00:54,  5.96it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 675/1000 [01:54<01:02,  5.17it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 676/1000 [01:54<01:08,  4.71it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 677/1000 [01:55<01:01,  5.24it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 678/1000 [01:55<00:56,  5.70it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 679/1000 [01:55<00:59,  5.39it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 680/1000 [01:55<01:00,  5.29it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 681/1000 [01:55<01:06,  4.77it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 682/1000 [01:55<00:58,  5.39it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 683/1000 [01:56<01:01,  5.16it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 684/1000 [01:56<00:56,  5.62it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 685/1000 [01:56<00:52,  5.98it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 686/1000 [01:56<00:50,  6.28it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 687/1000 [01:56<00:48,  6.50it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 688/1000 [01:56<00:46,  6.67it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 689/1000 [01:57<00:47,  6.55it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 690/1000 [01:57<00:43,  7.09it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 691/1000 [01:57<00:43,  7.04it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 692/1000 [01:57<00:43,  7.08it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 693/1000 [01:57<00:49,  6.16it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 694/1000 [01:57<00:47,  6.40it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 695/1000 [01:57<00:46,  6.58it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 696/1000 [01:58<00:45,  6.73it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 697/1000 [01:58<00:44,  6.85it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 698/1000 [01:58<00:50,  6.04it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 699/1000 [01:58<00:47,  6.34it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 700/1000 [01:58<00:45,  6.53it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 701/1000 [01:58<00:44,  6.68it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 702/1000 [01:59<00:51,  5.81it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 703/1000 [01:59<00:48,  6.18it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 704/1000 [01:59<00:45,  6.44it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 705/1000 [01:59<00:50,  5.84it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 706/1000 [01:59<00:47,  6.15it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 707/1000 [01:59<00:43,  6.73it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 708/1000 [01:59<00:42,  6.84it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 709/1000 [02:00<00:42,  6.90it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 710/1000 [02:00<00:39,  7.35it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 711/1000 [02:00<00:46,  6.15it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 712/1000 [02:00<00:44,  6.43it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 713/1000 [02:00<00:43,  6.61it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 714/1000 [02:00<00:48,  5.90it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 715/1000 [02:01<00:55,  5.11it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 716/1000 [02:01<00:48,  5.85it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 717/1000 [02:01<00:52,  5.37it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 718/1000 [02:01<00:54,  5.17it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 719/1000 [02:01<00:56,  4.99it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 720/1000 [02:02<00:51,  5.45it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 721/1000 [02:02<00:47,  5.84it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 722/1000 [02:02<00:43,  6.46it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 723/1000 [02:02<00:39,  7.02it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 724/1000 [02:02<00:36,  7.48it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 725/1000 [02:02<00:35,  7.81it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 726/1000 [02:02<00:35,  7.66it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 727/1000 [02:03<00:46,  5.90it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 728/1000 [02:03<00:55,  4.87it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 729/1000 [02:03<00:57,  4.75it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 730/1000 [02:03<00:50,  5.37it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 731/1000 [02:03<00:52,  5.09it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 732/1000 [02:04<00:54,  4.89it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 733/1000 [02:04<00:49,  5.37it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 734/1000 [02:04<00:46,  5.77it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 735/1000 [02:04<00:41,  6.41it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 736/1000 [02:04<00:40,  6.57it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 737/1000 [02:04<00:37,  7.08it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 738/1000 [02:05<00:43,  6.05it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 739/1000 [02:05<00:41,  6.34it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 740/1000 [02:05<00:39,  6.51it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 741/1000 [02:05<00:39,  6.63it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 742/1000 [02:05<00:38,  6.79it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 743/1000 [02:05<00:37,  6.85it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 744/1000 [02:06<00:42,  6.00it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 745/1000 [02:06<00:49,  5.13it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 746/1000 [02:06<00:50,  4.98it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 747/1000 [02:06<00:46,  5.47it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 748/1000 [02:06<00:42,  5.87it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 749/1000 [02:06<00:38,  6.49it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 750/1000 [02:07<00:37,  6.63it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 751/1000 [02:07<00:36,  6.75it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 752/1000 [02:07<00:34,  7.24it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 753/1000 [02:07<00:34,  7.22it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 754/1000 [02:07<00:39,  6.21it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 755/1000 [02:07<00:37,  6.48it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 756/1000 [02:07<00:42,  5.75it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 757/1000 [02:08<00:48,  5.05it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 758/1000 [02:08<00:43,  5.55it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 759/1000 [02:08<00:38,  6.25it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 760/1000 [02:08<00:37,  6.44it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 761/1000 [02:08<00:36,  6.55it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 762/1000 [02:09<00:43,  5.45it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 763/1000 [02:09<00:40,  5.86it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 764/1000 [02:09<00:38,  6.15it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 765/1000 [02:09<00:41,  5.60it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 766/1000 [02:09<00:39,  5.98it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 767/1000 [02:09<00:37,  6.28it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 768/1000 [02:09<00:35,  6.49it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 769/1000 [02:10<00:34,  6.68it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 770/1000 [02:10<00:33,  6.77it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 771/1000 [02:10<00:38,  6.00it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 772/1000 [02:10<00:41,  5.55it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 773/1000 [02:10<00:43,  5.20it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 774/1000 [02:11<00:41,  5.47it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 775/1000 [02:11<00:46,  4.88it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 776/1000 [02:11<00:41,  5.38it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 777/1000 [02:11<00:44,  5.06it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 778/1000 [02:11<00:40,  5.55it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 779/1000 [02:11<00:37,  5.93it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 780/1000 [02:12<00:35,  6.21it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 781/1000 [02:12<00:34,  6.40it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 782/1000 [02:12<00:38,  5.72it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 783/1000 [02:12<00:35,  6.19it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 784/1000 [02:12<00:33,  6.43it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 785/1000 [02:12<00:30,  6.98it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 786/1000 [02:12<00:30,  6.98it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 787/1000 [02:13<00:31,  6.74it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 788/1000 [02:13<00:32,  6.58it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 789/1000 [02:13<00:31,  6.70it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 790/1000 [02:13<00:30,  6.82it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 791/1000 [02:13<00:30,  6.87it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 792/1000 [02:13<00:29,  6.95it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 793/1000 [02:14<00:29,  6.94it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 794/1000 [02:14<00:34,  6.01it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 795/1000 [02:14<00:32,  6.29it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 796/1000 [02:14<00:31,  6.50it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 797/1000 [02:14<00:35,  5.72it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 798/1000 [02:14<00:33,  6.09it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 799/1000 [02:15<00:38,  5.19it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 800/1000 [02:15<00:35,  5.65it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 801/1000 [02:15<00:37,  5.36it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 802/1000 [02:15<00:34,  5.77it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 803/1000 [02:15<00:36,  5.47it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 804/1000 [02:15<00:33,  5.89it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 805/1000 [02:16<00:31,  6.16it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 806/1000 [02:16<00:30,  6.36it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 807/1000 [02:16<00:33,  5.72it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 808/1000 [02:16<00:36,  5.31it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 809/1000 [02:16<00:33,  5.74it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 810/1000 [02:17<00:35,  5.39it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 811/1000 [02:17<00:32,  5.79it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 812/1000 [02:17<00:34,  5.43it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 813/1000 [02:17<00:31,  5.85it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 814/1000 [02:17<00:30,  6.19it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 815/1000 [02:17<00:35,  5.26it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 816/1000 [02:18<00:32,  5.68it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 817/1000 [02:18<00:28,  6.33it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 818/1000 [02:18<00:28,  6.50it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 819/1000 [02:18<00:27,  6.65it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 820/1000 [02:18<00:30,  5.86it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 821/1000 [02:18<00:29,  6.16it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 822/1000 [02:19<00:32,  5.54it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 823/1000 [02:19<00:34,  5.18it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 824/1000 [02:19<00:31,  5.60it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 825/1000 [02:19<00:29,  5.99it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 826/1000 [02:19<00:31,  5.56it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 827/1000 [02:20<00:35,  4.94it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 828/1000 [02:20<00:35,  4.81it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 829/1000 [02:20<00:36,  4.73it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 830/1000 [02:20<00:32,  5.24it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 831/1000 [02:20<00:29,  5.66it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 832/1000 [02:21<00:31,  5.29it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 833/1000 [02:21<00:29,  5.74it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 834/1000 [02:21<00:27,  6.08it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 835/1000 [02:21<00:25,  6.35it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 836/1000 [02:21<00:30,  5.36it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 837/1000 [02:21<00:28,  5.79it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 838/1000 [02:21<00:26,  6.14it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 839/1000 [02:22<00:29,  5.54it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 840/1000 [02:22<00:30,  5.21it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 841/1000 [02:22<00:28,  5.66it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 842/1000 [02:22<00:29,  5.33it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 843/1000 [02:23<00:32,  4.79it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 844/1000 [02:23<00:29,  5.31it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 845/1000 [02:23<00:32,  4.74it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 846/1000 [02:23<00:29,  5.26it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 847/1000 [02:23<00:25,  5.94it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 848/1000 [02:23<00:23,  6.54it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 849/1000 [02:24<00:26,  5.76it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 850/1000 [02:24<00:24,  6.09it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 851/1000 [02:24<00:23,  6.31it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 852/1000 [02:24<00:27,  5.29it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 853/1000 [02:24<00:25,  5.71it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 854/1000 [02:24<00:22,  6.37it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 855/1000 [02:25<00:27,  5.35it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 856/1000 [02:25<00:27,  5.16it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 857/1000 [02:25<00:28,  5.00it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 858/1000 [02:25<00:28,  4.90it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 859/1000 [02:25<00:25,  5.64it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 860/1000 [02:25<00:23,  5.97it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 861/1000 [02:26<00:22,  6.25it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 862/1000 [02:26<00:24,  5.72it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 863/1000 [02:26<00:22,  6.07it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 864/1000 [02:26<00:24,  5.57it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 865/1000 [02:26<00:22,  5.94it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 866/1000 [02:27<00:25,  5.16it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 867/1000 [02:27<00:28,  4.68it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 868/1000 [02:27<00:29,  4.42it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 869/1000 [02:27<00:29,  4.44it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 870/1000 [02:28<00:28,  4.52it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 871/1000 [02:28<00:25,  5.07it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 872/1000 [02:28<00:23,  5.51it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 873/1000 [02:28<00:21,  5.86it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 874/1000 [02:28<00:20,  6.15it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 875/1000 [02:28<00:19,  6.35it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 876/1000 [02:28<00:19,  6.49it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 877/1000 [02:29<00:18,  6.63it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 878/1000 [02:29<00:20,  5.84it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 879/1000 [02:29<00:19,  6.07it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 880/1000 [02:29<00:22,  5.45it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 881/1000 [02:29<00:22,  5.18it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 882/1000 [02:29<00:20,  5.62it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 883/1000 [02:30<00:19,  5.98it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 884/1000 [02:30<00:20,  5.53it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 885/1000 [02:30<00:21,  5.26it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 886/1000 [02:30<00:23,  4.77it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 887/1000 [02:31<00:23,  4.84it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 888/1000 [02:31<00:21,  5.13it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 889/1000 [02:31<00:20,  5.53it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 890/1000 [02:31<00:21,  5.16it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 891/1000 [02:31<00:19,  5.59it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 892/1000 [02:31<00:18,  5.94it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 893/1000 [02:31<00:16,  6.63it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 894/1000 [02:32<00:15,  6.65it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 895/1000 [02:32<00:15,  6.66it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 896/1000 [02:32<00:19,  5.46it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 897/1000 [02:32<00:20,  5.11it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 898/1000 [02:32<00:18,  5.53it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 899/1000 [02:33<00:21,  4.67it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 900/1000 [02:33<00:19,  5.02it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 901/1000 [02:33<00:17,  5.79it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 902/1000 [02:33<00:18,  5.34it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 903/1000 [02:33<00:17,  5.55it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 904/1000 [02:33<00:15,  6.26it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 905/1000 [02:34<00:13,  6.88it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 906/1000 [02:34<00:16,  5.59it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 907/1000 [02:34<00:15,  5.91it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 908/1000 [02:34<00:15,  6.13it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 909/1000 [02:34<00:16,  5.52it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 910/1000 [02:34<00:15,  5.83it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 911/1000 [02:35<00:14,  6.14it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 912/1000 [02:35<00:16,  5.49it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 913/1000 [02:35<00:14,  5.88it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 914/1000 [02:35<00:13,  6.54it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 915/1000 [02:35<00:12,  6.64it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 916/1000 [02:35<00:14,  5.88it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 917/1000 [02:36<00:16,  5.11it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 918/1000 [02:36<00:14,  5.54it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 919/1000 [02:36<00:13,  5.88it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 920/1000 [02:36<00:13,  6.15it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 921/1000 [02:36<00:12,  6.37it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 922/1000 [02:36<00:12,  6.49it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 923/1000 [02:37<00:13,  5.72it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 924/1000 [02:37<00:11,  6.43it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 925/1000 [02:37<00:13,  5.39it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 926/1000 [02:37<00:12,  5.77it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 927/1000 [02:37<00:11,  6.46it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 928/1000 [02:37<00:10,  6.75it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 929/1000 [02:38<00:10,  6.77it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 930/1000 [02:38<00:12,  5.51it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 931/1000 [02:38<00:14,  4.89it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 932/1000 [02:38<00:12,  5.36it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 933/1000 [02:38<00:12,  5.52it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 934/1000 [02:39<00:11,  5.68it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 935/1000 [02:39<00:10,  5.98it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 936/1000 [02:39<00:10,  6.01it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 937/1000 [02:39<00:10,  6.00it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 938/1000 [02:39<00:11,  5.39it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 939/1000 [02:39<00:10,  5.77it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 940/1000 [02:40<00:09,  6.03it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 941/1000 [02:40<00:09,  6.03it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 942/1000 [02:40<00:10,  5.43it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 943/1000 [02:40<00:11,  5.05it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 944/1000 [02:40<00:10,  5.47it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 945/1000 [02:40<00:09,  5.84it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 946/1000 [02:41<00:09,  5.44it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 947/1000 [02:41<00:10,  5.18it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 948/1000 [02:41<00:10,  4.92it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 949/1000 [02:41<00:09,  5.38it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 950/1000 [02:41<00:08,  6.10it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 951/1000 [02:42<00:07,  6.76it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 952/1000 [02:42<00:06,  7.28it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 953/1000 [02:42<00:06,  6.88it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 954/1000 [02:42<00:07,  6.02it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 955/1000 [02:42<00:08,  5.55it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 956/1000 [02:42<00:07,  6.28it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 957/1000 [02:42<00:06,  6.43it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 958/1000 [02:43<00:06,  6.57it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 959/1000 [02:43<00:05,  7.16it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 960/1000 [02:43<00:05,  7.27it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 961/1000 [02:43<00:05,  7.10it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 962/1000 [02:43<00:06,  6.12it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 963/1000 [02:43<00:05,  6.33it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 964/1000 [02:43<00:05,  6.94it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 965/1000 [02:44<00:05,  6.91it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 966/1000 [02:44<00:05,  6.68it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 967/1000 [02:44<00:04,  6.73it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 968/1000 [02:44<00:04,  7.28it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 969/1000 [02:44<00:04,  7.73it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 970/1000 [02:44<00:04,  6.34it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 971/1000 [02:45<00:04,  6.48it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 972/1000 [02:45<00:04,  6.57it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 973/1000 [02:45<00:04,  5.86it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 974/1000 [02:45<00:04,  5.41it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 975/1000 [02:45<00:04,  5.60it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 976/1000 [02:45<00:04,  5.92it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 977/1000 [02:46<00:03,  6.58it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 978/1000 [02:46<00:03,  5.79it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 979/1000 [02:46<00:03,  6.09it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 980/1000 [02:46<00:03,  6.27it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 981/1000 [02:46<00:02,  6.44it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 982/1000 [02:46<00:03,  5.33it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 983/1000 [02:47<00:03,  5.53it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 984/1000 [02:47<00:03,  5.23it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 985/1000 [02:47<00:02,  5.63it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 986/1000 [02:47<00:02,  5.91it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 987/1000 [02:47<00:02,  6.15it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 988/1000 [02:47<00:01,  6.67it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 989/1000 [02:48<00:01,  6.76it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 990/1000 [02:48<00:01,  7.29it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 991/1000 [02:48<00:01,  5.68it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 992/1000 [02:48<00:01,  5.95it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 993/1000 [02:48<00:01,  6.34it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 994/1000 [02:48<00:01,  5.69it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 995/1000 [02:49<00:00,  5.80it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 996/1000 [02:49<00:00,  6.11it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 997/1000 [02:49<00:00,  5.47it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 998/1000 [02:49<00:00,  5.82it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 999/1000 [02:49<00:00,  6.10it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [02:49<00:00,  6.28it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [02:49<00:00,  5.89it/s]
DONE (2.28s)
DONE (8.16s)
loss_threshold ROC AUC: 0.6396919999999999, PR AUC: 0.6211159233919076, tpr_at_low_fpr: {0.001: 0.007, 0.01: 0.029}
min_k_threshold ROC AUC: 0.633433, PR AUC: 0.6146979676480047, tpr_at_low_fpr: {0.001: 0.007, 0.01: 0.042}
zlib_threshold ROC AUC: 0.5941369999999999, PR AUC: 0.5728986233710323, tpr_at_low_fpr: {0.001: 0.002, 0.01: 0.019}
ref-stablelm-base-alpha-3b-v2_threshold ROC AUC: 0.482982, PR AUC: 0.4879675598957619, tpr_at_low_fpr: {0.001: 0.003, 0.01: 0.012}
loss_threshold roc_auc: 0.640
min_k_threshold roc_auc: 0.633
zlib_threshold roc_auc: 0.594
ref-stablelm-base-alpha-3b-v2_threshold roc_auc: 0.483
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
results_new/neo125_github_experiment/meta-llama_Llama-2-70b-hf/temporal_wiki
Saving results to absolute path: /home/chawins/mimir/tmp_results/neo125_github_experiment/meta-llama_Llama-2-70b-hf/temporal_wiki
LOG: cache_dir is /home/chawins/.cache
Using cache dir /home/chawins/.cache
Loading BASE model meta-llama/Llama-2-70b-hf...
Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.embed_tokens.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:   7%|â–‹         | 1/15 [00:00<00:04,  3.30it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  13%|â–ˆâ–Ž        | 2/15 [00:00<00:03,  3.44it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 3/15 [00:00<00:03,  3.52it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:01<00:03,  3.53it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:01<00:02,  3.55it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.32.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.33.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:01<00:02,  3.54it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.34.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.35.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.36.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.37.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.38.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.39.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:01<00:02,  3.53it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.40.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.41.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.42.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.43.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.44.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:02<00:01,  3.52it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.45.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.46.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.47.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.48.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.49.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.50.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:02<00:01,  3.51it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.51.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.52.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.53.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.54.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.55.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.56.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:02<00:01,  3.51it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.57.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.58.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.59.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.60.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.61.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.62.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:03<00:01,  3.53it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.63.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.64.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.65.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.66.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.67.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:03<00:00,  3.54it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.68.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.69.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.70.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.71.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.72.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.73.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:03<00:00,  3.53it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.74.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.75.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.76.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.77.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.78.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.79.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:03<00:00,  3.54it/s]/home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for lm_head.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:04<00:00,  3.61it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:04<00:00,  3.54it/s]
  0%|          | 0/57 [00:00<?, ?w/s]model.embed_tokens.weight:   0%|          | 0/57 [00:00<?, ?w/s, dev=0]model.embed_tokens.weight:   2%|â–         | 1/57 [00:00<00:08,  6.77w/s, dev=0]model.layers.0.input_layernorm.weight:   2%|â–         | 1/57 [00:00<00:08,  6.76w/s, dev=0]model.layers.0.mlp.down_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:04, 13.49w/s, dev=0]  model.layers.0.mlp.down_proj.weight:   5%|â–Œ         | 3/57 [00:00<00:04, 11.05w/s, dev=0]model.layers.0.mlp.gate_proj.weight:   5%|â–Œ         | 3/57 [00:00<00:04, 11.04w/s, dev=0]model.layers.0.mlp.up_proj.weight:   7%|â–‹         | 4/57 [00:00<00:05,  9.95w/s, dev=0]  model.layers.0.mlp.up_proj.weight:   9%|â–‰         | 5/57 [00:00<00:05,  9.59w/s, dev=0]model.layers.0.post_attention_layernorm.weight:   9%|â–‰         | 5/57 [00:00<00:05,  9.59w/s, dev=0]model.layers.0.self_attn.k_proj.weight:  11%|â–ˆ         | 6/57 [00:00<00:04, 11.50w/s, dev=0]        model.layers.0.self_attn.o_proj.weight:  12%|â–ˆâ–        | 7/57 [00:00<00:03, 13.27w/s, dev=0]model.layers.0.self_attn.q_proj.weight:  14%|â–ˆâ–        | 8/57 [00:00<00:03, 14.11w/s, dev=0]model.layers.0.self_attn.rotary_emb.inv_freq:  16%|â–ˆâ–Œ        | 9/57 [00:00<00:03, 14.89w/s, dev=0]model.layers.0.self_attn.v_proj.weight:  18%|â–ˆâ–Š        | 10/57 [00:00<00:02, 16.54w/s, dev=0]     model.layers.1.input_layernorm.weight:  19%|â–ˆâ–‰        | 11/57 [00:00<00:02, 17.98w/s, dev=0] model.layers.1.mlp.down_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:00<00:02, 19.60w/s, dev=0]  model.layers.1.mlp.down_proj.weight:  23%|â–ˆâ–ˆâ–Ž       | 13/57 [00:00<00:02, 17.33w/s, dev=0]model.layers.1.mlp.gate_proj.weight:  23%|â–ˆâ–ˆâ–Ž       | 13/57 [00:00<00:02, 17.32w/s, dev=0]model.layers.1.mlp.up_proj.weight:  25%|â–ˆâ–ˆâ–       | 14/57 [00:00<00:02, 15.78w/s, dev=0]  model.layers.1.post_attention_layernorm.weight:  26%|â–ˆâ–ˆâ–‹       | 15/57 [00:01<00:02, 14.69w/s, dev=0]model.layers.1.self_attn.k_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:01<00:02, 15.66w/s, dev=0]        model.layers.1.self_attn.o_proj.weight:  30%|â–ˆâ–ˆâ–‰       | 17/57 [00:01<00:02, 16.55w/s, dev=0]model.layers.1.self_attn.q_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 18/57 [00:01<00:02, 16.96w/s, dev=0]model.layers.1.self_attn.rotary_emb.inv_freq:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 19/57 [00:01<00:02, 17.33w/s, dev=0]model.layers.1.self_attn.v_proj.weight:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 20/57 [00:01<00:02, 18.23w/s, dev=0]      model.layers.1.self_attn.v_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 21/57 [00:01<00:01, 19.04w/s, dev=0]model.layers.2.input_layernorm.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 21/57 [00:01<00:01, 19.03w/s, dev=0] model.layers.2.mlp.down_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:01<00:01, 19.94w/s, dev=0]  model.layers.2.mlp.gate_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 23/57 [00:01<00:01, 18.82w/s, dev=0]model.layers.2.mlp.up_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/57 [00:01<00:01, 17.86w/s, dev=0]  model.layers.2.post_attention_layernorm.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/57 [00:01<00:01, 16.95w/s, dev=0]model.layers.2.self_attn.k_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 26/57 [00:01<00:01, 17.62w/s, dev=0]        model.layers.2.self_attn.o_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 27/57 [00:01<00:01, 18.23w/s, dev=0]model.layers.2.self_attn.q_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:01<00:01, 18.48w/s, dev=0]model.layers.2.self_attn.q_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 29/57 [00:01<00:01, 18.72w/s, dev=0]model.layers.2.self_attn.rotary_emb.inv_freq:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 29/57 [00:01<00:01, 18.72w/s, dev=0]model.layers.2.self_attn.v_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 30/57 [00:01<00:01, 19.36w/s, dev=0]      model.layers.3.input_layernorm.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 31/57 [00:01<00:01, 19.93w/s, dev=0] model.layers.3.mlp.down_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 32/57 [00:01<00:01, 20.57w/s, dev=0]  model.layers.3.mlp.gate_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 33/57 [00:01<00:01, 19.72w/s, dev=0]model.layers.3.mlp.up_proj.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:01<00:01, 18.94w/s, dev=0]  model.layers.3.post_attention_layernorm.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/57 [00:01<00:01, 18.27w/s, dev=0]model.layers.3.self_attn.k_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 36/57 [00:01<00:01, 18.79w/s, dev=0]        model.layers.3.self_attn.k_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 37/57 [00:01<00:01, 19.26w/s, dev=0]model.layers.3.self_attn.o_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 37/57 [00:01<00:01, 19.26w/s, dev=0]model.layers.3.self_attn.q_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 38/57 [00:01<00:00, 19.43w/s, dev=0]model.layers.3.self_attn.rotary_emb.inv_freq:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 39/57 [00:01<00:00, 19.60w/s, dev=0]model.layers.3.self_attn.v_proj.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:01<00:00, 20.10w/s, dev=0]      model.layers.4.input_layernorm.weight:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/57 [00:01<00:00, 20.55w/s, dev=0] model.layers.4.mlp.down_proj.weight:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 42/57 [00:01<00:00, 21.05w/s, dev=0]  model.layers.4.mlp.gate_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 43/57 [00:02<00:00, 20.23w/s, dev=0]model.layers.4.mlp.up_proj.weight:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 44/57 [00:02<00:00, 19.48w/s, dev=0]  model.layers.4.mlp.up_proj.weight:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 45/57 [00:02<00:00, 18.90w/s, dev=0]model.layers.4.post_attention_layernorm.weight:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 45/57 [00:02<00:00, 18.90w/s, dev=0]model.layers.4.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:02<00:00, 19.32w/s, dev=0]        model.layers.4.self_attn.o_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 47/57 [00:02<00:00, 19.68w/s, dev=0]model.layers.4.self_attn.q_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/57 [00:02<00:00, 19.80w/s, dev=0]model.layers.4.self_attn.rotary_emb.inv_freq:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 49/57 [00:02<00:00, 19.91w/s, dev=0]model.layers.4.self_attn.v_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 50/57 [00:02<00:00, 20.31w/s, dev=0]      model.layers.5.mlp.gate_proj.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 51/57 [00:02<00:00, 20.66w/s, dev=0]   model.layers.5.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:02<00:00, 19.96w/s, dev=0]model.layers.5.self_attn.k_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 53/57 [00:02<00:00, 20.30w/s, dev=0]model.layers.5.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 53/57 [00:02<00:00, 20.30w/s, dev=0]model.layers.5.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/57 [00:02<00:00, 20.38w/s, dev=0]model.layers.5.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 55/57 [00:02<00:00, 20.43w/s, dev=0]model.layers.5.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 56/57 [00:02<00:00, 20.80w/s, dev=0]                                                                                                     0%|          | 0/59 [00:00<?, ?w/s]model.layers.5.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=0]model.layers.5.mlp.down_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 1546.57w/s, dev=0]model.layers.5.mlp.down_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:03, 14.59w/s, dev=0]  model.layers.5.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:03, 14.56w/s, dev=0]  model.layers.5.post_attention_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:05, 10.83w/s, dev=0]model.layers.5.post_attention_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:03, 14.42w/s, dev=0]model.layers.6.input_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:03, 14.41w/s, dev=0]         model.layers.6.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:02, 18.00w/s, dev=0]  model.layers.6.mlp.down_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:03, 14.85w/s, dev=0]model.layers.6.mlp.gate_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:03, 14.84w/s, dev=0]model.layers.6.mlp.up_proj.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:03, 13.41w/s, dev=0]  model.layers.6.mlp.up_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:04, 12.14w/s, dev=0]model.layers.6.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:04, 12.13w/s, dev=0]model.layers.6.self_attn.k_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:03, 13.64w/s, dev=0]        model.layers.6.self_attn.o_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:03, 15.00w/s, dev=0]model.layers.6.self_attn.q_proj.weight:  19%|â–ˆâ–Š        | 11/59 [00:00<00:03, 15.52w/s, dev=0]model.layers.6.self_attn.rotary_emb.inv_freq:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:02, 16.00w/s, dev=0]model.layers.6.self_attn.v_proj.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:02, 17.32w/s, dev=0]      model.layers.7.input_layernorm.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:02, 18.50w/s, dev=0] model.layers.7.mlp.down_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:02, 19.82w/s, dev=0]  model.layers.7.mlp.down_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:00<00:02, 18.68w/s, dev=0]model.layers.7.mlp.gate_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:00<00:02, 18.68w/s, dev=0]model.layers.7.mlp.up_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:00<00:02, 17.71w/s, dev=0]  model.layers.7.post_attention_layernorm.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:02, 17.21w/s, dev=0]model.layers.7.self_attn.k_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:02, 18.16w/s, dev=0]        model.layers.7.self_attn.o_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:02, 19.03w/s, dev=0]model.layers.7.self_attn.q_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:01<00:01, 19.32w/s, dev=0]model.layers.7.self_attn.rotary_emb.inv_freq:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:01, 19.56w/s, dev=0]model.layers.7.self_attn.v_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:01, 20.44w/s, dev=0]      model.layers.7.self_attn.v_proj.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:01, 21.19w/s, dev=0]model.layers.8.input_layernorm.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:01, 21.19w/s, dev=0] model.layers.8.mlp.down_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:01, 22.07w/s, dev=0]  model.layers.8.mlp.gate_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:01, 20.64w/s, dev=0]model.layers.8.mlp.up_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:01<00:01, 19.98w/s, dev=0]  model.layers.8.post_attention_layernorm.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:01, 19.48w/s, dev=0]model.layers.8.self_attn.k_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:01, 20.17w/s, dev=0]        model.layers.8.self_attn.o_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:01<00:01, 20.80w/s, dev=0]model.layers.8.self_attn.q_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:01<00:01, 21.12w/s, dev=0]model.layers.8.self_attn.q_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:01<00:01, 21.43w/s, dev=0]model.layers.8.self_attn.rotary_emb.inv_freq:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:01<00:01, 21.42w/s, dev=0]model.layers.8.self_attn.v_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:01<00:01, 22.09w/s, dev=0]      model.layers.9.input_layernorm.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:01<00:01, 22.69w/s, dev=0] model.layers.9.mlp.down_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:01<00:01, 23.36w/s, dev=0]  model.layers.9.mlp.gate_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:01<00:01, 22.51w/s, dev=0]model.layers.9.mlp.up_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:01<00:01, 21.04w/s, dev=0]  model.layers.9.post_attention_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:01<00:01, 20.60w/s, dev=0]model.layers.9.self_attn.k_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:01<00:00, 21.14w/s, dev=0]        model.layers.9.self_attn.k_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:01<00:00, 21.63w/s, dev=0]model.layers.9.self_attn.o_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:01<00:00, 21.63w/s, dev=0]model.layers.9.self_attn.q_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:01<00:00, 21.79w/s, dev=0]model.layers.9.self_attn.rotary_emb.inv_freq:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:01<00:00, 21.90w/s, dev=0]model.layers.9.self_attn.v_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:01<00:00, 22.42w/s, dev=0]      model.layers.10.input_layernorm.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:01<00:00, 22.87w/s, dev=0]model.layers.10.mlp.down_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:01<00:00, 23.39w/s, dev=0]  model.layers.10.mlp.gate_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:02<00:00, 22.56w/s, dev=0]model.layers.10.mlp.up_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:02<00:00, 21.91w/s, dev=0]  model.layers.10.mlp.up_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:02<00:00, 21.05w/s, dev=0]model.layers.10.post_attention_layernorm.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:02<00:00, 21.05w/s, dev=0]model.layers.10.self_attn.k_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:02<00:00, 21.49w/s, dev=0]        model.layers.10.self_attn.o_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:02<00:00, 21.87w/s, dev=0]model.layers.10.self_attn.q_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:02<00:00, 21.93w/s, dev=0]model.layers.10.self_attn.rotary_emb.inv_freq:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:02<00:00, 21.99w/s, dev=0]model.layers.10.self_attn.v_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:02<00:00, 22.41w/s, dev=0]      model.layers.11.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:02<00:00, 22.78w/s, dev=0]model.layers.11.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:02<00:00, 23.14w/s, dev=0]model.layers.11.self_attn.o_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:02<00:00, 23.28w/s, dev=0]model.layers.11.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:02<00:00, 23.28w/s, dev=0]model.layers.11.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:02<00:00, 23.42w/s, dev=0]model.layers.11.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:02<00:00, 23.82w/s, dev=0]                                                                                                      0%|          | 0/55 [00:00<?, ?w/s]model.layers.11.input_layernorm.weight:   0%|          | 0/55 [00:00<?, ?w/s, dev=0]model.layers.11.mlp.down_proj.weight:   2%|â–         | 1/55 [00:00<00:00, 1709.17w/s, dev=0]model.layers.11.mlp.down_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:02, 18.29w/s, dev=0]  model.layers.11.mlp.gate_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:02, 18.25w/s, dev=0]model.layers.11.mlp.up_proj.weight:   5%|â–Œ         | 3/55 [00:00<00:03, 13.42w/s, dev=0]  model.layers.11.mlp.up_proj.weight:   7%|â–‹         | 4/55 [00:00<00:06,  8.43w/s, dev=0]model.layers.11.post_attention_layernorm.weight:   7%|â–‹         | 4/55 [00:00<00:06,  8.42w/s, dev=0]model.layers.12.input_layernorm.weight:   9%|â–‰         | 5/55 [00:00<00:04, 10.52w/s, dev=0]         model.layers.12.mlp.down_proj.weight:  11%|â–ˆ         | 6/55 [00:00<00:03, 12.62w/s, dev=0]  model.layers.12.mlp.gate_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:03, 12.23w/s, dev=0]model.layers.12.mlp.gate_proj.weight:  15%|â–ˆâ–        | 8/55 [00:00<00:04, 11.54w/s, dev=0]model.layers.12.mlp.up_proj.weight:  15%|â–ˆâ–        | 8/55 [00:00<00:04, 11.54w/s, dev=0]  model.layers.12.post_attention_layernorm.weight:  16%|â–ˆâ–‹        | 9/55 [00:00<00:04, 10.95w/s, dev=0]model.layers.12.self_attn.k_proj.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:03, 12.16w/s, dev=0]       model.layers.12.self_attn.o_proj.weight:  20%|â–ˆâ–ˆ        | 11/55 [00:00<00:03, 11.24w/s, dev=0]model.layers.12.self_attn.o_proj.weight:  22%|â–ˆâ–ˆâ–       | 12/55 [00:01<00:03, 11.82w/s, dev=0]model.layers.12.self_attn.q_proj.weight:  22%|â–ˆâ–ˆâ–       | 12/55 [00:01<00:03, 11.81w/s, dev=0]model.layers.12.self_attn.rotary_emb.inv_freq:  24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:01<00:03, 12.34w/s, dev=0]model.layers.12.self_attn.v_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 14/55 [00:01<00:03, 13.28w/s, dev=0]      model.layers.13.input_layernorm.weight:  27%|â–ˆâ–ˆâ–‹       | 15/55 [00:01<00:02, 14.06w/s, dev=0] model.layers.13.mlp.down_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 16/55 [00:01<00:02, 14.99w/s, dev=0]  model.layers.13.mlp.down_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:01<00:02, 14.25w/s, dev=0]model.layers.13.mlp.gate_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:01<00:02, 14.24w/s, dev=0]model.layers.13.mlp.up_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/55 [00:01<00:02, 13.66w/s, dev=0]  model.layers.13.post_attention_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–      | 19/55 [00:01<00:02, 13.34w/s, dev=0]model.layers.13.self_attn.k_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:01<00:02, 14.04w/s, dev=0]        model.layers.13.self_attn.o_proj.weight:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/55 [00:01<00:02, 13.13w/s, dev=0]model.layers.13.self_attn.o_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/55 [00:01<00:02, 13.41w/s, dev=0]model.layers.13.self_attn.q_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/55 [00:01<00:02, 13.41w/s, dev=0]model.layers.13.self_attn.rotary_emb.inv_freq:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 23/55 [00:01<00:02, 13.68w/s, dev=0]model.layers.13.self_attn.v_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 24/55 [00:01<00:02, 14.27w/s, dev=0]      model.layers.14.input_layernorm.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:01<00:02, 14.76w/s, dev=0] model.layers.14.mlp.down_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 26/55 [00:01<00:01, 15.35w/s, dev=0]  model.layers.14.mlp.down_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 27/55 [00:01<00:01, 14.82w/s, dev=0]model.layers.14.mlp.gate_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 27/55 [00:01<00:01, 14.81w/s, dev=0]model.layers.14.mlp.up_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 28/55 [00:01<00:01, 14.33w/s, dev=0]  model.layers.14.post_attention_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/55 [00:02<00:01, 14.05w/s, dev=0]model.layers.14.self_attn.k_proj.weight:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 30/55 [00:02<00:01, 14.53w/s, dev=0]        model.layers.14.self_attn.o_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/55 [00:02<00:01, 13.89w/s, dev=0]model.layers.14.self_attn.o_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 32/55 [00:02<00:01, 14.12w/s, dev=0]model.layers.14.self_attn.q_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 32/55 [00:02<00:01, 14.12w/s, dev=0]model.layers.14.self_attn.rotary_emb.inv_freq:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/55 [00:02<00:01, 14.36w/s, dev=0]model.layers.14.self_attn.v_proj.weight:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/55 [00:02<00:01, 14.79w/s, dev=0]      model.layers.15.input_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:02<00:01, 15.18w/s, dev=0] model.layers.15.mlp.down_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 36/55 [00:02<00:01, 15.61w/s, dev=0]  model.layers.15.mlp.down_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 37/55 [00:02<00:01, 15.30w/s, dev=0]model.layers.15.mlp.gate_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 37/55 [00:02<00:01, 15.30w/s, dev=0]model.layers.15.mlp.up_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 38/55 [00:02<00:01, 15.15w/s, dev=0]  model.layers.15.post_attention_layernorm.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 39/55 [00:02<00:01, 14.94w/s, dev=0]model.layers.15.self_attn.k_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 40/55 [00:02<00:00, 15.32w/s, dev=0]        model.layers.15.self_attn.o_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/55 [00:02<00:00, 14.89w/s, dev=0]model.layers.15.self_attn.o_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 42/55 [00:02<00:00, 15.09w/s, dev=0]model.layers.15.self_attn.q_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 42/55 [00:02<00:00, 15.09w/s, dev=0]model.layers.15.self_attn.rotary_emb.inv_freq:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 43/55 [00:02<00:00, 15.28w/s, dev=0]model.layers.15.self_attn.v_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 44/55 [00:02<00:00, 15.64w/s, dev=0]      model.layers.16.input_layernorm.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:02<00:00, 15.96w/s, dev=0] model.layers.16.mlp.down_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 46/55 [00:02<00:00, 16.31w/s, dev=0]  model.layers.16.mlp.down_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 47/55 [00:02<00:00, 16.09w/s, dev=0]model.layers.16.mlp.gate_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 47/55 [00:02<00:00, 16.09w/s, dev=0]model.layers.16.mlp.up_proj.weight:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 48/55 [00:03<00:00, 15.84w/s, dev=0]  model.layers.16.post_attention_layernorm.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 49/55 [00:03<00:00, 15.56w/s, dev=0]model.layers.16.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 50/55 [00:03<00:00, 15.88w/s, dev=0]        model.layers.16.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 51/55 [00:03<00:00, 15.78w/s, dev=0]model.layers.16.self_attn.o_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 52/55 [00:03<00:00, 15.93w/s, dev=0]model.layers.16.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 52/55 [00:03<00:00, 15.92w/s, dev=0]model.layers.16.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 53/55 [00:03<00:00, 16.08w/s, dev=0]model.layers.16.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 54/55 [00:03<00:00, 16.38w/s, dev=0]                                                                                                      0%|          | 0/57 [00:00<?, ?w/s]model.layers.17.input_layernorm.weight:   0%|          | 0/57 [00:00<?, ?w/s, dev=0]model.layers.17.mlp.down_proj.weight:   2%|â–         | 1/57 [00:00<00:00, 1236.16w/s, dev=0]model.layers.17.mlp.down_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:02, 18.71w/s, dev=0]  model.layers.17.mlp.gate_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:02, 18.67w/s, dev=0]model.layers.17.mlp.up_proj.weight:   5%|â–Œ         | 3/57 [00:00<00:06,  8.80w/s, dev=0]  model.layers.17.mlp.up_proj.weight:   7%|â–‹         | 4/57 [00:00<00:05,  9.11w/s, dev=0]model.layers.17.post_attention_layernorm.weight:   7%|â–‹         | 4/57 [00:00<00:05,  9.10w/s, dev=0]model.layers.17.self_attn.k_proj.weight:   9%|â–‰         | 5/57 [00:00<00:04, 11.37w/s, dev=0]        model.layers.17.self_attn.o_proj.weight:  11%|â–ˆ         | 6/57 [00:00<00:03, 13.27w/s, dev=0]model.layers.17.self_attn.q_proj.weight:  12%|â–ˆâ–        | 7/57 [00:00<00:03, 14.05w/s, dev=0]model.layers.17.self_attn.rotary_emb.inv_freq:  14%|â–ˆâ–        | 8/57 [00:00<00:03, 14.83w/s, dev=0]model.layers.17.self_attn.rotary_emb.inv_freq:  16%|â–ˆâ–Œ        | 9/57 [00:00<00:02, 16.67w/s, dev=0]model.layers.17.self_attn.v_proj.weight:  16%|â–ˆâ–Œ        | 9/57 [00:00<00:02, 16.67w/s, dev=0]      model.layers.18.input_layernorm.weight:  18%|â–ˆâ–Š        | 10/57 [00:00<00:02, 18.28w/s, dev=0]model.layers.18.mlp.down_proj.weight:  19%|â–ˆâ–‰        | 11/57 [00:00<00:02, 20.10w/s, dev=0]  model.layers.18.mlp.gate_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:00<00:02, 18.46w/s, dev=0]model.layers.18.mlp.up_proj.weight:  23%|â–ˆâ–ˆâ–Ž       | 13/57 [00:00<00:02, 17.23w/s, dev=0]  model.layers.18.mlp.up_proj.weight:  25%|â–ˆâ–ˆâ–       | 14/57 [00:00<00:02, 16.16w/s, dev=0]model.layers.18.post_attention_layernorm.weight:  25%|â–ˆâ–ˆâ–       | 14/57 [00:00<00:02, 16.15w/s, dev=0]model.layers.18.self_attn.k_proj.weight:  26%|â–ˆâ–ˆâ–‹       | 15/57 [00:00<00:02, 17.30w/s, dev=0]        model.layers.18.self_attn.o_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:00<00:02, 18.35w/s, dev=0]model.layers.18.self_attn.q_proj.weight:  30%|â–ˆâ–ˆâ–‰       | 17/57 [00:00<00:02, 18.91w/s, dev=0]model.layers.18.self_attn.rotary_emb.inv_freq:  32%|â–ˆâ–ˆâ–ˆâ–      | 18/57 [00:00<00:02, 19.18w/s, dev=0]model.layers.18.self_attn.v_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 19/57 [00:00<00:01, 20.24w/s, dev=0]      model.layers.19.input_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 20/57 [00:00<00:01, 21.14w/s, dev=0] model.layers.19.mlp.down_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 21/57 [00:00<00:01, 22.19w/s, dev=0]  model.layers.19.mlp.down_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:01<00:01, 20.36w/s, dev=0]model.layers.19.mlp.gate_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:01<00:01, 20.35w/s, dev=0]model.layers.19.mlp.up_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 23/57 [00:01<00:01, 19.11w/s, dev=0]  model.layers.19.post_attention_layernorm.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/57 [00:01<00:01, 18.16w/s, dev=0]model.layers.19.self_attn.k_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/57 [00:01<00:01, 18.91w/s, dev=0]        model.layers.19.self_attn.o_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 26/57 [00:01<00:01, 19.59w/s, dev=0]model.layers.19.self_attn.q_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 27/57 [00:01<00:01, 19.84w/s, dev=0]model.layers.19.self_attn.rotary_emb.inv_freq:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:01<00:01, 20.06w/s, dev=0]model.layers.19.self_attn.v_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 29/57 [00:01<00:01, 20.77w/s, dev=0]      model.layers.19.self_attn.v_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 30/57 [00:01<00:01, 21.39w/s, dev=0]model.layers.20.input_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 30/57 [00:01<00:01, 18.34w/s, dev=1] model.layers.20.mlp.down_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 31/57 [00:01<00:01, 18.94w/s, dev=1]  model.layers.20.mlp.gate_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 32/57 [00:01<00:01, 18.11w/s, dev=1]model.layers.20.mlp.up_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 33/57 [00:01<00:01, 17.47w/s, dev=1]  model.layers.20.post_attention_layernorm.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:02<00:01, 16.83w/s, dev=1]model.layers.20.self_attn.k_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/57 [00:02<00:01, 17.32w/s, dev=1]        model.layers.20.self_attn.o_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 36/57 [00:02<00:01, 17.76w/s, dev=1]model.layers.20.self_attn.q_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 37/57 [00:02<00:01, 17.93w/s, dev=1]model.layers.20.self_attn.q_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 38/57 [00:02<00:01, 18.09w/s, dev=1]model.layers.20.self_attn.rotary_emb.inv_freq:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 38/57 [00:02<00:01, 18.09w/s, dev=1]model.layers.20.self_attn.v_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 39/57 [00:02<00:00, 18.57w/s, dev=1]      model.layers.21.input_layernorm.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:02<00:01, 15.92w/s, dev=1] model.layers.21.mlp.down_proj.weight:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/57 [00:02<00:00, 16.31w/s, dev=1]  model.layers.21.mlp.gate_proj.weight:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 42/57 [00:03<00:01, 13.34w/s, dev=1]model.layers.21.mlp.up_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 43/57 [00:03<00:01, 12.26w/s, dev=1]  model.layers.21.post_attention_layernorm.weight:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 44/57 [00:03<00:01, 11.49w/s, dev=1]model.layers.21.self_attn.k_proj.weight:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 45/57 [00:03<00:01, 11.75w/s, dev=1]        model.layers.21.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:03<00:00, 11.98w/s, dev=1]model.layers.21.self_attn.o_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:03<00:00, 11.98w/s, dev=1]model.layers.21.self_attn.q_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 47/57 [00:03<00:00, 12.11w/s, dev=1]model.layers.21.self_attn.rotary_emb.inv_freq:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/57 [00:03<00:00, 12.24w/s, dev=1]model.layers.21.self_attn.v_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 49/57 [00:03<00:00, 12.50w/s, dev=1]      model.layers.22.mlp.down_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 50/57 [00:03<00:00, 12.73w/s, dev=1]   model.layers.22.mlp.gate_proj.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 51/57 [00:04<00:00, 12.56w/s, dev=1]model.layers.22.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:04<00:00, 12.40w/s, dev=1]model.layers.22.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 53/57 [00:04<00:00, 12.62w/s, dev=1]model.layers.22.self_attn.o_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/57 [00:04<00:00, 12.74w/s, dev=1]model.layers.22.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/57 [00:04<00:00, 12.74w/s, dev=1]model.layers.22.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 55/57 [00:04<00:00, 12.85w/s, dev=1]model.layers.22.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 56/57 [00:04<00:00, 13.08w/s, dev=1]                                                                                                      0%|          | 0/59 [00:00<?, ?w/s]model.layers.22.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=1]model.layers.22.mlp.up_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 1349.08w/s, dev=1]model.layers.22.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:03, 16.13w/s, dev=1]  model.layers.22.post_attention_layernorm.weight:   3%|â–Ž         | 2/59 [00:00<00:03, 16.10w/s, dev=1]model.layers.23.input_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:02, 24.08w/s, dev=1]         model.layers.23.mlp.down_proj.weight:   7%|â–‹         | 4/59 [00:00<00:01, 32.04w/s, dev=1]  model.layers.23.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:02, 20.05w/s, dev=1]model.layers.23.mlp.gate_proj.weight:   8%|â–Š         | 5/59 [00:00<00:02, 20.03w/s, dev=1]model.layers.23.mlp.up_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:03, 16.42w/s, dev=1]  model.layers.23.post_attention_layernorm.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:03, 14.41w/s, dev=1]model.layers.23.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:03, 16.46w/s, dev=1]model.layers.23.self_attn.k_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:03, 16.45w/s, dev=1]        model.layers.23.self_attn.o_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:02, 18.25w/s, dev=1]model.layers.23.self_attn.q_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:02, 18.86w/s, dev=1]model.layers.23.self_attn.rotary_emb.inv_freq:  19%|â–ˆâ–Š        | 11/59 [00:00<00:02, 19.43w/s, dev=1]model.layers.23.self_attn.v_proj.weight:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:02, 21.18w/s, dev=1]      model.layers.24.input_layernorm.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:02, 22.77w/s, dev=1] model.layers.24.mlp.down_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:01, 24.50w/s, dev=1]  model.layers.24.mlp.down_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:01, 22.10w/s, dev=1]model.layers.24.mlp.gate_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:01, 22.09w/s, dev=1]model.layers.24.mlp.up_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:00<00:02, 19.76w/s, dev=1]  model.layers.24.post_attention_layernorm.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:00<00:02, 18.52w/s, dev=1]model.layers.24.self_attn.k_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:00<00:02, 19.60w/s, dev=1]        model.layers.24.self_attn.o_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:00<00:01, 20.52w/s, dev=1]model.layers.24.self_attn.q_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:00<00:01, 20.72w/s, dev=1]model.layers.24.self_attn.rotary_emb.inv_freq:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:01<00:01, 20.89w/s, dev=1]model.layers.24.self_attn.rotary_emb.inv_freq:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:01, 21.87w/s, dev=1]model.layers.24.self_attn.v_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:01, 21.87w/s, dev=1]      model.layers.25.input_layernorm.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:01, 22.72w/s, dev=1] model.layers.25.mlp.down_proj.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:01, 23.70w/s, dev=1]  model.layers.25.mlp.gate_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:01, 22.18w/s, dev=1]model.layers.25.mlp.up_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:01, 20.95w/s, dev=1]  model.layers.25.post_attention_layernorm.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:01<00:01, 19.85w/s, dev=1]model.layers.25.self_attn.k_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:01, 20.57w/s, dev=1]        model.layers.25.self_attn.k_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:01, 21.20w/s, dev=1]model.layers.25.self_attn.o_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:01, 21.20w/s, dev=1]model.layers.25.self_attn.q_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:01<00:01, 21.35w/s, dev=1]model.layers.25.self_attn.rotary_emb.inv_freq:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:01<00:01, 21.47w/s, dev=1]model.layers.25.self_attn.v_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:01<00:01, 22.16w/s, dev=1]      model.layers.26.input_layernorm.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:01<00:01, 22.74w/s, dev=1] model.layers.26.mlp.down_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:01<00:01, 23.43w/s, dev=1]  model.layers.26.mlp.gate_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:01<00:01, 22.20w/s, dev=1]model.layers.26.mlp.gate_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:01<00:01, 21.18w/s, dev=1]model.layers.26.mlp.up_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:01<00:01, 21.18w/s, dev=1]  model.layers.26.post_attention_layernorm.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:01<00:01, 20.30w/s, dev=1]model.layers.26.self_attn.k_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:01<00:01, 20.84w/s, dev=1]        model.layers.26.self_attn.o_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:01<00:00, 21.32w/s, dev=1]model.layers.26.self_attn.q_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:01<00:00, 21.46w/s, dev=1]model.layers.26.self_attn.rotary_emb.inv_freq:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:01<00:00, 21.57w/s, dev=1]model.layers.26.self_attn.v_proj.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:01<00:00, 22.09w/s, dev=1]      model.layers.26.self_attn.v_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:01<00:00, 22.56w/s, dev=1]model.layers.27.input_layernorm.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:01<00:00, 22.55w/s, dev=1] model.layers.27.mlp.down_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:01<00:00, 23.08w/s, dev=1]  model.layers.27.mlp.gate_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:02<00:00, 22.14w/s, dev=1]model.layers.27.mlp.up_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:02<00:00, 21.55w/s, dev=1]  model.layers.27.post_attention_layernorm.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:02<00:00, 20.85w/s, dev=1]model.layers.27.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:02<00:00, 21.29w/s, dev=1]        model.layers.27.self_attn.o_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:02<00:00, 21.69w/s, dev=1]model.layers.27.self_attn.o_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:02<00:00, 21.82w/s, dev=1]model.layers.27.self_attn.q_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:02<00:00, 21.81w/s, dev=1]model.layers.27.self_attn.rotary_emb.inv_freq:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:02<00:00, 21.90w/s, dev=1]model.layers.27.self_attn.v_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:02<00:00, 22.33w/s, dev=1]      model.layers.28.mlp.gate_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:02<00:00, 22.70w/s, dev=1]   model.layers.28.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:02<00:00, 21.90w/s, dev=1]model.layers.28.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:02<00:00, 22.25w/s, dev=1]model.layers.28.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:02<00:00, 22.31w/s, dev=1]model.layers.28.self_attn.q_proj.weight:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:02<00:00, 22.35w/s, dev=1]model.layers.28.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:02<00:00, 22.35w/s, dev=1]model.layers.28.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:02<00:00, 22.73w/s, dev=1]                                                                                                      0%|          | 0/59 [00:00<?, ?w/s]model.layers.28.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=1]model.layers.28.mlp.down_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 1127.80w/s, dev=1]model.layers.28.mlp.down_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:03, 15.42w/s, dev=1]  model.layers.28.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:03, 15.39w/s, dev=1]  model.layers.28.post_attention_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:04, 11.83w/s, dev=1]model.layers.28.post_attention_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:03, 15.75w/s, dev=1]model.layers.29.input_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:03, 15.74w/s, dev=1]         model.layers.29.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:02, 19.66w/s, dev=1]  model.layers.29.mlp.down_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:03, 16.82w/s, dev=1]model.layers.29.mlp.gate_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:03, 16.80w/s, dev=1]model.layers.29.mlp.up_proj.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:03, 14.90w/s, dev=1]  model.layers.29.mlp.up_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:03, 13.76w/s, dev=1]model.layers.29.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:03, 13.76w/s, dev=1]model.layers.29.self_attn.k_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:03, 15.47w/s, dev=1]        model.layers.29.self_attn.o_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:02, 17.04w/s, dev=1]model.layers.29.self_attn.q_proj.weight:  19%|â–ˆâ–Š        | 11/59 [00:00<00:02, 17.88w/s, dev=1]model.layers.29.self_attn.rotary_emb.inv_freq:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:02, 18.53w/s, dev=1]model.layers.29.self_attn.v_proj.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:02, 20.06w/s, dev=1]      model.layers.30.input_layernorm.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:02, 21.46w/s, dev=1] model.layers.30.mlp.down_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:01, 22.98w/s, dev=1]  model.layers.30.mlp.down_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:00<00:02, 21.22w/s, dev=1]model.layers.30.mlp.gate_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:00<00:02, 21.21w/s, dev=1]model.layers.30.mlp.up_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:00<00:02, 19.61w/s, dev=1]  model.layers.30.post_attention_layernorm.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:00<00:02, 18.43w/s, dev=1]model.layers.30.self_attn.k_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:00<00:02, 19.44w/s, dev=1]        model.layers.30.self_attn.o_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:00<00:01, 20.36w/s, dev=1]model.layers.30.self_attn.q_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:01<00:01, 20.72w/s, dev=1]model.layers.30.self_attn.rotary_emb.inv_freq:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:01, 21.02w/s, dev=1]model.layers.30.self_attn.v_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:01, 21.97w/s, dev=1]      model.layers.30.self_attn.v_proj.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:01, 22.82w/s, dev=1]model.layers.31.input_layernorm.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:01, 22.82w/s, dev=1] model.layers.31.mlp.down_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:01, 23.76w/s, dev=1]  model.layers.31.mlp.gate_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:01, 22.13w/s, dev=1]model.layers.31.mlp.up_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:01<00:01, 20.92w/s, dev=1]  model.layers.31.post_attention_layernorm.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:01, 19.72w/s, dev=1]model.layers.31.self_attn.k_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:01, 20.42w/s, dev=1]        model.layers.31.self_attn.o_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:01<00:01, 21.05w/s, dev=1]model.layers.31.self_attn.q_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:01<00:01, 21.25w/s, dev=1]model.layers.31.self_attn.q_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:01<00:01, 21.45w/s, dev=1]model.layers.31.self_attn.rotary_emb.inv_freq:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:01<00:01, 21.44w/s, dev=1]model.layers.31.self_attn.v_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:01<00:01, 22.11w/s, dev=1]      model.layers.32.input_layernorm.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:01<00:01, 22.70w/s, dev=1] model.layers.32.mlp.down_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:01<00:01, 23.36w/s, dev=1]  model.layers.32.mlp.gate_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:01<00:01, 22.11w/s, dev=1]model.layers.32.mlp.up_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:01<00:01, 21.03w/s, dev=1]  model.layers.32.post_attention_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:01<00:01, 19.75w/s, dev=1]model.layers.32.self_attn.k_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:01<00:00, 20.26w/s, dev=1]        model.layers.32.self_attn.k_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:01<00:00, 20.69w/s, dev=1]model.layers.32.self_attn.o_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:01<00:00, 20.69w/s, dev=1]model.layers.32.self_attn.q_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:01<00:00, 20.73w/s, dev=1]model.layers.32.self_attn.rotary_emb.inv_freq:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:02<00:00, 20.78w/s, dev=1]model.layers.32.self_attn.v_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:02<00:00, 21.27w/s, dev=1]      model.layers.33.input_layernorm.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:02<00:00, 21.69w/s, dev=1] model.layers.33.mlp.down_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:02<00:00, 22.18w/s, dev=1]  model.layers.33.mlp.gate_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:02<00:00, 21.16w/s, dev=1]model.layers.33.mlp.up_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:02<00:00, 20.50w/s, dev=1]  model.layers.33.mlp.up_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:02<00:00, 19.91w/s, dev=1]model.layers.33.post_attention_layernorm.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:02<00:00, 19.91w/s, dev=1]model.layers.33.self_attn.k_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:02<00:00, 20.32w/s, dev=1]        model.layers.33.self_attn.o_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:02<00:00, 20.70w/s, dev=1]model.layers.33.self_attn.q_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:02<00:00, 20.82w/s, dev=1]model.layers.33.self_attn.rotary_emb.inv_freq:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:02<00:00, 20.95w/s, dev=1]model.layers.33.self_attn.v_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:02<00:00, 21.34w/s, dev=1]      model.layers.34.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:02<00:00, 21.70w/s, dev=1]model.layers.34.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:02<00:00, 22.05w/s, dev=1]model.layers.34.self_attn.o_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:02<00:00, 22.15w/s, dev=1]model.layers.34.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:02<00:00, 22.15w/s, dev=1]model.layers.34.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:02<00:00, 22.18w/s, dev=1]model.layers.34.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:02<00:00, 22.57w/s, dev=1]                                                                                                      0%|          | 0/55 [00:00<?, ?w/s]model.layers.34.input_layernorm.weight:   0%|          | 0/55 [00:00<?, ?w/s, dev=1]model.layers.34.mlp.down_proj.weight:   2%|â–         | 1/55 [00:00<00:00, 1334.92w/s, dev=1]model.layers.34.mlp.down_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:03, 17.50w/s, dev=1]  model.layers.34.mlp.gate_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:03, 17.47w/s, dev=1]model.layers.34.mlp.up_proj.weight:   5%|â–Œ         | 3/55 [00:00<00:04, 12.44w/s, dev=1]  model.layers.34.mlp.up_proj.weight:   7%|â–‹         | 4/55 [00:00<00:04, 11.20w/s, dev=1]model.layers.34.post_attention_layernorm.weight:   7%|â–‹         | 4/55 [00:00<00:04, 11.19w/s, dev=1]model.layers.35.input_layernorm.weight:   9%|â–‰         | 5/55 [00:00<00:03, 13.97w/s, dev=1]         model.layers.35.mlp.down_proj.weight:  11%|â–ˆ         | 6/55 [00:00<00:02, 16.76w/s, dev=1]  model.layers.35.mlp.down_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:03, 14.43w/s, dev=1]model.layers.35.mlp.gate_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:03, 14.42w/s, dev=1]model.layers.35.mlp.up_proj.weight:  15%|â–ˆâ–        | 8/55 [00:00<00:03, 13.63w/s, dev=1]  model.layers.35.post_attention_layernorm.weight:  16%|â–ˆâ–‹        | 9/55 [00:00<00:03, 12.73w/s, dev=1]model.layers.35.post_attention_layernorm.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:03, 14.13w/s, dev=1]model.layers.35.self_attn.k_proj.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:03, 14.13w/s, dev=1]        model.layers.35.self_attn.o_proj.weight:  20%|â–ˆâ–ˆ        | 11/55 [00:00<00:02, 15.43w/s, dev=1]model.layers.35.self_attn.q_proj.weight:  22%|â–ˆâ–ˆâ–       | 12/55 [00:00<00:02, 16.00w/s, dev=1]model.layers.35.self_attn.rotary_emb.inv_freq:  24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:00<00:02, 16.48w/s, dev=1]model.layers.35.self_attn.v_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 14/55 [00:00<00:02, 17.74w/s, dev=1]      model.layers.36.input_layernorm.weight:  27%|â–ˆâ–ˆâ–‹       | 15/55 [00:00<00:02, 18.81w/s, dev=1] model.layers.36.mlp.down_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 16/55 [00:00<00:01, 20.06w/s, dev=1]  model.layers.36.mlp.down_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:00<00:02, 18.43w/s, dev=1]model.layers.36.mlp.gate_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:00<00:02, 18.43w/s, dev=1]model.layers.36.mlp.up_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/55 [00:01<00:02, 17.51w/s, dev=1]  model.layers.36.post_attention_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–      | 19/55 [00:01<00:02, 16.71w/s, dev=1]model.layers.36.self_attn.k_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:01<00:01, 17.58w/s, dev=1]        model.layers.36.self_attn.o_proj.weight:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/55 [00:01<00:01, 18.38w/s, dev=1]model.layers.36.self_attn.q_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/55 [00:01<00:01, 18.70w/s, dev=1]model.layers.36.self_attn.rotary_emb.inv_freq:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 23/55 [00:01<00:01, 19.01w/s, dev=1]model.layers.36.self_attn.rotary_emb.inv_freq:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 24/55 [00:01<00:01, 19.83w/s, dev=1]model.layers.36.self_attn.v_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 24/55 [00:01<00:01, 19.83w/s, dev=1]      model.layers.37.input_layernorm.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:01<00:01, 20.58w/s, dev=1] model.layers.37.mlp.down_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 26/55 [00:01<00:01, 21.39w/s, dev=1]  model.layers.37.mlp.gate_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 27/55 [00:01<00:01, 20.30w/s, dev=1]model.layers.37.mlp.up_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 28/55 [00:01<00:01, 19.39w/s, dev=1]  model.layers.37.post_attention_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/55 [00:01<00:01, 18.52w/s, dev=1]model.layers.37.self_attn.k_proj.weight:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 30/55 [00:01<00:01, 19.16w/s, dev=1]        model.layers.37.self_attn.k_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/55 [00:01<00:01, 19.73w/s, dev=1]model.layers.37.self_attn.o_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/55 [00:01<00:01, 19.73w/s, dev=1]model.layers.37.self_attn.q_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 32/55 [00:01<00:01, 19.95w/s, dev=1]model.layers.37.self_attn.rotary_emb.inv_freq:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/55 [00:01<00:01, 20.14w/s, dev=1]model.layers.37.self_attn.v_proj.weight:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/55 [00:01<00:01, 20.74w/s, dev=1]      model.layers.38.input_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:01<00:00, 21.29w/s, dev=1] model.layers.38.mlp.down_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 36/55 [00:01<00:00, 21.89w/s, dev=1]  model.layers.38.mlp.gate_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 37/55 [00:01<00:00, 21.12w/s, dev=1]model.layers.38.mlp.gate_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 38/55 [00:01<00:00, 20.22w/s, dev=1]model.layers.38.mlp.up_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 38/55 [00:01<00:00, 20.22w/s, dev=1]  model.layers.38.post_attention_layernorm.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 39/55 [00:01<00:00, 19.55w/s, dev=1]model.layers.38.self_attn.k_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 40/55 [00:01<00:00, 20.05w/s, dev=1]        model.layers.38.self_attn.o_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/55 [00:02<00:00, 20.49w/s, dev=1]model.layers.38.self_attn.q_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 42/55 [00:02<00:00, 20.65w/s, dev=1]model.layers.38.self_attn.rotary_emb.inv_freq:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 43/55 [00:02<00:00, 20.84w/s, dev=1]model.layers.38.self_attn.v_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 44/55 [00:02<00:00, 21.32w/s, dev=1]      model.layers.38.self_attn.v_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:02<00:00, 21.75w/s, dev=1]model.layers.39.input_layernorm.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:02<00:00, 21.75w/s, dev=1] model.layers.39.mlp.down_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 46/55 [00:02<00:00, 22.23w/s, dev=1]  model.layers.39.mlp.gate_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 47/55 [00:02<00:00, 21.49w/s, dev=1]model.layers.39.mlp.up_proj.weight:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 48/55 [00:02<00:00, 20.67w/s, dev=1]  model.layers.39.post_attention_layernorm.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 49/55 [00:02<00:00, 20.17w/s, dev=1]model.layers.39.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 50/55 [00:02<00:00, 20.57w/s, dev=1]        model.layers.39.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 51/55 [00:02<00:00, 20.94w/s, dev=1]model.layers.39.self_attn.o_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 52/55 [00:02<00:00, 21.08w/s, dev=1]model.layers.39.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 52/55 [00:02<00:00, 21.07w/s, dev=1]model.layers.39.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 53/55 [00:02<00:00, 21.18w/s, dev=1]model.layers.39.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 54/55 [00:02<00:00, 21.58w/s, dev=1]                                                                                                      0%|          | 0/57 [00:00<?, ?w/s]model.layers.40.input_layernorm.weight:   0%|          | 0/57 [00:00<?, ?w/s, dev=1]model.layers.40.mlp.down_proj.weight:   2%|â–         | 1/57 [00:00<00:00, 1241.65w/s, dev=1]model.layers.40.mlp.down_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:03, 16.11w/s, dev=1]  model.layers.40.mlp.gate_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:03, 16.08w/s, dev=1]model.layers.40.mlp.up_proj.weight:   5%|â–Œ         | 3/57 [00:00<00:04, 13.49w/s, dev=1]  model.layers.40.mlp.up_proj.weight:   7%|â–‹         | 4/57 [00:00<00:04, 12.50w/s, dev=1]model.layers.40.post_attention_layernorm.weight:   7%|â–‹         | 4/57 [00:00<00:04, 12.50w/s, dev=1]model.layers.40.self_attn.k_proj.weight:   9%|â–‰         | 5/57 [00:00<00:03, 15.60w/s, dev=1]        model.layers.40.self_attn.o_proj.weight:  11%|â–ˆ         | 6/57 [00:00<00:02, 18.45w/s, dev=1]model.layers.40.self_attn.q_proj.weight:  12%|â–ˆâ–        | 7/57 [00:00<00:02, 19.80w/s, dev=1]model.layers.40.self_attn.rotary_emb.inv_freq:  14%|â–ˆâ–        | 8/57 [00:00<00:02, 20.92w/s, dev=1]model.layers.40.self_attn.v_proj.weight:  16%|â–ˆâ–Œ        | 9/57 [00:00<00:02, 23.50w/s, dev=1]      model.layers.41.input_layernorm.weight:  18%|â–ˆâ–Š        | 10/57 [00:00<00:02, 21.99w/s, dev=2]model.layers.41.input_layernorm.weight:  19%|â–ˆâ–‰        | 11/57 [00:00<00:01, 24.15w/s, dev=2]model.layers.41.mlp.down_proj.weight:  19%|â–ˆâ–‰        | 11/57 [00:00<00:01, 24.14w/s, dev=2]  model.layers.41.mlp.gate_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:00<00:02, 19.18w/s, dev=2]model.layers.41.mlp.up_proj.weight:  23%|â–ˆâ–ˆâ–Ž       | 13/57 [00:00<00:02, 16.49w/s, dev=2]  model.layers.41.post_attention_layernorm.weight:  25%|â–ˆâ–ˆâ–       | 14/57 [00:00<00:02, 14.82w/s, dev=2]model.layers.41.self_attn.k_proj.weight:  26%|â–ˆâ–ˆâ–‹       | 15/57 [00:00<00:02, 15.87w/s, dev=2]        model.layers.41.self_attn.o_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:00<00:02, 16.81w/s, dev=2]model.layers.41.self_attn.q_proj.weight:  30%|â–ˆâ–ˆâ–‰       | 17/57 [00:00<00:02, 17.02w/s, dev=2]model.layers.41.self_attn.q_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 18/57 [00:01<00:02, 17.16w/s, dev=2]model.layers.41.self_attn.rotary_emb.inv_freq:  32%|â–ˆâ–ˆâ–ˆâ–      | 18/57 [00:01<00:02, 17.16w/s, dev=2]model.layers.41.self_attn.v_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 19/57 [00:01<00:02, 18.11w/s, dev=2]      model.layers.42.input_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 20/57 [00:01<00:01, 18.91w/s, dev=2] model.layers.42.mlp.down_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 21/57 [00:01<00:01, 19.85w/s, dev=2]  model.layers.42.mlp.gate_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:01<00:01, 17.97w/s, dev=2]model.layers.42.mlp.up_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 23/57 [00:01<00:02, 16.52w/s, dev=2]  model.layers.42.post_attention_layernorm.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/57 [00:01<00:02, 15.41w/s, dev=2]model.layers.42.post_attention_layernorm.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/57 [00:01<00:01, 16.05w/s, dev=2]model.layers.42.self_attn.k_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/57 [00:01<00:01, 16.05w/s, dev=2]        model.layers.42.self_attn.o_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 26/57 [00:01<00:01, 16.62w/s, dev=2]model.layers.42.self_attn.q_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 27/57 [00:01<00:01, 16.77w/s, dev=2]model.layers.42.self_attn.rotary_emb.inv_freq:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:01<00:01, 16.90w/s, dev=2]model.layers.42.self_attn.v_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 29/57 [00:01<00:01, 17.50w/s, dev=2]      model.layers.43.input_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 30/57 [00:01<00:01, 18.03w/s, dev=2] model.layers.43.mlp.down_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 31/57 [00:01<00:01, 18.63w/s, dev=2]  model.layers.43.mlp.down_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 32/57 [00:01<00:01, 17.61w/s, dev=2]model.layers.43.mlp.gate_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 32/57 [00:01<00:01, 17.60w/s, dev=2]model.layers.43.mlp.up_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 33/57 [00:01<00:01, 16.60w/s, dev=2]  model.layers.43.post_attention_layernorm.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:02<00:01, 15.67w/s, dev=2]model.layers.43.self_attn.k_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/57 [00:02<00:01, 16.13w/s, dev=2]        model.layers.43.self_attn.o_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 36/57 [00:02<00:01, 16.54w/s, dev=2]model.layers.43.self_attn.q_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 37/57 [00:02<00:01, 16.60w/s, dev=2]model.layers.43.self_attn.rotary_emb.inv_freq:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 38/57 [00:02<00:01, 16.65w/s, dev=2]model.layers.43.self_attn.rotary_emb.inv_freq:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 39/57 [00:02<00:01, 17.09w/s, dev=2]model.layers.43.self_attn.v_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 39/57 [00:02<00:01, 17.09w/s, dev=2]      model.layers.44.input_layernorm.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:02<00:00, 17.46w/s, dev=2] model.layers.44.mlp.down_proj.weight:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/57 [00:02<00:00, 17.89w/s, dev=2]  model.layers.44.mlp.gate_proj.weight:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 42/57 [00:02<00:00, 17.01w/s, dev=2]model.layers.44.mlp.up_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 43/57 [00:02<00:00, 16.35w/s, dev=2]  model.layers.44.post_attention_layernorm.weight:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 44/57 [00:02<00:00, 15.80w/s, dev=2]model.layers.44.self_attn.k_proj.weight:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 45/57 [00:02<00:00, 16.15w/s, dev=2]        model.layers.44.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:02<00:00, 16.47w/s, dev=2]model.layers.44.self_attn.o_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:02<00:00, 16.47w/s, dev=2]model.layers.44.self_attn.q_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 47/57 [00:02<00:00, 16.55w/s, dev=2]model.layers.44.self_attn.rotary_emb.inv_freq:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/57 [00:02<00:00, 16.63w/s, dev=2]model.layers.44.self_attn.v_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 49/57 [00:02<00:00, 16.98w/s, dev=2]      model.layers.45.mlp.down_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 50/57 [00:02<00:00, 17.29w/s, dev=2]   model.layers.45.mlp.gate_proj.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 51/57 [00:03<00:00, 16.76w/s, dev=2]model.layers.45.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:03<00:00, 16.24w/s, dev=2]model.layers.45.self_attn.k_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 53/57 [00:03<00:00, 16.50w/s, dev=2]model.layers.45.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 53/57 [00:03<00:00, 16.50w/s, dev=2]model.layers.45.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/57 [00:03<00:00, 16.52w/s, dev=2]model.layers.45.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 55/57 [00:03<00:00, 16.57w/s, dev=2]model.layers.45.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 56/57 [00:03<00:00, 16.87w/s, dev=2]                                                                                                      0%|          | 0/59 [00:00<?, ?w/s]model.layers.45.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=2]model.layers.45.mlp.up_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 1202.50w/s, dev=2]model.layers.45.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 13.03w/s, dev=2]  model.layers.45.post_attention_layernorm.weight:   3%|â–Ž         | 2/59 [00:00<00:04, 13.01w/s, dev=2]model.layers.46.input_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:02, 19.46w/s, dev=2]         model.layers.46.mlp.down_proj.weight:   7%|â–‹         | 4/59 [00:00<00:02, 25.91w/s, dev=2]  model.layers.46.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 15.90w/s, dev=2]model.layers.46.mlp.gate_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 15.89w/s, dev=2]model.layers.46.mlp.up_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:04, 12.71w/s, dev=2]  model.layers.46.post_attention_layernorm.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:04, 11.16w/s, dev=2]model.layers.46.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:04, 12.75w/s, dev=2]model.layers.46.self_attn.k_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:04, 12.75w/s, dev=2]        model.layers.46.self_attn.o_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:03, 14.19w/s, dev=2]model.layers.46.self_attn.q_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:03, 14.71w/s, dev=2]model.layers.46.self_attn.rotary_emb.inv_freq:  19%|â–ˆâ–Š        | 11/59 [00:00<00:03, 15.13w/s, dev=2]model.layers.46.self_attn.v_proj.weight:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:02, 16.50w/s, dev=2]      model.layers.46.self_attn.v_proj.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:02, 17.68w/s, dev=2]model.layers.47.input_layernorm.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:02, 17.67w/s, dev=2] model.layers.47.mlp.down_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:02, 19.03w/s, dev=2]  model.layers.47.mlp.gate_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:02, 16.75w/s, dev=2]model.layers.47.mlp.up_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:02, 15.17w/s, dev=2]  model.layers.47.post_attention_layernorm.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:01<00:02, 14.00w/s, dev=2]model.layers.47.post_attention_layernorm.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:02, 14.82w/s, dev=2]model.layers.47.self_attn.k_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:02, 14.82w/s, dev=2]        model.layers.47.self_attn.o_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:02, 15.54w/s, dev=2]model.layers.47.self_attn.q_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:02, 15.75w/s, dev=2]model.layers.47.self_attn.rotary_emb.inv_freq:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:01<00:02, 15.94w/s, dev=2]model.layers.47.self_attn.v_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:02, 16.70w/s, dev=2]      model.layers.47.self_attn.v_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:02, 17.37w/s, dev=2]model.layers.48.input_layernorm.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:02, 17.37w/s, dev=2] model.layers.48.mlp.down_proj.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:01, 18.12w/s, dev=2]  model.layers.48.mlp.gate_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:02, 16.89w/s, dev=2]model.layers.48.mlp.up_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:02, 15.96w/s, dev=2]  model.layers.48.post_attention_layernorm.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:01<00:02, 15.10w/s, dev=2]model.layers.48.post_attention_layernorm.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:01, 15.66w/s, dev=2]model.layers.48.self_attn.k_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:01, 15.66w/s, dev=2]        model.layers.48.self_attn.o_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:01, 16.15w/s, dev=2]model.layers.48.self_attn.q_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:01<00:01, 16.29w/s, dev=2]model.layers.48.self_attn.rotary_emb.inv_freq:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:01<00:01, 16.42w/s, dev=2]model.layers.48.self_attn.v_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:01<00:01, 16.95w/s, dev=2]      model.layers.48.self_attn.v_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:01<00:01, 17.42w/s, dev=2]model.layers.49.input_layernorm.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:01<00:01, 17.42w/s, dev=2] model.layers.49.mlp.down_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:01<00:01, 17.94w/s, dev=2]  model.layers.49.mlp.gate_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:02<00:01, 17.05w/s, dev=2]model.layers.49.mlp.up_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:02<00:01, 16.31w/s, dev=2]  model.layers.49.post_attention_layernorm.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:02<00:01, 15.69w/s, dev=2]model.layers.49.post_attention_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:02<00:01, 16.11w/s, dev=2]model.layers.49.self_attn.k_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:02<00:01, 16.11w/s, dev=2]        model.layers.49.self_attn.o_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:02<00:01, 16.49w/s, dev=2]model.layers.49.self_attn.q_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:02<00:01, 16.60w/s, dev=2]model.layers.49.self_attn.rotary_emb.inv_freq:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:02<00:01, 16.70w/s, dev=2]model.layers.49.self_attn.v_proj.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:02<00:00, 17.10w/s, dev=2]      model.layers.49.self_attn.v_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:02<00:00, 17.46w/s, dev=2]model.layers.50.input_layernorm.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:02<00:00, 17.45w/s, dev=2] model.layers.50.mlp.down_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:02<00:00, 17.86w/s, dev=2]  model.layers.50.mlp.gate_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:02<00:00, 17.15w/s, dev=2]model.layers.50.mlp.up_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:02<00:00, 16.59w/s, dev=2]  model.layers.50.post_attention_layernorm.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:02<00:00, 16.08w/s, dev=2]model.layers.50.post_attention_layernorm.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:02<00:00, 16.42w/s, dev=2]model.layers.50.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:02<00:00, 16.42w/s, dev=2]        model.layers.50.self_attn.o_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:02<00:00, 16.72w/s, dev=2]model.layers.50.self_attn.q_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:02<00:00, 16.80w/s, dev=2]model.layers.50.self_attn.rotary_emb.inv_freq:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:03<00:00, 16.86w/s, dev=2]model.layers.50.self_attn.v_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:03<00:00, 17.19w/s, dev=2]      model.layers.50.self_attn.v_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:03<00:00, 17.48w/s, dev=2]model.layers.51.mlp.gate_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:03<00:00, 17.48w/s, dev=2]   model.layers.51.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:03<00:00, 16.99w/s, dev=2]model.layers.51.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:03<00:00, 16.95w/s, dev=2]model.layers.51.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:03<00:00, 17.02w/s, dev=2]model.layers.51.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:03<00:00, 17.06w/s, dev=2]model.layers.51.self_attn.rotary_emb.inv_freq:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:03<00:00, 17.35w/s, dev=2]model.layers.51.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:03<00:00, 17.35w/s, dev=2]                                                                                                      0%|          | 0/59 [00:00<?, ?w/s]model.layers.51.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=2]model.layers.51.mlp.down_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 1108.14w/s, dev=2]model.layers.51.mlp.down_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:05, 11.28w/s, dev=2]  model.layers.51.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:05, 11.26w/s, dev=2]  model.layers.51.post_attention_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:06,  8.86w/s, dev=2]model.layers.51.post_attention_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:04, 11.79w/s, dev=2]model.layers.52.input_layernorm.weight:   7%|â–‹         | 4/59 [00:00<00:04, 11.79w/s, dev=2]         model.layers.52.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:03, 14.73w/s, dev=2]  model.layers.52.mlp.down_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:05,  9.29w/s, dev=2]model.layers.52.mlp.gate_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:05,  9.29w/s, dev=2]model.layers.52.mlp.up_proj.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:05,  8.83w/s, dev=2]  model.layers.52.mlp.up_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:06,  8.44w/s, dev=2]model.layers.52.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:06,  8.44w/s, dev=2]model.layers.52.self_attn.k_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:05,  9.49w/s, dev=2]        model.layers.52.self_attn.k_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:01<00:05,  8.89w/s, dev=2]model.layers.52.self_attn.o_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:01<00:05,  8.89w/s, dev=2]model.layers.52.self_attn.q_proj.weight:  19%|â–ˆâ–Š        | 11/59 [00:01<00:05,  9.38w/s, dev=2]model.layers.52.self_attn.q_proj.weight:  20%|â–ˆâ–ˆ        | 12/59 [00:01<00:04,  9.79w/s, dev=2]model.layers.52.self_attn.rotary_emb.inv_freq:  20%|â–ˆâ–ˆ        | 12/59 [00:01<00:04,  9.79w/s, dev=2]model.layers.52.self_attn.v_proj.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:01<00:04, 10.60w/s, dev=2]      model.layers.53.input_layernorm.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:01<00:03, 11.33w/s, dev=2] model.layers.53.mlp.down_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:01<00:03, 12.13w/s, dev=2]  model.layers.53.mlp.down_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:03, 11.45w/s, dev=2]model.layers.53.mlp.gate_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:03, 11.45w/s, dev=2]model.layers.53.mlp.up_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:01<00:03, 10.95w/s, dev=2]  model.layers.53.post_attention_layernorm.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:04,  9.70w/s, dev=2]model.layers.53.self_attn.k_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:03, 10.24w/s, dev=2]        model.layers.53.self_attn.k_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:03, 10.73w/s, dev=2]model.layers.53.self_attn.o_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:03, 10.73w/s, dev=2]model.layers.53.self_attn.q_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:01<00:03, 10.97w/s, dev=2]model.layers.53.self_attn.rotary_emb.inv_freq:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:03, 11.15w/s, dev=2]model.layers.53.self_attn.v_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:03, 11.66w/s, dev=2]      model.layers.53.self_attn.v_proj.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:02, 12.12w/s, dev=2]model.layers.54.input_layernorm.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:02, 12.12w/s, dev=2] model.layers.54.mlp.down_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:02, 12.62w/s, dev=2]  model.layers.54.mlp.gate_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:02<00:02, 12.14w/s, dev=2]model.layers.54.mlp.up_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:02<00:02, 11.62w/s, dev=2]  model.layers.54.mlp.up_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:02<00:02, 11.29w/s, dev=2]model.layers.54.post_attention_layernorm.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:02<00:02, 11.28w/s, dev=2]model.layers.54.self_attn.k_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:02<00:02, 11.69w/s, dev=2]        model.layers.54.self_attn.o_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:02<00:02, 11.57w/s, dev=2]model.layers.54.self_attn.q_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:02<00:02, 11.76w/s, dev=2]model.layers.54.self_attn.q_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:02<00:02, 11.93w/s, dev=2]model.layers.54.self_attn.rotary_emb.inv_freq:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:02<00:02, 11.93w/s, dev=2]model.layers.54.self_attn.v_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:02<00:02, 12.30w/s, dev=2]      model.layers.55.input_layernorm.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:02<00:01, 12.63w/s, dev=2] model.layers.55.mlp.down_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:02<00:01, 13.00w/s, dev=2]  model.layers.55.mlp.down_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:02<00:01, 12.68w/s, dev=2]model.layers.55.mlp.gate_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:02<00:01, 12.68w/s, dev=2]model.layers.55.mlp.up_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:03<00:01, 12.33w/s, dev=2]  model.layers.55.post_attention_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:03<00:01, 11.59w/s, dev=2]model.layers.55.self_attn.k_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:03<00:01, 11.89w/s, dev=2]        model.layers.55.self_attn.k_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:03<00:01, 12.16w/s, dev=2]model.layers.55.self_attn.o_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:03<00:01, 12.16w/s, dev=2]model.layers.55.self_attn.q_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:03<00:01, 12.28w/s, dev=2]model.layers.55.self_attn.rotary_emb.inv_freq:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:03<00:01, 12.38w/s, dev=2]model.layers.55.self_attn.v_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:03<00:01, 12.68w/s, dev=2]      model.layers.55.self_attn.v_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:03<00:01, 12.94w/s, dev=2]model.layers.56.input_layernorm.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:03<00:01, 12.94w/s, dev=2] model.layers.56.mlp.down_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:03<00:01, 13.23w/s, dev=2]  model.layers.56.mlp.gate_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:03<00:01, 12.92w/s, dev=2]model.layers.56.mlp.up_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:03<00:00, 12.67w/s, dev=2]  model.layers.56.mlp.up_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:03<00:00, 12.44w/s, dev=2]model.layers.56.post_attention_layernorm.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:03<00:00, 12.44w/s, dev=2]model.layers.56.self_attn.k_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:03<00:00, 12.69w/s, dev=2]        model.layers.56.self_attn.o_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:03<00:00, 12.93w/s, dev=2]model.layers.56.self_attn.q_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:03<00:00, 13.03w/s, dev=2]model.layers.56.self_attn.q_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:03<00:00, 13.12w/s, dev=2]model.layers.56.self_attn.rotary_emb.inv_freq:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:03<00:00, 13.12w/s, dev=2]model.layers.56.self_attn.v_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:03<00:00, 13.37w/s, dev=2]      model.layers.57.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:03<00:00, 13.60w/s, dev=2]model.layers.57.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:03<00:00, 13.82w/s, dev=2]model.layers.57.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:04<00:00, 13.91w/s, dev=2]model.layers.57.self_attn.q_proj.weight:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:04<00:00, 14.00w/s, dev=2]model.layers.57.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:04<00:00, 14.00w/s, dev=2]model.layers.57.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:04<00:00, 14.25w/s, dev=2]                                                                                                      0%|          | 0/55 [00:00<?, ?w/s]model.layers.57.input_layernorm.weight:   0%|          | 0/55 [00:00<?, ?w/s, dev=2]model.layers.57.mlp.down_proj.weight:   2%|â–         | 1/55 [00:00<00:00, 1229.28w/s, dev=2]model.layers.57.mlp.down_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:04, 12.78w/s, dev=2]  model.layers.57.mlp.gate_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:04, 12.75w/s, dev=2]model.layers.57.mlp.up_proj.weight:   5%|â–Œ         | 3/55 [00:00<00:05,  9.70w/s, dev=2]  model.layers.57.mlp.up_proj.weight:   7%|â–‹         | 4/55 [00:00<00:05,  8.61w/s, dev=2]model.layers.57.post_attention_layernorm.weight:   7%|â–‹         | 4/55 [00:00<00:05,  8.60w/s, dev=2]model.layers.58.input_layernorm.weight:   9%|â–‰         | 5/55 [00:00<00:04, 10.74w/s, dev=2]         model.layers.58.mlp.down_proj.weight:  11%|â–ˆ         | 6/55 [00:00<00:03, 12.88w/s, dev=2]  model.layers.58.mlp.down_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:04, 11.26w/s, dev=2]model.layers.58.mlp.gate_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:04, 11.26w/s, dev=2]model.layers.58.mlp.up_proj.weight:  15%|â–ˆâ–        | 8/55 [00:00<00:04, 10.31w/s, dev=2]  model.layers.58.post_attention_layernorm.weight:  16%|â–ˆâ–‹        | 9/55 [00:00<00:04,  9.64w/s, dev=2]model.layers.58.post_attention_layernorm.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:04, 10.70w/s, dev=2]model.layers.58.self_attn.k_proj.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:04, 10.70w/s, dev=2]        model.layers.58.self_attn.o_proj.weight:  20%|â–ˆâ–ˆ        | 11/55 [00:00<00:03, 11.69w/s, dev=2]model.layers.58.self_attn.q_proj.weight:  22%|â–ˆâ–ˆâ–       | 12/55 [00:00<00:03, 12.18w/s, dev=2]model.layers.58.self_attn.rotary_emb.inv_freq:  24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:01<00:03, 12.61w/s, dev=2]model.layers.58.self_attn.v_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 14/55 [00:01<00:03, 13.57w/s, dev=2]      model.layers.58.self_attn.v_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 15/55 [00:01<00:02, 14.46w/s, dev=2]model.layers.59.input_layernorm.weight:  27%|â–ˆâ–ˆâ–‹       | 15/55 [00:01<00:02, 14.45w/s, dev=2] model.layers.59.mlp.down_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 16/55 [00:01<00:02, 15.41w/s, dev=2]  model.layers.59.mlp.gate_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:01<00:02, 14.25w/s, dev=2]model.layers.59.mlp.up_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/55 [00:01<00:02, 13.44w/s, dev=2]  model.layers.59.post_attention_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–      | 19/55 [00:01<00:02, 12.76w/s, dev=2]model.layers.59.post_attention_layernorm.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:01<00:02, 13.42w/s, dev=2]model.layers.59.self_attn.k_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:01<00:02, 13.42w/s, dev=2]        model.layers.59.self_attn.o_proj.weight:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/55 [00:01<00:02, 14.02w/s, dev=2]model.layers.59.self_attn.q_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/55 [00:01<00:02, 14.25w/s, dev=2]model.layers.59.self_attn.rotary_emb.inv_freq:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 23/55 [00:01<00:02, 14.48w/s, dev=2]model.layers.59.self_attn.v_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 24/55 [00:01<00:02, 15.10w/s, dev=2]      model.layers.59.self_attn.v_proj.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:01<00:01, 15.66w/s, dev=2]model.layers.60.input_layernorm.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:01<00:01, 15.66w/s, dev=2] model.layers.60.mlp.down_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 26/55 [00:01<00:01, 16.29w/s, dev=2]  model.layers.60.mlp.gate_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 27/55 [00:01<00:01, 15.47w/s, dev=2]model.layers.60.mlp.up_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 28/55 [00:01<00:01, 14.78w/s, dev=2]  model.layers.60.post_attention_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/55 [00:02<00:01, 14.19w/s, dev=2]model.layers.60.post_attention_layernorm.weight:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 30/55 [00:02<00:01, 14.68w/s, dev=2]model.layers.60.self_attn.k_proj.weight:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 30/55 [00:02<00:01, 14.68w/s, dev=2]        model.layers.60.self_attn.o_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/55 [00:02<00:01, 15.12w/s, dev=2]model.layers.60.self_attn.q_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 32/55 [00:02<00:01, 15.27w/s, dev=2]model.layers.60.self_attn.rotary_emb.inv_freq:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/55 [00:02<00:01, 15.43w/s, dev=2]model.layers.60.self_attn.v_proj.weight:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/55 [00:02<00:01, 15.89w/s, dev=2]      model.layers.60.self_attn.v_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:02<00:01, 16.31w/s, dev=2]model.layers.61.input_layernorm.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:02<00:01, 16.31w/s, dev=2] model.layers.61.mlp.down_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 36/55 [00:02<00:01, 16.77w/s, dev=2]  model.layers.61.mlp.gate_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 37/55 [00:02<00:01, 16.10w/s, dev=2]model.layers.61.mlp.up_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 38/55 [00:02<00:01, 15.47w/s, dev=2]  model.layers.61.post_attention_layernorm.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 39/55 [00:02<00:01, 14.92w/s, dev=2]model.layers.61.post_attention_layernorm.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 40/55 [00:02<00:00, 15.30w/s, dev=2]model.layers.61.self_attn.k_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 40/55 [00:02<00:00, 15.30w/s, dev=2]        model.layers.61.self_attn.o_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/55 [00:02<00:00, 15.65w/s, dev=2]model.layers.61.self_attn.q_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 42/55 [00:02<00:00, 15.76w/s, dev=2]model.layers.61.self_attn.rotary_emb.inv_freq:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 43/55 [00:02<00:00, 15.87w/s, dev=2]model.layers.61.self_attn.v_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 44/55 [00:02<00:00, 16.23w/s, dev=2]      model.layers.61.self_attn.v_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:02<00:00, 16.56w/s, dev=2]model.layers.62.input_layernorm.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:03<00:00, 14.67w/s, dev=3] model.layers.62.mlp.down_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 46/55 [00:03<00:00, 14.99w/s, dev=3]  model.layers.62.mlp.gate_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 47/55 [00:03<00:00, 14.58w/s, dev=3]model.layers.62.mlp.up_proj.weight:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 48/55 [00:03<00:00, 14.20w/s, dev=3]  model.layers.62.post_attention_layernorm.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 49/55 [00:03<00:00, 13.84w/s, dev=3]model.layers.62.post_attention_layernorm.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 50/55 [00:03<00:00, 14.12w/s, dev=3]model.layers.62.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 50/55 [00:03<00:00, 14.12w/s, dev=3]        model.layers.62.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 51/55 [00:03<00:00, 14.38w/s, dev=3]model.layers.62.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 52/55 [00:03<00:00, 14.47w/s, dev=3]model.layers.62.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 53/55 [00:03<00:00, 14.57w/s, dev=3]model.layers.62.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 54/55 [00:03<00:00, 14.84w/s, dev=3]      model.layers.62.self_attn.v_proj.weight: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55/55 [00:03<00:00, 15.09w/s, dev=3]                                                                                                0%|          | 0/57 [00:00<?, ?w/s]model.layers.63.input_layernorm.weight:   0%|          | 0/57 [00:00<?, ?w/s, dev=3]model.layers.63.mlp.down_proj.weight:   2%|â–         | 1/57 [00:00<00:00, 920.41w/s, dev=3]model.layers.63.mlp.down_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:04, 12.63w/s, dev=3] model.layers.63.mlp.gate_proj.weight:   4%|â–Ž         | 2/57 [00:00<00:04, 12.61w/s, dev=3]model.layers.63.mlp.up_proj.weight:   5%|â–Œ         | 3/57 [00:00<00:08,  6.12w/s, dev=3]  model.layers.63.mlp.up_proj.weight:   7%|â–‹         | 4/57 [00:00<00:10,  5.10w/s, dev=3]model.layers.63.post_attention_layernorm.weight:   7%|â–‹         | 4/57 [00:00<00:10,  5.10w/s, dev=3]model.layers.63.self_attn.k_proj.weight:   9%|â–‰         | 5/57 [00:00<00:08,  6.37w/s, dev=3]        model.layers.63.self_attn.k_proj.weight:  11%|â–ˆ         | 6/57 [00:00<00:08,  6.31w/s, dev=3]model.layers.63.self_attn.o_proj.weight:  11%|â–ˆ         | 6/57 [00:00<00:08,  6.30w/s, dev=3]model.layers.63.self_attn.q_proj.weight:  12%|â–ˆâ–        | 7/57 [00:01<00:07,  6.92w/s, dev=3]model.layers.63.self_attn.q_proj.weight:  14%|â–ˆâ–        | 8/57 [00:01<00:08,  6.00w/s, dev=3]model.layers.63.self_attn.rotary_emb.inv_freq:  14%|â–ˆâ–        | 8/57 [00:01<00:08,  6.00w/s, dev=3]model.layers.63.self_attn.v_proj.weight:  16%|â–ˆâ–Œ        | 9/57 [00:01<00:07,  6.75w/s, dev=3]      model.layers.64.input_layernorm.weight:  18%|â–ˆâ–Š        | 10/57 [00:01<00:06,  7.45w/s, dev=3]model.layers.64.mlp.down_proj.weight:  19%|â–ˆâ–‰        | 11/57 [00:01<00:05,  8.20w/s, dev=3]  model.layers.64.mlp.down_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:01<00:05,  7.98w/s, dev=3]model.layers.64.mlp.gate_proj.weight:  21%|â–ˆâ–ˆ        | 12/57 [00:01<00:05,  7.98w/s, dev=3]model.layers.64.mlp.up_proj.weight:  23%|â–ˆâ–ˆâ–Ž       | 13/57 [00:01<00:05,  7.67w/s, dev=3]  model.layers.64.post_attention_layernorm.weight:  25%|â–ˆâ–ˆâ–       | 14/57 [00:01<00:05,  7.26w/s, dev=3]model.layers.64.self_attn.k_proj.weight:  26%|â–ˆâ–ˆâ–‹       | 15/57 [00:01<00:05,  7.77w/s, dev=3]        model.layers.64.self_attn.k_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:01<00:05,  8.02w/s, dev=3]model.layers.64.self_attn.o_proj.weight:  28%|â–ˆâ–ˆâ–Š       | 16/57 [00:01<00:05,  8.02w/s, dev=3]model.layers.64.self_attn.q_proj.weight:  30%|â–ˆâ–ˆâ–‰       | 17/57 [00:02<00:04,  8.26w/s, dev=3]model.layers.64.self_attn.rotary_emb.inv_freq:  32%|â–ˆâ–ˆâ–ˆâ–      | 18/57 [00:02<00:04,  7.89w/s, dev=3]model.layers.64.self_attn.v_proj.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 19/57 [00:02<00:04,  8.33w/s, dev=3]      model.layers.64.self_attn.v_proj.weight:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 20/57 [00:02<00:04,  8.74w/s, dev=3]model.layers.65.input_layernorm.weight:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 20/57 [00:02<00:04,  8.74w/s, dev=3] model.layers.65.mlp.down_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 21/57 [00:02<00:03,  9.17w/s, dev=3]  model.layers.65.mlp.gate_proj.weight:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [00:02<00:03,  8.96w/s, dev=3]model.layers.65.mlp.up_proj.weight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 23/57 [00:02<00:03,  8.79w/s, dev=3]  model.layers.65.mlp.up_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/57 [00:02<00:03,  8.67w/s, dev=3]model.layers.65.post_attention_layernorm.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/57 [00:02<00:03,  8.67w/s, dev=3]model.layers.65.self_attn.k_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/57 [00:02<00:03,  9.03w/s, dev=3]        model.layers.65.self_attn.o_proj.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 26/57 [00:02<00:03,  9.12w/s, dev=3]model.layers.65.self_attn.q_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 27/57 [00:02<00:03,  9.32w/s, dev=3]model.layers.65.self_attn.q_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:02<00:03,  9.50w/s, dev=3]model.layers.65.self_attn.rotary_emb.inv_freq:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [00:02<00:03,  9.50w/s, dev=3]model.layers.65.self_attn.v_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 29/57 [00:02<00:02,  9.84w/s, dev=3]      model.layers.66.input_layernorm.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 30/57 [00:02<00:02, 10.16w/s, dev=3] model.layers.66.mlp.down_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 31/57 [00:02<00:02, 10.49w/s, dev=3]  model.layers.66.mlp.down_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 32/57 [00:03<00:02, 10.25w/s, dev=3]model.layers.66.mlp.gate_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 32/57 [00:03<00:02, 10.25w/s, dev=3]model.layers.66.mlp.up_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 33/57 [00:03<00:02, 10.05w/s, dev=3]  model.layers.66.post_attention_layernorm.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [00:03<00:02,  9.57w/s, dev=3]model.layers.66.self_attn.k_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/57 [00:03<00:02,  9.85w/s, dev=3]        model.layers.66.self_attn.k_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 36/57 [00:03<00:02, 10.09w/s, dev=3]model.layers.66.self_attn.o_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 36/57 [00:03<00:02, 10.09w/s, dev=3]model.layers.66.self_attn.q_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 37/57 [00:03<00:01, 10.18w/s, dev=3]model.layers.66.self_attn.rotary_emb.inv_freq:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 38/57 [00:03<00:01, 10.32w/s, dev=3]model.layers.66.self_attn.v_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 39/57 [00:03<00:01, 10.59w/s, dev=3]      model.layers.66.self_attn.v_proj.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:03<00:01, 10.17w/s, dev=3]model.layers.67.input_layernorm.weight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [00:03<00:01, 10.17w/s, dev=3] model.layers.67.mlp.down_proj.weight:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/57 [00:03<00:01, 10.43w/s, dev=3]  model.layers.67.mlp.gate_proj.weight:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 42/57 [00:04<00:01, 10.26w/s, dev=3]model.layers.67.mlp.up_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 43/57 [00:04<00:01, 10.13w/s, dev=3]  model.layers.67.mlp.up_proj.weight:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 44/57 [00:04<00:01,  9.68w/s, dev=3]model.layers.67.post_attention_layernorm.weight:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 44/57 [00:04<00:01,  9.68w/s, dev=3]model.layers.67.self_attn.k_proj.weight:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 45/57 [00:04<00:01,  9.90w/s, dev=3]        model.layers.67.self_attn.o_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [00:04<00:01, 10.10w/s, dev=3]model.layers.67.self_attn.q_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 47/57 [00:04<00:00, 10.21w/s, dev=3]model.layers.67.self_attn.q_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/57 [00:04<00:00, 10.31w/s, dev=3]model.layers.67.self_attn.rotary_emb.inv_freq:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/57 [00:04<00:00, 10.31w/s, dev=3]model.layers.67.self_attn.v_proj.weight:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 49/57 [00:04<00:00, 10.53w/s, dev=3]      model.layers.68.mlp.down_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 50/57 [00:04<00:00, 10.39w/s, dev=3]   model.layers.68.mlp.gate_proj.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 51/57 [00:04<00:00, 10.26w/s, dev=3]model.layers.68.mlp.gate_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:05<00:00, 10.15w/s, dev=3]model.layers.68.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [00:05<00:00, 10.15w/s, dev=3]model.layers.68.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 53/57 [00:05<00:00, 10.24w/s, dev=3]model.layers.68.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/57 [00:05<00:00, 10.34w/s, dev=3]model.layers.68.self_attn.rotary_emb.inv_freq:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 55/57 [00:05<00:00, 10.44w/s, dev=3]model.layers.68.self_attn.rotary_emb.inv_freq:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 56/57 [00:05<00:00, 10.62w/s, dev=3]model.layers.68.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 56/57 [00:05<00:00, 10.62w/s, dev=3]                                                                                                      0%|          | 0/59 [00:00<?, ?w/s]model.layers.68.input_layernorm.weight:   0%|          | 0/59 [00:00<?, ?w/s, dev=3]model.layers.68.mlp.up_proj.weight:   2%|â–         | 1/59 [00:00<00:00, 915.99w/s, dev=3]model.layers.68.mlp.up_proj.weight:   3%|â–Ž         | 2/59 [00:00<00:07,  8.13w/s, dev=3] model.layers.68.post_attention_layernorm.weight:   3%|â–Ž         | 2/59 [00:00<00:07,  8.12w/s, dev=3]model.layers.69.input_layernorm.weight:   5%|â–Œ         | 3/59 [00:00<00:04, 12.16w/s, dev=3]         model.layers.69.mlp.down_proj.weight:   7%|â–‹         | 4/59 [00:00<00:03, 16.20w/s, dev=3]  model.layers.69.mlp.down_proj.weight:   8%|â–Š         | 5/59 [00:00<00:04, 12.70w/s, dev=3]model.layers.69.mlp.gate_proj.weight:   8%|â–Š         | 5/59 [00:00<00:04, 12.70w/s, dev=3]model.layers.69.mlp.up_proj.weight:  10%|â–ˆ         | 6/59 [00:00<00:04, 10.95w/s, dev=3]  model.layers.69.post_attention_layernorm.weight:  12%|â–ˆâ–        | 7/59 [00:00<00:05,  9.99w/s, dev=3]model.layers.69.post_attention_layernorm.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:04, 11.41w/s, dev=3]model.layers.69.self_attn.k_proj.weight:  14%|â–ˆâ–Ž        | 8/59 [00:00<00:04, 11.41w/s, dev=3]        model.layers.69.self_attn.o_proj.weight:  15%|â–ˆâ–Œ        | 9/59 [00:00<00:03, 12.68w/s, dev=3]model.layers.69.self_attn.q_proj.weight:  17%|â–ˆâ–‹        | 10/59 [00:00<00:03, 13.22w/s, dev=3]model.layers.69.self_attn.q_proj.weight:  19%|â–ˆâ–Š        | 11/59 [00:00<00:03, 13.67w/s, dev=3]model.layers.69.self_attn.rotary_emb.inv_freq:  19%|â–ˆâ–Š        | 11/59 [00:00<00:03, 13.67w/s, dev=3]model.layers.69.self_attn.v_proj.weight:  20%|â–ˆâ–ˆ        | 12/59 [00:00<00:03, 14.91w/s, dev=3]      model.layers.70.input_layernorm.weight:  22%|â–ˆâ–ˆâ–       | 13/59 [00:00<00:02, 16.01w/s, dev=3] model.layers.70.mlp.down_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:00<00:02, 17.24w/s, dev=3]  model.layers.70.mlp.down_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:02, 15.52w/s, dev=3]model.layers.70.mlp.gate_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:00<00:02, 15.52w/s, dev=3]model.layers.70.mlp.up_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:01<00:02, 14.37w/s, dev=3]  model.layers.70.post_attention_layernorm.weight:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:01<00:03, 13.49w/s, dev=3]model.layers.70.self_attn.k_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:01<00:02, 14.28w/s, dev=3]        model.layers.70.self_attn.k_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:02, 14.99w/s, dev=3]model.layers.70.self_attn.o_proj.weight:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:01<00:02, 14.99w/s, dev=3]model.layers.70.self_attn.q_proj.weight:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:01<00:02, 15.26w/s, dev=3]model.layers.70.self_attn.rotary_emb.inv_freq:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:01<00:02, 15.48w/s, dev=3]model.layers.70.self_attn.v_proj.weight:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:01<00:02, 16.21w/s, dev=3]      model.layers.71.input_layernorm.weight:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [00:01<00:02, 16.87w/s, dev=3] model.layers.71.mlp.down_proj.weight:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [00:01<00:01, 17.60w/s, dev=3]  model.layers.71.mlp.down_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:02, 16.57w/s, dev=3]model.layers.71.mlp.gate_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [00:01<00:02, 16.56w/s, dev=3]model.layers.71.mlp.up_proj.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [00:01<00:02, 15.70w/s, dev=3]  model.layers.71.post_attention_layernorm.weight:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [00:01<00:02, 14.97w/s, dev=3]model.layers.71.self_attn.k_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [00:01<00:01, 15.52w/s, dev=3]        model.layers.71.self_attn.o_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [00:01<00:01, 16.02w/s, dev=3]model.layers.71.self_attn.q_proj.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [00:01<00:01, 16.19w/s, dev=3]model.layers.71.self_attn.q_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:01<00:01, 16.35w/s, dev=3]model.layers.71.self_attn.rotary_emb.inv_freq:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [00:01<00:01, 16.35w/s, dev=3]model.layers.71.self_attn.v_proj.weight:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [00:01<00:01, 16.87w/s, dev=3]      model.layers.72.input_layernorm.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [00:01<00:01, 17.34w/s, dev=3] model.layers.72.mlp.down_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [00:01<00:01, 17.86w/s, dev=3]  model.layers.72.mlp.gate_proj.weight:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [00:02<00:01, 17.08w/s, dev=3]model.layers.72.mlp.up_proj.weight:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [00:02<00:01, 16.39w/s, dev=3]  model.layers.72.mlp.up_proj.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:02<00:01, 15.79w/s, dev=3]model.layers.72.post_attention_layernorm.weight:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [00:02<00:01, 15.79w/s, dev=3]model.layers.72.self_attn.k_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [00:02<00:01, 16.21w/s, dev=3]        model.layers.72.self_attn.o_proj.weight:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [00:02<00:01, 16.60w/s, dev=3]model.layers.72.self_attn.q_proj.weight:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [00:02<00:01, 16.72w/s, dev=3]model.layers.72.self_attn.rotary_emb.inv_freq:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [00:02<00:01, 16.84w/s, dev=3]model.layers.72.self_attn.v_proj.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [00:02<00:00, 17.25w/s, dev=3]      model.layers.73.input_layernorm.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [00:02<00:00, 17.62w/s, dev=3] model.layers.73.mlp.down_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [00:02<00:00, 18.03w/s, dev=3]  model.layers.73.mlp.down_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:02<00:00, 17.39w/s, dev=3]model.layers.73.mlp.gate_proj.weight:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [00:02<00:00, 17.39w/s, dev=3]model.layers.73.mlp.up_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [00:02<00:00, 16.79w/s, dev=3]  model.layers.73.post_attention_layernorm.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [00:02<00:00, 16.27w/s, dev=3]model.layers.73.self_attn.k_proj.weight:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [00:02<00:00, 16.62w/s, dev=3]        model.layers.73.self_attn.o_proj.weight:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [00:02<00:00, 16.93w/s, dev=3]model.layers.73.self_attn.q_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [00:02<00:00, 17.02w/s, dev=3]model.layers.73.self_attn.rotary_emb.inv_freq:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [00:02<00:00, 17.11w/s, dev=3]model.layers.73.self_attn.v_proj.weight:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [00:02<00:00, 17.44w/s, dev=3]      model.layers.73.self_attn.v_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:02<00:00, 17.74w/s, dev=3]model.layers.74.mlp.gate_proj.weight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [00:02<00:00, 17.74w/s, dev=3]   model.layers.74.self_attn.k_proj.weight:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [00:03<00:00, 17.23w/s, dev=3]model.layers.74.self_attn.o_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [00:03<00:00, 17.51w/s, dev=3]model.layers.74.self_attn.q_proj.weight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [00:03<00:00, 17.59w/s, dev=3]model.layers.74.self_attn.rotary_emb.inv_freq:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [00:03<00:00, 17.66w/s, dev=3]model.layers.74.self_attn.v_proj.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [00:03<00:00, 17.96w/s, dev=3]                                                                                                      0%|          | 0/55 [00:00<?, ?w/s]model.layers.74.input_layernorm.weight:   0%|          | 0/55 [00:00<?, ?w/s, dev=3]model.layers.74.mlp.down_proj.weight:   2%|â–         | 1/55 [00:00<00:00, 992.73w/s, dev=3]model.layers.74.mlp.down_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:04, 12.48w/s, dev=3] model.layers.74.mlp.up_proj.weight:   4%|â–Ž         | 2/55 [00:00<00:04, 12.46w/s, dev=3]  model.layers.74.post_attention_layernorm.weight:   5%|â–Œ         | 3/55 [00:00<00:05,  9.62w/s, dev=3]model.layers.74.post_attention_layernorm.weight:   7%|â–‹         | 4/55 [00:00<00:03, 12.81w/s, dev=3]model.layers.75.input_layernorm.weight:   7%|â–‹         | 4/55 [00:00<00:03, 12.80w/s, dev=3]         model.layers.75.mlp.down_proj.weight:   9%|â–‰         | 5/55 [00:00<00:03, 15.99w/s, dev=3]  model.layers.75.mlp.down_proj.weight:  11%|â–ˆ         | 6/55 [00:00<00:03, 12.93w/s, dev=3]model.layers.75.mlp.gate_proj.weight:  11%|â–ˆ         | 6/55 [00:00<00:03, 12.93w/s, dev=3]model.layers.75.mlp.up_proj.weight:  13%|â–ˆâ–Ž        | 7/55 [00:00<00:04, 11.38w/s, dev=3]  model.layers.75.mlp.up_proj.weight:  15%|â–ˆâ–        | 8/55 [00:00<00:04, 10.49w/s, dev=3]model.layers.75.post_attention_layernorm.weight:  15%|â–ˆâ–        | 8/55 [00:00<00:04, 10.49w/s, dev=3]model.layers.75.self_attn.k_proj.weight:  16%|â–ˆâ–‹        | 9/55 [00:00<00:03, 11.79w/s, dev=3]        model.layers.75.self_attn.o_proj.weight:  18%|â–ˆâ–Š        | 10/55 [00:00<00:03, 13.00w/s, dev=3]model.layers.75.self_attn.q_proj.weight:  20%|â–ˆâ–ˆ        | 11/55 [00:00<00:03, 13.52w/s, dev=3]model.layers.75.self_attn.rotary_emb.inv_freq:  22%|â–ˆâ–ˆâ–       | 12/55 [00:00<00:03, 14.00w/s, dev=3]model.layers.75.self_attn.v_proj.weight:  24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:00<00:02, 15.16w/s, dev=3]      model.layers.75.self_attn.v_proj.weight:  25%|â–ˆâ–ˆâ–Œ       | 14/55 [00:00<00:02, 16.21w/s, dev=3]model.layers.76.input_layernorm.weight:  25%|â–ˆâ–ˆâ–Œ       | 14/55 [00:00<00:02, 16.21w/s, dev=3] model.layers.76.mlp.down_proj.weight:  27%|â–ˆâ–ˆâ–‹       | 15/55 [00:00<00:02, 17.36w/s, dev=3]  model.layers.76.mlp.gate_proj.weight:  29%|â–ˆâ–ˆâ–‰       | 16/55 [00:01<00:02, 15.80w/s, dev=3]model.layers.76.mlp.up_proj.weight:  31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:01<00:02, 14.62w/s, dev=3]  model.layers.76.post_attention_layernorm.weight:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/55 [00:01<00:02, 13.74w/s, dev=3]model.layers.76.self_attn.k_proj.weight:  35%|â–ˆâ–ˆâ–ˆâ–      | 19/55 [00:01<00:02, 14.49w/s, dev=3]        model.layers.76.self_attn.k_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:01<00:02, 15.19w/s, dev=3]model.layers.76.self_attn.o_proj.weight:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:01<00:02, 15.18w/s, dev=3]model.layers.76.self_attn.q_proj.weight:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/55 [00:01<00:02, 15.44w/s, dev=3]model.layers.76.self_attn.rotary_emb.inv_freq:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/55 [00:01<00:02, 15.68w/s, dev=3]model.layers.76.self_attn.v_proj.weight:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 23/55 [00:01<00:01, 16.38w/s, dev=3]      model.layers.77.input_layernorm.weight:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 24/55 [00:01<00:01, 17.02w/s, dev=3] model.layers.77.mlp.down_proj.weight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:01<00:01, 17.73w/s, dev=3]  model.layers.77.mlp.down_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 26/55 [00:01<00:01, 16.68w/s, dev=3]model.layers.77.mlp.gate_proj.weight:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 26/55 [00:01<00:01, 16.68w/s, dev=3]model.layers.77.mlp.up_proj.weight:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 27/55 [00:01<00:01, 15.75w/s, dev=3]  model.layers.77.post_attention_layernorm.weight:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 28/55 [00:01<00:01, 14.97w/s, dev=3]model.layers.77.self_attn.k_proj.weight:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/55 [00:01<00:01, 15.51w/s, dev=3]        model.layers.77.self_attn.o_proj.weight:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 30/55 [00:01<00:01, 15.99w/s, dev=3]model.layers.77.self_attn.q_proj.weight:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/55 [00:01<00:01, 16.15w/s, dev=3]model.layers.77.self_attn.q_proj.weight:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 32/55 [00:01<00:01, 16.30w/s, dev=3]model.layers.77.self_attn.rotary_emb.inv_freq:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 32/55 [00:01<00:01, 16.30w/s, dev=3]model.layers.77.self_attn.v_proj.weight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/55 [00:01<00:01, 16.80w/s, dev=3]      model.layers.78.input_layernorm.weight:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/55 [00:01<00:01, 17.26w/s, dev=3] model.layers.78.mlp.down_proj.weight:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:01<00:01, 17.77w/s, dev=3]  model.layers.78.mlp.gate_proj.weight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 36/55 [00:02<00:01, 16.96w/s, dev=3]model.layers.78.mlp.up_proj.weight:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 37/55 [00:02<00:01, 16.29w/s, dev=3]  model.layers.78.mlp.up_proj.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 38/55 [00:02<00:01, 15.69w/s, dev=3]model.layers.78.post_attention_layernorm.weight:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 38/55 [00:02<00:01, 15.69w/s, dev=3]model.layers.78.self_attn.k_proj.weight:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 39/55 [00:02<00:00, 16.10w/s, dev=3]        model.layers.78.self_attn.o_proj.weight:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 40/55 [00:02<00:00, 16.47w/s, dev=3]model.layers.78.self_attn.q_proj.weight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/55 [00:02<00:00, 16.57w/s, dev=3]model.layers.78.self_attn.rotary_emb.inv_freq:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 42/55 [00:02<00:00, 16.66w/s, dev=3]model.layers.78.self_attn.v_proj.weight:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 43/55 [00:02<00:00, 17.05w/s, dev=3]      model.layers.78.self_attn.v_proj.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 44/55 [00:02<00:00, 17.40w/s, dev=3]model.layers.79.input_layernorm.weight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 44/55 [00:02<00:00, 17.40w/s, dev=3] model.layers.79.mlp.down_proj.weight:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:02<00:00, 17.79w/s, dev=3]  model.layers.79.mlp.gate_proj.weight:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 46/55 [00:02<00:00, 16.92w/s, dev=3]model.layers.79.mlp.up_proj.weight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 47/55 [00:02<00:00, 16.33w/s, dev=3]  model.layers.79.post_attention_layernorm.weight:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 48/55 [00:03<00:00, 15.87w/s, dev=3]model.layers.79.self_attn.k_proj.weight:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 49/55 [00:03<00:00, 16.20w/s, dev=3]        model.layers.79.self_attn.k_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 50/55 [00:03<00:00, 16.47w/s, dev=3]model.layers.79.self_attn.o_proj.weight:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 50/55 [00:03<00:00, 16.47w/s, dev=3]model.layers.79.self_attn.q_proj.weight:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 51/55 [00:03<00:00, 16.56w/s, dev=3]model.layers.79.self_attn.rotary_emb.inv_freq:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 52/55 [00:03<00:00, 16.65w/s, dev=3]model.layers.79.self_attn.v_proj.weight:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 53/55 [00:03<00:00, 16.97w/s, dev=3]      model.norm.weight:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 54/55 [00:03<00:00, 17.26w/s, dev=3]                                                                                                0%|          | 0/1 [00:00<?, ?w/s]lm_head.weight:   0%|          | 0/1 [00:00<?, ?w/s, dev=3]lm_head.weight: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  6.20w/s, dev=3]                                                                   self.device: cuda:0
Loading BASE model stabilityai/stablelm-base-alpha-3b-v2...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:04<00:04,  4.90s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:05<00:00,  2.34s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:05<00:00,  2.73s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[2] MOVING BASE MODEL TO GPU.../home/chawins/.conda/envs/mimir/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for iamgroot42/mimir contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/iamgroot42/mimir
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
Loading dataset the_pile...
Loading from HuggingFace!
Loading dataset the_pile...
Loading from HuggingFace!
Generating samples:   0%|          | 0/21 [00:00<?, ?it/s]Generating samples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:00<00:00, 866.02it/s]
NEW N_SAMPLES IS  1000
Writing raw data to tmp_results/neo125_github_experiment/meta-llama_Llama-2-70b-hf/temporal_wiki/raw_data.json
Writing raw data to tmp_results/neo125_github_experiment/meta-llama_Llama-2-70b-hf/temporal_wiki/raw_data_lens.json
Computing criterion:   0%|          | 0/20 [00:00<?, ?it/s]Computing criterion:   5%|â–Œ         | 1/20 [00:24<07:43, 24.38s/it]Computing criterion:  10%|â–ˆ         | 2/20 [00:40<05:55, 19.77s/it]Computing criterion:  15%|â–ˆâ–Œ        | 3/20 [00:58<05:17, 18.66s/it]Computing criterion:  20%|â–ˆâ–ˆ        | 4/20 [01:15<04:46, 17.90s/it]Computing criterion:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [01:32<04:23, 17.60s/it]Computing criterion:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [01:48<04:02, 17.30s/it]Computing criterion:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [02:05<03:44, 17.27s/it]Computing criterion:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [02:22<03:24, 17.00s/it]Computing criterion:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [02:39<03:07, 17.00s/it]Computing criterion:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [02:56<02:49, 16.99s/it]Computing criterion:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [03:13<02:33, 17.05s/it]Computing criterion:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [03:30<02:17, 17.13s/it]Computing criterion:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [03:47<01:59, 17.01s/it]Computing criterion:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [04:04<01:42, 17.01s/it]Computing criterion:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [04:21<01:25, 17.06s/it]Computing criterion:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [04:39<01:09, 17.26s/it]Computing criterion:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [04:56<00:51, 17.09s/it]Computing criterion:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [05:12<00:33, 16.89s/it]Computing criterion:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [05:30<00:17, 17.15s/it]Computing criterion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [05:47<00:00, 17.13s/it]Computing criterion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [05:47<00:00, 17.37s/it]
Ref scores:   0%|          | 0/1000 [00:00<?, ?it/s]Ref scores:   0%|          | 1/1000 [00:09<2:32:41,  9.17s/it]Ref scores:   0%|          | 2/1000 [00:09<1:04:24,  3.87s/it]Ref scores:   0%|          | 3/1000 [00:09<36:08,  2.17s/it]  Ref scores:   0%|          | 4/1000 [00:09<22:51,  1.38s/it]Ref scores:   0%|          | 5/1000 [00:09<15:16,  1.09it/s]Ref scores:   1%|          | 6/1000 [00:09<10:50,  1.53it/s]Ref scores:   1%|          | 7/1000 [00:10<08:19,  1.99it/s]Ref scores:   1%|          | 8/1000 [00:10<06:14,  2.65it/s]Ref scores:   1%|          | 9/1000 [00:10<04:50,  3.41it/s]Ref scores:   1%|          | 11/1000 [00:10<03:19,  4.96it/s]Ref scores:   1%|          | 12/1000 [00:10<03:07,  5.26it/s]Ref scores:   1%|â–         | 13/1000 [00:10<02:53,  5.69it/s]Ref scores:   1%|â–         | 14/1000 [00:10<02:43,  6.05it/s]Ref scores:   2%|â–         | 15/1000 [00:11<02:34,  6.36it/s]Ref scores:   2%|â–         | 16/1000 [00:11<02:19,  7.04it/s]Ref scores:   2%|â–         | 17/1000 [00:11<02:18,  7.09it/s]Ref scores:   2%|â–         | 18/1000 [00:11<02:16,  7.18it/s]Ref scores:   2%|â–         | 19/1000 [00:11<02:16,  7.20it/s]Ref scores:   2%|â–         | 20/1000 [00:11<02:20,  6.97it/s]Ref scores:   2%|â–         | 21/1000 [00:11<02:09,  7.56it/s]Ref scores:   2%|â–         | 22/1000 [00:11<02:09,  7.55it/s]Ref scores:   2%|â–         | 23/1000 [00:12<02:09,  7.52it/s]Ref scores:   2%|â–         | 24/1000 [00:12<02:11,  7.45it/s]Ref scores:   2%|â–Ž         | 25/1000 [00:12<02:11,  7.43it/s]Ref scores:   3%|â–Ž         | 26/1000 [00:12<02:04,  7.82it/s]Ref scores:   3%|â–Ž         | 27/1000 [00:12<02:06,  7.69it/s]Ref scores:   3%|â–Ž         | 28/1000 [00:12<02:08,  7.59it/s]Ref scores:   3%|â–Ž         | 29/1000 [00:12<02:14,  7.21it/s]Ref scores:   3%|â–Ž         | 30/1000 [00:13<02:13,  7.26it/s]Ref scores:   3%|â–Ž         | 31/1000 [00:13<02:13,  7.28it/s]Ref scores:   3%|â–Ž         | 32/1000 [00:13<02:04,  7.80it/s]Ref scores:   3%|â–Ž         | 33/1000 [00:13<02:06,  7.64it/s]Ref scores:   3%|â–Ž         | 34/1000 [00:13<02:00,  8.00it/s]Ref scores:   4%|â–Ž         | 36/1000 [00:13<01:53,  8.47it/s]Ref scores:   4%|â–Ž         | 37/1000 [00:13<01:57,  8.17it/s]Ref scores:   4%|â–         | 38/1000 [00:14<02:00,  8.00it/s]Ref scores:   4%|â–         | 39/1000 [00:14<01:56,  8.27it/s]Ref scores:   4%|â–         | 40/1000 [00:14<01:59,  8.06it/s]Ref scores:   4%|â–         | 41/1000 [00:14<02:01,  7.88it/s]Ref scores:   4%|â–         | 42/1000 [00:14<01:55,  8.29it/s]Ref scores:   4%|â–         | 43/1000 [00:14<01:59,  7.98it/s]Ref scores:   4%|â–         | 44/1000 [00:14<02:02,  7.82it/s]Ref scores:   4%|â–         | 45/1000 [00:14<02:08,  7.43it/s]Ref scores:   5%|â–         | 46/1000 [00:15<02:08,  7.41it/s]Ref scores:   5%|â–         | 47/1000 [00:15<02:07,  7.46it/s]Ref scores:   5%|â–         | 48/1000 [00:15<02:01,  7.86it/s]Ref scores:   5%|â–         | 49/1000 [00:15<02:03,  7.72it/s]Ref scores:   5%|â–Œ         | 50/1000 [00:15<02:05,  7.59it/s]Ref scores:   5%|â–Œ         | 51/1000 [00:15<02:06,  7.52it/s]Ref scores:   5%|â–Œ         | 53/1000 [00:15<01:55,  8.23it/s]Ref scores:   6%|â–Œ         | 55/1000 [00:16<01:54,  8.27it/s]Ref scores:   6%|â–Œ         | 56/1000 [00:16<02:02,  7.69it/s]Ref scores:   6%|â–Œ         | 57/1000 [00:16<01:58,  7.93it/s]Ref scores:   6%|â–Œ         | 58/1000 [00:16<02:00,  7.79it/s]Ref scores:   6%|â–Œ         | 59/1000 [00:16<02:02,  7.69it/s]Ref scores:   6%|â–Œ         | 60/1000 [00:16<02:04,  7.57it/s]Ref scores:   6%|â–Œ         | 61/1000 [00:16<01:58,  7.90it/s]Ref scores:   6%|â–Œ         | 62/1000 [00:17<01:55,  8.15it/s]Ref scores:   6%|â–‹         | 63/1000 [00:17<01:58,  7.92it/s]Ref scores:   6%|â–‹         | 64/1000 [00:17<02:01,  7.69it/s]Ref scores:   6%|â–‹         | 65/1000 [00:17<02:03,  7.60it/s]Ref scores:   7%|â–‹         | 66/1000 [00:17<01:57,  7.96it/s]Ref scores:   7%|â–‹         | 68/1000 [00:17<01:50,  8.45it/s]Ref scores:   7%|â–‹         | 69/1000 [00:17<01:48,  8.55it/s]Ref scores:   7%|â–‹         | 70/1000 [00:18<01:47,  8.62it/s]Ref scores:   7%|â–‹         | 71/1000 [00:18<01:52,  8.27it/s]Ref scores:   7%|â–‹         | 72/1000 [00:18<01:56,  7.99it/s]Ref scores:   7%|â–‹         | 73/1000 [00:18<01:59,  7.78it/s]Ref scores:   7%|â–‹         | 74/1000 [00:18<02:01,  7.62it/s]Ref scores:   8%|â–Š         | 75/1000 [00:18<02:07,  7.27it/s]Ref scores:   8%|â–Š         | 76/1000 [00:18<02:11,  7.03it/s]Ref scores:   8%|â–Š         | 77/1000 [00:19<02:09,  7.13it/s]Ref scores:   8%|â–Š         | 78/1000 [00:19<02:07,  7.25it/s]Ref scores:   8%|â–Š         | 79/1000 [00:19<02:07,  7.25it/s]Ref scores:   8%|â–Š         | 80/1000 [00:19<01:59,  7.67it/s]Ref scores:   8%|â–Š         | 81/1000 [00:19<02:06,  7.27it/s]Ref scores:   8%|â–Š         | 82/1000 [00:19<02:05,  7.31it/s]Ref scores:   8%|â–Š         | 83/1000 [00:19<02:06,  7.28it/s]Ref scores:   8%|â–Š         | 84/1000 [00:19<02:05,  7.29it/s]Ref scores:   8%|â–Š         | 85/1000 [00:20<02:26,  6.26it/s]Ref scores:   9%|â–Š         | 86/1000 [00:20<02:24,  6.31it/s]Ref scores:   9%|â–Š         | 87/1000 [00:20<02:18,  6.61it/s]Ref scores:   9%|â–‰         | 88/1000 [00:20<02:13,  6.82it/s]Ref scores:   9%|â–‰         | 90/1000 [00:20<01:57,  7.77it/s]Ref scores:   9%|â–‰         | 91/1000 [00:20<01:58,  7.65it/s]Ref scores:   9%|â–‰         | 92/1000 [00:21<02:00,  7.55it/s]Ref scores:   9%|â–‰         | 94/1000 [00:21<01:52,  8.09it/s]Ref scores:  10%|â–‰         | 95/1000 [00:21<02:00,  7.53it/s]Ref scores:  10%|â–‰         | 96/1000 [00:21<02:00,  7.48it/s]Ref scores:  10%|â–‰         | 97/1000 [00:21<02:01,  7.46it/s]Ref scores:  10%|â–‰         | 98/1000 [00:21<02:00,  7.47it/s]Ref scores:  10%|â–‰         | 99/1000 [00:22<01:52,  7.98it/s]Ref scores:  10%|â–ˆ         | 101/1000 [00:22<01:46,  8.43it/s]Ref scores:  10%|â–ˆ         | 102/1000 [00:22<01:49,  8.18it/s]Ref scores:  10%|â–ˆ         | 103/1000 [00:22<01:52,  7.94it/s]Ref scores:  10%|â–ˆ         | 104/1000 [00:22<01:55,  7.73it/s]Ref scores:  10%|â–ˆ         | 105/1000 [00:22<01:52,  7.94it/s]Ref scores:  11%|â–ˆ         | 106/1000 [00:22<01:55,  7.77it/s]Ref scores:  11%|â–ˆ         | 107/1000 [00:23<02:11,  6.79it/s]Ref scores:  11%|â–ˆ         | 108/1000 [00:23<02:08,  6.94it/s]Ref scores:  11%|â–ˆ         | 109/1000 [00:23<02:05,  7.10it/s]Ref scores:  11%|â–ˆ         | 110/1000 [00:23<02:04,  7.13it/s]Ref scores:  11%|â–ˆ         | 111/1000 [00:23<02:08,  6.93it/s]Ref scores:  11%|â–ˆ         | 112/1000 [00:23<02:06,  7.05it/s]Ref scores:  11%|â–ˆâ–        | 113/1000 [00:23<02:03,  7.15it/s]Ref scores:  11%|â–ˆâ–        | 114/1000 [00:24<01:55,  7.68it/s]Ref scores:  12%|â–ˆâ–        | 115/1000 [00:24<02:02,  7.25it/s]Ref scores:  12%|â–ˆâ–        | 116/1000 [00:24<02:01,  7.27it/s]Ref scores:  12%|â–ˆâ–        | 117/1000 [00:24<02:00,  7.30it/s]Ref scores:  12%|â–ˆâ–        | 118/1000 [00:24<01:59,  7.35it/s]Ref scores:  12%|â–ˆâ–        | 119/1000 [00:24<02:04,  7.09it/s]Ref scores:  12%|â–ˆâ–        | 120/1000 [00:24<02:02,  7.17it/s]Ref scores:  12%|â–ˆâ–        | 121/1000 [00:25<02:01,  7.23it/s]Ref scores:  12%|â–ˆâ–        | 122/1000 [00:25<02:00,  7.28it/s]Ref scores:  12%|â–ˆâ–        | 123/1000 [00:25<02:00,  7.28it/s]Ref scores:  12%|â–ˆâ–        | 124/1000 [00:25<01:59,  7.32it/s]Ref scores:  12%|â–ˆâ–Ž        | 125/1000 [00:25<01:58,  7.36it/s]Ref scores:  13%|â–ˆâ–Ž        | 126/1000 [00:25<01:59,  7.32it/s]Ref scores:  13%|â–ˆâ–Ž        | 127/1000 [00:25<02:05,  6.94it/s]Ref scores:  13%|â–ˆâ–Ž        | 128/1000 [00:26<02:08,  6.79it/s]Ref scores:  13%|â–ˆâ–Ž        | 129/1000 [00:26<02:25,  5.99it/s]Ref scores:  13%|â–ˆâ–Ž        | 130/1000 [00:26<02:17,  6.32it/s]Ref scores:  13%|â–ˆâ–Ž        | 131/1000 [00:26<02:05,  6.92it/s]Ref scores:  13%|â–ˆâ–Ž        | 132/1000 [00:26<02:03,  7.00it/s]Ref scores:  13%|â–ˆâ–Ž        | 133/1000 [00:26<02:02,  7.07it/s]Ref scores:  13%|â–ˆâ–Ž        | 134/1000 [00:26<02:01,  7.13it/s]Ref scores:  14%|â–ˆâ–Ž        | 135/1000 [00:27<02:00,  7.19it/s]Ref scores:  14%|â–ˆâ–Ž        | 136/1000 [00:27<01:53,  7.62it/s]Ref scores:  14%|â–ˆâ–Ž        | 137/1000 [00:27<01:54,  7.53it/s]Ref scores:  14%|â–ˆâ–        | 138/1000 [00:27<02:08,  6.72it/s]Ref scores:  14%|â–ˆâ–        | 139/1000 [00:27<01:59,  7.23it/s]Ref scores:  14%|â–ˆâ–        | 140/1000 [00:27<01:58,  7.25it/s]Ref scores:  14%|â–ˆâ–        | 141/1000 [00:27<01:52,  7.65it/s]Ref scores:  14%|â–ˆâ–        | 142/1000 [00:27<01:54,  7.50it/s]Ref scores:  14%|â–ˆâ–        | 143/1000 [00:28<01:54,  7.47it/s]Ref scores:  14%|â–ˆâ–        | 144/1000 [00:28<01:55,  7.41it/s]Ref scores:  14%|â–ˆâ–        | 145/1000 [00:28<02:09,  6.60it/s]Ref scores:  15%|â–ˆâ–        | 146/1000 [00:28<02:05,  6.82it/s]Ref scores:  15%|â–ˆâ–        | 147/1000 [00:28<02:02,  6.94it/s]Ref scores:  15%|â–ˆâ–        | 148/1000 [00:28<02:00,  7.05it/s]Ref scores:  15%|â–ˆâ–        | 149/1000 [00:28<01:53,  7.50it/s]Ref scores:  15%|â–ˆâ–Œ        | 150/1000 [00:29<01:47,  7.87it/s]Ref scores:  15%|â–ˆâ–Œ        | 151/1000 [00:29<01:49,  7.73it/s]Ref scores:  15%|â–ˆâ–Œ        | 152/1000 [00:29<01:51,  7.57it/s]Ref scores:  15%|â–ˆâ–Œ        | 153/1000 [00:29<01:53,  7.47it/s]Ref scores:  15%|â–ˆâ–Œ        | 154/1000 [00:29<01:53,  7.43it/s]Ref scores:  16%|â–ˆâ–Œ        | 155/1000 [00:29<01:54,  7.37it/s]Ref scores:  16%|â–ˆâ–Œ        | 156/1000 [00:29<01:46,  7.92it/s]Ref scores:  16%|â–ˆâ–Œ        | 157/1000 [00:30<01:54,  7.38it/s]Ref scores:  16%|â–ˆâ–Œ        | 158/1000 [00:30<01:47,  7.86it/s]Ref scores:  16%|â–ˆâ–Œ        | 159/1000 [00:30<01:43,  8.15it/s]Ref scores:  16%|â–ˆâ–Œ        | 160/1000 [00:30<01:46,  7.86it/s]Ref scores:  16%|â–ˆâ–Œ        | 161/1000 [00:30<01:53,  7.38it/s]Ref scores:  16%|â–ˆâ–Œ        | 162/1000 [00:30<02:00,  6.97it/s]Ref scores:  16%|â–ˆâ–‹        | 163/1000 [00:30<02:03,  6.79it/s]Ref scores:  16%|â–ˆâ–‹        | 165/1000 [00:31<01:44,  8.00it/s]Ref scores:  17%|â–ˆâ–‹        | 166/1000 [00:31<01:41,  8.19it/s]Ref scores:  17%|â–ˆâ–‹        | 167/1000 [00:31<01:44,  7.96it/s]Ref scores:  17%|â–ˆâ–‹        | 168/1000 [00:31<01:47,  7.77it/s]Ref scores:  17%|â–ˆâ–‹        | 169/1000 [00:31<01:48,  7.65it/s]Ref scores:  17%|â–ˆâ–‹        | 170/1000 [00:31<01:50,  7.53it/s]Ref scores:  17%|â–ˆâ–‹        | 171/1000 [00:31<01:50,  7.48it/s]Ref scores:  17%|â–ˆâ–‹        | 172/1000 [00:31<01:51,  7.40it/s]Ref scores:  17%|â–ˆâ–‹        | 173/1000 [00:32<01:56,  7.09it/s]Ref scores:  18%|â–ˆâ–Š        | 175/1000 [00:32<01:39,  8.28it/s]Ref scores:  18%|â–ˆâ–Š        | 176/1000 [00:32<01:45,  7.77it/s]Ref scores:  18%|â–ˆâ–Š        | 177/1000 [00:32<01:51,  7.39it/s]Ref scores:  18%|â–ˆâ–Š        | 178/1000 [00:32<01:52,  7.34it/s]Ref scores:  18%|â–ˆâ–Š        | 179/1000 [00:32<01:52,  7.31it/s]Ref scores:  18%|â–ˆâ–Š        | 180/1000 [00:33<01:56,  7.06it/s]Ref scores:  18%|â–ˆâ–Š        | 181/1000 [00:33<01:54,  7.14it/s]Ref scores:  18%|â–ˆâ–Š        | 182/1000 [00:33<01:53,  7.21it/s]Ref scores:  18%|â–ˆâ–Š        | 183/1000 [00:33<01:53,  7.22it/s]Ref scores:  18%|â–ˆâ–Š        | 184/1000 [00:33<01:52,  7.25it/s]Ref scores:  18%|â–ˆâ–Š        | 185/1000 [00:33<01:56,  7.00it/s]Ref scores:  19%|â–ˆâ–Š        | 186/1000 [00:33<01:48,  7.47it/s]Ref scores:  19%|â–ˆâ–‰        | 188/1000 [00:34<01:40,  8.11it/s]Ref scores:  19%|â–ˆâ–‰        | 189/1000 [00:34<01:37,  8.29it/s]Ref scores:  19%|â–ˆâ–‰        | 190/1000 [00:34<01:40,  8.05it/s]Ref scores:  19%|â–ˆâ–‰        | 191/1000 [00:34<01:43,  7.85it/s]Ref scores:  19%|â–ˆâ–‰        | 192/1000 [00:34<01:45,  7.66it/s]Ref scores:  19%|â–ˆâ–‰        | 193/1000 [00:34<01:40,  8.00it/s]Ref scores:  19%|â–ˆâ–‰        | 194/1000 [00:34<01:43,  7.77it/s]Ref scores:  20%|â–ˆâ–‰        | 195/1000 [00:35<01:45,  7.65it/s]Ref scores:  20%|â–ˆâ–‰        | 196/1000 [00:35<01:50,  7.27it/s]Ref scores:  20%|â–ˆâ–‰        | 197/1000 [00:35<01:50,  7.27it/s]Ref scores:  20%|â–ˆâ–‰        | 198/1000 [00:35<01:44,  7.68it/s]Ref scores:  20%|â–ˆâ–‰        | 199/1000 [00:35<01:45,  7.60it/s]Ref scores:  20%|â–ˆâ–ˆ        | 200/1000 [00:35<01:46,  7.51it/s]Ref scores:  20%|â–ˆâ–ˆ        | 201/1000 [00:35<01:41,  7.88it/s]Ref scores:  20%|â–ˆâ–ˆ        | 202/1000 [00:35<01:43,  7.73it/s]Ref scores:  20%|â–ˆâ–ˆ        | 203/1000 [00:36<01:44,  7.65it/s]Ref scores:  20%|â–ˆâ–ˆ        | 204/1000 [00:36<01:44,  7.60it/s]Ref scores:  20%|â–ˆâ–ˆ        | 205/1000 [00:36<01:45,  7.57it/s]Ref scores:  21%|â–ˆâ–ˆ        | 207/1000 [00:36<01:32,  8.60it/s]Ref scores:  21%|â–ˆâ–ˆ        | 209/1000 [00:36<01:30,  8.73it/s]Ref scores:  21%|â–ˆâ–ˆ        | 210/1000 [00:36<01:43,  7.60it/s]Ref scores:  21%|â–ˆâ–ˆ        | 211/1000 [00:37<01:48,  7.29it/s]Ref scores:  21%|â–ˆâ–ˆâ–       | 213/1000 [00:37<01:40,  7.86it/s]Ref scores:  21%|â–ˆâ–ˆâ–       | 214/1000 [00:37<01:41,  7.73it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 215/1000 [00:37<01:40,  7.84it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 216/1000 [00:37<01:52,  6.95it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 217/1000 [00:37<01:51,  7.05it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 218/1000 [00:38<01:53,  6.89it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 219/1000 [00:38<01:46,  7.36it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 220/1000 [00:38<01:45,  7.41it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 221/1000 [00:38<01:45,  7.38it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 222/1000 [00:38<01:45,  7.37it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 223/1000 [00:38<01:45,  7.37it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 224/1000 [00:38<01:40,  7.75it/s]Ref scores:  22%|â–ˆâ–ˆâ–Ž       | 225/1000 [00:39<01:53,  6.84it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 226/1000 [00:39<01:50,  6.99it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 227/1000 [00:39<01:49,  7.04it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 228/1000 [00:39<01:48,  7.10it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 229/1000 [00:39<01:47,  7.20it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 230/1000 [00:39<01:46,  7.24it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 231/1000 [00:39<01:58,  6.50it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 232/1000 [00:40<01:54,  6.72it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 233/1000 [00:40<01:51,  6.89it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 234/1000 [00:40<01:43,  7.42it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 235/1000 [00:40<01:43,  7.40it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 236/1000 [00:40<01:49,  6.96it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 237/1000 [00:40<01:47,  7.08it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 238/1000 [00:40<01:46,  7.15it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 239/1000 [00:40<01:39,  7.61it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 240/1000 [00:41<01:41,  7.51it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 241/1000 [00:41<01:47,  7.05it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 242/1000 [00:41<01:45,  7.17it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 243/1000 [00:41<01:39,  7.64it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 244/1000 [00:41<01:39,  7.57it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 245/1000 [00:41<01:42,  7.37it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 246/1000 [00:41<01:42,  7.34it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 247/1000 [00:42<01:42,  7.37it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 248/1000 [00:42<01:41,  7.37it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 249/1000 [00:42<01:45,  7.11it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 250/1000 [00:42<01:44,  7.18it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 251/1000 [00:42<01:47,  6.97it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 252/1000 [00:42<01:45,  7.10it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 253/1000 [00:42<01:43,  7.20it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 254/1000 [00:43<01:43,  7.23it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 255/1000 [00:43<01:47,  6.96it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 256/1000 [00:43<01:45,  7.03it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 257/1000 [00:43<01:44,  7.09it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 258/1000 [00:43<01:47,  6.91it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 259/1000 [00:43<01:45,  7.02it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 260/1000 [00:43<01:44,  7.09it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 261/1000 [00:44<01:43,  7.12it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 262/1000 [00:44<01:42,  7.20it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 264/1000 [00:44<01:49,  6.72it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 265/1000 [00:44<01:46,  6.88it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 266/1000 [00:44<01:44,  7.03it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 267/1000 [00:44<01:48,  6.76it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 268/1000 [00:45<01:39,  7.33it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 269/1000 [00:45<01:34,  7.73it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 270/1000 [00:45<01:35,  7.64it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 271/1000 [00:45<01:36,  7.59it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 272/1000 [00:45<01:36,  7.53it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 273/1000 [00:45<01:32,  7.85it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 274/1000 [00:45<01:34,  7.69it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 275/1000 [00:45<01:36,  7.54it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 276/1000 [00:46<01:31,  7.90it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 277/1000 [00:46<01:28,  8.19it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 278/1000 [00:46<01:30,  7.95it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 279/1000 [00:46<01:32,  7.80it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 280/1000 [00:46<01:27,  8.21it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 281/1000 [00:46<01:30,  7.92it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 282/1000 [00:46<01:33,  7.70it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 283/1000 [00:46<01:34,  7.57it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 284/1000 [00:47<01:34,  7.54it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 285/1000 [00:47<01:35,  7.47it/s]Ref scores:  29%|â–ˆâ–ˆâ–Š       | 286/1000 [00:47<01:35,  7.46it/s]Ref scores:  29%|â–ˆâ–ˆâ–Š       | 287/1000 [00:47<01:36,  7.40it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 289/1000 [00:47<01:27,  8.09it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 290/1000 [00:47<01:30,  7.89it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 291/1000 [00:47<01:31,  7.72it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 292/1000 [00:48<01:32,  7.62it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 293/1000 [00:48<01:29,  7.94it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 294/1000 [00:48<01:30,  7.80it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 295/1000 [00:48<01:27,  8.08it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 296/1000 [00:48<01:29,  7.83it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 297/1000 [00:48<01:31,  7.67it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 299/1000 [00:48<01:25,  8.23it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 300/1000 [00:49<01:50,  6.36it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 301/1000 [00:49<01:46,  6.59it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 302/1000 [00:49<01:46,  6.58it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 303/1000 [00:49<01:46,  6.57it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 304/1000 [00:49<01:42,  6.77it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 305/1000 [00:49<01:40,  6.94it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 306/1000 [00:50<01:38,  7.04it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 307/1000 [00:50<01:48,  6.37it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 308/1000 [00:50<01:44,  6.62it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 310/1000 [00:50<01:31,  7.57it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 311/1000 [00:50<01:27,  7.86it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 312/1000 [00:50<01:29,  7.71it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆâ–      | 313/1000 [00:50<01:24,  8.13it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆâ–      | 314/1000 [00:51<01:31,  7.49it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 315/1000 [00:51<01:32,  7.43it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 316/1000 [00:51<01:32,  7.39it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 317/1000 [00:51<01:31,  7.44it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 318/1000 [00:51<01:32,  7.39it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 319/1000 [00:51<01:35,  7.13it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 320/1000 [00:51<01:35,  7.15it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 322/1000 [00:52<01:34,  7.17it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 323/1000 [00:52<01:34,  7.19it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 324/1000 [00:52<01:33,  7.23it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 325/1000 [00:52<01:32,  7.27it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 326/1000 [00:52<01:36,  6.95it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 327/1000 [00:52<01:34,  7.11it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 328/1000 [00:53<01:33,  7.17it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 329/1000 [00:53<01:32,  7.23it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 330/1000 [00:53<01:31,  7.29it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 331/1000 [00:53<01:30,  7.35it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 332/1000 [00:53<01:30,  7.37it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 333/1000 [00:53<01:29,  7.42it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 334/1000 [00:53<01:30,  7.38it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 335/1000 [00:54<01:30,  7.38it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 336/1000 [00:54<01:24,  7.83it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 337/1000 [00:54<01:26,  7.69it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 338/1000 [00:54<01:27,  7.60it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 339/1000 [00:54<01:27,  7.51it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 340/1000 [00:54<01:28,  7.49it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 341/1000 [00:54<01:28,  7.47it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 342/1000 [00:54<01:28,  7.43it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 343/1000 [00:55<01:28,  7.41it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 344/1000 [00:55<01:23,  7.82it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 345/1000 [00:55<01:24,  7.71it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 346/1000 [00:55<01:26,  7.60it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 347/1000 [00:55<01:30,  7.19it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 348/1000 [00:55<01:30,  7.22it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 349/1000 [00:55<01:25,  7.65it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 350/1000 [00:56<01:25,  7.60it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 351/1000 [00:56<01:27,  7.39it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 352/1000 [00:56<01:27,  7.38it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 353/1000 [00:56<01:32,  7.00it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 354/1000 [00:56<01:31,  7.09it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 355/1000 [00:56<01:29,  7.17it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 356/1000 [00:56<01:22,  7.78it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 358/1000 [00:57<01:16,  8.43it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 359/1000 [00:57<01:18,  8.17it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 360/1000 [00:57<01:20,  7.95it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 361/1000 [00:57<01:25,  7.47it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 362/1000 [00:57<01:26,  7.38it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 363/1000 [00:57<01:22,  7.73it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 364/1000 [00:57<01:27,  7.29it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 365/1000 [00:58<01:30,  7.03it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 366/1000 [00:58<01:29,  7.10it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 367/1000 [00:58<01:32,  6.87it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 368/1000 [00:58<01:30,  6.99it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 369/1000 [00:58<01:24,  7.45it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 370/1000 [00:58<01:24,  7.46it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 371/1000 [00:58<01:18,  8.01it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 372/1000 [00:58<01:15,  8.33it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 373/1000 [00:59<01:18,  8.00it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 374/1000 [00:59<01:23,  7.50it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 375/1000 [00:59<01:23,  7.45it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 376/1000 [00:59<01:28,  7.04it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 377/1000 [00:59<01:27,  7.11it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 378/1000 [00:59<01:26,  7.16it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 379/1000 [00:59<01:29,  6.93it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 380/1000 [01:00<01:28,  7.03it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 381/1000 [01:00<01:26,  7.18it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 382/1000 [01:00<01:19,  7.79it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 383/1000 [01:00<01:30,  6.84it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 384/1000 [01:00<01:28,  6.96it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 386/1000 [01:00<01:15,  8.16it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 387/1000 [01:00<01:17,  7.93it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 388/1000 [01:01<01:18,  7.78it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 389/1000 [01:01<01:19,  7.64it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 390/1000 [01:01<01:19,  7.63it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 391/1000 [01:01<01:15,  8.08it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 392/1000 [01:01<01:16,  7.90it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 393/1000 [01:01<01:18,  7.74it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 395/1000 [01:02<01:19,  7.60it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 397/1000 [01:02<01:15,  7.97it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 398/1000 [01:02<01:17,  7.82it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 399/1000 [01:02<01:17,  7.71it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 400/1000 [01:02<01:15,  7.95it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 401/1000 [01:02<01:16,  7.78it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 402/1000 [01:02<01:18,  7.64it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 403/1000 [01:03<01:19,  7.51it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 404/1000 [01:03<01:23,  7.15it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 406/1000 [01:03<01:15,  7.87it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 407/1000 [01:03<01:19,  7.48it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 408/1000 [01:03<01:16,  7.76it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 409/1000 [01:03<01:12,  8.16it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 410/1000 [01:03<01:09,  8.49it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 411/1000 [01:04<01:12,  8.10it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 412/1000 [01:04<01:14,  7.85it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 413/1000 [01:04<01:11,  8.26it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 414/1000 [01:04<01:13,  7.92it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 415/1000 [01:04<01:15,  7.71it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 416/1000 [01:04<01:17,  7.54it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 417/1000 [01:04<01:18,  7.47it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 418/1000 [01:04<01:13,  7.92it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 419/1000 [01:05<01:15,  7.74it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 420/1000 [01:05<01:16,  7.60it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 421/1000 [01:05<01:17,  7.48it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 422/1000 [01:05<01:21,  7.13it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 423/1000 [01:05<01:20,  7.15it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 424/1000 [01:05<01:15,  7.59it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 425/1000 [01:05<01:21,  7.08it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 426/1000 [01:06<01:16,  7.53it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 427/1000 [01:06<01:16,  7.45it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 428/1000 [01:06<01:21,  7.03it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 429/1000 [01:06<01:19,  7.15it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 430/1000 [01:06<01:28,  6.47it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 431/1000 [01:06<01:24,  6.70it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 432/1000 [01:06<01:22,  6.90it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 433/1000 [01:07<01:20,  7.02it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 434/1000 [01:07<01:19,  7.12it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 435/1000 [01:07<01:18,  7.19it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 436/1000 [01:07<01:22,  6.85it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 437/1000 [01:07<01:20,  6.99it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 438/1000 [01:07<01:22,  6.79it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 439/1000 [01:07<01:20,  6.97it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 440/1000 [01:08<01:19,  7.08it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 441/1000 [01:08<01:17,  7.17it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 442/1000 [01:08<01:17,  7.20it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 443/1000 [01:08<01:16,  7.26it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 444/1000 [01:08<01:19,  7.00it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 445/1000 [01:08<01:18,  7.03it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 446/1000 [01:08<01:14,  7.47it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 447/1000 [01:09<01:14,  7.39it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 448/1000 [01:09<01:10,  7.89it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 449/1000 [01:09<01:20,  6.84it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 450/1000 [01:09<01:14,  7.35it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 451/1000 [01:09<01:14,  7.37it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 452/1000 [01:09<01:15,  7.29it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 454/1000 [01:09<01:10,  7.77it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 455/1000 [01:10<01:11,  7.64it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 456/1000 [01:10<01:11,  7.60it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 457/1000 [01:10<01:15,  7.23it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 458/1000 [01:10<01:14,  7.26it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 459/1000 [01:10<01:10,  7.64it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 460/1000 [01:10<01:11,  7.54it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 461/1000 [01:10<01:12,  7.46it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 462/1000 [01:11<01:12,  7.41it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 463/1000 [01:11<01:15,  7.13it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 464/1000 [01:11<01:14,  7.17it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 465/1000 [01:11<01:13,  7.23it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 466/1000 [01:11<01:13,  7.30it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 467/1000 [01:11<01:15,  7.04it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 468/1000 [01:11<01:15,  7.09it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 469/1000 [01:12<01:14,  7.17it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 470/1000 [01:12<01:16,  6.97it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 471/1000 [01:12<01:14,  7.08it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 472/1000 [01:12<01:14,  7.13it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 473/1000 [01:12<01:13,  7.20it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 474/1000 [01:12<01:12,  7.23it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 475/1000 [01:12<01:08,  7.64it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 476/1000 [01:12<01:09,  7.54it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 477/1000 [01:13<01:09,  7.48it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 478/1000 [01:13<01:05,  7.93it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 479/1000 [01:13<01:07,  7.73it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 480/1000 [01:13<01:08,  7.62it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 481/1000 [01:13<01:08,  7.54it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 482/1000 [01:13<01:09,  7.49it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 483/1000 [01:13<01:09,  7.42it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 484/1000 [01:14<01:09,  7.38it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 485/1000 [01:14<01:10,  7.36it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 486/1000 [01:14<01:10,  7.33it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 487/1000 [01:14<01:09,  7.34it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 488/1000 [01:14<01:09,  7.36it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 489/1000 [01:14<01:05,  7.85it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 490/1000 [01:14<01:09,  7.37it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 491/1000 [01:14<01:09,  7.36it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 492/1000 [01:15<01:09,  7.32it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 493/1000 [01:15<01:09,  7.31it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 494/1000 [01:15<01:09,  7.33it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 495/1000 [01:15<01:08,  7.35it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 496/1000 [01:15<01:08,  7.37it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 497/1000 [01:15<01:08,  7.35it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 498/1000 [01:15<01:08,  7.35it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 499/1000 [01:16<01:08,  7.34it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 500/1000 [01:16<01:08,  7.34it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 501/1000 [01:16<01:07,  7.36it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 502/1000 [01:16<01:07,  7.36it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 503/1000 [01:16<01:04,  7.75it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 504/1000 [01:16<01:04,  7.64it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 506/1000 [01:16<01:00,  8.22it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 507/1000 [01:17<00:58,  8.37it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 508/1000 [01:17<00:57,  8.51it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 509/1000 [01:17<01:00,  8.15it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 510/1000 [01:17<01:02,  7.84it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 511/1000 [01:17<01:03,  7.69it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 512/1000 [01:17<01:07,  7.18it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 513/1000 [01:17<01:10,  6.95it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 514/1000 [01:18<01:05,  7.39it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 515/1000 [01:18<01:06,  7.35it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 516/1000 [01:18<01:08,  7.06it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 517/1000 [01:18<01:07,  7.11it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 518/1000 [01:18<01:06,  7.26it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 519/1000 [01:18<01:06,  7.22it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 520/1000 [01:18<01:05,  7.28it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 521/1000 [01:19<01:04,  7.38it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 522/1000 [01:19<01:04,  7.38it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 523/1000 [01:19<01:04,  7.38it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 524/1000 [01:19<01:03,  7.44it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 525/1000 [01:19<01:00,  7.86it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 526/1000 [01:19<00:57,  8.17it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 527/1000 [01:19<00:59,  7.91it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 528/1000 [01:19<01:00,  7.81it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 529/1000 [01:20<01:00,  7.73it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 530/1000 [01:20<01:01,  7.68it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 531/1000 [01:20<01:01,  7.64it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 532/1000 [01:20<00:57,  8.16it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 533/1000 [01:20<00:59,  7.83it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 534/1000 [01:20<01:03,  7.31it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 535/1000 [01:20<01:06,  6.97it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 536/1000 [01:20<01:04,  7.14it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 537/1000 [01:21<01:04,  7.20it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 538/1000 [01:21<01:07,  6.87it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 539/1000 [01:21<01:05,  6.99it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 541/1000 [01:21<00:58,  7.85it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 542/1000 [01:21<00:56,  8.12it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 543/1000 [01:21<00:57,  7.91it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 544/1000 [01:22<00:58,  7.75it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 545/1000 [01:22<00:59,  7.64it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 546/1000 [01:22<00:56,  8.03it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 547/1000 [01:22<00:57,  7.85it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 548/1000 [01:22<00:58,  7.78it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 550/1000 [01:22<00:53,  8.37it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 551/1000 [01:22<00:52,  8.61it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 552/1000 [01:23<00:56,  7.95it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 553/1000 [01:23<00:56,  7.85it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 554/1000 [01:23<01:00,  7.37it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 555/1000 [01:23<00:59,  7.46it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 556/1000 [01:23<00:59,  7.51it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 557/1000 [01:23<00:59,  7.51it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 559/1000 [01:23<00:53,  8.32it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 560/1000 [01:24<00:54,  8.10it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 561/1000 [01:24<00:52,  8.36it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 562/1000 [01:24<00:56,  7.81it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 563/1000 [01:24<00:56,  7.75it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 564/1000 [01:24<00:56,  7.67it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 565/1000 [01:24<00:56,  7.64it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 566/1000 [01:24<00:57,  7.59it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 567/1000 [01:24<00:54,  8.02it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 568/1000 [01:25<01:01,  7.04it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 569/1000 [01:25<01:03,  6.81it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 570/1000 [01:25<01:01,  6.98it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 571/1000 [01:25<01:00,  7.12it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 572/1000 [01:25<00:59,  7.19it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 573/1000 [01:25<00:54,  7.78it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 574/1000 [01:25<00:55,  7.71it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 576/1000 [01:26<00:47,  8.93it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 577/1000 [01:26<00:49,  8.47it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 578/1000 [01:26<00:51,  8.23it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 579/1000 [01:26<00:48,  8.62it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 580/1000 [01:26<00:51,  8.21it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 581/1000 [01:26<00:52,  7.94it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 582/1000 [01:26<00:53,  7.82it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 583/1000 [01:27<00:57,  7.28it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 584/1000 [01:27<01:03,  6.56it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 585/1000 [01:27<01:11,  5.81it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 586/1000 [01:27<01:06,  6.22it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 587/1000 [01:27<01:02,  6.57it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 588/1000 [01:27<01:00,  6.82it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 589/1000 [01:27<01:00,  6.77it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 590/1000 [01:28<00:55,  7.32it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 591/1000 [01:28<00:55,  7.36it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 592/1000 [01:28<00:54,  7.45it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 593/1000 [01:28<00:56,  7.16it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 594/1000 [01:28<00:56,  7.21it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 595/1000 [01:28<00:55,  7.33it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 596/1000 [01:28<00:54,  7.41it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 597/1000 [01:29<00:54,  7.43it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 598/1000 [01:29<00:54,  7.44it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 599/1000 [01:29<00:50,  7.87it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 600/1000 [01:29<00:48,  8.32it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 601/1000 [01:29<00:50,  7.95it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 602/1000 [01:29<00:51,  7.78it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 603/1000 [01:29<00:51,  7.73it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 604/1000 [01:29<00:58,  6.73it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 605/1000 [01:30<00:56,  6.99it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 607/1000 [01:30<00:47,  8.30it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 608/1000 [01:30<00:48,  8.12it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 609/1000 [01:30<00:46,  8.48it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 610/1000 [01:30<00:44,  8.80it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 611/1000 [01:30<00:46,  8.44it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 612/1000 [01:30<00:44,  8.66it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 613/1000 [01:31<00:46,  8.32it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 614/1000 [01:31<00:47,  8.09it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 615/1000 [01:31<00:48,  7.97it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 616/1000 [01:31<00:49,  7.83it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 617/1000 [01:31<00:49,  7.79it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 618/1000 [01:31<00:49,  7.78it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 619/1000 [01:31<00:46,  8.16it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 620/1000 [01:31<00:49,  7.65it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 621/1000 [01:32<00:46,  8.19it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 622/1000 [01:32<00:47,  8.00it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 623/1000 [01:32<00:47,  7.90it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 624/1000 [01:32<00:47,  7.84it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 625/1000 [01:32<00:48,  7.73it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 626/1000 [01:32<00:49,  7.62it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 627/1000 [01:32<00:49,  7.59it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 628/1000 [01:32<00:49,  7.59it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 629/1000 [01:33<00:52,  7.12it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 630/1000 [01:33<00:47,  7.73it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 631/1000 [01:33<00:48,  7.63it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 632/1000 [01:33<00:48,  7.62it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 633/1000 [01:33<00:51,  7.07it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 634/1000 [01:33<00:51,  7.17it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 635/1000 [01:33<00:52,  7.00it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 636/1000 [01:34<00:51,  7.12it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 637/1000 [01:34<00:50,  7.24it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 638/1000 [01:34<00:49,  7.29it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 639/1000 [01:34<00:46,  7.76it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 640/1000 [01:34<00:48,  7.39it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 641/1000 [01:34<00:45,  7.94it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 642/1000 [01:34<00:42,  8.34it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 643/1000 [01:34<00:41,  8.70it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 644/1000 [01:35<00:45,  7.80it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 645/1000 [01:35<00:46,  7.71it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 646/1000 [01:35<00:51,  6.82it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 647/1000 [01:35<00:50,  7.06it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 648/1000 [01:35<00:48,  7.24it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 649/1000 [01:35<00:47,  7.33it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 650/1000 [01:35<00:47,  7.43it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 651/1000 [01:36<00:46,  7.47it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 652/1000 [01:36<00:46,  7.51it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 653/1000 [01:36<00:46,  7.50it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 654/1000 [01:36<00:52,  6.62it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 655/1000 [01:36<00:50,  6.89it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 656/1000 [01:36<00:45,  7.55it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 657/1000 [01:36<00:51,  6.66it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 658/1000 [01:37<00:49,  6.89it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 659/1000 [01:37<00:50,  6.69it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 660/1000 [01:37<00:51,  6.56it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 661/1000 [01:37<00:55,  6.11it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 662/1000 [01:37<00:52,  6.48it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 663/1000 [01:37<00:49,  6.75it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 664/1000 [01:37<00:48,  6.95it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 665/1000 [01:38<00:46,  7.14it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 666/1000 [01:38<00:46,  7.23it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 667/1000 [01:38<00:45,  7.33it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 668/1000 [01:38<00:44,  7.42it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 670/1000 [01:38<00:36,  9.11it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 671/1000 [01:38<00:36,  9.12it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 672/1000 [01:38<00:37,  8.66it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 673/1000 [01:39<00:39,  8.34it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 674/1000 [01:39<00:38,  8.57it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 675/1000 [01:39<00:36,  8.80it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 676/1000 [01:39<00:38,  8.37it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 677/1000 [01:39<00:39,  8.12it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 678/1000 [01:39<00:38,  8.46it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 679/1000 [01:39<00:39,  8.15it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 680/1000 [01:39<00:40,  7.98it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 681/1000 [01:40<00:42,  7.51it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 682/1000 [01:40<00:42,  7.46it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 683/1000 [01:40<00:42,  7.43it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 684/1000 [01:40<00:42,  7.43it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 685/1000 [01:40<00:43,  7.17it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 686/1000 [01:40<00:43,  7.28it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 687/1000 [01:40<00:42,  7.38it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 688/1000 [01:40<00:41,  7.45it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 689/1000 [01:41<00:43,  7.14it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 690/1000 [01:41<00:42,  7.27it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 691/1000 [01:41<00:40,  7.71it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 692/1000 [01:41<00:40,  7.69it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 693/1000 [01:41<00:40,  7.65it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 694/1000 [01:41<00:40,  7.64it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 695/1000 [01:41<00:40,  7.56it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 696/1000 [01:42<00:40,  7.55it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 697/1000 [01:42<00:40,  7.52it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 698/1000 [01:42<00:39,  7.55it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 699/1000 [01:42<00:39,  7.57it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 700/1000 [01:42<00:39,  7.53it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 701/1000 [01:42<00:39,  7.52it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 702/1000 [01:42<00:39,  7.47it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 704/1000 [01:43<00:37,  7.88it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 705/1000 [01:43<00:37,  7.79it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 706/1000 [01:43<00:38,  7.73it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 707/1000 [01:43<00:36,  8.10it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 708/1000 [01:43<00:38,  7.64it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 709/1000 [01:43<00:35,  8.18it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 710/1000 [01:43<00:38,  7.56it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 711/1000 [01:44<00:39,  7.27it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 712/1000 [01:44<00:40,  7.06it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 713/1000 [01:44<00:39,  7.23it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 714/1000 [01:44<00:38,  7.34it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 715/1000 [01:44<00:40,  7.08it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 716/1000 [01:44<00:40,  6.93it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 717/1000 [01:44<00:40,  7.06it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 718/1000 [01:45<00:39,  7.20it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 719/1000 [01:45<00:40,  6.87it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 721/1000 [01:45<00:37,  7.47it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 722/1000 [01:45<00:37,  7.46it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 723/1000 [01:45<00:36,  7.51it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 724/1000 [01:45<00:38,  7.11it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 725/1000 [01:45<00:37,  7.24it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 726/1000 [01:46<00:38,  7.03it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 728/1000 [01:46<00:34,  7.91it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 729/1000 [01:46<00:34,  7.81it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 730/1000 [01:46<00:34,  7.73it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 731/1000 [01:46<00:35,  7.61it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 732/1000 [01:46<00:35,  7.56it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 733/1000 [01:47<00:35,  7.55it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 734/1000 [01:47<00:35,  7.55it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 735/1000 [01:47<00:35,  7.53it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 736/1000 [01:47<00:33,  7.97it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 737/1000 [01:47<00:33,  7.87it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 738/1000 [01:47<00:31,  8.29it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 739/1000 [01:47<00:32,  7.97it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 740/1000 [01:47<00:33,  7.82it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 741/1000 [01:48<00:33,  7.71it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 742/1000 [01:48<00:31,  8.12it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 743/1000 [01:48<00:32,  7.89it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 744/1000 [01:48<00:33,  7.70it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 746/1000 [01:48<00:28,  8.96it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 747/1000 [01:48<00:29,  8.57it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 748/1000 [01:48<00:30,  8.24it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 749/1000 [01:48<00:31,  8.02it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 750/1000 [01:49<00:35,  7.05it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 751/1000 [01:49<00:32,  7.56it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 752/1000 [01:49<00:32,  7.53it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 753/1000 [01:49<00:32,  7.54it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 754/1000 [01:49<00:32,  7.47it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 755/1000 [01:49<00:32,  7.48it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 756/1000 [01:49<00:33,  7.19it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 757/1000 [01:50<00:33,  7.27it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 758/1000 [01:50<00:31,  7.72it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 759/1000 [01:50<00:31,  7.64it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 760/1000 [01:50<00:31,  7.63it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 761/1000 [01:50<00:31,  7.55it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 762/1000 [01:50<00:31,  7.47it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 763/1000 [01:50<00:31,  7.44it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 764/1000 [01:51<00:31,  7.44it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 765/1000 [01:51<00:31,  7.46it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 766/1000 [01:51<00:32,  7.18it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 767/1000 [01:51<00:33,  7.00it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 768/1000 [01:51<00:32,  7.09it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 769/1000 [01:51<00:30,  7.61it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 770/1000 [01:51<00:30,  7.60it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 771/1000 [01:51<00:28,  7.97it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 772/1000 [01:52<00:29,  7.78it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 773/1000 [01:52<00:29,  7.60it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 774/1000 [01:52<00:31,  7.25it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 775/1000 [01:52<00:30,  7.31it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 776/1000 [01:52<00:30,  7.32it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 777/1000 [01:52<00:28,  7.78it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 778/1000 [01:52<00:29,  7.65it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 779/1000 [01:53<00:29,  7.53it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 780/1000 [01:53<00:30,  7.19it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 781/1000 [01:53<00:38,  5.62it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 782/1000 [01:53<00:37,  5.79it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 783/1000 [01:53<00:34,  6.24it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 784/1000 [01:53<00:33,  6.54it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 785/1000 [01:54<00:32,  6.54it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 786/1000 [01:54<00:31,  6.79it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 787/1000 [01:54<00:28,  7.35it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 788/1000 [01:54<00:28,  7.31it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 789/1000 [01:54<00:28,  7.35it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 790/1000 [01:54<00:28,  7.32it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 791/1000 [01:54<00:26,  7.87it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 792/1000 [01:54<00:27,  7.66it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 793/1000 [01:55<00:30,  6.71it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 794/1000 [01:55<00:29,  6.93it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 795/1000 [01:55<00:27,  7.41it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 796/1000 [01:55<00:27,  7.43it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 797/1000 [01:55<00:27,  7.38it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 798/1000 [01:55<00:27,  7.38it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 799/1000 [01:55<00:27,  7.37it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 800/1000 [01:56<00:27,  7.36it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 801/1000 [01:56<00:26,  7.37it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 802/1000 [01:56<00:24,  7.95it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 803/1000 [01:56<00:25,  7.80it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 804/1000 [01:56<00:25,  7.68it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 805/1000 [01:56<00:25,  7.53it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 806/1000 [01:56<00:24,  7.91it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 807/1000 [01:56<00:27,  6.91it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 808/1000 [01:57<00:27,  7.00it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 809/1000 [01:57<00:26,  7.08it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 810/1000 [01:57<00:26,  7.15it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 811/1000 [01:57<00:26,  7.19it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 812/1000 [01:57<00:27,  6.87it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 813/1000 [01:57<00:26,  7.04it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 814/1000 [01:57<00:26,  7.09it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 815/1000 [01:58<00:24,  7.66it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 817/1000 [01:58<00:22,  8.21it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 818/1000 [01:58<00:24,  7.57it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 819/1000 [01:58<00:24,  7.52it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 820/1000 [01:58<00:23,  7.50it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 821/1000 [01:58<00:24,  7.44it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 822/1000 [01:59<00:24,  7.41it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 823/1000 [01:59<00:22,  7.79it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 824/1000 [01:59<00:22,  7.69it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 825/1000 [01:59<00:22,  7.62it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 826/1000 [01:59<00:23,  7.53it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 827/1000 [01:59<00:21,  8.04it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 828/1000 [01:59<00:23,  7.44it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 829/1000 [01:59<00:21,  7.86it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 830/1000 [02:00<00:22,  7.66it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 831/1000 [02:00<00:22,  7.55it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 832/1000 [02:00<00:22,  7.55it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 833/1000 [02:00<00:22,  7.53it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 834/1000 [02:00<00:22,  7.49it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 835/1000 [02:00<00:22,  7.45it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 836/1000 [02:00<00:22,  7.39it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 837/1000 [02:00<00:22,  7.40it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 838/1000 [02:01<00:20,  7.80it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 839/1000 [02:01<00:19,  8.25it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 840/1000 [02:01<00:20,  7.91it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 841/1000 [02:01<00:20,  7.71it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 842/1000 [02:01<00:21,  7.31it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 844/1000 [02:01<00:19,  8.16it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 845/1000 [02:01<00:19,  7.92it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 846/1000 [02:02<00:19,  7.80it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 847/1000 [02:02<00:19,  7.69it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 848/1000 [02:02<00:18,  8.00it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 849/1000 [02:02<00:19,  7.85it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 850/1000 [02:02<00:20,  7.30it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 851/1000 [02:02<00:20,  7.32it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 853/1000 [02:03<00:18,  8.01it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 854/1000 [02:03<00:19,  7.57it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 855/1000 [02:03<00:19,  7.26it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 856/1000 [02:03<00:22,  6.53it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 857/1000 [02:03<00:21,  6.74it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 858/1000 [02:03<00:20,  6.92it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 859/1000 [02:03<00:21,  6.71it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 860/1000 [02:04<00:20,  6.88it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 861/1000 [02:04<00:19,  7.03it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 862/1000 [02:04<00:18,  7.60it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 863/1000 [02:04<00:17,  7.93it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 864/1000 [02:04<00:16,  8.21it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 865/1000 [02:04<00:17,  7.93it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 866/1000 [02:04<00:17,  7.79it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 867/1000 [02:04<00:17,  7.66it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 868/1000 [02:05<00:16,  7.99it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 869/1000 [02:05<00:15,  8.25it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 870/1000 [02:05<00:16,  8.01it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 871/1000 [02:05<00:16,  7.83it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 872/1000 [02:05<00:15,  8.15it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 873/1000 [02:05<00:16,  7.85it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 874/1000 [02:05<00:17,  7.33it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 875/1000 [02:05<00:17,  7.34it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 876/1000 [02:06<00:17,  7.04it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 877/1000 [02:06<00:16,  7.51it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 879/1000 [02:06<00:14,  8.55it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 880/1000 [02:06<00:14,  8.25it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 881/1000 [02:06<00:14,  8.04it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 882/1000 [02:06<00:14,  8.38it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 883/1000 [02:06<00:14,  8.09it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 884/1000 [02:07<00:14,  7.90it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 885/1000 [02:07<00:14,  8.15it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 886/1000 [02:07<00:14,  7.88it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 887/1000 [02:07<00:13,  8.27it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 888/1000 [02:07<00:13,  8.02it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 889/1000 [02:07<00:14,  7.80it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 890/1000 [02:07<00:14,  7.65it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 892/1000 [02:08<00:12,  8.69it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 893/1000 [02:08<00:12,  8.87it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 894/1000 [02:08<00:13,  8.09it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 895/1000 [02:08<00:13,  7.86it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 897/1000 [02:08<00:12,  8.41it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 898/1000 [02:08<00:13,  7.84it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 899/1000 [02:08<00:12,  8.10it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 901/1000 [02:09<00:12,  8.06it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 902/1000 [02:09<00:12,  7.65it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 903/1000 [02:09<00:13,  7.30it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 904/1000 [02:09<00:13,  7.27it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 905/1000 [02:09<00:12,  7.32it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 906/1000 [02:09<00:12,  7.32it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 907/1000 [02:10<00:12,  7.34it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 908/1000 [02:10<00:12,  7.40it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 909/1000 [02:10<00:12,  7.38it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 910/1000 [02:10<00:12,  7.42it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 911/1000 [02:10<00:11,  7.82it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 912/1000 [02:10<00:11,  7.40it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 913/1000 [02:10<00:11,  7.39it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 914/1000 [02:10<00:11,  7.80it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 915/1000 [02:11<00:11,  7.66it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 916/1000 [02:11<00:11,  7.54it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 917/1000 [02:11<00:11,  7.50it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 918/1000 [02:11<00:11,  7.44it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 919/1000 [02:11<00:10,  7.41it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 920/1000 [02:11<00:11,  7.10it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 921/1000 [02:11<00:10,  7.20it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 922/1000 [02:12<00:10,  7.63it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 923/1000 [02:12<00:10,  7.53it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 924/1000 [02:12<00:10,  7.44it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 925/1000 [02:12<00:10,  7.44it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 926/1000 [02:12<00:09,  7.43it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 927/1000 [02:12<00:09,  7.95it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 928/1000 [02:12<00:09,  7.78it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 929/1000 [02:12<00:09,  7.33it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 930/1000 [02:13<00:09,  7.32it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 931/1000 [02:13<00:09,  7.30it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 932/1000 [02:13<00:09,  7.29it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 933/1000 [02:13<00:09,  7.03it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 934/1000 [02:13<00:09,  7.11it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 935/1000 [02:13<00:09,  6.92it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 936/1000 [02:13<00:09,  7.08it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 937/1000 [02:14<00:08,  7.15it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 938/1000 [02:14<00:08,  7.21it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 939/1000 [02:14<00:08,  7.25it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 940/1000 [02:14<00:08,  7.32it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 941/1000 [02:14<00:07,  7.89it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 942/1000 [02:14<00:07,  7.75it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 943/1000 [02:14<00:07,  7.31it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 944/1000 [02:15<00:08,  6.57it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 945/1000 [02:15<00:07,  7.13it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 946/1000 [02:15<00:07,  6.91it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 947/1000 [02:15<00:07,  7.04it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 948/1000 [02:15<00:06,  7.68it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 949/1000 [02:15<00:08,  6.34it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 950/1000 [02:15<00:07,  6.62it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 951/1000 [02:16<00:07,  6.80it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 952/1000 [02:16<00:06,  7.00it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 954/1000 [02:16<00:05,  7.90it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 955/1000 [02:16<00:05,  7.74it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 956/1000 [02:16<00:05,  8.02it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 957/1000 [02:16<00:05,  7.83it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 958/1000 [02:16<00:05,  7.66it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 959/1000 [02:17<00:05,  7.99it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 960/1000 [02:17<00:04,  8.29it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 961/1000 [02:17<00:04,  8.03it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 962/1000 [02:17<00:05,  7.39it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 963/1000 [02:17<00:04,  7.77it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 964/1000 [02:17<00:05,  6.39it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 965/1000 [02:17<00:05,  6.63it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 966/1000 [02:18<00:05,  6.79it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 967/1000 [02:18<00:04,  6.69it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 968/1000 [02:18<00:04,  6.87it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 969/1000 [02:18<00:04,  6.98it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 970/1000 [02:18<00:04,  7.10it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 971/1000 [02:18<00:03,  7.56it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 972/1000 [02:18<00:03,  7.21it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 973/1000 [02:19<00:03,  7.21it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 974/1000 [02:19<00:03,  7.23it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 975/1000 [02:19<00:03,  7.69it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 976/1000 [02:19<00:02,  8.04it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 977/1000 [02:19<00:03,  7.53it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 978/1000 [02:19<00:02,  7.49it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 979/1000 [02:19<00:02,  7.15it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 980/1000 [02:20<00:02,  6.84it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 981/1000 [02:20<00:02,  6.99it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 982/1000 [02:20<00:02,  7.46it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 983/1000 [02:20<00:02,  7.41it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 984/1000 [02:20<00:02,  7.39it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 985/1000 [02:20<00:02,  7.39it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 986/1000 [02:20<00:01,  7.38it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 987/1000 [02:20<00:01,  7.40it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 988/1000 [02:21<00:01,  7.10it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 989/1000 [02:21<00:01,  7.17it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 990/1000 [02:21<00:01,  7.22it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 991/1000 [02:21<00:01,  7.27it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 992/1000 [02:21<00:01,  7.04it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 993/1000 [02:21<00:00,  7.15it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 994/1000 [02:21<00:00,  6.80it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 995/1000 [02:22<00:00,  6.69it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 996/1000 [02:22<00:00,  6.86it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 997/1000 [02:22<00:00,  7.04it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 998/1000 [02:22<00:00,  6.87it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 999/1000 [02:22<00:00,  6.73it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [02:22<00:00,  6.92it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [02:22<00:00,  7.00it/s]
DONE (8.98s)
DONE (11.30s)
Computing criterion:   0%|          | 0/20 [00:00<?, ?it/s]Computing criterion:   5%|â–Œ         | 1/20 [00:19<06:19, 19.97s/it]Computing criterion:  10%|â–ˆ         | 2/20 [00:40<06:02, 20.16s/it]Computing criterion:  15%|â–ˆâ–Œ        | 3/20 [01:00<05:40, 20.01s/it]Computing criterion:  20%|â–ˆâ–ˆ        | 4/20 [01:21<05:26, 20.38s/it]Computing criterion:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [01:40<05:03, 20.21s/it]Computing criterion:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [02:01<04:44, 20.30s/it]Computing criterion:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [02:21<04:23, 20.24s/it]Computing criterion:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [02:41<04:01, 20.13s/it]Computing criterion:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [03:01<03:41, 20.16s/it]Computing criterion:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [03:21<03:19, 19.95s/it]Computing criterion:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [03:40<02:57, 19.74s/it]Computing criterion:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [03:58<02:34, 19.26s/it]Computing criterion:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [04:16<02:12, 18.98s/it]Computing criterion:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [04:36<01:54, 19.09s/it]Computing criterion:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [04:55<01:35, 19.16s/it]Computing criterion:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [05:14<01:15, 18.96s/it]Computing criterion:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [05:34<00:57, 19.30s/it]Computing criterion:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [05:52<00:38, 19.10s/it]Computing criterion:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [06:11<00:19, 19.12s/it]Computing criterion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [06:30<00:00, 19.06s/it]Computing criterion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [06:30<00:00, 19.54s/it]
Ref scores:   0%|          | 0/1000 [00:00<?, ?it/s]Ref scores:   0%|          | 1/1000 [00:03<53:57,  3.24s/it]Ref scores:   0%|          | 2/1000 [00:03<23:38,  1.42s/it]Ref scores:   0%|          | 3/1000 [00:03<13:53,  1.20it/s]Ref scores:   0%|          | 4/1000 [00:03<09:08,  1.81it/s]Ref scores:   0%|          | 5/1000 [00:03<06:40,  2.48it/s]Ref scores:   1%|          | 6/1000 [00:03<05:21,  3.09it/s]Ref scores:   1%|          | 7/1000 [00:04<04:21,  3.80it/s]Ref scores:   1%|          | 8/1000 [00:04<03:42,  4.46it/s]Ref scores:   1%|          | 9/1000 [00:04<03:13,  5.12it/s]Ref scores:   1%|          | 10/1000 [00:04<02:56,  5.62it/s]Ref scores:   1%|          | 11/1000 [00:04<02:44,  6.00it/s]Ref scores:   1%|          | 12/1000 [00:04<02:36,  6.33it/s]Ref scores:   1%|â–         | 13/1000 [00:04<02:37,  6.28it/s]Ref scores:   1%|â–         | 14/1000 [00:05<02:36,  6.29it/s]Ref scores:   2%|â–         | 15/1000 [00:05<02:21,  6.95it/s]Ref scores:   2%|â–         | 16/1000 [00:05<02:25,  6.76it/s]Ref scores:   2%|â–         | 17/1000 [00:05<02:22,  6.90it/s]Ref scores:   2%|â–         | 18/1000 [00:05<02:20,  7.01it/s]Ref scores:   2%|â–         | 19/1000 [00:05<02:17,  7.12it/s]Ref scores:   2%|â–         | 20/1000 [00:05<02:17,  7.13it/s]Ref scores:   2%|â–         | 21/1000 [00:06<02:16,  7.16it/s]Ref scores:   2%|â–         | 22/1000 [00:06<02:17,  7.10it/s]Ref scores:   2%|â–         | 23/1000 [00:06<02:09,  7.57it/s]Ref scores:   2%|â–         | 24/1000 [00:06<02:11,  7.42it/s]Ref scores:   2%|â–Ž         | 25/1000 [00:06<02:13,  7.29it/s]Ref scores:   3%|â–Ž         | 26/1000 [00:06<02:31,  6.41it/s]Ref scores:   3%|â–Ž         | 27/1000 [00:06<02:33,  6.36it/s]Ref scores:   3%|â–Ž         | 28/1000 [00:07<02:28,  6.54it/s]Ref scores:   3%|â–Ž         | 29/1000 [00:07<02:41,  6.01it/s]Ref scores:   3%|â–Ž         | 30/1000 [00:07<02:33,  6.31it/s]Ref scores:   3%|â–Ž         | 31/1000 [00:07<02:27,  6.58it/s]Ref scores:   3%|â–Ž         | 32/1000 [00:07<02:43,  5.94it/s]Ref scores:   3%|â–Ž         | 33/1000 [00:07<02:26,  6.60it/s]Ref scores:   3%|â–Ž         | 34/1000 [00:08<02:23,  6.73it/s]Ref scores:   4%|â–Ž         | 35/1000 [00:08<02:20,  6.87it/s]Ref scores:   4%|â–Ž         | 36/1000 [00:08<02:18,  6.98it/s]Ref scores:   4%|â–Ž         | 37/1000 [00:08<02:15,  7.09it/s]Ref scores:   4%|â–         | 38/1000 [00:08<02:07,  7.56it/s]Ref scores:   4%|â–         | 39/1000 [00:08<02:01,  7.93it/s]Ref scores:   4%|â–         | 40/1000 [00:08<01:56,  8.25it/s]Ref scores:   4%|â–         | 41/1000 [00:08<02:06,  7.59it/s]Ref scores:   4%|â–         | 42/1000 [00:09<02:08,  7.43it/s]Ref scores:   4%|â–         | 43/1000 [00:09<02:10,  7.33it/s]Ref scores:   4%|â–         | 44/1000 [00:09<02:31,  6.31it/s]Ref scores:   4%|â–         | 45/1000 [00:09<02:26,  6.52it/s]Ref scores:   5%|â–         | 46/1000 [00:09<02:21,  6.74it/s]Ref scores:   5%|â–         | 47/1000 [00:09<02:19,  6.85it/s]Ref scores:   5%|â–         | 48/1000 [00:09<02:16,  6.99it/s]Ref scores:   5%|â–         | 49/1000 [00:10<02:20,  6.78it/s]Ref scores:   5%|â–Œ         | 50/1000 [00:10<02:17,  6.91it/s]Ref scores:   5%|â–Œ         | 51/1000 [00:10<02:58,  5.32it/s]Ref scores:   5%|â–Œ         | 52/1000 [00:10<02:44,  5.76it/s]Ref scores:   5%|â–Œ         | 53/1000 [00:10<02:35,  6.11it/s]Ref scores:   5%|â–Œ         | 54/1000 [00:11<03:03,  5.16it/s]Ref scores:   6%|â–Œ         | 55/1000 [00:11<02:55,  5.37it/s]Ref scores:   6%|â–Œ         | 56/1000 [00:11<02:41,  5.86it/s]Ref scores:   6%|â–Œ         | 57/1000 [00:11<02:29,  6.29it/s]Ref scores:   6%|â–Œ         | 58/1000 [00:11<02:22,  6.63it/s]Ref scores:   6%|â–Œ         | 59/1000 [00:11<02:17,  6.86it/s]Ref scores:   6%|â–Œ         | 60/1000 [00:11<02:12,  7.07it/s]Ref scores:   6%|â–Œ         | 61/1000 [00:12<02:09,  7.22it/s]Ref scores:   6%|â–Œ         | 62/1000 [00:12<02:07,  7.35it/s]Ref scores:   6%|â–‹         | 63/1000 [00:12<02:07,  7.35it/s]Ref scores:   6%|â–‹         | 64/1000 [00:12<02:06,  7.39it/s]Ref scores:   6%|â–‹         | 65/1000 [00:12<02:05,  7.43it/s]Ref scores:   7%|â–‹         | 66/1000 [00:12<02:04,  7.48it/s]Ref scores:   7%|â–‹         | 67/1000 [00:12<02:04,  7.50it/s]Ref scores:   7%|â–‹         | 68/1000 [00:13<02:04,  7.51it/s]Ref scores:   7%|â–‹         | 69/1000 [00:13<02:04,  7.50it/s]Ref scores:   7%|â–‹         | 70/1000 [00:13<02:22,  6.54it/s]Ref scores:   7%|â–‹         | 71/1000 [00:13<02:16,  6.82it/s]Ref scores:   7%|â–‹         | 72/1000 [00:13<02:12,  7.03it/s]Ref scores:   7%|â–‹         | 73/1000 [00:13<02:09,  7.18it/s]Ref scores:   7%|â–‹         | 74/1000 [00:13<02:06,  7.31it/s]Ref scores:   8%|â–Š         | 75/1000 [00:14<02:10,  7.10it/s]Ref scores:   8%|â–Š         | 76/1000 [00:14<02:07,  7.24it/s]Ref scores:   8%|â–Š         | 77/1000 [00:14<01:59,  7.73it/s]Ref scores:   8%|â–Š         | 78/1000 [00:14<02:00,  7.65it/s]Ref scores:   8%|â–Š         | 79/1000 [00:14<02:00,  7.66it/s]Ref scores:   8%|â–Š         | 80/1000 [00:14<02:00,  7.62it/s]Ref scores:   8%|â–Š         | 81/1000 [00:14<02:00,  7.61it/s]Ref scores:   8%|â–Š         | 82/1000 [00:14<02:00,  7.60it/s]Ref scores:   8%|â–Š         | 83/1000 [00:15<02:00,  7.59it/s]Ref scores:   8%|â–Š         | 84/1000 [00:15<02:06,  7.22it/s]Ref scores:   8%|â–Š         | 85/1000 [00:15<02:04,  7.36it/s]Ref scores:   9%|â–Š         | 86/1000 [00:15<02:02,  7.44it/s]Ref scores:   9%|â–Š         | 87/1000 [00:15<02:01,  7.49it/s]Ref scores:   9%|â–‰         | 88/1000 [00:15<02:27,  6.17it/s]Ref scores:   9%|â–‰         | 89/1000 [00:15<02:18,  6.56it/s]Ref scores:   9%|â–‰         | 90/1000 [00:16<02:13,  6.83it/s]Ref scores:   9%|â–‰         | 91/1000 [00:16<02:09,  7.01it/s]Ref scores:   9%|â–‰         | 92/1000 [00:16<02:06,  7.20it/s]Ref scores:   9%|â–‰         | 93/1000 [00:16<01:57,  7.72it/s]Ref scores:   9%|â–‰         | 94/1000 [00:16<01:57,  7.71it/s]Ref scores:  10%|â–‰         | 95/1000 [00:16<01:57,  7.70it/s]Ref scores:  10%|â–‰         | 96/1000 [00:16<01:57,  7.69it/s]Ref scores:  10%|â–‰         | 97/1000 [00:16<01:58,  7.61it/s]Ref scores:  10%|â–‰         | 98/1000 [00:17<01:58,  7.60it/s]Ref scores:  10%|â–‰         | 99/1000 [00:17<01:58,  7.58it/s]Ref scores:  10%|â–ˆ         | 100/1000 [00:17<01:58,  7.56it/s]Ref scores:  10%|â–ˆ         | 101/1000 [00:17<01:58,  7.57it/s]Ref scores:  10%|â–ˆ         | 102/1000 [00:17<02:39,  5.62it/s]Ref scores:  10%|â–ˆ         | 103/1000 [00:17<02:26,  6.11it/s]Ref scores:  10%|â–ˆ         | 104/1000 [00:18<02:17,  6.51it/s]Ref scores:  10%|â–ˆ         | 105/1000 [00:18<02:11,  6.79it/s]Ref scores:  11%|â–ˆ         | 106/1000 [00:18<02:07,  7.01it/s]Ref scores:  11%|â–ˆ         | 107/1000 [00:18<02:11,  6.78it/s]Ref scores:  11%|â–ˆ         | 108/1000 [00:18<02:01,  7.35it/s]Ref scores:  11%|â–ˆ         | 109/1000 [00:18<02:00,  7.39it/s]Ref scores:  11%|â–ˆ         | 110/1000 [00:18<01:59,  7.45it/s]Ref scores:  11%|â–ˆ         | 111/1000 [00:18<01:58,  7.51it/s]Ref scores:  11%|â–ˆ         | 112/1000 [00:19<01:57,  7.56it/s]Ref scores:  11%|â–ˆâ–        | 113/1000 [00:19<01:57,  7.57it/s]Ref scores:  11%|â–ˆâ–        | 114/1000 [00:19<01:56,  7.57it/s]Ref scores:  12%|â–ˆâ–        | 115/1000 [00:19<01:50,  7.99it/s]Ref scores:  12%|â–ˆâ–        | 116/1000 [00:19<01:59,  7.42it/s]Ref scores:  12%|â–ˆâ–        | 117/1000 [00:19<02:04,  7.08it/s]Ref scores:  12%|â–ˆâ–        | 118/1000 [00:19<02:08,  6.85it/s]Ref scores:  12%|â–ˆâ–        | 119/1000 [00:20<02:05,  7.03it/s]Ref scores:  12%|â–ˆâ–        | 120/1000 [00:20<02:07,  6.93it/s]Ref scores:  12%|â–ˆâ–        | 121/1000 [00:20<02:03,  7.09it/s]Ref scores:  12%|â–ˆâ–        | 122/1000 [00:20<02:01,  7.24it/s]Ref scores:  12%|â–ˆâ–        | 123/1000 [00:20<01:59,  7.34it/s]Ref scores:  12%|â–ˆâ–        | 124/1000 [00:20<02:04,  7.05it/s]Ref scores:  12%|â–ˆâ–Ž        | 125/1000 [00:20<02:07,  6.86it/s]Ref scores:  13%|â–ˆâ–Ž        | 126/1000 [00:21<01:57,  7.43it/s]Ref scores:  13%|â–ˆâ–Ž        | 127/1000 [00:21<01:56,  7.47it/s]Ref scores:  13%|â–ˆâ–Ž        | 128/1000 [00:21<02:02,  7.12it/s]Ref scores:  13%|â–ˆâ–Ž        | 129/1000 [00:21<02:00,  7.23it/s]Ref scores:  13%|â–ˆâ–Ž        | 130/1000 [00:21<01:58,  7.33it/s]Ref scores:  13%|â–ˆâ–Ž        | 131/1000 [00:21<01:57,  7.42it/s]Ref scores:  13%|â–ˆâ–Ž        | 132/1000 [00:21<01:50,  7.87it/s]Ref scores:  13%|â–ˆâ–Ž        | 133/1000 [00:21<01:51,  7.80it/s]Ref scores:  13%|â–ˆâ–Ž        | 134/1000 [00:22<01:51,  7.75it/s]Ref scores:  14%|â–ˆâ–Ž        | 135/1000 [00:22<01:51,  7.74it/s]Ref scores:  14%|â–ˆâ–Ž        | 136/1000 [00:22<01:46,  8.13it/s]Ref scores:  14%|â–ˆâ–Ž        | 137/1000 [00:22<01:47,  7.99it/s]Ref scores:  14%|â–ˆâ–        | 138/1000 [00:22<01:54,  7.55it/s]Ref scores:  14%|â–ˆâ–        | 139/1000 [00:22<01:53,  7.56it/s]Ref scores:  14%|â–ˆâ–        | 140/1000 [00:22<01:53,  7.55it/s]Ref scores:  14%|â–ˆâ–        | 141/1000 [00:23<01:58,  7.24it/s]Ref scores:  14%|â–ˆâ–        | 142/1000 [00:23<01:57,  7.28it/s]Ref scores:  14%|â–ˆâ–        | 143/1000 [00:23<02:14,  6.37it/s]Ref scores:  14%|â–ˆâ–        | 144/1000 [00:23<02:08,  6.66it/s]Ref scores:  14%|â–ˆâ–        | 145/1000 [00:23<02:04,  6.87it/s]Ref scores:  15%|â–ˆâ–        | 146/1000 [00:23<02:01,  7.04it/s]Ref scores:  15%|â–ˆâ–        | 147/1000 [00:23<01:53,  7.55it/s]Ref scores:  15%|â–ˆâ–        | 148/1000 [00:24<01:53,  7.50it/s]Ref scores:  15%|â–ˆâ–        | 149/1000 [00:24<01:53,  7.49it/s]Ref scores:  15%|â–ˆâ–Œ        | 150/1000 [00:24<01:57,  7.22it/s]Ref scores:  15%|â–ˆâ–Œ        | 151/1000 [00:24<01:56,  7.29it/s]Ref scores:  15%|â–ˆâ–Œ        | 152/1000 [00:24<01:59,  7.10it/s]Ref scores:  15%|â–ˆâ–Œ        | 153/1000 [00:24<01:58,  7.18it/s]Ref scores:  15%|â–ˆâ–Œ        | 154/1000 [00:25<02:34,  5.47it/s]Ref scores:  16%|â–ˆâ–Œ        | 155/1000 [00:25<02:27,  5.71it/s]Ref scores:  16%|â–ˆâ–Œ        | 156/1000 [00:25<02:17,  6.15it/s]Ref scores:  16%|â–ˆâ–Œ        | 157/1000 [00:25<02:14,  6.28it/s]Ref scores:  16%|â–ˆâ–Œ        | 158/1000 [00:25<02:07,  6.63it/s]Ref scores:  16%|â–ˆâ–Œ        | 159/1000 [00:25<02:02,  6.85it/s]Ref scores:  16%|â–ˆâ–Œ        | 160/1000 [00:25<01:59,  7.05it/s]Ref scores:  16%|â–ˆâ–Œ        | 161/1000 [00:26<02:13,  6.27it/s]Ref scores:  16%|â–ˆâ–Œ        | 162/1000 [00:26<02:07,  6.59it/s]Ref scores:  16%|â–ˆâ–‹        | 163/1000 [00:26<02:06,  6.61it/s]Ref scores:  16%|â–ˆâ–‹        | 164/1000 [00:26<02:02,  6.85it/s]Ref scores:  16%|â–ˆâ–‹        | 165/1000 [00:26<01:58,  7.06it/s]Ref scores:  17%|â–ˆâ–‹        | 166/1000 [00:26<01:56,  7.17it/s]Ref scores:  17%|â–ˆâ–‹        | 167/1000 [00:26<01:54,  7.26it/s]Ref scores:  17%|â–ˆâ–‹        | 168/1000 [00:27<02:10,  6.40it/s]Ref scores:  17%|â–ˆâ–‹        | 169/1000 [00:27<02:04,  6.67it/s]Ref scores:  17%|â–ˆâ–‹        | 170/1000 [00:27<02:05,  6.62it/s]Ref scores:  17%|â–ˆâ–‹        | 171/1000 [00:27<02:00,  6.89it/s]Ref scores:  17%|â–ˆâ–‹        | 172/1000 [00:27<02:03,  6.72it/s]Ref scores:  17%|â–ˆâ–‹        | 173/1000 [00:27<01:59,  6.92it/s]Ref scores:  17%|â–ˆâ–‹        | 174/1000 [00:27<02:01,  6.79it/s]Ref scores:  18%|â–ˆâ–Š        | 175/1000 [00:28<01:58,  6.95it/s]Ref scores:  18%|â–ˆâ–Š        | 176/1000 [00:28<01:56,  7.06it/s]Ref scores:  18%|â–ˆâ–Š        | 177/1000 [00:28<01:54,  7.19it/s]Ref scores:  18%|â–ˆâ–Š        | 178/1000 [00:28<01:53,  7.25it/s]Ref scores:  18%|â–ˆâ–Š        | 179/1000 [00:28<01:52,  7.32it/s]Ref scores:  18%|â–ˆâ–Š        | 180/1000 [00:28<01:51,  7.38it/s]Ref scores:  18%|â–ˆâ–Š        | 181/1000 [00:28<01:50,  7.41it/s]Ref scores:  18%|â–ˆâ–Š        | 182/1000 [00:29<01:54,  7.16it/s]Ref scores:  18%|â–ˆâ–Š        | 183/1000 [00:29<01:52,  7.25it/s]Ref scores:  18%|â–ˆâ–Š        | 184/1000 [00:29<01:51,  7.35it/s]Ref scores:  18%|â–ˆâ–Š        | 185/1000 [00:29<01:54,  7.11it/s]Ref scores:  19%|â–ˆâ–Š        | 186/1000 [00:29<01:53,  7.17it/s]Ref scores:  19%|â–ˆâ–Š        | 187/1000 [00:29<02:08,  6.31it/s]Ref scores:  19%|â–ˆâ–‰        | 188/1000 [00:29<02:07,  6.38it/s]Ref scores:  19%|â–ˆâ–‰        | 189/1000 [00:30<02:02,  6.64it/s]Ref scores:  19%|â–ˆâ–‰        | 190/1000 [00:30<01:58,  6.86it/s]Ref scores:  19%|â–ˆâ–‰        | 191/1000 [00:30<01:55,  7.03it/s]Ref scores:  19%|â–ˆâ–‰        | 192/1000 [00:30<01:52,  7.15it/s]Ref scores:  19%|â–ˆâ–‰        | 193/1000 [00:30<01:51,  7.21it/s]Ref scores:  19%|â–ˆâ–‰        | 194/1000 [00:30<02:14,  5.99it/s]Ref scores:  20%|â–ˆâ–‰        | 195/1000 [00:30<02:06,  6.36it/s]Ref scores:  20%|â–ˆâ–‰        | 196/1000 [00:31<02:25,  5.52it/s]Ref scores:  20%|â–ˆâ–‰        | 197/1000 [00:31<02:08,  6.26it/s]Ref scores:  20%|â–ˆâ–‰        | 198/1000 [00:31<02:01,  6.59it/s]Ref scores:  20%|â–ˆâ–‰        | 199/1000 [00:31<01:56,  6.85it/s]Ref scores:  20%|â–ˆâ–ˆ        | 200/1000 [00:31<01:53,  7.03it/s]Ref scores:  20%|â–ˆâ–ˆ        | 201/1000 [00:31<01:51,  7.15it/s]Ref scores:  20%|â–ˆâ–ˆ        | 202/1000 [00:32<01:50,  7.24it/s]Ref scores:  20%|â–ˆâ–ˆ        | 203/1000 [00:32<01:53,  7.03it/s]Ref scores:  20%|â–ˆâ–ˆ        | 204/1000 [00:32<01:51,  7.17it/s]Ref scores:  20%|â–ˆâ–ˆ        | 205/1000 [00:32<01:49,  7.24it/s]Ref scores:  21%|â–ˆâ–ˆ        | 206/1000 [00:32<01:48,  7.30it/s]Ref scores:  21%|â–ˆâ–ˆ        | 207/1000 [00:32<01:42,  7.77it/s]Ref scores:  21%|â–ˆâ–ˆ        | 208/1000 [00:32<01:43,  7.64it/s]Ref scores:  21%|â–ˆâ–ˆ        | 209/1000 [00:32<01:44,  7.59it/s]Ref scores:  21%|â–ˆâ–ˆ        | 210/1000 [00:33<01:57,  6.72it/s]Ref scores:  21%|â–ˆâ–ˆ        | 211/1000 [00:33<01:48,  7.28it/s]Ref scores:  21%|â–ˆâ–ˆ        | 212/1000 [00:33<01:47,  7.35it/s]Ref scores:  21%|â–ˆâ–ˆâ–       | 213/1000 [00:33<01:46,  7.37it/s]Ref scores:  21%|â–ˆâ–ˆâ–       | 214/1000 [00:33<01:51,  7.05it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 215/1000 [00:33<01:44,  7.53it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 216/1000 [00:33<01:44,  7.50it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 217/1000 [00:34<01:49,  7.16it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 218/1000 [00:34<01:48,  7.23it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 219/1000 [00:34<01:47,  7.26it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 220/1000 [00:34<01:47,  7.28it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 221/1000 [00:34<01:46,  7.33it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 222/1000 [00:34<01:45,  7.36it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 223/1000 [00:34<01:49,  7.10it/s]Ref scores:  22%|â–ˆâ–ˆâ–       | 224/1000 [00:35<01:47,  7.22it/s]Ref scores:  22%|â–ˆâ–ˆâ–Ž       | 225/1000 [00:35<01:46,  7.30it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 226/1000 [00:35<01:50,  6.99it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 227/1000 [00:35<01:48,  7.14it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 228/1000 [00:35<01:46,  7.26it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 229/1000 [00:35<01:45,  7.33it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 230/1000 [00:35<01:48,  7.07it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 231/1000 [00:35<01:47,  7.18it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 232/1000 [00:36<01:45,  7.26it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 233/1000 [00:36<01:48,  7.04it/s]Ref scores:  23%|â–ˆâ–ˆâ–Ž       | 234/1000 [00:36<01:47,  7.13it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 235/1000 [00:36<01:45,  7.23it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 236/1000 [00:36<01:44,  7.31it/s]Ref scores:  24%|â–ˆâ–ˆâ–Ž       | 237/1000 [00:36<01:43,  7.40it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 238/1000 [00:36<01:47,  7.10it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 239/1000 [00:37<01:46,  7.17it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 240/1000 [00:37<01:44,  7.27it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 241/1000 [00:37<01:43,  7.30it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 242/1000 [00:37<01:48,  6.96it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 243/1000 [00:37<01:47,  7.07it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 244/1000 [00:37<01:50,  6.82it/s]Ref scores:  24%|â–ˆâ–ˆâ–       | 245/1000 [00:37<01:48,  6.98it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 246/1000 [00:38<01:46,  7.10it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 247/1000 [00:38<01:59,  6.29it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 248/1000 [00:38<01:57,  6.37it/s]Ref scores:  25%|â–ˆâ–ˆâ–       | 249/1000 [00:38<02:09,  5.79it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 250/1000 [00:38<02:05,  6.00it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 251/1000 [00:38<01:57,  6.36it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 252/1000 [00:39<01:52,  6.64it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 253/1000 [00:39<02:11,  5.70it/s]Ref scores:  25%|â–ˆâ–ˆâ–Œ       | 254/1000 [00:39<02:02,  6.11it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 255/1000 [00:39<01:54,  6.48it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 256/1000 [00:39<01:50,  6.72it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 257/1000 [00:39<01:52,  6.62it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 258/1000 [00:40<01:48,  6.85it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 259/1000 [00:40<01:44,  7.07it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 260/1000 [00:40<01:46,  6.92it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 261/1000 [00:40<01:44,  7.05it/s]Ref scores:  26%|â–ˆâ–ˆâ–Œ       | 262/1000 [00:40<01:47,  6.89it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 263/1000 [00:40<01:44,  7.08it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 264/1000 [00:40<01:46,  6.93it/s]Ref scores:  26%|â–ˆâ–ˆâ–‹       | 265/1000 [00:41<01:43,  7.10it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 266/1000 [00:41<01:41,  7.27it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 267/1000 [00:41<01:40,  7.33it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 268/1000 [00:41<01:39,  7.32it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 269/1000 [00:41<01:39,  7.35it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 270/1000 [00:41<01:38,  7.40it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 271/1000 [00:41<01:37,  7.44it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 272/1000 [00:41<01:37,  7.43it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 273/1000 [00:42<01:41,  7.17it/s]Ref scores:  27%|â–ˆâ–ˆâ–‹       | 274/1000 [00:42<01:40,  7.25it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 275/1000 [00:42<01:38,  7.33it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 276/1000 [00:42<01:39,  7.31it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 277/1000 [00:42<01:38,  7.35it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 278/1000 [00:42<01:37,  7.39it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 279/1000 [00:42<01:37,  7.38it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 280/1000 [00:43<01:36,  7.44it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 281/1000 [00:43<01:35,  7.52it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 282/1000 [00:43<01:35,  7.51it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 283/1000 [00:43<01:55,  6.21it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 284/1000 [00:43<02:04,  5.76it/s]Ref scores:  28%|â–ˆâ–ˆâ–Š       | 285/1000 [00:43<01:54,  6.22it/s]Ref scores:  29%|â–ˆâ–ˆâ–Š       | 286/1000 [00:43<01:48,  6.56it/s]Ref scores:  29%|â–ˆâ–ˆâ–Š       | 287/1000 [00:44<01:44,  6.84it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 288/1000 [00:44<01:41,  7.04it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 289/1000 [00:44<01:53,  6.24it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 290/1000 [00:44<01:53,  6.28it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 291/1000 [00:44<01:46,  6.63it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 292/1000 [00:44<01:42,  6.88it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 293/1000 [00:45<01:44,  6.78it/s]Ref scores:  29%|â–ˆâ–ˆâ–‰       | 294/1000 [00:45<01:45,  6.72it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 295/1000 [00:45<01:46,  6.60it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 296/1000 [00:45<01:42,  6.84it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 297/1000 [00:45<01:40,  7.00it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 298/1000 [00:45<01:43,  6.79it/s]Ref scores:  30%|â–ˆâ–ˆâ–‰       | 299/1000 [00:45<02:01,  5.79it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 300/1000 [00:46<01:59,  5.84it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 301/1000 [00:46<01:54,  6.13it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 302/1000 [00:46<01:48,  6.46it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 303/1000 [00:46<01:43,  6.71it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 304/1000 [00:46<01:54,  6.07it/s]Ref scores:  30%|â–ˆâ–ˆâ–ˆ       | 305/1000 [00:46<01:48,  6.43it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 306/1000 [00:47<01:44,  6.67it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 307/1000 [00:47<01:35,  7.24it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 308/1000 [00:47<01:34,  7.31it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 309/1000 [00:47<01:33,  7.40it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 310/1000 [00:47<01:32,  7.42it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 311/1000 [00:47<01:32,  7.45it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆ       | 312/1000 [00:47<01:31,  7.48it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆâ–      | 313/1000 [00:48<01:45,  6.49it/s]Ref scores:  31%|â–ˆâ–ˆâ–ˆâ–      | 314/1000 [00:48<02:01,  5.63it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 315/1000 [00:48<01:52,  6.06it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 316/1000 [00:48<01:46,  6.41it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 317/1000 [00:48<02:02,  5.59it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 318/1000 [00:48<01:52,  6.04it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 319/1000 [00:49<01:46,  6.41it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 320/1000 [00:49<01:41,  6.70it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 321/1000 [00:49<01:38,  6.90it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 322/1000 [00:49<01:35,  7.08it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 323/1000 [00:49<01:34,  7.15it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–      | 324/1000 [00:49<01:32,  7.27it/s]Ref scores:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 325/1000 [00:49<01:37,  6.94it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 326/1000 [00:49<01:35,  7.08it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 327/1000 [00:50<01:37,  6.94it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 328/1000 [00:50<01:34,  7.09it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 329/1000 [00:50<01:38,  6.81it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 330/1000 [00:50<01:31,  7.35it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 331/1000 [00:50<01:30,  7.37it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 332/1000 [00:50<01:35,  7.02it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 333/1000 [00:50<01:33,  7.14it/s]Ref scores:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 334/1000 [00:51<01:51,  5.96it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 335/1000 [00:51<01:49,  6.07it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 336/1000 [00:51<01:43,  6.43it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 337/1000 [00:51<01:43,  6.40it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 338/1000 [00:51<01:43,  6.40it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 339/1000 [00:51<01:39,  6.67it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 340/1000 [00:52<01:35,  6.88it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 341/1000 [00:52<01:33,  7.04it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 342/1000 [00:52<01:31,  7.17it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 343/1000 [00:52<01:30,  7.24it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 344/1000 [00:52<01:29,  7.29it/s]Ref scores:  34%|â–ˆâ–ˆâ–ˆâ–      | 345/1000 [00:52<01:29,  7.31it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 346/1000 [00:52<01:29,  7.34it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 347/1000 [00:53<01:27,  7.42it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 348/1000 [00:53<01:27,  7.48it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–      | 349/1000 [00:53<01:27,  7.45it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 350/1000 [00:53<01:26,  7.51it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 351/1000 [00:53<01:26,  7.50it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 352/1000 [00:53<01:29,  7.21it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 353/1000 [00:53<01:28,  7.31it/s]Ref scores:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 354/1000 [00:53<01:23,  7.75it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 355/1000 [00:54<01:27,  7.36it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 356/1000 [00:54<01:27,  7.38it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 357/1000 [00:54<01:22,  7.82it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 358/1000 [00:54<01:26,  7.41it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 359/1000 [00:54<01:25,  7.47it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 360/1000 [00:54<01:29,  7.15it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 361/1000 [00:54<01:23,  7.63it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 362/1000 [00:55<01:24,  7.56it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 363/1000 [00:55<01:28,  7.20it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 364/1000 [00:55<01:27,  7.28it/s]Ref scores:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 365/1000 [00:55<01:26,  7.32it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 366/1000 [00:55<01:29,  7.07it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 367/1000 [00:55<01:28,  7.18it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 368/1000 [00:55<01:27,  7.26it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 369/1000 [00:56<01:26,  7.29it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 370/1000 [00:56<01:44,  6.04it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 371/1000 [00:56<01:38,  6.38it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 372/1000 [00:56<01:34,  6.66it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 373/1000 [00:56<01:31,  6.87it/s]Ref scores:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 374/1000 [00:56<01:29,  7.00it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 375/1000 [00:57<01:46,  5.87it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 376/1000 [00:57<01:39,  6.26it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 377/1000 [00:57<01:34,  6.60it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 378/1000 [00:57<01:31,  6.80it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 379/1000 [00:57<01:29,  6.95it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 380/1000 [00:57<01:30,  6.86it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 381/1000 [00:57<01:28,  7.01it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 382/1000 [00:57<01:26,  7.13it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 383/1000 [00:58<01:25,  7.21it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 384/1000 [00:58<01:24,  7.30it/s]Ref scores:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 385/1000 [00:58<01:19,  7.76it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 386/1000 [00:58<01:20,  7.65it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 387/1000 [00:58<01:25,  7.20it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 388/1000 [00:58<01:24,  7.27it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 389/1000 [00:58<01:23,  7.33it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 390/1000 [00:59<01:25,  7.10it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 391/1000 [00:59<01:20,  7.60it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 392/1000 [00:59<01:20,  7.52it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 393/1000 [00:59<01:20,  7.55it/s]Ref scores:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 394/1000 [00:59<01:20,  7.54it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 395/1000 [00:59<01:24,  7.13it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 396/1000 [00:59<01:27,  6.94it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 397/1000 [01:00<01:28,  6.83it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 398/1000 [01:00<01:26,  6.97it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 399/1000 [01:00<01:28,  6.77it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 400/1000 [01:00<01:26,  6.95it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 401/1000 [01:00<01:24,  7.06it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 402/1000 [01:00<01:41,  5.92it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 403/1000 [01:00<01:34,  6.33it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 404/1000 [01:01<01:29,  6.65it/s]Ref scores:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 405/1000 [01:01<01:26,  6.89it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 406/1000 [01:01<01:24,  7.04it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 407/1000 [01:01<01:22,  7.16it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 408/1000 [01:01<01:22,  7.21it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 409/1000 [01:01<01:21,  7.28it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 410/1000 [01:01<01:20,  7.36it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 411/1000 [01:02<01:23,  7.08it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 412/1000 [01:02<01:17,  7.59it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 413/1000 [01:02<01:17,  7.54it/s]Ref scores:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 414/1000 [01:02<01:17,  7.55it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 415/1000 [01:02<01:17,  7.51it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 416/1000 [01:02<01:18,  7.47it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 417/1000 [01:02<01:18,  7.46it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 418/1000 [01:02<01:18,  7.46it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 419/1000 [01:03<01:17,  7.46it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 420/1000 [01:03<01:17,  7.45it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 421/1000 [01:03<01:40,  5.78it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 422/1000 [01:03<01:36,  5.98it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 423/1000 [01:03<01:34,  6.12it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 424/1000 [01:04<01:51,  5.16it/s]Ref scores:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 425/1000 [01:04<01:45,  5.44it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 426/1000 [01:04<01:40,  5.71it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 427/1000 [01:04<01:33,  6.12it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 428/1000 [01:04<01:28,  6.46it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 429/1000 [01:04<01:28,  6.44it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 430/1000 [01:04<01:24,  6.73it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 431/1000 [01:05<01:18,  7.29it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 432/1000 [01:05<01:12,  7.83it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 433/1000 [01:05<01:16,  7.41it/s]Ref scores:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 434/1000 [01:05<01:32,  6.09it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 435/1000 [01:05<01:23,  6.76it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 436/1000 [01:05<01:21,  6.94it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 437/1000 [01:05<01:23,  6.77it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 438/1000 [01:06<01:21,  6.93it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 439/1000 [01:06<01:15,  7.46it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 440/1000 [01:06<01:11,  7.83it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 441/1000 [01:06<01:12,  7.67it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 442/1000 [01:06<01:13,  7.59it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 443/1000 [01:06<01:13,  7.54it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 444/1000 [01:06<01:16,  7.23it/s]Ref scores:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 445/1000 [01:07<01:16,  7.30it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 446/1000 [01:07<01:18,  7.08it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 447/1000 [01:07<01:17,  7.16it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 448/1000 [01:07<01:19,  6.99it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 449/1000 [01:07<01:17,  7.11it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 450/1000 [01:07<01:16,  7.21it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 451/1000 [01:07<01:18,  6.99it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 452/1000 [01:08<01:28,  6.22it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 453/1000 [01:08<01:26,  6.31it/s]Ref scores:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 454/1000 [01:08<01:22,  6.58it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 455/1000 [01:08<01:19,  6.85it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 456/1000 [01:08<01:17,  7.02it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 457/1000 [01:08<01:12,  7.50it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 458/1000 [01:08<01:12,  7.47it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 459/1000 [01:09<01:12,  7.47it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 460/1000 [01:09<01:08,  7.86it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 461/1000 [01:09<01:20,  6.66it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 462/1000 [01:09<01:18,  6.89it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 463/1000 [01:09<01:16,  7.03it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 464/1000 [01:09<01:15,  7.12it/s]Ref scores:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 465/1000 [01:09<01:14,  7.18it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 466/1000 [01:10<01:13,  7.30it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 467/1000 [01:10<01:12,  7.33it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 468/1000 [01:10<01:15,  7.09it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 469/1000 [01:10<01:13,  7.23it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 470/1000 [01:10<01:12,  7.34it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 471/1000 [01:10<01:11,  7.40it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 472/1000 [01:10<01:13,  7.15it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 473/1000 [01:10<01:12,  7.25it/s]Ref scores:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 474/1000 [01:11<01:11,  7.34it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 475/1000 [01:11<01:11,  7.37it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 476/1000 [01:11<01:06,  7.84it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 477/1000 [01:11<01:08,  7.62it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 478/1000 [01:11<01:19,  6.53it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 479/1000 [01:11<01:39,  5.21it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 480/1000 [01:12<01:26,  6.01it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 481/1000 [01:12<01:21,  6.38it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 482/1000 [01:12<01:17,  6.67it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 483/1000 [01:12<01:15,  6.89it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 484/1000 [01:12<01:12,  7.08it/s]Ref scores:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 485/1000 [01:12<01:22,  6.26it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 486/1000 [01:12<01:14,  6.91it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 487/1000 [01:13<01:12,  7.08it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 488/1000 [01:13<01:11,  7.15it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 489/1000 [01:13<01:10,  7.21it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 490/1000 [01:13<01:09,  7.33it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 491/1000 [01:13<01:09,  7.37it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 492/1000 [01:13<01:08,  7.40it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 493/1000 [01:13<01:08,  7.40it/s]Ref scores:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 494/1000 [01:13<01:08,  7.43it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 495/1000 [01:14<01:07,  7.46it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 496/1000 [01:14<01:07,  7.49it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 497/1000 [01:14<01:10,  7.09it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 498/1000 [01:14<01:09,  7.23it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 499/1000 [01:14<01:18,  6.35it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 500/1000 [01:14<01:18,  6.36it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 501/1000 [01:15<01:14,  6.65it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 502/1000 [01:15<01:15,  6.56it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 503/1000 [01:15<01:12,  6.85it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 504/1000 [01:15<01:10,  7.03it/s]Ref scores:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 505/1000 [01:15<01:11,  6.92it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 506/1000 [01:15<01:09,  7.09it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 507/1000 [01:16<01:26,  5.73it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 508/1000 [01:16<01:19,  6.16it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 509/1000 [01:16<01:15,  6.53it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 510/1000 [01:16<01:12,  6.77it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 511/1000 [01:16<01:12,  6.73it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 512/1000 [01:16<01:13,  6.68it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 513/1000 [01:16<01:13,  6.61it/s]Ref scores:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 514/1000 [01:17<01:19,  6.15it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 515/1000 [01:17<01:17,  6.28it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 516/1000 [01:17<01:13,  6.62it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 517/1000 [01:17<01:10,  6.83it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 518/1000 [01:17<01:20,  6.02it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 519/1000 [01:17<01:15,  6.38it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 520/1000 [01:17<01:12,  6.65it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 521/1000 [01:18<01:09,  6.87it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 522/1000 [01:18<01:07,  7.05it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 523/1000 [01:18<01:09,  6.91it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 524/1000 [01:18<01:07,  7.05it/s]Ref scores:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 525/1000 [01:18<01:06,  7.16it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 526/1000 [01:18<01:19,  5.97it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 527/1000 [01:19<01:25,  5.56it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 528/1000 [01:19<01:18,  6.04it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 529/1000 [01:19<01:13,  6.38it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 530/1000 [01:19<01:10,  6.69it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 531/1000 [01:19<01:10,  6.64it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 532/1000 [01:19<01:08,  6.85it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 533/1000 [01:19<01:06,  7.04it/s]Ref scores:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 534/1000 [01:20<01:04,  7.18it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 535/1000 [01:20<01:04,  7.25it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 536/1000 [01:20<01:03,  7.31it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 537/1000 [01:20<01:02,  7.37it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 538/1000 [01:20<01:02,  7.39it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 539/1000 [01:20<01:19,  5.83it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 540/1000 [01:20<01:13,  6.25it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 541/1000 [01:21<01:10,  6.55it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 542/1000 [01:21<01:07,  6.81it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 543/1000 [01:21<01:08,  6.67it/s]Ref scores:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 544/1000 [01:21<01:06,  6.87it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 545/1000 [01:21<01:04,  7.04it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 546/1000 [01:21<01:03,  7.19it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 547/1000 [01:21<01:05,  6.92it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 548/1000 [01:22<01:03,  7.11it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 549/1000 [01:22<00:59,  7.60it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 550/1000 [01:22<01:01,  7.28it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 551/1000 [01:22<01:01,  7.35it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 552/1000 [01:22<01:00,  7.38it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 553/1000 [01:22<01:00,  7.41it/s]Ref scores:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 554/1000 [01:22<01:02,  7.12it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 555/1000 [01:23<01:01,  7.21it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 556/1000 [01:23<01:01,  7.28it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 557/1000 [01:23<01:10,  6.30it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 558/1000 [01:23<01:09,  6.35it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 559/1000 [01:23<01:06,  6.62it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 560/1000 [01:23<01:04,  6.83it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 561/1000 [01:23<01:05,  6.73it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 562/1000 [01:24<01:03,  6.89it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 563/1000 [01:24<01:04,  6.81it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 564/1000 [01:24<01:02,  7.01it/s]Ref scores:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 565/1000 [01:24<01:02,  6.91it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 566/1000 [01:24<01:00,  7.12it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 567/1000 [01:24<00:59,  7.23it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 568/1000 [01:24<01:02,  6.94it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 569/1000 [01:25<01:10,  6.15it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 570/1000 [01:25<01:05,  6.52it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 571/1000 [01:25<01:05,  6.57it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 572/1000 [01:25<01:02,  6.82it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 573/1000 [01:25<01:00,  7.04it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 574/1000 [01:25<00:56,  7.57it/s]Ref scores:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 575/1000 [01:25<00:58,  7.29it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 576/1000 [01:26<00:54,  7.77it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 577/1000 [01:26<00:54,  7.76it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 578/1000 [01:26<00:54,  7.70it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 579/1000 [01:26<00:54,  7.67it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 580/1000 [01:26<00:55,  7.58it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 581/1000 [01:26<01:07,  6.18it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 582/1000 [01:26<01:04,  6.51it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 583/1000 [01:27<01:01,  6.79it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 584/1000 [01:27<00:59,  6.94it/s]Ref scores:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 585/1000 [01:27<00:58,  7.08it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 586/1000 [01:27<00:57,  7.22it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 587/1000 [01:27<00:56,  7.28it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 588/1000 [01:27<00:56,  7.31it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 589/1000 [01:27<00:55,  7.36it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 590/1000 [01:28<00:55,  7.37it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 591/1000 [01:28<00:55,  7.40it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 592/1000 [01:28<00:55,  7.41it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 593/1000 [01:28<00:54,  7.43it/s]Ref scores:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 594/1000 [01:28<00:54,  7.48it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 595/1000 [01:28<00:53,  7.51it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 596/1000 [01:28<00:54,  7.45it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 597/1000 [01:28<00:53,  7.46it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 598/1000 [01:29<00:53,  7.46it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 599/1000 [01:29<00:55,  7.19it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 600/1000 [01:29<00:55,  7.27it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 601/1000 [01:29<00:54,  7.32it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 602/1000 [01:29<00:56,  7.04it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 603/1000 [01:29<00:55,  7.17it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 604/1000 [01:30<01:02,  6.30it/s]Ref scores:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 605/1000 [01:30<01:02,  6.37it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 606/1000 [01:30<00:59,  6.64it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 607/1000 [01:30<00:59,  6.62it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 608/1000 [01:30<00:56,  6.88it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 609/1000 [01:30<00:55,  7.02it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 610/1000 [01:30<00:54,  7.17it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 611/1000 [01:30<00:53,  7.24it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 612/1000 [01:31<00:53,  7.31it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 613/1000 [01:31<00:54,  7.09it/s]Ref scores:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 614/1000 [01:31<00:53,  7.19it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 615/1000 [01:31<00:52,  7.28it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 616/1000 [01:31<00:52,  7.36it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 617/1000 [01:31<00:52,  7.35it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 618/1000 [01:31<00:52,  7.35it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 619/1000 [01:32<00:51,  7.41it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 620/1000 [01:32<00:59,  6.43it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 621/1000 [01:32<00:56,  6.71it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 622/1000 [01:32<00:54,  6.92it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 623/1000 [01:32<00:53,  7.10it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 624/1000 [01:32<00:52,  7.22it/s]Ref scores:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 625/1000 [01:32<00:51,  7.27it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 626/1000 [01:33<00:53,  6.98it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 627/1000 [01:33<00:53,  7.01it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 628/1000 [01:33<00:51,  7.18it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 629/1000 [01:33<00:50,  7.29it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 630/1000 [01:33<00:58,  6.36it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 631/1000 [01:33<00:55,  6.70it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 632/1000 [01:33<00:53,  6.93it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 633/1000 [01:34<00:53,  6.84it/s]Ref scores:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 634/1000 [01:34<00:51,  7.07it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 635/1000 [01:34<00:50,  7.20it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 636/1000 [01:34<00:49,  7.35it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 637/1000 [01:34<00:49,  7.40it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 638/1000 [01:34<00:48,  7.48it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 639/1000 [01:34<00:48,  7.52it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 640/1000 [01:35<00:49,  7.22it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 641/1000 [01:35<00:49,  7.30it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 642/1000 [01:35<00:48,  7.39it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 643/1000 [01:35<00:50,  7.07it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 644/1000 [01:35<00:49,  7.19it/s]Ref scores:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 645/1000 [01:35<00:48,  7.26it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 646/1000 [01:35<00:48,  7.35it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 647/1000 [01:36<00:55,  6.41it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 648/1000 [01:36<00:52,  6.72it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 649/1000 [01:36<00:50,  6.91it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 650/1000 [01:36<00:49,  7.06it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 651/1000 [01:36<00:50,  6.88it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 652/1000 [01:36<00:49,  7.09it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 653/1000 [01:36<00:47,  7.23it/s]Ref scores:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 654/1000 [01:37<00:47,  7.26it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 655/1000 [01:37<00:54,  6.35it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 656/1000 [01:37<00:49,  6.98it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 657/1000 [01:37<00:55,  6.17it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 658/1000 [01:37<00:52,  6.50it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 659/1000 [01:37<00:50,  6.75it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 660/1000 [01:37<00:48,  6.99it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 661/1000 [01:38<00:47,  7.11it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 662/1000 [01:38<00:46,  7.21it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 663/1000 [01:38<00:43,  7.68it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 664/1000 [01:38<00:44,  7.61it/s]Ref scores:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 665/1000 [01:38<00:41,  7.99it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 666/1000 [01:38<00:42,  7.82it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 667/1000 [01:38<00:44,  7.43it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 668/1000 [01:39<00:44,  7.44it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 669/1000 [01:39<00:54,  6.07it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 670/1000 [01:39<00:51,  6.43it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 671/1000 [01:39<01:01,  5.39it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 672/1000 [01:39<01:02,  5.21it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 673/1000 [01:39<00:58,  5.56it/s]Ref scores:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 674/1000 [01:40<00:54,  6.00it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 675/1000 [01:40<00:51,  6.37it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 676/1000 [01:40<00:48,  6.69it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 677/1000 [01:40<00:54,  5.96it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 678/1000 [01:40<00:52,  6.13it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 679/1000 [01:40<00:52,  6.11it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 680/1000 [01:41<00:51,  6.26it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 681/1000 [01:41<00:54,  5.82it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 682/1000 [01:41<00:52,  6.01it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 683/1000 [01:41<01:01,  5.18it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 684/1000 [01:41<00:55,  5.72it/s]Ref scores:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 685/1000 [01:41<00:51,  6.16it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 686/1000 [01:42<00:50,  6.28it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 687/1000 [01:42<00:47,  6.63it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 688/1000 [01:42<00:43,  7.20it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 689/1000 [01:42<00:44,  7.03it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 690/1000 [01:42<00:43,  7.14it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 691/1000 [01:42<00:42,  7.23it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 692/1000 [01:42<00:40,  7.62it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 693/1000 [01:43<00:40,  7.56it/s]Ref scores:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 694/1000 [01:43<00:40,  7.56it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 695/1000 [01:43<00:40,  7.51it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 696/1000 [01:43<00:40,  7.44it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 697/1000 [01:43<00:38,  7.87it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 698/1000 [01:43<00:39,  7.74it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 699/1000 [01:43<00:39,  7.66it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 700/1000 [01:43<00:39,  7.61it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 701/1000 [01:44<00:41,  7.19it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 702/1000 [01:44<00:41,  7.25it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 703/1000 [01:44<00:40,  7.36it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 704/1000 [01:44<00:42,  7.00it/s]Ref scores:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 705/1000 [01:44<00:46,  6.38it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 706/1000 [01:44<00:44,  6.65it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 707/1000 [01:44<00:42,  6.88it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 708/1000 [01:45<00:41,  7.03it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 709/1000 [01:45<00:40,  7.15it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 710/1000 [01:45<00:39,  7.25it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 711/1000 [01:45<00:41,  6.96it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 712/1000 [01:45<00:49,  5.82it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 713/1000 [01:45<00:46,  6.24it/s]Ref scores:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 714/1000 [01:46<00:43,  6.58it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 715/1000 [01:46<00:43,  6.60it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 716/1000 [01:46<00:43,  6.47it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 717/1000 [01:46<00:53,  5.29it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 718/1000 [01:46<00:49,  5.74it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 719/1000 [01:46<00:47,  5.87it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 720/1000 [01:47<00:46,  5.98it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 721/1000 [01:47<00:44,  6.26it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 722/1000 [01:47<00:44,  6.26it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 723/1000 [01:47<00:40,  6.89it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 724/1000 [01:47<00:39,  7.00it/s]Ref scores:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 725/1000 [01:47<00:39,  7.03it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 726/1000 [01:47<00:38,  7.08it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 727/1000 [01:48<00:38,  7.12it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 728/1000 [01:48<00:35,  7.60it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 729/1000 [01:48<00:36,  7.49it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 730/1000 [01:48<00:36,  7.41it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 731/1000 [01:48<00:38,  7.03it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 732/1000 [01:48<00:38,  7.04it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 733/1000 [01:48<00:37,  7.04it/s]Ref scores:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 734/1000 [01:49<00:37,  7.04it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 735/1000 [01:49<00:37,  7.07it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 736/1000 [01:49<00:37,  7.07it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 737/1000 [01:49<00:44,  5.88it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 738/1000 [01:49<00:47,  5.50it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 739/1000 [01:49<00:44,  5.90it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 740/1000 [01:50<00:41,  6.21it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 741/1000 [01:50<00:40,  6.45it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 742/1000 [01:50<00:40,  6.40it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 743/1000 [01:50<00:38,  6.61it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 744/1000 [01:50<00:37,  6.78it/s]Ref scores:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 745/1000 [01:50<00:36,  6.90it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 746/1000 [01:50<00:36,  6.95it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 747/1000 [01:51<00:36,  6.99it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 748/1000 [01:51<00:37,  6.71it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 749/1000 [01:51<00:41,  5.98it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 750/1000 [01:51<00:39,  6.26it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 751/1000 [01:51<00:38,  6.48it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 752/1000 [01:51<00:37,  6.69it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 753/1000 [01:51<00:36,  6.79it/s]Ref scores:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 754/1000 [01:52<00:35,  6.87it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 755/1000 [01:52<00:35,  6.94it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 756/1000 [01:52<00:34,  6.98it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 757/1000 [01:52<00:34,  7.02it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 758/1000 [01:52<00:34,  7.06it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 759/1000 [01:52<00:34,  7.07it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 760/1000 [01:52<00:33,  7.06it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 761/1000 [01:53<00:33,  7.07it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 762/1000 [01:53<00:33,  7.20it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 763/1000 [01:53<00:33,  7.15it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 764/1000 [01:53<00:32,  7.16it/s]Ref scores:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 765/1000 [01:53<00:32,  7.14it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 766/1000 [01:53<00:34,  6.73it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 767/1000 [01:53<00:34,  6.83it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 768/1000 [01:54<00:33,  6.91it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 769/1000 [01:54<00:33,  6.99it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 770/1000 [01:54<00:32,  6.97it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 771/1000 [01:54<00:32,  7.02it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 772/1000 [01:54<00:32,  7.04it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 773/1000 [01:54<00:33,  6.77it/s]Ref scores:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 774/1000 [01:54<00:34,  6.61it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 775/1000 [01:55<00:34,  6.52it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 776/1000 [01:55<00:33,  6.68it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 777/1000 [01:55<00:34,  6.45it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 778/1000 [01:55<00:31,  7.03it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 779/1000 [01:55<00:31,  7.08it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 780/1000 [01:55<00:31,  7.08it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 781/1000 [01:56<00:36,  5.92it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 782/1000 [01:56<00:35,  6.20it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 783/1000 [01:56<00:33,  6.42it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 784/1000 [01:56<00:32,  6.62it/s]Ref scores:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 785/1000 [01:56<00:29,  7.17it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 786/1000 [01:56<00:30,  7.11it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 787/1000 [01:56<00:29,  7.15it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 788/1000 [01:57<00:29,  7.20it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 789/1000 [01:57<00:29,  7.15it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 790/1000 [01:57<00:29,  7.10it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 791/1000 [01:57<00:29,  6.97it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 792/1000 [01:57<00:29,  7.01it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 793/1000 [01:57<00:33,  6.15it/s]Ref scores:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 794/1000 [01:58<00:38,  5.41it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 795/1000 [01:58<00:35,  5.82it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 796/1000 [01:58<00:33,  6.17it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 797/1000 [01:58<00:31,  6.43it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 798/1000 [01:58<00:34,  5.84it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 799/1000 [01:58<00:32,  6.16it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 800/1000 [01:58<00:31,  6.40it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 801/1000 [01:59<00:31,  6.32it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 802/1000 [01:59<00:31,  6.32it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 803/1000 [01:59<00:30,  6.53it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 804/1000 [01:59<00:27,  7.10it/s]Ref scores:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 805/1000 [01:59<00:29,  6.71it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 806/1000 [01:59<00:32,  6.04it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 807/1000 [02:00<00:34,  5.60it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 808/1000 [02:00<00:32,  5.99it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 809/1000 [02:00<00:30,  6.32it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 810/1000 [02:00<00:29,  6.55it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 811/1000 [02:00<00:29,  6.43it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 812/1000 [02:00<00:28,  6.63it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 813/1000 [02:00<00:27,  6.76it/s]Ref scores:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 814/1000 [02:01<00:28,  6.52it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 815/1000 [02:01<00:27,  6.66it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 816/1000 [02:01<00:27,  6.79it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 817/1000 [02:01<00:27,  6.61it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 818/1000 [02:01<00:26,  6.75it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 819/1000 [02:01<00:27,  6.58it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 820/1000 [02:02<00:27,  6.46it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 821/1000 [02:02<00:26,  6.65it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 822/1000 [02:02<00:26,  6.77it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 823/1000 [02:02<00:32,  5.46it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 824/1000 [02:02<00:29,  5.87it/s]Ref scores:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 825/1000 [02:02<00:28,  6.23it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 826/1000 [02:03<00:34,  5.02it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 827/1000 [02:03<00:32,  5.29it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 828/1000 [02:03<00:30,  5.70it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 829/1000 [02:03<00:29,  5.86it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 830/1000 [02:03<00:27,  6.16it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 831/1000 [02:03<00:27,  6.21it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 832/1000 [02:04<00:26,  6.46it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 833/1000 [02:04<00:25,  6.61it/s]Ref scores:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 834/1000 [02:04<00:30,  5.38it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 835/1000 [02:04<00:29,  5.62it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 836/1000 [02:04<00:26,  6.08it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 837/1000 [02:04<00:25,  6.32it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 838/1000 [02:05<00:24,  6.51it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 839/1000 [02:05<00:23,  6.72it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 840/1000 [02:05<00:23,  6.84it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 841/1000 [02:05<00:22,  6.94it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 842/1000 [02:05<00:27,  5.83it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 843/1000 [02:05<00:28,  5.53it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 844/1000 [02:06<00:26,  5.91it/s]Ref scores:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 845/1000 [02:06<00:23,  6.51it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 846/1000 [02:06<00:23,  6.68it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 847/1000 [02:06<00:22,  6.81it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 848/1000 [02:06<00:24,  6.12it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 849/1000 [02:06<00:24,  6.16it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 850/1000 [02:06<00:23,  6.42it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 851/1000 [02:07<00:23,  6.26it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 852/1000 [02:07<00:22,  6.49it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 853/1000 [02:07<00:22,  6.63it/s]Ref scores:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 854/1000 [02:07<00:20,  7.19it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 855/1000 [02:07<00:21,  6.87it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 856/1000 [02:07<00:20,  6.99it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 857/1000 [02:07<00:20,  7.05it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 858/1000 [02:08<00:20,  7.01it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 859/1000 [02:08<00:22,  6.22it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 860/1000 [02:08<00:21,  6.43it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 861/1000 [02:08<00:21,  6.58it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 862/1000 [02:08<00:21,  6.51it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 863/1000 [02:08<00:20,  6.68it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 864/1000 [02:09<00:19,  6.83it/s]Ref scores:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 865/1000 [02:09<00:20,  6.62it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 866/1000 [02:09<00:19,  6.77it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 867/1000 [02:09<00:19,  6.86it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 868/1000 [02:09<00:18,  6.96it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 869/1000 [02:09<00:19,  6.61it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 870/1000 [02:09<00:19,  6.74it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 871/1000 [02:10<00:17,  7.27it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 872/1000 [02:10<00:17,  7.25it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 873/1000 [02:10<00:17,  7.21it/s]Ref scores:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 874/1000 [02:10<00:17,  7.16it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 875/1000 [02:10<00:17,  7.13it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 876/1000 [02:10<00:17,  7.13it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 877/1000 [02:10<00:17,  7.12it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 878/1000 [02:11<00:17,  7.12it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 879/1000 [02:11<00:19,  6.20it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 880/1000 [02:11<00:21,  5.67it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 881/1000 [02:11<00:19,  6.03it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 882/1000 [02:11<00:18,  6.35it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 883/1000 [02:11<00:17,  6.57it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 884/1000 [02:12<00:19,  5.91it/s]Ref scores:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 885/1000 [02:12<00:18,  6.23it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 886/1000 [02:12<00:17,  6.44it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 887/1000 [02:12<00:20,  5.60it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 888/1000 [02:12<00:18,  5.97it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 889/1000 [02:12<00:17,  6.37it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 890/1000 [02:12<00:15,  6.97it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 891/1000 [02:13<00:15,  6.96it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 892/1000 [02:13<00:15,  7.00it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 893/1000 [02:13<00:15,  7.07it/s]Ref scores:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 894/1000 [02:13<00:14,  7.13it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 895/1000 [02:13<00:14,  7.14it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 896/1000 [02:13<00:13,  7.62it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 897/1000 [02:13<00:13,  7.48it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 898/1000 [02:14<00:14,  7.28it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 899/1000 [02:14<00:13,  7.25it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 900/1000 [02:14<00:14,  6.90it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 901/1000 [02:14<00:14,  6.98it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 902/1000 [02:14<00:13,  7.13it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 903/1000 [02:14<00:13,  7.13it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 904/1000 [02:14<00:13,  7.10it/s]Ref scores:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 905/1000 [02:15<00:13,  7.07it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 906/1000 [02:15<00:13,  7.08it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 907/1000 [02:15<00:13,  7.06it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 908/1000 [02:15<00:13,  6.78it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 909/1000 [02:15<00:13,  6.85it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 910/1000 [02:15<00:13,  6.91it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 911/1000 [02:15<00:12,  7.02it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 912/1000 [02:16<00:13,  6.67it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 913/1000 [02:16<00:13,  6.53it/s]Ref scores:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 914/1000 [02:16<00:15,  5.42it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 915/1000 [02:16<00:14,  5.82it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 916/1000 [02:16<00:12,  6.49it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 917/1000 [02:16<00:12,  6.68it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 918/1000 [02:17<00:11,  6.84it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 919/1000 [02:17<00:11,  6.91it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 920/1000 [02:17<00:11,  6.97it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 921/1000 [02:17<00:11,  6.65it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 922/1000 [02:17<00:11,  6.77it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 923/1000 [02:17<00:11,  6.59it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 924/1000 [02:18<00:12,  6.02it/s]Ref scores:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 925/1000 [02:18<00:11,  6.28it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 926/1000 [02:18<00:11,  6.46it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 927/1000 [02:18<00:11,  6.38it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 928/1000 [02:18<00:11,  6.50it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 929/1000 [02:18<00:10,  6.66it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 930/1000 [02:18<00:10,  6.54it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 931/1000 [02:19<00:10,  6.68it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 932/1000 [02:19<00:10,  6.78it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 933/1000 [02:19<00:09,  6.87it/s]Ref scores:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 934/1000 [02:19<00:09,  6.93it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 935/1000 [02:19<00:09,  6.94it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 936/1000 [02:19<00:09,  6.99it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 937/1000 [02:19<00:08,  7.48it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 938/1000 [02:20<00:08,  7.29it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 939/1000 [02:20<00:08,  7.22it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 940/1000 [02:20<00:09,  6.34it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 941/1000 [02:20<00:10,  5.84it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 942/1000 [02:20<00:11,  5.08it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 943/1000 [02:20<00:10,  5.56it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 944/1000 [02:21<00:09,  5.96it/s]Ref scores:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 945/1000 [02:21<00:08,  6.26it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 946/1000 [02:21<00:08,  6.48it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 947/1000 [02:21<00:07,  6.70it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 948/1000 [02:21<00:07,  6.81it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 949/1000 [02:21<00:07,  6.62it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 950/1000 [02:21<00:07,  6.78it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 951/1000 [02:22<00:07,  6.91it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 952/1000 [02:22<00:06,  7.01it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 953/1000 [02:22<00:06,  7.03it/s]Ref scores:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 954/1000 [02:22<00:06,  7.02it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 955/1000 [02:22<00:06,  7.05it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 956/1000 [02:22<00:06,  7.09it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 957/1000 [02:22<00:06,  6.75it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 958/1000 [02:23<00:06,  6.87it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 959/1000 [02:23<00:05,  6.94it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 960/1000 [02:23<00:06,  6.60it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 961/1000 [02:23<00:06,  6.47it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 962/1000 [02:23<00:05,  6.63it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 963/1000 [02:23<00:05,  6.53it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 964/1000 [02:24<00:06,  5.92it/s]Ref scores:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 965/1000 [02:24<00:05,  6.33it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 966/1000 [02:24<00:05,  6.27it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 967/1000 [02:24<00:05,  6.25it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 968/1000 [02:24<00:04,  6.46it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 969/1000 [02:24<00:04,  6.67it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 970/1000 [02:24<00:04,  6.53it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 971/1000 [02:25<00:04,  6.70it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 972/1000 [02:25<00:04,  6.56it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 973/1000 [02:25<00:04,  6.47it/s]Ref scores:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 974/1000 [02:25<00:04,  5.60it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 975/1000 [02:25<00:04,  5.99it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 976/1000 [02:25<00:03,  6.20it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 977/1000 [02:26<00:03,  6.21it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 978/1000 [02:26<00:03,  6.43it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 979/1000 [02:26<00:03,  6.55it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 980/1000 [02:26<00:03,  6.47it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 981/1000 [02:26<00:02,  6.65it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 982/1000 [02:26<00:02,  6.75it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 983/1000 [02:27<00:02,  6.56it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 984/1000 [02:27<00:02,  6.70it/s]Ref scores:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 985/1000 [02:27<00:02,  6.58it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 986/1000 [02:27<00:02,  6.69it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 987/1000 [02:27<00:01,  7.25it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 988/1000 [02:27<00:01,  7.26it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 989/1000 [02:27<00:01,  7.21it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 990/1000 [02:27<00:01,  7.65it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 991/1000 [02:28<00:01,  7.45it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 992/1000 [02:28<00:01,  7.37it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 993/1000 [02:28<00:01,  6.37it/s]Ref scores:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 994/1000 [02:28<00:00,  6.58it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 995/1000 [02:28<00:00,  6.73it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 996/1000 [02:28<00:00,  6.85it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 997/1000 [02:29<00:00,  6.94it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 998/1000 [02:29<00:00,  6.73it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 999/1000 [02:29<00:00,  6.86it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [02:29<00:00,  7.38it/s]Ref scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [02:29<00:00,  6.69it/s]
DONE (3.09s)
DONE (7.94s)
loss_threshold ROC AUC: 0.644574, PR AUC: 0.6203121607140045, tpr_at_low_fpr: {0.001: 0.005, 0.01: 0.061}
min_k_threshold ROC AUC: 0.659802, PR AUC: 0.645705074551883, tpr_at_low_fpr: {0.001: 0.005, 0.01: 0.071}
zlib_threshold ROC AUC: 0.573466, PR AUC: 0.526905492225301, tpr_at_low_fpr: {0.001: 0.014, 0.01: 0.04}
ref-stablelm-base-alpha-3b-v2_threshold ROC AUC: 0.5002, PR AUC: 0.4710981724214728, tpr_at_low_fpr: {0.001: 0.017, 0.01: 0.04}
loss_threshold roc_auc: 0.645
min_k_threshold roc_auc: 0.660
zlib_threshold roc_auc: 0.573
ref-stablelm-base-alpha-3b-v2_threshold roc_auc: 0.500
Finished.
